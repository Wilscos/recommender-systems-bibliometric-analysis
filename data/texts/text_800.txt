The Morris worm [10, 46] was one of the ﬁrst worms spread via the internet. It was spread on November 2, 1988, and changed how computer security was viewed by computer professionals as well as general public [3]. Since its inception the Morris worm has been studied extensively from the security point of view [46, 37, 43, 10] and is still a point of interest [31, 36]. This paper summarizes the eﬀects, impacts, and lessons learned from the episode. There are other copies of this paper present at [20, 21] and [18]. However, I recommend using the arXiv version only. This paper was started as part of CS 52600 Information Security class at Purdue by prof. Eugene H. Spaﬀord. Thanks to prof. Spafto thank my f r iend Ben Harsha for helping Lovepreet Singh for all his help. On November 2, 1988 at around 6 PM a Cornell university grad-student, Robert Morris [39, 40 ], launched a computer worm ( see §8 for explanation of worm) with the intention(he claims) o f mapping the Internet. The worm was self-replicating and selfpropagating and took advantage of exploits in the Unix services, described in detail in §2.1. Though Robert claims that the worm was only intended for educational purposes, the worm disrupted the whole Internet [31] ( more in §2.2). Being ﬁrst of its kind, the incident was very interesting for computer scientist. Many researchers at Berkely, MIT and Purdue studied the worm and uncovered how it works and released some ﬁxes and patches within a day. Technical details of the patches are discussed in §3. The worm also impacted the way computer security was perceived, right from the formation of CERT to people being more cautious and thoughtful about security. Some people even term the episode as the big bang of cybersecurity [35]. §4 and §5 talk mor e about learning and changes which followed the event. At that time there were roughly 60,000 ma chines on the Internet, and it is estimated that the episode resulted in a loss of $9 8 million. Another estimate says that by the time incident was isolated around 5-10 % machines on the Internet were victimized [3]. How drastic could such an incident be with many companies and much of our day to day life activities solely relying on computers? §6 discusses this in detail. §7 is summary and §8 talks about some other but related interesting f acts and terminologies. As soon as the worm was noticed individual researchers, University committees, and government agencies all started ana lyzing the vulnerabilities exploited by the worm. [12, 46, 6, 43]. This section discusses what went wrong and what were the loopholes? Which exploits of BSD UNIX the worm took advantage of to reach its target? Did Morris made some innocent mistakes in writing the replicating routine? Or was the worm intent ionally designed to spread like forest ﬁre? Along with many o ther ﬂaws, the episode exposed a few speciﬁc loopholes in services provided by BSD-derived versions of UNIX. Researchers, engineers and system administrators identiﬁed and reported [46, 43, 41] these bugs within 2 days of the outbreak. The following vulnerabilities were exploited. 1. Sendmail – Sendmail is a mailer program used to route emails on the Internet. The program was capable o f running in various modes, one of them was being a background daemon. In this mode the program used to l i sten on a TCP connection for an incoming mail. Sendmail allowed mail to be delivered to a process (the background daemon) instead of the mailbox ﬁles, which was used for purpose like setting up automatic vacation responses. Also, while ﬁxing some o t her security bugs in the sendmail [43] accident ally a new misfeature was introduced in it. The new bug was that if the sendmail is compiled with DEBUG ﬂag on and if the sender of a mail asks the daemon t o go in debug mode by sending a debug command, then the s endmail allowed the sender to send a sequence of commands instead o f a recipients address. A combination of these two features was exploited in the worm. 2. ﬁngerd – The ﬁngerd the utility was used to obtain general user information like name, username and current login status of other users on the system. Like sendmail , ﬁngerd also ran as a background daemon. The worm exploited ﬁngerd by overrunning the buﬀer the process used as its input. The source of the bug was the gets routine in the standard C library. A call to gets writes input to a buﬀer and the ﬂaw was that the function implementing g e ts assumed that the buﬀer passed to it is large enough for the input to be written. This fault was not only in gets, but also, exists in other input-output routines like scanf and fscanf. 3. rsh and rexec – rsh and rexec are services which oﬀer remote command inthentication purposes Rsh used permission ﬁles and a privileged source port worm exploited the fact that there is a high possibility that a passwo rd for a local user for an account on a remote machine will be same as its local password. Another likelihood was that a remote account for a user will have rsh permission ﬁles for the local account o f the user. The worm exploited above two ideas to penetrate into remote machines. Use of rsh was very simple – just look for an account on a remote machine for a user who is running the worm locally. Use of rexec was not that simpler. To use rexec for penetrating the worm used the local password to do a remote login. So for doing this the worm had to crack local password ﬁrst. Following subsection discusses the password cracking. 4. Passwords – One of the key requirements for the worm to be able to spread was to be able to break the password of its current host. This was done by exploiting the fact that encrypted user passwords were stored in a publically readable ﬁle. However, in his technical report [46] Spaﬀord says that the passwords were not (he means in eﬀect) encrypted as a block of zero bits were repeatedly encrypted using the user password and the result thus obta ined was stored in the publically readable ﬁle. Interestingly Morris had done a case study [33] on this before the attack. So, breaking passwords was easy by guessing a list of passwords, then encrypting them and comparing the output with the stored value. This subsection talks about the aspect that whether the intent was malicious or not . Though the Cornell Commission [12] states that the worm did not harm any user data or system ﬁles, it did ma ke infected systems slow. However, the commission also does not fail to mention that ”given Morris’s evident knowledge of systems and netwo r ks, he knew or clearly should have known that such a consequence was certain, given the design of the worm”. The commission report also states that Morris made only minimal eﬀorts to halt the worm once it started spreading and also accuses him o f not informing any responsible authority about it. However, according to these press articles, Morris did try to t alk to a friend, Graham, and Harvard University who informed Andy Sudduth [31, 17]. Morris did suggest Sudduth some steps to protect Harvard computers from the worm. Sudduth also says that after some time Morris again called him realizing that he had made a ’colo ssal’ mistake, asking him to publicly publish an anonymous apology with instructions on how to ﬁx things. However, Sudduth was also the ﬁrst witness for the defense in the law-suit against the worm and, in response to a question by prosecutor Mark D. Rasch, he said [17] that ”He (Morris) wanted that I should keep it quiet(a major security bug in a ﬁle transfer program in the BSD UNIX), yes”. This fact gives a reason to doubt that Morris might have had notorious intents. With above contrasting information, it is not clear whether Morris was only performing an innocent educational experiment with no malicious intent. However, one thing is clear even if it was an innocent act the Internet had grown to such an extent that even innocence can cause great damage. Another interesting fact is that Eric Allman, developer of the sendmail and delivermail, in a personal communication to Donn Seeley said, ”The trap door resulted from two distinct ’features’ that, although innocent by themselves, were deadly when combined(kind of like binary nerve gas)” [43]. Exploit of sendmail is another example of how innocent mistakes can be har mful. This section provides high-level overview of working of the worm i.e. answers the following question, ”What exactly did the worm do that led it to cause an epidemic?”. This section will also brieﬂy discuss some of the early patches released to target this problem. The worm can be considered as divided into two maj or parts a bootstrap(vector) program and a main program. The vector program is a 99-line C program and the main program is a large relocatable object ﬁle that is compatible with VAX and Sun-3 systems [43]. The bootstrap program is included in the appendix in [46]. Once the worm is established on a machine it then tr ies to locate a host and, most importantly, target accounts on the host to infect new machines which it then exploits via one of the loop holes discussed in §2 to pass a copy of the worm to the r emote machine. The worm tries to obtain the address of potential target hosts by reading various system tables like /etc/hosts.equiv and /.rh osts and user ﬁles like .forward and .rhosts, ordered in such a way such that it r eads ﬁles having the name of local machines ﬁrst. It might be doing this as local machines are more likely to give access without authentication. For a ﬁxed address the worm can try to penetrate in one of the following ways: • Exploiting the bug in the ﬁnger server which lets the worm download its code instead of a ﬁnger request and then tricking the server to execute it. • Using the bug in the debugging code of the sendm ail SMTP mail service. Infected machines slowed down because of uncontro llable replication of the worm because the worm was only using the main memory (see §3.1.3) for its entire processing, this lead to memory clogging [12] and resulted in the machines slowing down. Following steps summarize some of the masquerading steps taken by the worm described in [43] • On startup, the worm used to delete its argument list and set t he very ﬁrst argument as sh in an attempt to pretend to be a shell command interpreter. • Used to fork itself so that it doesn’t have to stay with the same process i.d. for very long. • It read all the ﬁles which are part of t he worm program into memory and deletes them so that no evidence is left • Turns oﬀ core-dump generation, so that if the worm program crashes no dump ﬁles, leaving evidences behind are generated. This also helps in preventing analysis of the worm. • While loading the wor m ﬁle most of the non-essential symbol table entries were deleted to ensure that even if the worm ﬁle is caught before deleting, it will be harder to guess what the routines are doing. The program was a worm, and not a virus (see §8.1 and §8.2 for diﬀerence in a worm and a virus). It did not a t t empt to modify any other program or ﬁles on the system. It also, didn’t delete any system ﬁles [43, 12], and in-fact did not attempt to incapacitate the system by deleting any of the alr eady existing ﬁles, it only deleted ﬁles created by itself. It, also neither installed Trojan Horses nor transmitted decrypted passwords anywhere. It didn’t try to get superuser privileges. Scientist and engineers at major institutions like MIT, NASA, Purdue University, UC Berkeley, and many others started realizing that something was wrong late night on Nov. 2, 1988, or early morning Nov. 3, 1988, and started taking immediate action. The ﬁrst formal public posting about the virus was sent by Peter Yee of NASA Ames at 2:28 a m, on Nov. 2, 1988, via the mailing list “tcpip@sri-nic.arpa” [41]. Reports gave a detailed timeline of the maj or event s and actions from the assumed ”beginning” of t he worm until its full decompiled code was installed at Berkeley [43, 41]. On Thursday, Nov. 3 morning at 5:58 am several patches to ﬁx the worm were released by Keith Bostic of UC Berkeley [30], one of the key people in t he history of BSD UNIX, via the tcp-ip mailing list which was a lso forwarded by several ot hers[43, 41]. E.H. Spafford [1 3] from Purdue University a nalyzed the worm and released some patches in his detailed technical repor t on the worm[46]. Patches for ﬁ ngerd and sendm ail are discussed in detail in this section. ing the event, the National Computer Security Center(NCSC) called a meeting of scientists, oﬃcers, faculty members from institutions including the National Institute of Science and Technology, the Defence Communication Agency, the DARPA, the Department of Energy, the Ballistics Research Laboratory, the Lawrence Livermore National Laboratory, the CIA, the UC Berkeley, the MIT, SRI Internationa l, the FBI, and various other stake holders. The thr ee fourth of the day was spent in analyzing the event from the perspective of all the participants and the remaining t ime was used to discuss learnings from the event and what actions to be taken [41]. Some of the actions taken by NCSC are discussed in section §5.2. On the Thursday following the a t tack, Keith Bostic sent following two ﬁxes or suggestions for sendmail: nic.arpa which provided the compile without the debug command ﬁx to sendmail [41 ]. This posting also suggested to rename the UNIX C compiler(cc) and loader(ld), this worked as the worm needed the path to them to compile itself and helped in protecting against the non-sendmail attack. comp.4bsd.ucb-ﬁxes. This ﬁx suggested to use 0xﬀ instead of 0x00 in the binary patch to the sendmail. This was needed to support the previous patch. The previous patch was eﬀective however would have fallen to debug mode if an empty command line wa s sent. He also asked to look for string “debug” in the sendmail bin ary using the UNIX strings command and mentioned that if there is no presence of the string t hen the version is deﬁnitely safe. The patch for sendmail can be found in the appendix of this [46] report. On the Thursday night following the attack at 10:18 p.m., Bostic sent out a ﬁx for ﬁngerd. Overall, this wa s the t hir d ﬁx posted by him. This ﬁx had new source code for ﬁngerd which used fgets instead of gets and instead of return used exit. This bug-ﬁx post also had anot her version of sendmail which §8.4 to know more about why ge ts cannot be ﬁxed. A pat ch for ﬁ ngerd can be f ound in the appendix o f this [46] report. On Friday, following the attack at 5:05 p.m., Bostic released his fourth bug-ﬁx. This ﬁx was diﬀerent than the previous ones, It was a ﬁx to the worm [41]. Use of wo r m-like capability was not new in this case researchers have tried it to enable automated operating system pat ches across multiple networks[35]. This incident showed that, with technology advancement, unintended or undesirable consequences can follow. Being the ﬁrst of its kind this event gave many lessons to scientists, engineers, technical agencies as well as general public. If we look at the broader timescale around this event, this was the time when the use of the Internet was growing rapidly outside of research. Press reporting of the event (like this [17] and many others) made the general public (non-research community) aware of computer networking and, most importantly, it made many people aware of the fact that malicious computer programs can be written [32] and also made the community aware of computer and network security issues and a widespread concern grew out among people that whether the network no which many essential things like transportation, commerce, a nd high-risk services like national defense and space missions r ely upon is secure. The shocking slowdown of the system and the awareness created ma de government agencies, universities, and all other stakeholders ana lyze the incident carefully and think a bout computer and network security from all perspectives, and ﬁgure out learnings from the episode for future safety [38], [41] etc. Some of the views put forwar d by diﬀerent people were contrasting. However, based on my readings I found that there was more or less a general consensus among all the stakeholders regarding the following points. Diversifying the options - If we make an analogy with biological fact, biolog ical genetic diversity makes species more robust. Having diversity in computer programs will make the network more robust as an at tack exploiting ﬂaws of any particular software will have its eﬀect limited only to machines running or using that software, and will prevent it from bringing the whole network down. Least Privilege - The basic security principle of least privi l ege says that any entity should only be g iven just enough privilege to carry out t he work they are supposed to. There was a consensus a mo ng people that the principle of least privilege should not be ignored for computer security. Ignoring this might lead to disasters. Defense Mechanism - Defense mechanisms should be installed at end-hosts, in this particular event the network performed well and faults were in the application programs. Need of a Response and coordinating team - Just like fo r any other critical situation there should be an emergency response team for responding to computer security threats No limited connectivity - Researchers outright denied suggestions of limiting the connectivity and agreed to the argument that ”the cure shouldn’t be worse than the disease” [41]. Limiting connectivity will negatively impact t he progress of the research. In that period, 1988 - 1990, for spreading the worm which slowed down some machines Morris was sentenced to 3 years of Probation and $3276 to cover the cost of the probation, a ﬁne of $10 ,500 and 400 hours of urgency among people – many technologies and security-related(including non-computer security) agencies met within days of the event[38]. Teams like CERT were created and many other actions described in §5 were taken, public awareness on network security spiked. Now, 29 years la ter, we are in a world where hackers are mostly known as someone who does money related fraud or steals digital information a nd uses it for some unintended purpose. The Morris worm has become a history lesson, a forerunner that put everyone on guard from a demon ( However, if executed even now the worm will still need daemons for carrying out its task! Pun intended). As discussed earlier in this section, the event gave many immediate lessons but have we been able t o keep up with learnings the forerunner gave us. Have we learned a hidden lesson, that if things can then they will go wrong? Have we been able to scale up the learnings and concern for computer and network security with growing use of digital devices and computer networks? In my opinion, the answer is yes and no. To me, it seems that in many cases we are on the ”best effort” mode for computer and networ k security. Many core-technical companies do understand the importance of security and are working towards making systems and designs working towards product security [7]. System designers are taking care of security issues in advance while designing new systems or architecture. Security considerations in the design of ICN and NDN [48, 5 ] are great examples. Anti-virus and anti-malware have a big ma rket, and many products with varying prices, features and options available. They are also being widely used by people. However, there are still many attacks happening and the number is increasing day by day. If we take a look at some of the top data breach attacks of 21century [47] more than 70% of the victim companies are not core technical companies and are just using technology as a tool for their business. Attacks like Wanna Cry ransomeware [49], which are of much greater impact and severity as compared to the Morris Worm, are occurring. In many cases, we are not even able to track the attacker, a conviction is out of the questhose which are speciﬁcally designed for security) highlight security f eat ures in the product description. The following could be the potential reasons behind the above pro blems that either leadership of the non-core tech companies and other regulatory authorit ies are not aware of or are not able to judge or willfully ignore to a cknowledge the severity of risks involved in the computer security. Legal policies are not strong enough or detailed and precise enough to capture t he computer security requirements [16, 11]. Another potential reason could be that people are not able to correctly sense and measure the risk involved with the digital world as it is still very new. People do not use security features as one o f the selection criteria for digital products. Or maybe agencies still believe and agree with the statement that ”It may b e more expensive t o prevent such attacks t ha n it is to clean up after them” [41]. When the worm incident ha ppened most of the people using computers and networks were professionals and had signiﬁcant knowledge of design and wo rking of the system. However, today this not at all the case computers are just like any ot her equipment and many people with little, if any, knowledge about how system works are using it. There has been no signiﬁcant step taken yet to educate such people. Schools and other public and private institutions do arrange time-to-time ﬁre drills, and drills to handle other disaster events. However, I am not awa re of or have heard of computer security drills. Even many Computer Science departments do not have a single mandat ory course on computer security [1]. However there are a large number of vacancies fo r cybersecurity experts, as per [29] every year in the United States., 40,000 jobs for information security analysts go unﬁlled, and institutions are struggling to ﬁll 200,000 other cyber-security related positions. I am not sure about lack of awareness but there deﬁnitely is lack of attention to computer security. Computer security is not getting the importance it should. Before the incident the Internet was a closed community, and ethical and responsible behavior and good intent were assumed. The worm episode changed people’s behavior and attitude towards computer security. Various actions were taken by diﬀerent organizations [10, 6], government agencies, and universities, and of course, an event with such huge impact will deﬁnitely bring some changes in practice and perspective of people. This section discusses these changes following the event. Post worm people had this sense tha t the Internet is no longer a closed community. As published in an interview by Intel news room [32] – The day after the wor m an awful lot of people were shocked that such abuse could happen. This is evident from the following statement in [12] ”A community of scholars should not have to build walls as high as the sky to protect reasonable expectation of privacy, particularly when such walls will equally impede the free ﬂow of information.” The statement clearly shows the disappointment of the community on observing the breach of trust. This incident made a lot of people outside the academic community aware of the f act that malicious software could be written. From being a closed and trusted community of researchers the Internet started becoming a large community, and it had t o accept uncontrollable sociopaths as it members. Computer science departments all across the world started deﬁning the appropriate and inappropriate usage of the Internet resources. This subsection discusses more concrete and administrative changes and other objective changes made in response to the episode. There were many administrative actions suggested and taken [10, 6, 38]. Like the formation of Computer Emergency Response Team(CERT) at Carnegie Mellon University with funding support from DARPA. CERT was formed by orga nizing computer scientists with aim of isolating such problems and preventing them from happening in the future. Though the ﬁrewall technology already existed before the event, it saw a surge after t he disturbing event [37]. Some people at DEC started putting a n eﬀort into corporate network protection. In a report to the Chairman of Subcommittee on Telecommunications and Finance, Committee o n Energy and Commerce, House of Representatives US General Accounting Oﬃce recommended formation of an interagency group including agencies funding research networks on the Internet, under the coordination of President’s Science Advisor, with following goals (extracted from the report [6]). 1. provide Internet-wide security policy, direction and coordination. 2. support o ngoing eﬀorts to enhance Internet security. 3. obtain the involvement of Internet users, software vendors, technical advisory groups, and federal agencies regarding security issues. 4. become an integral part of the structure that emerges to manage the National Research Netwo r k. This section subjectively discusses t he direct or indirect costs incurred due to the worm and what it would cost for such an event in today’s scenario. With approximately only 6000 (10% of total machines) [12] machines being aﬀected a nd the size of the networ k being only 60,000 machines, use of computers was esoteric and primarily for research, with high chances of the attacker not having criminal intent the losses were in millions of dollars. Experts estimated that the per machine loss could be up to $53,000 [9] and another estimate in [12] said around 6000 machines were infected, so the to t al loss could be as high as $318 million. In contr ast today there are 1) billions of digital devices in use and mo st of them are connected to the Internet, 2) more than 51% of total world population 3) Computers are being used widely ranging trivial day to day work like managing traﬃc, listening to music, and education to high-risk tasks like medical treatment, na t ional and internal security. A large number of businesses entirely depend on computers and network 4) cyber-attacks being made with criminal intent [49, 47]. It is hard to imag ine what will be the impact of an attack involving 10% of to t al machines on the Internet. It could simply break traﬃc in an entire city, state or maybe of whole nation. It could bring down several businesses or can even lead to large number of deaths if medical systems or security systems get into ambit of the attack. According to cyber risk modeling from Cyence [2], economic losses fr om a recent cyb er attack, WannaCry ransomeware, [49], could reach $4 billion. The attack began on 12May 2017 and impacted j ust 300,000 devices(less than 0.01% of t otal digital devices) and 20 0,000 people (less than .003%) of world population [49]. Even if extrapolate linearly the losses due t o an attack involving 10% of total devices on the network could be as high as $4 trillion. This study compels me to agree to the statement (or it s variations) by Prof. Spaﬀord that “The only truly secure system is one that is powered oﬀ , cast in a block of concrete and sealed in a lead-lined room with armed guards — and even then I have my doubts” [3 2]. Any new system starting with a closed group will have to deal with security issues as it expands. This is precisely what the Morris worm showed. An attack like this was inevitable, had Morris not done it someone else would have. New upcoming systems are now more cautious about security, ICN is an example. However, that is not enough as discussed in §4.1 we are not yet fully ready to eﬃciently tackle cyber attacks there is pressing need to make general masses and policy makers realize the importance of computer and Internet security. D emo citesjajooSLearn, jajo oPhilae, jajooG r aviton, jajooSLearn, saathTechReport, phdthesis, philaeTechReport, slearnTechReport The concept of self-propagating worm program was ﬁrst described by John Brunner in his ﬁctional novel The Shockwave Rider [4]. Worm in the novel was used as a tool for taking revenge! [8]. A computer wor m is a complete piece of program that replicates itself in o r der to spread to other machines. Certainly, other than the very weak claim made in [45] that war m might have been written to take subconscious revenge on his father. I didn’t ﬁnd any source claiming that Morris wrote the worm to take revenge. More about worms here [50]. A computer virus is a malicious software. O n execution, it replicates itself by ”infecting” other programs i.e. by inserting its own code fected progra ms can include data ﬁles or system progr ams or even boot sector of the hard drive. Robert Tapan Morris’s father, Robert Morris, was a computer scientist at Bell labs and helped in designing Multics and Unix and later he became chief scientist at National Computer Security Center, a division of the NSA. Some people claim [3] that Junior Robert was trying to get away fro m his father’s image and have one of his own. We can say that functions implementing APIs these function will only get a pointer to a buﬀer( char* ) and not the size of the buﬀer and since there is no implicit size bound to a buﬀer in C, it will be impossible for the function to bound the size of data being read. This point is very clearly explained in the linux man page of gets [14]. Never use gets(). Because it is impossible to tell without knowing the data in advance how many chara cters gets() will read, and because gets() will continue to store characters pa st the end of the buﬀer, it is extremely dangerous to use. It has been used to break computer security. Use fgets() instead.