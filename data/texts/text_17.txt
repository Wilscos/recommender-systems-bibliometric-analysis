Consider the pipeline to generate a list of personalized contents (e.g., videos, ads) to users in a real (probably oversimpliﬁed) recommendation system. Such pipelines typically include two phases: 1) predicting the probability of user’s immediate actions and 2) calculating a rank score for ﬁnal recommendation, as illustrated in Figure 1. In the ﬁrst phase, the goal is to accurately predict various immediate user actions such as clicking on an ad or liking a recommended content. The prediction functions to these actions can be directly learned from the user’s behavior data. However, it is not optimal to use any single one of these predicted scores to rank and display the ﬁnal contents because the ultimate goal of recommender is to improve multiple downstream business goals. For example, the goals could be to maximize the average user engagement time while the average content like rate does not decrease. These business purposes are usually the global and overall metric, which can hardly be modeled by users’ immediate actions and are not directly learnable. Therefore, in the second phase, one needs an aggregation function to compute an overall score weicongding@kuaishou.com tanghl1994@gmail.com jingsf@uw.edu ji.liu.uwisc@gmail.com , Hanlin Tang, JingShuo Feng, Lei Yuan, Sen Yang, Guangxu Yang, Jie Zheng, Jing Wang, , Dong Zheng, Xuezhong Qiu, Yongqi Liu, Yuxuan Chen, Yang Liu, Chao Song, Dongying Kong, Kai Real-world recommendation systems often consist of two phases. In the ﬁrst phase, multiple predictive models produce the probability of different immediate user actions. In the second phase, these predictions are aggregated according to a set of ‘strategic parameters’ to meet a diverse set of business goals, such as longer user engagement, higher revenue potential, or more community/network interactions. In addition to building accurate predictive models, it is also crucial to optimize this set of ‘strategic parameters’ so that primary goals are optimized while secondary guardrails are not hurt. In this setting with multiple and constrained goals, this paper discovers that a probabilistic strategic parameter regime can achieve better value compared to the standard regime of ﬁnding a single deterministic parameter. The new probabilistic regime is to learn the best distribution over strategic parameter choices and sample one strategic parameter from the distribution when each user visits the platform. To pursue the optimal probabilistic solution, we formulate the problem into a stochastic compositional optimization problem, in which the unbiased stochastic gradient is unavailable. Our approach is applied in a popular social network platform with hundreds of millions of daily users and achieves+0.22%lift of user engagement in a recommendation task and+1.7%lift in revenue in an advertising optimization scenario comparing to using the best deterministic parameter strategy. Figure 1: A illustrating example of strategic parameters in a simpliﬁed recommendation system with two phases. The 1st phase has multiple predictors for click-through-rate (pctr), like-rate (plr), etc. The 2nd phase is an aggregation layer with strategic parameter to generate the ﬁnal recommendation score. Note that the strategic parameters can be in many other components of a recommendation system. for each user-item pair (a, b) based on the predictions from the ﬁrst phase, wherepctr(a, b) function weighted sum [22, 30, 36] where β are the mixing weights. Our paper focuses on the strategic parameters. Examples like Figure 1 is ubiquitous in industrial recommendation systems. The strategic parameter many components of a practical recommendation system and are often in the form like weights, thresholds, or quotas. While the problem of building predictive models (e.g, models in the 1st phase in Figure 1) has been extensively studied[10 is generally classiﬁed into a standard black-box optimization problem – tuning the strategic parameters to optimize the overall business metrics. Manual tuning via controlled experiments are popular in practice [ sophisticated methods like bandit optimization [ Bayesian optimization [ strategic parameter; that is, the same strategic parameter system. In this paper, we argue that the deterministic (single) strategic parameter is not the optimal solution. We discover that a probabilistic choice for the strategic parameter can be superior to any deterministic one, especially when there are multiple business goals to pursue simultaneously. In the probabilistic solution, we learn an optimal distribution over all the candidate strategic parameter choices. The system then works as follows: when there is a request to the recommender (e.g., a user visit), we ﬁrst sample one of the multiple strategic parameters using the learned distribution and then apply the randomly selected strategic parameter in the recommendation pipeline. A common recommender is visited by users hundreds of millions of times daily. Therefore, the proposed solution can be viewed as a probabilistic mixture over multiple strategic parameters. The probabilistic solution achieves supreme performance over deterministic solution since the latter can therefore be viewed as restricting the distribution vector in the probabilistic solution to a one-hot vector. We demonstrate the supreme performance using extensive simulation studies and online AB testing results later in this paper. The challenges of ﬁnding the optimal probabilistic parameters are two folded. On the one hand, the distribution of the unknown metrics can only be learned by interacting with the online customer-facing systems and then observing sparse and noisy samples via multiple iterations. On the other hand, the optimization problem to ﬁnd the best probabilistic distribution falls into the family of stochastic compositional optimization, in which the unbiased stochastic gradient is unavailable. We propose to use an average of the unbiased estimator over the history in approximating the unbiased stochastic gradient. This trick also helped to address the sparse observation and reduced the noise. We also incorporated proximal projection to remove the simplex constraint. The proposed approach, Probabilistic pArameter optimization with unbiased STOchastic gradient approximation ( order of O ,plr(a, b)are the predicted content click through rate, like rate, etc.βis the hyper parameter of the R(), which is referred as thestrategic parameterin recommendation. A simple version ofR(·)can be a ,35], deciding the strategic parameter has not yet received equal attention. It is mainly because the problem In sum, the key contributions of this paper are, •We discover a probabilistic strategic parameter solution that outperforms the classic deterministic strategic parameter when we are pursuing multiple goals in recommendation systems. •We formulate the problem of ﬁnding the optimal probabilistic parameter solution as a compositional stochastic optimization task, and developed an efﬁcient stochastic gradient algorithm. We proved that the proposed algorithm converges to the optimal probabilistic distribution at a rate of where T is the number of iterations. •We implement the proposed probabilistic strategic parameter solution at a leading social network platform with hundreds of millions of daily active users and tens of billions annual revenue. Note that in a platform of this scale, a slight percentage gain provides enormous business value. The proposed approach achieved user engagement in a recommendation scenario and compared with the optimal deterministic parameter choice. The rest of this paper is organized as follows. We ﬁrst discuss related literature in Section 2. In Section 3 we formulate the strategic parameter searching as an optimization problem and argue why the probabilistic regime is better. We discussed our solution in Section 4 and provide the theoretical analysis in Section 5. We include a series of simulation studies in Section 6, and present online AB testing results in Section 7. Black-box strategic parameter tuning has been extensively studied using both heuristic and Bayesian approaches to ﬁnd the maximizer of an unknown and noisy objective function. Popular heuristic approaches such as Genetic Algorithm [24], Cross-Entropy-Methods [ [16,19] proposed to use the Bayesian approach to sequentially explore the strategic parameters. [ use the Bayesian approach with contextual information in industrial recommendation systems. Multi-armed bandits (MAB) and bandit optimization problems are also widely used in searching the best deterministic strategic parameter [4,31]. The contextual bandit setting considers environmental conditions and can select a deterministic parameter for each different context [ detailed discussion). Multi-task learning has raised attention recently in recommendation systems [ literature is in the phase of predicting immediate user actions [ architecture for model-parameter sharing, loss sharing, etc., have been explored [ multiple losses have also been studied, such as weighted sum [ Regarding learning deterministic strategic parameters with multiple objectives, [ include a diversity indicator on top of item rating evaluation; [ reinforcement learning. Recently, much attention has been paid to ﬁnding the Pareto frontier of multiple goals [ where a set of Pareto optimal items is selected, and no alternative can improve every objective simultaneously. And Pareto efﬁcient algorithms can help coordinates multiple objectives [ search [3, 29], expected hyper-volume improvement [7], etc. Recently, [ or device type. However, in their proposed settings, deterministic single-best parameters are still the solution for each user group or context. We also note that [ treatment/parameters. Our paper considered a dynamic and iterative approach that is more effective in industrial applications. Now we formally deﬁne the probabilistic strategic parameter solution and show its advantage over the deterministic one. We start by formulating the strategic parameter selection problem into an optimization task. Then we show that the proposed probabilistic solution deﬁnes a larger feasible domain than the deterministic solution, implying its superior performance. 3.1 Optimization View of the Strategic Parameter Tuning Problem Recall the motivating example in Section 1 where we tune the strategic parameter goals, e.g., 32] proposed to personalize the strategic parameter assignment based on the user’s demographic group • maximize the expected averaged engagement time (µ • maximize the expected averaged engagement time (µ • maximize a utility function of both the expected averaged engagement time (µ In this example, to be more precise, we denote by the strategic parameter a group of randomly selected users, namely, HereE(ˆu in our observation. In general, we use guardrail metrics we do not want to drop. Next we can formulate the the form of a numerical optimization problem where B is the set of all possible strategic parameters. For different goals, f can be designed as, • f(x, y) = x if we only have one metric to optimize. This is less common in practice. • f(x, y) = x − h(y; c) strictly requiring metric µ • f(x, y) = x − λ min(0, y − c) X while imposing a square penalty if the metric Y drops below a pre-deﬁne threshold c. Now to illustrate our proposed probabilistic solution, we ﬁrst reformulate Here we assume that the number of possible strategic parameter options Kfor simplicity. The inﬁnite scenario follows the same spirit. parameters compactly in vector form And re-write the optimization Eq (1) in an equivalent form, It is worth noting that the optimal solution to Eq. to this as the deterministic solution since the same strategy parameter is applied to all the recommendation requests. Next we are ready to propose the probabilistic solution. Revisiting selection variable K-dimensional probability simplex and p ∈ The pmf view of Say for example we randomly select either parameter to generate ﬁnal recommendation list for the user. If we consider the average effect over hundreds of millions of daily requests in a industrial recommender, the inner product metricX propose to pursue the optimal probabilistic solution by solving, See appendix for discussion on continuous parameter space. µ(β)is difﬁcult to obtain. We can usually obtain an unbiased sample (denote asˆu(β)) by applyingβto (β) :=1#usersuser j’s X metric (i.e., daily engagement time) applying strategic parameter β. (β)) = µ(β)is a unbiased observation ofµ(β)whose variance depends on the number of users/requests pto a larger set,¯S = {p ∈ [0, 1]|Σp= 1}, which is the convex hull formed byS.¯Sis the psuggests a newprobabilisticsolution of applying strategic parameters in recommendation systems. K = 3andp = [0.8, 0.2, 0]. When a user visits the platform and a recommendation request is created, (here the expectation is over both the randomness of sampling frompand the noise inˆu). Formally, we 3.3 Probabilistic Solution Achieves better Objective Value than Singleton Solution Sincefis in general a non-linear in Eq andµ’s in (probabilistic parameter) solution to Eq. straightforward to verify that, Observation 1 parameter solution achieves better reward compared to the deterministic parameter. Figure 2: An Illustrating Example on why probabilistic parameter can achieve better overall rewards than single deterministic parameter. probabilistic parameter mixing weight that samples the overall rewards achieved with p. An Illustrating Example the ﬁrst choice more on the relative gain over some baseline. So setting time over baseline. We also set whereh results in selecting the second parameter case, we can choose f = 1. Therefore, the p The illustrating example we’ve discussed represents a common real-world scenario, that is, one strategic parameter choice (β vice versa for another strategic parameter choice ( applications often compete with each other (given limited user attention on the platform). We will demonstrate this in Section 6 and 7. Before concluding this section, note that we used two metrics typically multiple metrics X’s to improve and various metrics Y ’s to protect. The formulation and conclusions in this section directly extends to this general setting. We now discuss how to solve the probabilistic parameter optimization problem in Eq constrained stochastic compositional optimization [ rewardsµ strategic parameters (say and observe the empirical average metrics engagement time and his/her like rate), and repeat this for multiple rounds/iterations. The observation at each iteration is a sparse sample of certain time. Say, if hourly engagement time is our goal, then, we need to apply (2)and(3), letpbe the optimal (deterministic parameter) solution to Eq.(2)andpbe the optimal The optimal objective valuef(µp, µp) ≤ f(µp, µp)). Namely, the probabilistic X, Yrefer to two different business metrics,β, βare two possible strategic parameter choices.p = [0.5, 0.5]is a βhas an expected engagement time of2and the engagement time is0forβ). In practice we focus imposes inﬁnity penalty wheny < 0. For deterministic solutions, selecting ﬁrst parameterp= [1, 0] f = −∞since the metricYis below the thresholdc = 0for ﬁrst strategic parameterβ. On the other hand, in this example) improves metricXbut at the same time results in a lower average value of metricY, and (β)andµ(β)are unknown. To obtain an empirical sampleˆuin practice, one needs to choose one of the We choose to solve our problem using stochastic gradient approach. The sparse and noisy observation in our problem raises two technical challenges: 1) the unbiased stochastic gradient is generally hard to obtain; 2) the constraint is quite tricky to enforce. We next discuss our technical solutions to address them. In solving Eq approximate the true expectations form and compactly write could be more than 2 rows in as 2 throughout this section. Since vector from the common stochastic optimization admitting the form of the family of stochastic compositional objectives and the unbiased stochastic gradient is generally not achievable [ More speciﬁcally, the true gradient of (3) at learning step t admits the form which requires us to obtain unbiased estimator of the term at each round metrics ˆu and verify that compute is unbiased, because case, our solution is to use the averaged value of all historical (unbiased) estimator which gets more and more accurate as training continues. We then approximate the unbiased gradient using We note that our gradient estimation is different than the stochastic compositional gradient descent (SCGD) framework Since we used the history average generic SCGD solution [33] (see Section 6 for simulation study). The second technical challenge is to handle the simplex constraint after the (approximate) stochastic gradient descent step. We choose KL divergence (other than the Euclidean distance) as the Bregman distance which can naturally ensure the next iterate considering the simplex constraint. More speciﬁcally, a given step-size γ resulting in the following closed form update rule, (3), an unbiased stochastic gradient for is hard to obtain since the our observation in each round to µare the expectation of the noisy metric observations, the optimization objectivef(µp)in(3)is different t, we can sample one strategic parameterβfrom the current distributionp, and observed empirical (β), ˆu(β), etc. We can then construct E(ˆU) = µis unbiased estimator ofµ. However, this cannot ensure that the stochastic gradient we ]. This difference is due to the linear formµpinside theffunction, a special case of the generic SCGD. We summarize the proposed robabilistic Parameter Optimization with Unbiased Stochastic Gradient Approximation in Algorithm 1 as an iterative exploration and optimization procedure. At each round, we ﬁrst sample one strategic parameter from the current probabilistic pmf the gradient optimization. Note that instead of using the do converges to the optimal rewards. Algorithm 1 Probabilistic pArameter Optimization with unbiased STOchastic Gradient Approximation (PASTO) Require: total number of iteration steps T ; Ensure: 1: Initialize w 2: for each iteration t = 1, . . . , T do Note that following [ a non-zero chance of being selected for exploration and to cap the unbiased estimation also introduces more exploration of the strategic parameters. Similarly, the step-size exploration. We now present the theoretical analysis for Algorithm 1. Without loss of generality, we assume that problem conditions on our objective function f and the underlying ground-truth rewards µ’s. Assumption 1. We make the following commonly used assumptions for f(·) and µ(B): • The gradient of objective f(·) deﬁned in Eq (3) is bounded, and f(·) is L • The Frobenius norm of µ(B) deﬁned in Eq (4) is bounded, i.e., kµ(B)k 5.1 Convergence Analysis convergence of Algorithm 1. Namely, we want to show the estimate vector as f (µp), can converge to the optimal reward of Eq (3) (the convergence is in the sense of expectation). Formally, Note that f represents the rewards to be maximized in application. Therefore, we assume f being concave in our analysis. pas the estimated best probabilistic solution, which is more stable in practice. We show this average Initial estimator of rewardsbU(2 × Kmatrix), learning rateγ, smoothing parameter, t = 1, . . . , T, the = [1, . . . , 1]=: 1 ∈ R,ˆV←ˆU based on Eq (5) ˆV=ˆV+ˆU(This is the same as Eq (6) but in a recursive form.) (3)is a concave objective function.For the results to hold, we ﬁrst introduce a few common technical is what we deploy in the actual production system after the learning process, ourprimaryfocus here is the Tgoes to inﬁnity. Equivalently, we need to show that the overall reward that can be achieved by¯p, i.e., Here the expectation is over all the randomness in the gradient history {p In short, the estimation value the order of rewards of the probabilistic solution. Next, we analyze the regrets bound of the iterative learning process. In this analysis, we assume the ground-truth rewardsµ overall trends are constantly changing. Concretely, we denote by (the same format as in Eq (4)). From a practice viewpoint, tillP can re-select a solution gap between this reward objective with is from Algorithm 1. Similarly, the regret can be also deﬁned as the gap the global optimal and the empirical loss, which is more close to the empirical regrets in the procedure of Algorithm 1. With all these notations, we show the following bounds on the total regret, Theorem 2. (Regret of Alg. 1) For Algorithm 1, under Assumption 1,  max and max As Theorem 2 suggests, our Algorithm 1 can achieve a regret at the order of O( supplementary. We conduct simulation to demonstrate the experiments, we choose objective of the form if there are multiple guardrail metrics goal is to compare the performance of probabilistic solution against the deterministic one. Therefore, when reporting the performance of the deterministic(single) solution, we always assume having access to the noise-less rewards and knowing which single strategic parameter yields the highest expected objective value, which will be the upper bound for any deterministic parameter algorithm. Parallel Querying: in each round of Algorithm 1, we could choose fromp. Practically, we can randomly split the users or requests into parameter for each group accordingly. We ﬁrst verify that the proposed Algorithm 1 does converge empirically using the following setup: (Convergence of Alg. 1) Under Assumption 1, set=andγ =, further, assuming that the noisy ˆUandˆUfrom two different iterations t, s are independent. Then, O. Theorem 1 implies that using the¯pin the online production system can yield the optimal could be different over time. This is often the case in real-world applications since customer preferences and ˆUthe empirical observation. Given this,f(µp)represents the (ideal) objective value if one (A)the illustrating example in Section 3 where queried in one round, we add a Gaussian noise of ˆuand ˆu (B) K = 100 is selected. We set parallel querying Q = 10. Figure 3: Empirical convergence of our proposed Algorithm 1 and S-SCGD on the example A. Shaded area indicate the 75th and 25th percentile of the objective in each round in 1000 Monte Carlo runs. For the ﬁrst toy example (A), we conduct To evaluate the efﬁciency of the proposed approach, we also compared against the stochastic compositional gradient descent approach [ probabilistic solution. As one can easily verify from the simulation setting, this probabilistic reward is indeed better than the single parameter solution. We also note that the convergence of Algorithm 1 is faster than the simple S-SCGD due to its reduction of noise. We will only use proposed Algorithm 1 in later sections due to its efﬁciency and robustness to sparse observations. Figure 4: Relative gain of Algorithm 1 on synthetic dataset with different noise level random runs are reported. For the second simulation setting (B), we also conduct 1000 Monte Carlo runs to generate ground-truth our algorithm. To make it comparable across different ground-truth setting probabilistic parameter over the single best parameter, deﬁned as parameter normalized by the ideal objective gap between probabilistic parameter and single parameter. achieving a better objective than the single-best parameter. Figure 4 summarizes the average relative gain as the number of iteration for different noise levels. The error bars indicate the standard deviation of the 1000 Monte Carlo runs. We note that convergence speed of the approach. Motivated by [ of them has customer-item rating interactions as well as price, genre information. We binaries the 5-star rating and set https://grouplens.org/datasets/movielens/20m/ . We set parallel querying Q = 1. andµ,µ,µfork = 1, . . . , Kare sampled independently and uniformly within[−1, 1]. We set = 0.5. An additive Gaussian noiseN(0, σ)is added to each round’s observation if a strategic parameter TASCOdoes converge and approaches the best possible gain. The noise of observation does impact the 22,23], we simulate on two publicly available datasets, Amazon Books [25] and MovieLens 20M. Each ratings≥ 3 training and test with 70%, 30%. We considered three targeted metrics: metrics,2) as Revenue@K or REV@K) (for Amazon dataset), and ‘Documentary’ genre (for Movielens 20M dataset) (R-D@K). Higher metrics indicate better performance. We build Variational Auto-Encoder (VAE) [ each of the 3 targets. All the setups are summarized in Table 1. The simulation runs as following. For each dataset, objective 1 in Table 1 is viewed as underlying VAE models we have built predict the scores using a power-based function strategic parameter. We discretize the parameter space of To simulated the noise in the online reward collection regime, for each round 10 folds, apply random subsets of testing data. Table 2: Results on real world data simulation. Threshold is 4.28 for REV@20 an 0.080 for R-D@20. We report the ﬁnal rewards and constraints metrics in Table 2. The single best parameter, as discussed before, is identiﬁed by iterating through all possible choices using the entire testing set. We also report the reward by having non-constrained single obj.1 to understand the upper limit of our prediction models. Table 2 shows our probabilistic solution can achieve overall better results over the deterministic best parameter choices. We present two industrial applications on a leading social networking platform with hundreds of millions of daily active users and the AB-testing results. Table 3: AB Test Result of Online Content Recommendation Task, Watch time is primary reward we would like to optimize. Like and Sharing are two guardrail constraints. A soft threshold of constraint metrics. All results are reported as lift w.r.t. the baseline model at the time of experiment. Ensemble Sort in Content Recommendation scenarios. The strategic parameters are ensemble weights of multiple recommendation queues, each optimized for a particular customer target events. This is similar to the illustrating example in Figure 1. The goal is to increase the average user engagement (time) while not dropping the ‘like’ action rate and the ‘share’ action rate. The existing as positive. Users and items with at least 5 ratings are kept in the processed data. Data were then split into the revenue achieved by ranking measured by the recall metrics weighted by the actual price of item (denoted Q = 10parallel strategic parameters, and then compute the empirical average objective metrics on these baseline parameter is a deterministic parameter and is extensively optimized. The AB testing results are outlined in Table 3. Note that the probabilistic setting improves not only the primary metrics but also the constraints metrics. This is due to the fact that some of the parameters in our probabilistic mix do yield higher gain in the ‘like’ and ‘sharing’ metrics. Quota-Based Ads Retrieval Systems platform. In this system, there are multiple modules, each attempting to retrieve a set of relevant ad contents with different types. One needs to combine these candidate sets into a single and smaller group to feed the downstream ad ranking and pricing models. Due to the limitation on computation time and power, quotas need to be set on the maximum number of ad contents generated by each module. Our goal here is to improve the overall advertising revenue and not to hurt a number of speciﬁc categories such as cold-start ad content. We implement our probabilistic parameter solution in this ads system and compare it against an existing single-parameter baseline that has been extensively optimized. In our online AB test, the algorithm achieves a revenue improvement of +1.7% without signiﬁcantly hurting the imposed constraints. This paper argues that the probabilistic strategic parameter achieves better rewards than the deterministic parameter solution. We present an algorithm ( theoretical guarantees. Both simulation and online applications have shown improvement over the deterministic best arm choice.