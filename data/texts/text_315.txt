Matrix completion is the study of recovering an underlying matrix from a sparse subset of noisy observations. Traditionally, it is assumed that the entries of the matrix are “missing completely at random” (MCAR), i.e., each entry is revealed at random, independent of everything else, with uniform probability. This is likely unrealistic due to the presence of “latent confounders”, i.e., unobserved factors that determine both the entries of the underlying matrix and the missingness pattern in the observed matrix. For example, in the context of movie recommender systems—a canonical application for matrix completion—a user who vehemently dislikes horror ﬁlms is unlikely to ever watch horror ﬁlms. In general, these confounders yield “missing not at random” (MNAR) data, which can severely impact any inference procedure that does not correct for this bias. We develop a formal causal model for matrix completion through the language of potential outcomes, and provide novel identiﬁcation arguments for a variety of causal estimands of interest. We design a procedure, which we call “synthetic nearest neighbors” (SNN), to estimate these causal estimands. We prove ﬁnite-sample consistency and asymptotic normality of our estimator. Our analysis also leads to new theoretical results for the matrix completion literature. In particular, we establish entry-wise, i.e., max-norm, ﬁnite-sample consistency and asymptotic normality results for matrix completion with MNAR data. As a special case, this also provides entry-wise bounds for matrix completion with MCAR data. Across simulated and real data, we demonstrate the eﬃcacy of our proposed estimator. Matrix completion is the study of recovering an underlying matrix from its noisy and partial observations. Given its widespread applicability, the ﬁeld of matrix completion has grown tremendously in recent years. To establish statistical guarantees for the various algorithms that exist for matrix completion, it is typically assumed that: (i) the underlying noiseless matrix has latent structure, e.g., it is low-rank, and (ii) the entries of this matrix are missing completely at random (MCAR), i.e., an entry is missing independent of everything else and with uniform probability. However, numerous modern applications of interest violate the latter assumption. Below, we consider two motivating examples. First, arguably the most well-known application of matrix completion is recommender systems, which are ubiquitous in modern online platforms. Typically, data is collected in the form of a matrix, where the rows index users and columns index items; the (i, j)-th entry, therefore, corresponds to the rating supplied by user i for item j. In such scenarios, observations are often subject to selection-biases. For instance, in movie recommendations, a fan of fantasy ﬁction will almost certainly watch and highly rate the Harry Potter series. Similarly, in restaurant recommendations, a vegetarian is unlikely to enjoy nor rate a steakhouse restaurant. While these examples demonstrate self-selection biases from the end of the users, systems also exhibit targeted suggestions. For example, when a user searches for trails at the Grand Canyon, an ad placement system is more likely to display an ad for hiking boots than wedding shoes; in turn, this can increase the user’s likelihood to purchase and rate hiking boots. In all of these cases, the user’s preferences and/or the system’s beliefs in its users’ preferences, inﬂuence the sparsity pattern of the observation matrix. A second example is panel data settings in econometrics. Here, observations of units (e.g., individuals, geographic locations) are collected over time as they undergo diﬀerent interventions (e.g., promotions, socioeconomic policies). The induced matrix has rows index units and columns index time-intervention pairs; the (i, (a, t))-th entry then corresponds to the potential outcome of unit i under the a-th intervention at time step t; here (a, t) represents the j-th column, i.e., columns are double indexed by both intervention and time. As with recommender systems, observations in panel data settings are unlikely to occur completely at random. For instance, policy-makers strategically recommend programs that are designed to achieve certain desirable outcomes based on numerous socio-economic factors surrounding the geographic region under their purview. Further, competing programs with disagreeing agendas cannot be simultaneously adopted for a speciﬁc region during the same time period, i.e., if the (i, (a, t))-th entry is observed, then the (i, (a entry must be missing. Notably, similar matrices and observation patterns can arise in sequential decisionmaking paradigms within machine learning such as online learning, contextual bandits, and reinforcement learning with time-intervention pairs being replaced by state-action pairs. In both examples, the missingness pattern of the matrix is dependent on the underlying values in that matrix, and observing the outcome of one entry can alter the probability of observing another. That is, the entries are missing not at random (MNAR). To address the above challenges, there has been exciting recent progress on matrix completion with MNAR data, including Schnabel et al. (2016); Ma and Chen (2019); Zhu et al. (2019); Sportisse et al. (2020a,b); Wang et al. (2020); Yang et al. (2021); Bhattacharya and Chatterjee (2021). Through numerous empirical studies, these works have shown that algorithms that account for MNAR data outperform conventional algorithms that are designed for MCAR data. With respect to theoretical analysis, however, critical aspects of matrix completion with MNAR data remain to be explored. In particular, as highlighted in Ma and Chen (2019), there are two common limiting assumptions in the literature: (i) the revelation of each entry in the matrix is independent of all other entries, and (ii) each entry has a nonzero probability of being observed. Another recent exciting line of work that we build upon is that of panel data and matrix completion, see Amjad et al. (2018, 2019); Arkhangelsky et al. (2019); Bai and Ng (2019); Fern´andez-Val et al. (2020); Athey et al. (2021); Agarwal et al. (2021c,a,b); Agarwal and Singh (2021). Some of these works allow for MNAR data and entries of a matrix to be deterministically missing. However, they consider very restricted sparsity patterns that are not particularly suitable for important applications of matrix completion. For example, the most common sparsity pattern considered in the panel data literature is where for a given row i, if a column j is missing, then entries for all columns j recommendation systems or sequential decision-making. Further, to the best of our knowledge, none of these works within the panel data literature provide meaningful results for matrix completion with MCAR data. The statistical parameters these works aim to estimate are also less meaningful for these other applications of matrix completion. The most common statistical parameter these works consider is the average outcome for all missing entries in a given row i; in say recommendation systems, this would correspond to the average rating a user i would have given for all movies they did not rate. This is not particularly meaningful for an online platform—ideally, a platform would like to do accurate inference for each (i, j) pair. The focus of this work is to propose a formal causal framework and an algorithm with provable guarantees to analyze matrix completion with MNAR data where the probability that an entry of the matrix is missing can: (i) depend on the underlying values in the matrix itself; (ii) depend on which other entries are missing; (iii) potentially be deterministically zero. Further, we want to allow for more general missingness patterns and estimate more reﬁned statistical parameters than considered in the panel data literature thus far. We hope this work further bridges the rich and growing ﬁelds of causal inference and matrix completion. As further motivation for why it is important to carefully think about the underlying mechanism for why data is missing, we now provide illustrative empirical simulations. In particular we run three experiments, each with a diﬀerent mechanism for how data is missing. In Experiment 1, data is missing via a MCAR mechanism i.e., each entry is missing independently at random with probability 0.35; the induced sparsity pattern is depicted in Figure 1a. In Experiment 2, data is missing in a MNAR fashion, i.e., each entry has a diﬀerent probability of being missing; the induced sparsity pattern is depicted in Figure 1b. However, we ensure key assumptions made thus far in the matrix completion literature with MNAR data are maintained; in particular, (i) the revelation of entries are entry-wise independent and (ii) each entry has a nonzero probability of being observed. In Experiment 3, data is missing in a MNAR fashion, but we violate conditions (i) and (ii) above; the induced sparsity pattern is depicted in Figure 1c. For exact details on the missingness mechanism in Experiment 2 and 3, refer to Section 6.2.1 and 6.2.2, respectively. In all experiments, we ﬁrst create a sample of true “ratings”, which are invariant across all three experiments. We enforce these ratings to go from 1 to 5, as is standard in many online platforms. The distribution of true/revealed ratings are plotted in light/dark blue in Figures 2a, 3a, and 4a, respectively. As expected, the distribution of the revealed ratings in the MCAR setup matches that of the true ratings. However, the set of ratings that are revealed in both MNAR settings are severely biased, i.e., their distribution does not match that of the true underlying ratings. We use three matrix completion algorithms and see whether they can recover the distribution of true ratings given the revelead entries in all three experiments. The algorithms are: Universal singular value thresholding (USVT) Chatterjee (2015), which is a popular spectral based method; (2015), which is a popular optimization based method; Synthetic nearest neighbours (SNN), which is our proposed method for matrix completion with MNAR data, and is a combination of the approach taken in nearest neighbour style and panel data methods in econometrics. USVT and softImpute are not designed for MNAR data, as is, but we de-bias them for MNAR data as is done in Bhattacharya and Chatterjee (2021) and Ma and Chen (2019), respectively. See details in Section 6.3. We see that in Figure 2, under the MCAR setting, softImpute and SNN both recover the distribution of true ratings very well, while USVT cannot. Once we go to the limited MNAR setting, depicted in Figure 3, where conditions (i) and (ii) are upheld, SNN is still able to recover the underlying distribution of true ratings, but now both softImpute and USVT have non-negligible bias. In the general MNAR setting, depicted in Figure 4, SNN continues to accurately recover the distribution, but the bias of softImpute is signiﬁcantly worsened. This empirical illustration highlights the sensitivity of these traditional matrix completion methods to the missingness mechanism and strongly motivates the need for a rigorous framework for tackling the general MNAR setting where conditions (i) and (ii) above are violated. Providing such a framework is what we set out to do in this work. Figure 2: MCAR: recovered ratings distributions under (modiﬁed) USVT, (modiﬁed) softImpute, and SNN. Figure 3: Limited MNAR: recovered ratings distributions under (modiﬁed) USVT, (modiﬁed) softImpute, and SNN. Figure 4: More general MNAR: recovered ratings distributions under (modiﬁed) USVT, (modiﬁed) softImpute, and SNN. We now formally introduce our setup. Consider a signal matrix A = [A [ε] ∈ R unobserved. Let Y = [Y We assume Y itself is partially observed. In particular, we denote D = [D as the missingness mask matrix that indicates which entries of Y are observed. For convenience, we encode our observations into In words, if D let us return to the recommender system example. Here, A represents the expected rating for every user-item pair and P dictates the probability these expected ratings are revealed, both of which are unknown. Y in relation to A then models the inherent randomness in how users rate items; that is, Y can be interpreted as a “noisy” instance of A. Another interpretation of what ε allow users to input integer valued ratings (e.g. integer between 1 to 5 or a binary 0/1). Hence, Y interpreted as a “noisy” discretized observation of A continuous interval [1, 5] or [0, 1]). Observationally, we have access to D and collection of ratings users have supplied to the system while the latter refers to the corresponding realized “noisy” ratings. Finally, we remark that (1) also agrees with standard panel data setups in econometrics, where each observation is assumed to be corrupted by an idiosyncratic shock, which is represented by ε In terms of the type of MNAR data this work considers, we allow for D and Y to be dependent, provided D ⊥⊥ Y |A, where A is latent. In fact, we allow D to be any arbitrary function of A, random or deterministic, subject to suitable observation patterns which we discuss in the forthcoming sections. Notably, our framework also allows the entries in D to be dependent with each other across both rows and columns, and the minimum value of P to be 0, which are important departures from the current matrix completion literature. Under these conditions, we propose an algorithm that provably recovers A from guarantees. Section 2: Related works. We provide an overview of the current literature on matrix completion under the diﬀerent models of missingness proposed by Rubin (1976); Little and Rubin (2019): (i) missing completely at random (MCAR); (ii) missing at random (MAR); (iii) missing not at random (MNAR). We note the usage of the terms MCAR, MAR, and MNAR is inconsistent across the previous works on matrix completion, and so we hope that our literature survey helps give a more comprehensive and uniﬁed overview of the diﬀerent regimes of missingness considered in these works. Section 3: Causal framework for matrix completion. We propose a formal causal framework for matrix completion using the language of potential outcomes, see Neyman (1923); Rubin (1974). We interpret Y as the matrix of potential outcomes and P as the matrix of intervention assignments. Building upon the recent work of Agarwal et al. (2021b), we propose a framework that allows (i) correlation between D and Y , i.e., hidden confounding; (ii) correlation between the entries of D; (iii) the minimum value of P to be 0, i.e., entries of required in the panel data literature, i.e., we consider signiﬁcantly more general missingness patterns. To the best of our knowledge, our framework, and associated algorithm, is the ﬁrst within the MNAR matrix completion literature that allows for conditions (i)-(iv) to simultaneously hold. Additionally, we do not make any parametric or distributional assumptions on P , as is common in previous works on matrix completion. Nevertheless, we establish an identiﬁcation result in Theorem 1, which eﬀectively states that A can be learned from for a variety of applications that can be posed as matrix completion problems with MNAR data. , and a propensity score matrix P = [p] ∈ [0, 1]. All three matrices are entirely latent, i.e., Section 4: An algorithmic solution. We combine the nearest neighbours approach for matrix completion —popularly known as collaborative ﬁltering—with the synthetic controls approach for panel data, to design a novel two-step algorithm, which we call “synthetic nearest neighbors” (SNN), to estimate A from eY . Pleasingly, each step of SNN enjoys a simple closed-form solution. In order to eﬃciently execute SNN in practice, we provide an algorithm to automatically ﬁnd the “neighbors” for any (i, j) pair in a data-driven manner. To do so, we relate this task to the well-known problem of ﬁnding the “maximum” biclique in a bipartite graph. Since SNN is a generalization of the recently proposed synthetic interventions (SI) estimator of Agarwal et al. (2021b), which itself is a generalization of the popular synthetic controls algorithm of Abadie and Gardeazabal (2003); Abadie et al. (2010), this subroutine may be of independent interest to the synthetic controls and panel data literatures. Section 5: Theoretical results. We establish entry-wise ﬁnite-sample consistency and asymptotic normality of SNN, i.e., we provide theoretical guarantees for A new theoretical results, in a max-norm sense, for the literature on matrix completion with MNAR data. As a special case, this also provides novel entry-wise ﬁnite-sample consistency and asymptotic normality results for the traditional matrix completion with MCAR data literature. Collectively, our identiﬁcation, consistency, and asymptotic normality results, coupled with SNN, can be seen as a generalization of the SI framework proposed in Agarwal et al. (2021b). Section 6: Experimental validation. We run comprehensive experiments, both with simulated and realworld data, to test the empirical eﬃcacy of SNN against a collection of state-of-the-art matrix completion algorithms for MNAR data. Some key takeaways are as follows: (i) SNN is robust to the various forms of missingness across all experiments, while the previous methods are relatively sensitive to it. (ii) we ﬁnd the approaches to de-bias estimators for MNAR data are not particularly eﬀective, i.e., their performance is similar to their MCAR analogues; this is in line with the empirical ﬁndings of Ma and Chen (2019). For a matrix X ∈ R norms as kXk v ∈ R kvk a, let [a] = {1, . . . , a}. For index sets I whose rows and columns are indexed by I sub-matrix of X that retains the columns of X but only considers those rows indexed by I analogously. Unless stated otherwise, we index rows with i ∈ [m] and columns with j ∈ [n]. Let f and g be two functions deﬁned on the same space. We say f (n) = O(g(n)) if and only if there exists a positive real number M and a real number n f(n) = Θ(g(n)) if and only if there exists positive real numbers m, M such that for all n ≥ n |f(n)|≤ M |g(n)|; f (n) = o(g(n)) if for any m > 0, there exists n We adopt the standard notations and deﬁnitions for stochastic convergences. As such, we denote as convergences in distribution and probability, respectively. We will also make use of O probabilistic versions of the commonly used deterministic O and o notations. More formally, for any sequence of random vectors X P(kX probability”. Similarly, X every n ≥ n We say a sequence of events E i.e., for any ε > 0, there exists a n sequence of events E min{n variance σ denote the projection matrix onto the subspace spanned by the columns of X. For a vector , let kvkdenote its `-norm. For a random variable v, we deﬁne its sub-gaussian (Orlicz) norm as . Let ◦ denote component-wise multiplication and let ⊗ denote the outer product. For a positive integer k> Ca) < ε for every n ≥ n; equivalently, we say (1/a)Xis “uniformly tight” or “bounded in . Therefore, X= o(1) ⇐⇒ X−→ 0. Additionally, we denote: plim X= a ⇐⇒ X−→ a. , . . . , n} → ∞. We also use N(µ, σ) to denote a normal or Gaussian distribution with mean µ and —we call it standard normal if µ = 0 and σ= 1. Given the vastness of the matrix completion literature, we do not strive to do an exhaustive review of it. Instead, we focus on a few representative works that propose and analyze algorithms designed for the three diﬀerent models of missingness: MCAR, MAR, and MNAR. In Section 2.1, we give an overview of the type of algorithms for matrix completion studied thus far in existing works. In Section 2.2, we discuss the diﬀerent models of missingness considered in the matrix completion literature, and representative algorithms for these various models. Finally, in Section 2.3, we discuss the growing literature exploring the intersection of matrix completion and causal inference; in particular, the panel data literature in econometrics. Algorithms for matrix completion broadly fall into two classes: empirical risk minimization (ERM) methods and matching (i.e., collaborative ﬁltering) methods, with ERM methods being relatively more popular. We give an overview of both class of methods below. Empirical Risk Minimization (ERM) Methods. Empirical risk minimization (ERM) is arguably the de facto approach to recover the underlying signal matrix A given solve the following program: Here, Ω ⊆ [m] × [n], d(·, ·) is an appropriate distance measure (e.g., squared loss), T formation of hyper-parameter. For certain algorithms, they replace the regularizer (i.e., set λ = 0) with a constraint, constraint(·). In order to prove statistical guarantees about these various estimators, structure is placed on A. The assumptions made guide the speciﬁc choices of the above parameters, which then deﬁne the algorithm. For instance, if the singular values of A are assumed to be moderately sparse (i.e., only few are non-zero), then a natural convex regularizer would penalize solutions with large nuclear norm, i.e., regularize(Q) = kQk Cand`es and Tao (2010); Recht (2011). Indeed, choosing Ω = {(i, j) : D entries, T et al. (2010); Hastie et al. (2015). As another example, if A is assumed to be exactly low-rank, then a natural constraint would be the rank of the output matrix. More speciﬁcally, contraint(Q) can be deﬁned as rank(Q) ≤ µ for some pre-speciﬁed integer µ > 0. Then, choosing Ω = [m] × [n], T Gavish and Donoho (2014); Chatterjee (2015). Other notable algorithms within the broader ERM class include maximum-margin matrix factorization (MMMF) Srebro et al. (2004), probabilistic matrix factorization (PMF) Mnih and Salakhutdinov (2008), and SVD++ Koren (2008) to name a few. Broadly speaking, it is commonly assumed that A follows some form of a latent variable model; in particular, A= f (u low-dimensional latent variables associated with row i and column j, respectively. Such latent variable models imply that A is (approximately) low-rank, i.e., A e.g., Xu (2017); Udell and Townsend (2019); Agarwal et al. (2021c). For an excellent overview on standard assumptions made on A and the subsequent guarantees proven for the estimation error, please refer to Davenport and Romberg (2016). When every entry is revealed with uniform probability (i.e., p loss function with all entries revealed (i.e., D is an all ones matrix). When p works have provably and empirically shown that (2) is biased Schnabel et al. (2016); Ma and Chen (2019). As such, these works advocate to de-bias the standard ERM objective by re-weighting each observation eY(e.g. 1(D= 1) ·eY), regularize(·) is a regularization term and λ > 0 is the regularization =eY, and d(·, ·) as the squared loss yields the popular softImpute algorithm of Mazumder , and d(·, ·) as the squared loss yields a suite of spectral based methods Keshavan et al. (2010a,b); , v), where f is a suﬃciently “smooth” latent function (e.g., H¨older continuous), and u, vare inversely by its propensity score p inverse propensity scoring (IPS) or weighting (IPW), see Imbens and Rubin (2015); Little and Rubin (2019). This yields the following adapted program: where bp of (2). Faithful matrix recovery under more general missingness patterns thus requires structure on not only A, but also P and D. We overview standard assumptions on these quantities in Section 2.2. Matching methods. For traditional applications of matrix completion, such as recommendation systems, K nearest neighbour (KNN) methods have been popular (e.g., Goldberg et al. (1992); Linden et al. (2003); Kleinberg and Sandler (2008); Koren and Bell (2015); Lee et al. (2016); Chen et al. (2018)). In KNN, to impute a missing entry (i, j), the ﬁrst step is to select K rows for which the entry in the j-th column is not missing. Of all the rows for which the j-th column is not missing, the K rows are selected such that they are the “closest” to row i. In particular, a hyper-parameter of KNN is the metric that is chosen to deﬁne “closeness” between any two given rows; the most commonly used metric is the mean squared distance between the commonly revealed entries for a given two rows. Once these K “neighbour rows” are chosen, the estimate for the missing entry (i, j) is the average KNN methods is that they do not require imputing missing values by 0. A related literature that shares similarities with KNN is that of synthetic controls Abadie and Gardeazabal (2003); Abadie et al. (2010). A key diﬀerence is that to impute (i, j), uniform weights (i.e., 1/K) are not used for the neighbouring rows; classically in synthetic controls, these weights are constrained to lie within the simplex, i.e., the weights are non-negative and sum to 1 (if the weights are restricted to be 1/K, this is known in the panel data literature as “diﬀerence-in-diﬀerences”). However, as discussed earlier, synthetic controls methods have been designed to handle restricted sparsity patterns naturally arising in the panel data setting. Given the growing literature on synthetic controls, we do a detailed literature review of it in Section 2.3. Below, we utilize the useful taxonomy set in Rubin (1976); Little and Rubin (2019) to discuss the three primary mechanisms that lead to missing data and how previous works ﬁt within these regimes. Missing completely at random (MCAR). MCAR is the most standard model of missingness assumed in the matrix completion literature and is characterized by the following properties: (i) D ⊥⊥ Y ; (ii) D for all (i, j) 6= (a, b); (iii) p independent and identically distributed (i.i.d.) Bernoulli random variable (r.v.) with parameter p ∈ (0, 1]. This implies that the missingness pattern is independent of the values in Y . We note that this condition p> 0 is known in the causal inference literature as “positivity”, see Imbens and Rubin (2015). It follows that the maximum likelihood estimator bp As previously mentioned, given MCAR data, (2) is an unbiased estimator of the ideal loss function where all entries observed. Though MCAR is likely unrealistic outside experimental settings, the MCAR regime remains a popular abstraction in machine learning and statistics to study the inherent trade-oﬀs between the observation probability p, properties of the noise E, and the structure imposed on the signal A, in terms of the estimation error between values in Y (denoted as ?) by 0 and re-weight all non-missing values in Y by 1/bp, where bp is the fraction of observed entries. This can be interpreted as a form of uniform IPW. Other methods such as nuclear norm minimization, alternating least squares, and nearest neighbour methods do not require imputing missing values by 0. However, existing theoretical analysis of these algorithms do still require that E[D that D is an estimate of p. In words, (3) requires learning P prior to carrying out the standard ERM is independent of the all other randomness in the model. Missing at random (MAR). of MAR are as follows. (i) D ⊥⊥ Y | O, where O represents observed covariates about the rows and columns of the matrix (e.g., covariates about users and movies in the context of recommender systems)—concretely, these observed variables, O, often include features or covariates (X and column j, respectively, and observed outcomes all (i, j). Here, the entries of D continue to obey positivity and remain independent Bernoulli r.v.’s. Below, we overview two popular propensity estimation techniques of Schnabel et al. (2016). To aid the following discussion, let X = {(X the set of hidden features. The ﬁrst approach is via Naive Bayes, which assumes that p E[D however, such an approach requires a small sample of MCAR data, see Schnabel et al. (2016). The second estimation strategy is based on logistic regression. Here, it is assumed that there exists model parameters φ such that p as “selection on observables”, see Imbens and Rubin (2015). Typically, it is posited that φ = (ω and E[D the sigmoid function. Some notable works in the MAR literature include Liang et al. (2016); Wang et al. (2018a,b, 2019). Missing not at random (MNAR). MNAR is the most challenging missingness model in matrix completion with a comparatively sparser literature. In its fullest generality, in MNAR the following conditions are allowed: (i) D can depend on Y and other unobserved variables; (ii) D (i, j) 6= (a, b); (iii) min p on observed covariates. The second condition allows the revelation of one outcome to alter the probability of another outcome being revealed. Finally, the third condition can restrict certain outcomes from ever being revealed. Hence, the literature has thus far only considered a limited version of MNAR with conditions cf. Ma and Chen (2019); Bhattacharya and Chatterjee (2021); Yang et al. (2021). In particular, they continue to make the following assumptions: p each entry of D is an independent (not necessarily identically distributed) Bernoulli r.v. with a strictly positive probability of being revealed, which are the assumptions as in MAR. These assumption are what allow the weighted ERM framework of (3) to continue being valid. The methods proposed in Ma and Chen (2019); Bhattacharya and Chatterjee (2021); Yang et al. (2021) work for this limited MNAR setting by positing that P is (approximately) low-rank, and recovers P from D via matrix completion algorithms. This is a generalization of the MAR setting as such an approach circumvents the requirement of meaningful auxiliary features X to conduct propensity score estimation. Additional works within the MNAR literature include Zhu et al. (2019); Sportisse et al. (2020a,b); Wang et al. (2020). As previously mentioned, our work operates under greater generality than the limited MNAR regime thus far considered in the literature. More speciﬁcally, our framework allows D and Y to be dependent, provided D ⊥⊥ Y |A, and for D to be any arbitrary function of A, subject to suitable observation patterns. We also allow for conditions (ii) and (iii) described above to hold, i.e., the entries in D can be highly correlated and the minimum probability of observation can be deterministically set to 0. In Section 3, we will formally introduce our causal framework to rigorously discuss these properties. Summary of matrix completion results. Across the various models of missingness, the key theoretical results for low-rank matrix completion typically have error bounds that scale in the following form (see Davenport and Romberg (2016)): for δ ≥ 0 and where poly(·) denotes polynomial dependence. Here, p (approximate) rank of A. The most studied metric in the literature is the average error across all entries, (1/mn)k |eY]. Under this assumption, the maximum likelihood estimator bpcan be solved using Bayes formula; = E[D|X, H, Y ] = E[D|X,˜X, φ]; within the causal inference literature, this is often known |X,˜X, φ] = σ(hω, Xi + hω,˜Xi + α+ γ), where σ(·) takes a simple parametric form such as bA − Ak, though recent works have begun to analyze stronger metrics such as the maximum average error across all columns, (1/m)k (2021)), and the maximum entry-wise error, k bounds scale with the inverse of poly(p p= 0, i.e., condition (iii) of MNAR above. Finally, we remark that the literature studying the asymptotic properties of k asymptotic analyses of matrix completion estimators under MCAR include Chen et al. (2019); Cai et al. (2020); Bhattacharya and Chatterjee (2021). In Section 3, we propose a causal framework for matrix completion that draws inspiration from the rich and growing literature in econometrics on panel data and matrix completion; some relevant works include Amjad et al. (2018, 2019); Arkhangelsky et al. (2019); Bai and Ng (2019); Fern´andez-Val et al. (2020); Athey et al. (2021); Agarwal et al. (2021c,b); Agarwal and Singh (2021). As is common in matrix completion, these works impose a (approximate) low-rank factor model on the signal matrix (i.e., A), also known as an interactive ﬁxed eﬀects model, to capture structure across units and time (i.e., the rows and columns of the matrix, respectively). Panel data & matrix completion: an overview. As described in Section 1, the sparsity structure considered in these works is one where for each row i, there is a column j j < j they are all missing. The motivation for such a sparsity pattern comes from socio-economic policy making where Y intervention has yet been applied on unit i. The time steps [1, j under control, and time steps [j Yfor j > j unit i had it remained under control during [j literature as “synthetic contorls” Abadie and Gardeazabal (2003); Abadie et al. (2010). The statistical/causal parameter that is most commonly studied is for a “treated” unit i, to estimate average potential outcome of unit i under control during the “post-intervention” period. Most of these works make the additional assumption that each unit either remains under control for the entire time period under consideration, or undergoes an intervention at a time step that is common across all units. Athey et al. (2021) is one notable work that allows for diﬀerent post-intervention periods for each unit. Connections to matrix completion with MNAR data. An attractive quality of this literature is that in some ways it allows for more relaxed conditions on D and P than those considered in the matrix completion with MNAR data literature discussed earlier, see Ma and Chen (2019); Yang et al. (2021); Sportisse et al. (2020b,a); Wang et al. (2019). In particular, the panel data literature allows the entries of D to be correlated, e.g., if D on A. On the other hand, the sparsity pattern considered in the panel data literature is far more restrictive compared to the works on matrix completion with MNAR data—as discussed above, in panel data settings, all columns for a given row are observed till a speciﬁc point, after which they are all missing (i.e., D for all j < j is unrealistic for many important applications for matrix completion, including recommendation systems and sequential decision-making. Further, it is not straightforward to see how the target statistical/causal parameter with this work is to combine the best of both worlds, where we: (i) allow entries of D to be correlated; (ii) allow min p allow for general missingness patterns in the matrix that includes MCAR data as a special case. Further the target parameter we aim to estimate (in expectation) is each entry Y bridging the panel data literature to more classical applications of matrix completion such as recommendation systems, we hope this spurs further investigation into the unexplored connections between these two ﬁelds. Comparison with synthetic interventions. Our proposed framework framework builds upon the recent work of Agarwal et al. (2021b), called synthetic interventions (SI). SI is a causal inference method to do tensor and D= 0 for j ≥ j. That is, all entries for a given row i are observed till some column j, after which represents unit i’s potential outcome at time step j under “control”, i.e., if no socio-economic = 0, then D= 0 for j> j. Further, min pis allowed to be 0 and jis allowed to depend and D= 0 for j ≥ j). Note that this also implies that P is low-rank. Such a sparsity pattern completion with MNAR data, where the dimensions of the order-3 tensor of interest are units, measurements, and interventions. That is, an entry Y the i-th unit, its j-th measurement, under the d-th intervention. Their setup can be made a special case of ours by eﬀectively ﬂattening the tensor into a matrix, where the rows of the induced matrix still correspond to units, but a column is a double index for a measurement and an intervention, i.e., the (i, j, d)-th entry of the tensor corresponds to the (i, (j, d))-th entry of the induced matrix. Given this simple reduction, we generalize the framework, algorithm, and theoretical results in Agarwal et al. (2021b) in the following ways. First, we formally extend the SI framework, to recover matrices under more general missingness patterns than that considered in Agarwal et al. (2021b). Doing so allows us to apply our framework to a wider variety of applications such as recommender systems, while the SI framework was introduced in the context of personalized policy evaluation and synthetic A/B testing. Third, this work establishes point-wise ﬁnitesample consistency and asymptotic normality of our proposed SNN algorithm, which was absent in Agarwal et al. (2021b) with respect to the SI algorithm. Indeed, in the context of the panel data literature, establishing point-wise asymptotic normality for each unit, (intervention, time)-tuple is of independent interest. In this section, we develop a formal causal framework for matrix completion with MNAR data. In Section 3.1, we show how to causally interpret matrix completion with MNAR data using the language of potential outcomes in Section 3.1. We then state and justify our assumptions in Section 3.2, deﬁne our causal estimand in Section 3.3, and present our identiﬁcation result in Section 3.4. We follow the potential outcomes framework of Neyman (1923); Rubin (1974). In particular, we let the r.v. Y revealed. For instance, in the case of recommender systems, Y have given to item j had they rated it. In the context of healthcare for example, Y i’s health metric of interest (e.g. heart rate) had they been given treatment j. Finally, in the case of panel data setting, as discussed in Section 2.3, Y generated, socio-economic indicator), if they would have received the a-th socio-economic policy at time step t; here, (a, t) represents the j-th column. If D That is, in the language of potential outcomes, we can interpret D as the matrix of intervention assignments. Through this perspective, we remark that (1) is an implicit assumption that is known in the causal inference literature as “consistency” or “stable-unit-treatment-value assumption” (SUTVA). As discussed earlier, the fact that Y 6⊥⊥ D (e.g. a user’s preference for a movie can determine whether they rate it) means that the potential outcomes are not independent of the intervention assignments. This dependence is known in the causal inference literature as “confounding”. Lastly, as alluded to earlier, we generalize the standard potential outcomes framework in that a given unit can receive multiple interventions. Traditionally, it is assumed that a unit receives exactly one intervention. However, in applications like movie recommendation systems, a user can “intervene” and rate multiple movies. Lastly, this framework also generalizes panel data settings, as we allow each unit to receive diﬀerent interventions at diﬀerent time steps; as discussed earlier, it is typically assumed that units are in control for a period of time, and then some subset of units receive one intervention for the remaining time steps. Below, we state our causal assumptions and then provide their corresponding interpretations. ∈ R, as deﬁned in Section 1.2, denote the potential outcome associated with each pair (i, j) if it is = 1, then by (1) we see that we actually do observe the (i, j)-th potential outcome, i.e.eY= Y. Assumption 1 (Low-rank factor model). For every pair (i, j), let where u U ∈ R Assumption 2 (Selection on latent factors). We have that for any intervention assignment D, Neighbourhood rows and columns. For the remainder of this work, for a given column j, we refer to NR(j) = {a ∈ [m] : D for a given row i, we refer to NC(i) = {b ∈ [n] : D entries in row i are not missing. See Figure 5b for a visual depiction of NR(j) and NC(i). Assumption 3 (Linear span inclusion). Conditioned on D, for a given pair (i, j) and any I ⊆ NR(j), if |I|≥ µ, then u Interpretation of Assumptions 1 to 3 By the tower law, Assumption 2 implies that E[E|U , V ] = 0. This together with Assumption 1 posits that E[Y |U , V ] is a low-rank matrix with rank r. As discussed in Section 2, this is a standard assumption within the matrix completion literature. Next, we remark that Assumption 2, coupled with Assumption 1, implies that That is, the potential outcomes are mean independent of the intervention assignments, conditioned on the latent row and column factors. This has been termed as “selection on latent factors”, see Agarwal et al. (2021b). Similar conditional independence conditions have been explored in Athey et al. (2021); Kallus et al. (2018). Lastly, given Assumption 1, it follows that Assumption 3 is rather mild. To see this, suppose span({u generally, if the rows of U are randomly sampled sub-gaussian vectors, then span({u set I holds w.h.p., provided µ ≥ r is chosen to be suﬃciently large; see Vershynin (2018) for details. Note that given Assumptions 1 and 2, the deﬁnition of A in (4) is consistent with the deﬁnition of A used in Section 1.2. We are now equipped to deﬁne our target causal estimand, which is A of the paper we focus on a particular pair (i, j), without loss of generality. Note given Assumptions 1 and 2, we can write In words, A column latent vectors (u rating user i would supply for item j, conditioned on the latent features that characterize user i and item j. In panel data settings, letting j = (a, t), A a-th intervention at time step t. , v∈ Rare latent vectors. Equivalently, we say Y = U V+ E, where urefers to the i-th row of , and vrefers to the j-th row of V ∈ R. : ` ∈ I}) = R, i.e., rank(U) = r. Then, Assumption 3 immediately holds as u∈ R. More The following identiﬁcation results establishes that each entry of A can be learned from observable quantities, i.e., from possible. Theorem 1. Let Assumptions 1 to 3 hold. For a given pair (i, j) and I ⊆ NR(j) with |I|≥ µ, suppose β deﬁned with respect to I as in Assumption 3, is known. Then, Interpretation. Theorem 1 states that despite the missingness pattern being MNAR, if Assumptions 1 to 3 hold, and given knowledge of the linear model parameter β, the causal estimand A of quantities that can be estimated from observed data, namely E[ literature as “identiﬁcation”. Note, I is deterministic given D. The key requirement of the missingness pattern D is that I ⊆ NR(j) is suﬃciently large, which is parameterized by µ, i.e., we require µ  r where r is the rank of A. That is, the number of rows for which column j is observed is suﬃciently large. Thus, Theorem 1 suggests that the key quantity that enables the recovery of A algorithm to estimate β, which in turn, allows us to estimate A In this section, we introduce an algorithm, synthetic nearest neighbors (SNN), for matrix completion with MNAR data. Towards this, we introduce helpful notation that will be used for the remainder of this work. Again, without loss of generality, we consider imputing the (i, j)-th entry of the matrix . Notation. Let AR ⊆ NR(j) and AC ⊆ NC(i) denote a subset of rows and columns, respectively, of satisfy D pair (i, j), respectively. Collectively, AR and AC form a fully observed sub-matrix of We refer to this |AR|×|AC| sub-matrix as S of AR, AC, and S. Note, by construction S is such that if entries from row a are present in S, then D similarly, if entries from column b are present in S, then D refers to the rows in column j which correspond to AR. By construction, all the elements in q and x are not missing. See Figure 5d for a visual depiction of q and x. eY . Practically speaking, this means that matrix completion with MNAR data for any pair (i, j) is = 1 for all (a, b) ∈ AR×AC. We refer to AR and AC as the “anchor rows” and “anchor columns” of eY: a ∈ AR]. q ∈ Rrefers to the columns in row i which correspond to AC; similarly, x ∈ R Figure 5: We visually depict the various quantities needed to deﬁne the SNN algorithm. Figure 5a depicts a particular sparsity pattern in our matrixeY with entry (i, j) missing. Figure 5b depicts NR(j) and NC(i). Figure 5c depicts AR, AC, and S. Figure 5d depicts the SNN algorithm with K = 1; for K > 1, we partition the rows in S into K mutually disjoint sets. We now present SNN in Algorithm 1 to impute the (i, j)-th entry. It has K ∈ N and λ hyper-parameters. Algorithm 1 SNN(i, j) Note, for ease of notation, in Algorithm 1 we suppress the dependence on i and j in the deﬁnitions of {(AC which (i, j)-th entry of the matrix we aim to impute. We continue to suppress this dependence for the remainder of the paper. For a visual depiction of the SNN algorithm for K = 1, refer to Figure 5d. For K > 1, we simply re-run the SNN algorithm seperately for the K disjoint subsets {AR the average of the estimates Interpretation. SNN draws inspiration from the popular K Nearest Neighbour (KNN) algorithm, described in Section 2. However, the key assumption underlying KNN is that there do exist K rows that are close to identical to the i-th row, with respect to some pre-deﬁned metric. However, it is not necessary that these K rows exist even for a rank 1 matrix. As a simple example, consider a matrix M ∈ R M= [i, 2i, . . . , ni]. By construction M is rank 1, but for any row, there does not exist any other row that is close to it in a mean squared sense; hence, it has no nearest neighbours. The SNN algorithm overcomes this hurdle by ﬁrst constructing K “synthetic” neighbors of row i from NR(j), where the k-th synthetic neighboring row is formed by a linear combination, deﬁned by that are associated with the K synthetic neighbors of row i. In words, linear weights that best recreates the observed outcomes of row i from the rows in AR from the columns in AC synthetic controls literature—see Section 2.3 for details. To ensure the linear ﬁt is appropriately regularized, a spectral sparsity constraint is imposed on S is known in the literature as principal component regression (PCR) (see Agarwal et al. (2021c,a,b); Agarwal and Singh (2021)). We note that in lieu of requiring that there exist K close neighbouring rows as in KNN, SNN requires that the i-th row lies in the linear span of the rows in AR we require |AR satisfy this linear span inclusion condition, i.e., µ = 1 since M is rank 1. Choosing λ a few here. As is standard within the statistics and ML literatures, the most popular data-driven approach is to use cross-validation. Another common approach is to use a universal thresholding scheme that preserves the singular values above a precomputed threshold (see Gavish and Donoho (2014); Chatterjee (2015)). Finally, a human-in-the-loop approach is to inspect the spectral characteristics of S be the natural “elbow” point that partitions the singular values into those of large and small magnitudes; in such a setting, the large magnitude singular values, which typically correspond to signal, are retained while the small magnitude singular values, which are often induced by noise, are ﬁltered out. See the exposition on choosing the hyper-parameter for PCR in Agarwal et al. (2021c,a,b). 1. Deﬁne S= [eY: (a, b) ∈ AR× AC]P 2. Compute S←bτbu⊗ bv , AR) : k ∈ [K]}, S,bβ, q, and x. That is, these quantities will change depending on . Then, similar to KNN, SNN estimates Aby taking an average of the observed outcomes for column j Proposition 1, we demonstrate that A column j from NC(i) through a simple “transposition” of Algorithm 1. Then, Note the SNN algorithm takes as input {(AC the matrix D, how to ﬁnd these anchor rows and columns, with the additional constraint that the K set of anchor rows {AR AnchorSubMatrix in Algorithm 2 to ﬁnd {(AC we discuss some motivating applications where anchor rows and columns are naturally induced. 4.2.1 Algorithmically Finding Anchor Rows and Columns via Maximum Biclique Search In particular, we reduce our task of ﬁnding anchor rows and columns to a well-known problem in the graph theory literature known as ﬁnding “maximum bicliques”. We brieﬂy explain how to do this simple reduction. We ﬁrst introduce some standard notation from graph theory. Let G = (V where (V between v (or adjacency matrix). In particular, B a biclique, then we denote it as BC ⊂ G, i.e., there is an edge between any pair of nodes (v Now to see how to do the reduction between ﬁnding anchor rows and columns to the maximum biclique problem, recall D ∈ {0, 1} bipartite graph with |V of D, respectively. We deﬁne E as follows, (v B ∈ {0, 1} Given this reduction, we now describe how to practically implement the AnchorSubMatrix algorithm. We assume access to two algorithms: createGraph and maxBiclique. The former, createGraph : B → G, takes as input a bipartite incidence matrix B (or adjacency matrix) and returns a bipartite graph G; we note that the Python package NetworkX is an excellent resource to generate such graphs. The latter, maxBiclique : G → {BC we refer the interested reader to Alexe et al. (2003); Zhang et al. (2014); Lyu et al. (2020); Lu et al. (2020) and references therein for example algorithms. Algorithm 2 AnchorSubMatrix(i, j) Input: createGraph, maxBiclique 1. Find NR(j) and NR(i) 2. Assign B ← [D 3. Generate G ← createGraph(B) 4. Compute {BC 5. Assign BC 6. Output AR ← V , V) are the disjoint vertex sets and E ∈ V× Vis the edge set, i.e., (v, v) ∈ E if there an edge and v. Another way of representing G is via a bipartite incidence matrix B ∈ {0, 1} }, takes as input a bipartite graph G and returns a set of L maximal bicliques {BC}; ← AC for every k, i.e., the anchor columns for each subgroup k are all identically equal to AC. Second, we (randomly) partition AR into K subgroups of equal size and then assign AR AR such that |AR Note, for the purposes of theoretical analysis, we do not necessarily need to have AC all K. In Section 5, we show how the estimation error of SNN scales with |AR theoretical results suggest that we want {(AC we choose |AC Step 5 of Algorithm 2 is doing. 4.2.2 Applications where Anchor Rows and Columns are Naturally Induced In this section, we discuss the typical sparsity pattern in recommender systems and sequential decision-making paradigms, which include panel data settings, reinforcement learning, and sequential A/B testing. We argue why these applications have a sparsity pattern where anchor rows and columns are naturally induced. Recommender systems. As stated earlier, one of the key motivating applications for matrix completion is recommender systems. It has been noted in Ma and Chen (2019) that real-world recommender systems exhibit block-sparse structure; further the sparsity pattern is such that there is dependent missingness (i.e., D Dand zero probability of observing certain entries (i.e., p would induce a sparsity structure as shown in Figure 6a. Within the context of movie recommender systems, a narrative for this missingness pattern is one where users only watch ﬁlms that belong to genre(s) that they like and nothing else. However, in many recommender system applications, there exists a dense sub-matrix which corresponds to items that all users commonly rate—this corresponds to the rightmost columns of Figure 6a. This could occur if say a platform ask new users to indicate a subset of ﬁlms that they enjoy. Indeed, this is a common practice for online platforms such as Hulu, Netﬂix, StitchFix to quickly learn a new user’s preferences in order to provide a “warm-start” to their recommendation engine. Alternatively, many a time there are a small subset of iconic ﬁlms (e.g., Titanic or Star Wars) that a large majority of users have watched. In this example in Figure 6a, all users can be used as anchor rows, and the set of items that are commonly rated across all users can be used as anchor columns. Further, we remark that in this example P is not low-rank, thus violating the key assumption required to learn p (2020); Bhattacharya and Chatterjee (2021). Sequential decision-making. As described earlier, in sequential decision-making, data is collected across units (e.g., individuals, customer types, geographic locations) over time in a sequential manner, where each unit is likely to be observed under a single or small set of interventions out of many at any time period. Many sequential decision-making problems can be phrased this way, including (i) panel data settings in econometrics; (ii) reinforcement learning and its variants (e.g., online learning, contextual bandits); (iii) sequential A/B testing. In (ii), an intervention denotes both the action picked and the observed state for that given time period; meanwhile in (iii), platforms run experiments on diﬀerent customer types in a sequential and/or adaptive manner over time. The induced matrix in these settings has rows index units and columns index time-intervention pairs. It is common in many of these sequential decision-making settings that there is a time period when all units are under the same intervention. This is usually done to collect “control” data about each unit to establish its baseline. For example, in an e-commerce setting, companies Figure 6: In both 6a, 6b, and 6c, observed entries are shown in yellow while unobserved entries are shown in white. Further, commonly estimate the baseline engagement level of a customer to understand the treatment eﬀect of a discount policy; similarly, in clinical trials, pharmaceutical companies collect health metrics of patients to establish the treatment eﬀect a particular therapy has. Further, the assumption that such a control period exists is standard in the synthetic controls literature. For an illustration of the sparsity pattern in the induced matrix with a control period, see Figure 6b. Hence, this “control” period in sequential decision-making can serve as our anchor columns, and all units can serve as anchor rows. Below, we establish the statistical properties of the SNN algorithm. Without loss of generality, we consider a speciﬁc pair (i, j). Recall from our discussion earlier, we suppress dependencies on (i, j), e.g., all anchor rows and columns AR required to establish the theoretical results. In Sections 5.2 and 5.3, we establish ﬁnite-sample consistency and asymptotic normality of the SNN algorithm for a given entry (i, j). In Sections 5.4 and 5.5, we discuss our assumptions and theoretical results, respectively. Notation. For every vector v ∈ R E = {U, V , D}, i.e., the collection of latent factors and the observed missingness pattern. Recall the deﬁnition of S E[S here, r and right singular vectors, respectively, i.e., u and V We state additional assumptions required to establish guarantees for the SNN algorithm. In Section 5.4 we provide interpretations for Assumptions 6 and 7; Assumptions 4 and 5 are relatively standard and selfexplanatory. Below, k is indexed over [K], where recall K is a hyper-parameter of the SNN algorithm. Assumption 4 (Sub-gaussian noise). Conditioned on E, ε with E[ε Assumption 5 (Bounded expected potential outcomes). Conditioned on E, A Assumption 6 (Well-balanced spectra). Conditioned on E and given a pair (i, j) as well as subgroup k, the rnonzero singular values τ that satisfy Assumption 7 (Subspace inclusion). Conditioned on E and given a pair (i, j) as well as subgroup k, where we recall x = rank(E[S| E]). We denote U∈ Rand V∈ Ras the matrices of left , respectively. ] = σ≤ σand kεk≤ Cσfor some constants C > 0 and σ > 0. The following result establishes that the SNN algorithm outputs entry-wise consistent estimates of A, i.e., we establish consistency in k·k the constant within O Theorem 2. Conditioned on E, for a given pair (i, j) and subgroup k ∈ [K], suppose |AR let Assumptions 1 to 7 hold. Further, let K = o(min rank(E[S where assume k Note, Theorem 2 does not require N → ∞ to establish consistency of the SNN estimator. Rather, that |AR we state Corollary 1 to help further interpret our results in Section 5.5. Implication for matrix completion with MCAR data. Proposition 2 below shows that SNN provides uniform entry-wise consistency for matrix completion with MCAR data as a special case if p, the probability of observing an entry, is suﬃciently large. Proposition 2 (SNN for matrix completion with MCAR data). Let the setup of Theorem 2 hold. Further, let m = n = L. Assume each entry (i, j) is revealed with uniform probability p ∈ (0, 1], independent of everything else. Fix any δ > 0. Let with Q = C The following establishes that the entry-wise estimate around the target causal parameter A Theorem 3. For a given pair (i, j) and subgroup k, let the setup of Theorem 2 hold. Deﬁne ]), where λis deﬁned as in Algorithm 1. Then, eβ= Pβis the projection of βonto the subspace spanned by the columns of U. We eβk≥ c, for some absolute constant c ≥ 0. = maxr. Let the setup of Theorem 2 hold. Then, |, |AC| is growing on average (ignoring logarithmic factors and dependence on β, r, σ). However, δ, where Cis a function only of β, rfor k ∈ [K], σ, and log(L). (i) K → ∞; Then conditioned on E, Remark 1. Recall the notation in Corollary 1. Then one can easily verify a suﬃcient property for condition (iii) in Theorem 3 is Practically, this can be interpreted as saying that to ensure valid conﬁdence intervals, the number of synthetic nearest neighbours, i.e., K, we construct in SNN cannot scale too quickly relative to the number of anchor rows and columns, i.e., |AR Interpretation of Assumption 6. Assumption 6 requires that the nonzero singular values of E[S are well-balanced. Such an assumption is quite standard with the econometrics factor model and matrix completion literature. For example, it is analogous to incoherence-style conditions; see Assumption A of Bai and Ng (2019) and the discussion of theoretical results in Agarwal et al. (2021c). It is also closely related to the notion of pervasiveness, see Proposition 3.2 of Fan et al. (2018). Indeed, the assumption that there is a gap between the top few singular values of a matrix of interest, and the remaining singular values has been widely adopted in the econometrics literature of large dimensional factor analysis dating back to Chamberlain and Rothschild (1983). Crucially though, these works within econometrics (e.g. Bai and Ng (2019), Fan et al. (2018), Chamberlain and Rothschild (1983)) aim to accurately estimate the factors themselves, which require making additional assumptions about the spectra of the matrix of interest to ensure these factors are uniquely identiﬁable. Instead we simply require that these low-rank factors exist, but do not explicitly require accurately estimating them. Assumption 6 has also been shown to hold with high-probability for the canonical probabilistic generating process used to analyze probabilistic principal component analysis in Bishop (1999) and Tipping and Bishop (1999); here, the observations are assumed to be a high-dimensional embedding of a low-rank matrix with independent sub-Gaussian entries (see Proposition 4.2 of Agarwal et al. (2021c)). Within the matrix/tensor completion literature, for an overview of where the well-balanced spectra assumption is utilized, see Cai et al. (2021) and references therein. Practically speaking, Assumption 6 can be |, |AR|→ ∞ for each k; keβklog(|AC||AR|) = o(min{|AC|, |AR|}) for each k; empirically validated by plotting the spectrum of S point in the singular spectrum of S large and approximately equal magnitude, and the remaining singular values are signiﬁcantly smaller, then Assumption 6 is likely to hold. For further discussion of this empirical robustness check, please refer to the related discussion in Agarwal et al. (2021b). Interpretation of Assumption 7. Recall from Algorithm 1 that we learn the model the j-th column of the rows AR qand S such generalization requires making distributional assumptions about the training data (i.e., S testing data (i.e., x want to make such an assumption as it is unrealistic in setting such as recommendation systems, e.g., the ratings users give diﬀerent movies is likely to be neither identically nor independently distributed. Indeed, by conditioning on E, we are implicitly conditioning on U and V , which requires our analysis to be instance dependent, i.e., has to hold for the speciﬁc sampling of the latent factors U and V . To circumvent making any distribution assumptions, we make the natural assumption that in expectation, x span of S to 0, then no meaningful model et al. (2021a,b); Agarwal and Singh (2021). In particular, in Agarwal et al. (2021b) the authors provide a data-driven hypothesis test to verify when such a condition holds. To ease the discussion of the interpretation of the results, we will ignore dependence on logarithmic factors, and β Sample complexity. Note that even if D observe A is corrupted by noise and we only get a single sample of it. Remarkably, despite having access to (at most) a single noisy sample of A normal around A Hypothetically, if we get K independent noisy samples of A likelihood estimator would be the empirical mean, mean would concentrate around A obtain an additive error of O(δ), we would need K = Ω(δ Now in comparison to the hypothetical scenario above where we have access to K independent samples, Corollary 1 eﬀectively establishes that with access to at most N mator scales as O(max(N 1. This implies for any δ > 0, A and K = Ω(δ additional cost of N rate even though we either do not observe a sample of A instantiation of it in (across diﬀerent entries). Further, in the hypothetical scenario where we get K independent noisy copies for each (i, j), if we wanted to estimate A In contrast, for SNN, if we assume that for all (i, j), we can use the same set of anchor rows and columns, i.e., {|AC of observations we need to recover each A .bAis then estimated by applying the modelbβon the outcomes in x(i.e., the entries in , generalize well to accurately estimate Ausing hx,bβi. Normally, in statistical learning, . Such a condition is necessary as well for generalization, e.g., if every entry of E[S| E] is equal , r, σ. , rather we only observe Y, where Y= A+ε. That is, even if D= 1, our observation of A to within error O(δ) for all (i, j), this would require m ×n ×K observations, with K = Ω(δ). |, |AR|}can be chosen to be the same for all (i, j), then one can easily verify that the number with N = Ω(δ this holds. Thus, for any ﬁxed δ > 0, we can recover every entry A access to only O(m + n) observations, rather than O(m × n) observations as would be naively required. Connections to causal transportability, transfer learning, learning with distribution shift. We note that this problem of generalizing well without making an i.i.d assumption is known by a variety of terms across many ﬁelds of study; these include “causal transportability”, “transfer learning”, “learning with distribution shift”. Given that subspace inclusion, i.e., Assumption 7 holds, we show that generalization is possible without making any distributional assumptions about the underlying signal matrix A. Indeed, our theoretical results in Theorem 2 and 3 can be interpreted as point-wise out-of-sample generalization error bounds, which are distribution free (i.e., instance dependent). This might be of independent interest. The objective of this section is to compare the imputation accuracy of SNN against the state-of-the-art matrix completion algorithms for MNAR data. We describe these algorithms in Section 6.1. We do two case studies. In Section 6.2, we apply these various algorithms in the setting of recommender systems with diﬀerent missingness patterns. In Section 6.3, we do the same but using data from a classic panel data case study in the econometrics literature called “California Prop 99” Abadie et al. (2010). In particular, we compare two types of algorithms for matrix completion against SNN; we choose these benchmarks to be in line with those considered in Ma and Chen (2019). The ﬁrst group of algorithms does not account for entries being MNAR; these include PMF (Mnih and Salakhutdinov (2008)), SVD (Funk (2006)), SVD++ (Koren (2008)), softImpute (Hastie et al. (2015)), and KNN (Lee et al. (2016)); we remark that the algorithm proposed in Athey et al. (2021) is similar to softImpute with the addition of separate ﬁxed eﬀects terms. In particular, both the algorithm design and associated analysis of these algorithms is for MCAR data. In contrast, the second group does account for the limited MNAR setting as described in Section 2.2; these include MaxNorm (Cai and Zhou (2016)), ExpoMF (Liang et al. (2016)), and WTN (Srebro and Salakhutdinov (2010)). With the exception of ExpoMF, we further consider IPW-variants of the other benchmark algorithms, i.e., for each algorithm, we ﬁrst de-bias the loss function given in (3) via propensity scores. We do not do so with ExpoMF as their algorithm does not lend itself to be de-based via propensity scores in a straightforward manner (also see Ma and Chen (2019)).The propensity scores are estimated in two ways, which are in line with the MAR and limited MNAR setting described in Section 2.2. (i) MAR setting: We provide meaningful additional covariates (X completion algorithm is de-biased in this way, we add LR in front of it, e.g., LR-PMF means the PMF algorithm Figure 7: Sparsity pattern for which minimum number of observations required for entry-wise recovery. is used to estimate We do not provide additional covariates and directly estimate bp done using the 1bitMC algorithm in Davenport et al. (2014) and algorithms de-biased in this manner has a preﬁx of 1bitMC added to them.; this approach to de-bias MNAR data is in line with what is proposed in Ma and Chen (2019); Yang et al. (2021); Bhattacharya and Chatterjee (2021). We consider two error metrics, root mean-squared-error (RMSE) and mean-absolute-error (MAE). For all benchmark algorithms, we use 5-fold cross validation to tune their hyper-parameters through grid search for every error metric, i.e., for each benchmark algorithm, we ﬁnd its best performing hyper-parameters with respect to RMSE and MAE on the validation set and report the error metric-speciﬁc hyper-parameters on the test set for each error metric. For SNN, we choose K = 1 and λ we do not tune the hyper-parameters of SNN nor do we optimize it for each error metric. We emphasize that only the algorithms with the LR preﬁx use the additional row and column covariates X We begin with recommendation systems, which is arguably the canonical matrix completion application. Through the recommendation systems setting, we present two MNAR missingness patterns—one obeys the standard assumptions on MNAR in the literature (which we often refer to as limited MNAR) while the other considers a more general MNAR setting. To better understand the eﬀect of the underlying mechanism which leads to missingness on each algorithm’s ability to perform imputation, we consider the “noiseless” case, i.e., setting in Section 6.3. 6.2.1 Limited MNAR Setting: Positivity & Independent Missingness In our ﬁrst illustration, our observation pattern reﬂects the self-selection bias phenomena where most users tend to provide ratings if they particularly liked or disliked an item. However, they are much less inclined to provide a rating for an item that they are lukewarm about. Our simulated setup also consists of “core users” and “core movies”. We use core users to represent movie fanatics or critics, for instance, who provide explicit feedback for a signiﬁcant number of ﬁlms. In the setting of movie recommendations, we use core items to represent iconic movies such as Star Wars or Titanic that have inﬂuenced future ﬁlms and popular culture, and are largely viewed by the general audience. These can also represent the subset of items that online platforms such as Hulu, Netﬂix, Stichﬁx display to new users when prompting for their preferences. Experimental setup. We consider m = 80 users and n = 80 movies. We choose the dimension of the latent space as r = 5. We generate the latent user matrix U ∈ R users and construct U we construct U a Dirichlet distribution, which ensures that the new factors lie in the same intervals as the factors in U doing so, every row of U combination of that of core users U core users. We then deﬁne U = [U construct V = [V of latent factors associated with core movies and standard movies, respectively; here, we choose n We form A = U V low-rank matrix. Finally, we generate user and movie covariates matrices X = U Q the entries in Q normalize the columns in Q Next, we describe our generative model for the propensity matrix P ∈ R we denote C if D= 1 andeY= ? otherwise. We study the eﬀect of additional noise εin the panel data i ≤ m as the subset of standard users and core movies, and C of standard users and standard movies. These will represent our four cohorts of interest. Next, for some t ∈ (1, 5), κ In our setting, we choose our threshold t = 2.3. Here, α α= 1 is MCAR while α α= 0.35 for (i, j) ∈ C the expected number of revealed ratings within the cohort is equal to some value. We choose the expected number of observations within C as 5%. This sampling process ensures the two key assumptions in the limited MNAR setting of the entries of D being independent and p pattern under this missingness mechanism. Results. In the following simulations, we obey the generative process above. In particular, we sample A and P once, as well as X and the sparsity pattern, i.e., we observe 10 independent realizations of D. We report the average RMSEs and MAEs, as well as their respective standard deviations, over the 10 experimental runs in Table 1. We ﬁnd that with respect to MAE, SNN achieves the best result along with MaxNorm (and its variants); with respect to RMSE, SNN is a close second with SVD++ (and its variants), after MaxNorm (and its variants). Although positivity and independence between entries in D are upheld, we remark that debiasing via 1bitMC and LRdo not always yield stronger results, e.g., see PMF and softImpute. 6.2.2 A More General MNAR Setting: Violating Positivity & Independence Assumptions In this simulation, we violate two key assumptions in the current literature on MNAR data: (i) positivity and (ii) independence between the entries in D. Towards this, we continue the notion of core movies, for which all users provide ratings. For the remaining movies, users only provide ratings if a movie belong to their favorite genre. This deterministically sets every entry in P (and thus D) to either 0 or 1, and correlates the entries in D, which yields a sparsity pattern similar to that shown in Figure 6a. See Figure 1c for a visual depiction of empirical sparsity pattern under this missingness mechanism. Experimental setup. In particular, we consider m = 80 users and n = 80 items. We choose the dimension of the latent space as r = 5. We generate the latent user matrix U ∈ R standard normal distribution. To generate the latent item matrix V ∈ R items (to be deﬁned in greater detail below) and construct V standard normal. Next, we construct V sampled i.i.d. from a Dirichlet distribution. In doing so, every row of V V, i.e., every item can be expressed as a weighted combination of core items. We then deﬁne V = [V such that the ﬁrst n values to lie within the interval [1, 5]. Finally, we generate user and item feature matrices X = U Q andX = V Q normal N(0, 1); additionally, we normalize the columns in Q higher dimensional covariates X and denoted by the preﬁx LR, which use this additional information to estimate the propensities. To describe the generating process for the observation pattern D, we begin by providing an interpretation of the above quantities. First, we interpret r as the number of latent genres. In turn, the (i, k)-th entry in U can be interpreted as user i’s preference for genre k; similarly, the (j, k)-th entry in V can be interpreted as the level to which item j is composed of genre k. We consider the setting where all users provide ratings for all core items, i.e., D in D, we posit that every user will only rate items from their favorite genre. More speciﬁcally, given the above interpretation, we deﬁne user i’s favorite genre k , j > n} as the subset of core users and standard movies, C:= {(i, j) : i > m, j ≤ n} > 0, and α∈ (0, 1], ˜∈ R, where the entries in Q∈ Rand Q∈ Rsampled i.i.d. from a standard an item j as belonging to genre k item j > n standard operating assumptions within the current MNAR literature as entries in D are deterministically set to 0 (i.e., the minimum element in P is 0), and are dependent on one another. Results. In the following simulations, we obey the generative process above. In particular, we sample V and X once, and perform 10 experimental repeats where the only randomization lies in the re-sampling of U; this is done to model new users coming into the system with the movies ﬁxed. We report the average RMSEs and MAEs, as well as their respective standard deviations, over the 10 experimental runs in Table 1. We ﬁnd that SNN achieves the best RMSE and MAE, with 1bitMC-MaxNorm as a close second with respect to RMSE and MAE. As with the limited MNAR setting experiment, we ﬁnd that de-biasing does not always improve results. This is reasonable given that our generative process violates the typical assumptions underlying propensity estimation methods. The relative improvement of SNN shows its robustness to the general MNAR setting, where entry-wise positivity and independence of D are violated. The fact that KNN performs relatively poorly indicates that matching via linear weights is indeed more expressive than matching with uniforms weights as done in KNN. WE also note that the various state-of-the-art algorithms are still relatively robust to the general MNAR setting. This may warrant further investigation into the potential gap between theory and practice on the robustness of these methods to diﬀerent missingness patterns. We now compare SNN against the same benchmark matrix completion algorithms using a classic case study of California smoking data of Abadie et al. (2010), which has been widely utilized within the econometrics literature. We do so as this setting has a MNAR sparsity pattern which is quite distinct from what is seen in recommendation systems. We now give a brief overview of the case study. In 1988, California introduced the ﬁrst modern-time large-scale anti-tobacco legislation in the United States (Proposition 99). There was interest in estimating the eﬀect of this legislation on tobacco sales in California. Towards this, per-capita Table 1: RMSEs and MAEs of matrix completion methods on a recommender system experiment and a panel data experiment. cigarette sales data was collected across 39 U.S. states from 1970 to 2000. Among the 39 states, there was one “treated” state, California, which implemented the legislation; the remaining 38 states were chosen as “control” states as they neither instituted a tobacco control program nor raised cigarette sales taxes by 50 cents or more. These other 38 control states were then used to build a “synthetic California”, i.e., a synthetic trajectory of cigarette sales in California if it had not introduced any tobacco legislation. Experimental setup. We consider the time horizon of n = 31 years and restrict our focus to the m = 38 control units in the original dataset. This data is encoded into a 38 × 31 matrix, Y , where the entry Y represents the potential outcome of per-capita cigarette sales (in packs) for state i in year j under control, i.e., without any intervention in place. To generate MNAR data, we artiﬁcially introduce interventions to a subset of states in 1989, where the probability a state adopts an intervention (e.g., tobacco control program) depends on their change in cigarette sales pre- and post-1989. More speciﬁcally, we consider the following treatment adoption protocol: First, we cluster states into three categories—mild, moderate, or severe—based on their change in average cigarette sales during 1989-2000 compared to that during 1970-1988; we note that in this context, a negative change means that the cigarette sales in the post-intervention period are lower than that in the pre-intervention period. As such, we deﬁne (i) mild states as those whose change is at least one standard deviation above the average change across all states; (ii) severe states as those whose change is at least one standard deviation below the average change across all states; (iii) and moderate states as the remaining states whose change is within one standard deviation. We then designate the probability of intervention for mild, moderate, and severe states as 10%, 30%, and 50%, respectively. In words, this setup reﬂects the scenario in which a state is more likely to adopt an intervention if their average sales in the post-intervention period is relatively closer to their pre-intervention sales compared to that of their peer states. In the language of causal inference, this is exactly confounding, i.e., there is a correlation between the treatment assignment and the eventual outcome. For an example of a mild, moderate, and severe state, please see Figure 8. Additionally, we remark that once an intervention is adopted, all sales under control during the post-intervention period are, by deﬁnition, unobserved, i.e., pattern shown in Figure 6b. Finally, to employ logistic regression, i.e., LR to de-bias the estimates, we use state covariate data from Abadie et al. (2010), which include average retail price of cigarettes, per capita state personal income (logged), the percentage of the population age 15-24, and per capita beer consumption. We note that SNN does not use this auxiliary data. Results. Using the above setup, we apply the various matrix completion methods to impute the missing counterfactual cigarette sales associated with the artiﬁcial intervention states during the post-intervention period. We report the average root mean-squared-errors (RMSEs) and mean absolute errors (MAEs), as well as their respective standard deviations, over 10 experimental runs in Table 1. As the table shows, SNN signiﬁcantly outperforms all baseline algorithms under both error metrics. The only exception is KNN, which performs similarly to SNN; this is interesting as KNN is in essence, the diﬀerence-in-diﬀerences estimator, a standard method within the panel data econometrics literature. Further, SVD++ and MaxNorm (and its variants), which performed strongly in the recommendation systems example, now incur a signiﬁcant error. We display a few representative results in Figure 8. Collectively across all three studies, we ﬁnd that SNN is Figure 8: True observations are represented in black, SNN estimates shown in blue, KNN estimates shown in orange, SVD estimates robust under varying missingness mechanisms.