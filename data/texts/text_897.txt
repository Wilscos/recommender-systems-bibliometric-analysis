Eun Som Jeon, Student Member, IEEE, Anirudh Som, Ankita Shukla, Kristina Hasanaj, Matthew P. Buman, Abstract—Deep neural networks are parametrized by several thousands or millions of parameters, and have shown tremendous success in many classiﬁcation problems. However, the large number of parameters makes it difﬁcult to integrate these models into edge devices such as smartphones and wearable devices. To address this problem, knowledge distillation (KD) has been widely employed, that uses a pre-trained high capacity network to train a much smaller network, suitable for edge devices. In this paper, for the ﬁrst time, we study the applicability and challenges of using KD for time-series data for wearable devices. Successful application of KD requires speciﬁc choices of data augmentation methods during training. However, it is not yet known if there exists a coherent strategy for choosing an augmentation approach during KD. In this paper, we report the results of a detailed study that compares and contrasts various common choices and some hybrid data augmentation strategies in KD based human activity analysis. Research in this area is often limited as there are not many comprehensive databases available in the public domain from wearable devices. Our study considers databases from small scale publicly available to one derived from a large scale interventional study into human activity and sedentary behavior. We ﬁnd that the choice of data augmentation techniques during KD have a variable level of impact on end performance, and ﬁnd that the optimal network choice as well as data augmentation strategies are speciﬁc to a dataset at hand. However, we also conclude with a general set of recommendations that can provide a strong baseline performance across databases. Index Terms—Knowledge Distillation, Data Augmentation, time-series, Wearable Sensor Data. EEP LEARNING has achieved state-of-the-art performance in various ﬁelds, including computer vision [1], [2], [3], [4], speech recognition [5], [6], and wearable sensors analysis [7], [8]. In general, stacking more layers or increasing the number of learnable parameters causes deep networks to exhibit improved performance [2], [3], [4], [8], [9], [10]. However, this causes the model to become large resulting in additional need for compute and power resources, for training, storage, and deployment. These challenges can hinder the ability to incorporate such models into edge devices. Many studies have explored techniques such as network pruning [11], [12], quantization [12], [13], low-rank factorization [14], and Knowledge Distillation (KD) [15] to compress deep learning models. At the cost of lower classiﬁcation accuracy, some of these methods help to make the deep learning model smaller and increase the speed of inference on the edge devices. Posttraining or ﬁne-tuning strategies can be applied to recover the lost classiﬁcation performance [12], [13]. On the contrary, KD does not require ﬁne-tuning nor is subjected to any posttraining processes. KD is a simple and popular technique that is used to develop smaller and efﬁcient models by distilling the learnt knowledge/weights from a larger and more complex model. The smaller and larger models are referred to as student and teacher models, respectively. KD allows the student model to retain the classiﬁcation performance of the larger teacher model. Recently, different variants of KD have been proposed [16], [17]. These variations rely on different choices of network architectures, teacher models, and various features used to train the student model. Alongside, teacher models trained by early stopping for KD (ESKD) have been explored, which have helped improving the efﬁcacy of KD [18]. However, to the best of our knowledge, there is no previous study that explores the effects, challenges, and beneﬁts of KD for human activity recognition using wearable sensor data. In this paper, we ﬁrstly study KD for human activity recognition from time-series data collected from wearable sensors. Secondly, we also evaluate the role of data augmentation techniques in KD. This is evaluated by using several time domain data augmentation strategies for training as well as for testing phase. The key highlights and ﬁndings from our study are summarized below: time-series data and conclude that EKSD performs better as compared to other techniques. of teacher and student networks. We corroborate results from previous studies that suggest that the performance of a higher capacity teacher model is not necessarily better. both teacher and student models. We do this to identify which combination of augmentation methods give the most beneﬁt in terms of classiﬁcation performance. task and is conducted on a small scale publicly available dataset as well as a large scale dataset. This ensures the observations are reliable irrespective of the dataset sizes. The rest of the paper is organized as follows. In Section II, we provide a brief overview of KD techniques as well as data augmentation strategies. In Section III, we present which augmentation methods are used and its effects on time-series data. In Section IV, we describe our experimental results and analysis. In Section V, we discuss our ﬁndings and conclusions. 1) Knowledge Distillation: The goal of KD is to supervise a small student network by a large teacher network, such that the student network achieves comparable or improved performance over teacher model. This idea was ﬁrstly explored by Buciluˇa et al. [19] followed by several developments like Hinton et al. [15]. The main idea of KD is to use the soft labels which are outputs, soft probabilities, of a trained teacher network and contain more information than just a class label, which is illustrated in Fig. 1. For instance, if two classes have high probabilities for a data, the data has to lie close to a decision boundary between these two classes. Therefore, mimicking these probabilities helps student models to get knowledge of teachers that have been trained with labeled data (hard labels) alone. During training, the loss function L for a student network is deﬁned as: where Lis the standard cross entropy loss, Lis KD loss, and λ is hyper-parameter; 0 < λ < 1. In supervised learning, the error between the output of the softmax layer of a student network and ground-truth label is penalized by the cross-entropy loss: where H(·) denotes a cross entropy loss function, ais logits of a student (inputs to the ﬁnal softmax), and yis a ground truth label. In the process of KD, instead of using peaky probability distributions which may produce less accurate results, Hinton et al. [15] proposed to use probabilities with temperature scaling, i.e., output of a teacher network given by f= softmax(a/τ) and a student f= softmax(a/τ) are softened by hyperparameter τ, where τ > 1. The teacher and student try to match these probabilities by a KL-divergence loss: where KL(·) is the KL-divergence loss function. There has been lots of approaches to improve the performance of distillation. Previous methods focus on adding more losses on intermediate layers of a student network to be closer to a teacher [20], [21]. Averaging consecutive student models tends to produce better performance of students [22]. By implementing KD repetitively, the performance of KD is improved, which is called sequential knowledge distillation [23]. Recently, learning procedures for improved efﬁcacy of KD has been presented. Goldblum et al. [24] suggested adversarially robust distillation (ARD) loss function by minimizing dependencies between output features of a teacher. The method used perturbed data as adversarial data to train the student network. Interestingly, ARD students even show higher accuracy than their teacher. We adopt augmentation methods to create data which is similar to adversarial data of ARD. Based on ARD, the effect of using adversarial data for KD can be veriﬁed, however, which data augmentation is useful for training KD is not well explored. Unlike ARD, to ﬁgure out the role of augmentation methods for KD and which method improves the performance of KD, we use augmentation methods generating different kinds of transformed data for teachers and students. In detail, by adopting augmentation methods, we can generate various combinations of teachers and students which are trained with the same or different augmentation method. It provides to understand which transformation and combinations can improve the performance of KD. We explain the augmentation method for KD in Section III with details. Additionally, KD tends to show an efﬁcacy with transferring information from early stopped model of a teacher, where training strategy is called ESKD [18]. Early stopped teachers produce better students than the standard knowledge distillation (Full KD) using fully-trained teachers. Cho et al. [18] presented the efﬁcacy of ESKD with image datasets. We implement ESKD on time-series data and investigate its efﬁcacy on training with data transformed by various augmentation methods. We explain more details in Section III and discuss the efﬁciency of ESKD in later sections. In general, many studies focus on the structure of networks and adding loss functions to existing framework of KD [25], [26]. However, the performance of most approaches depends on the capacity of student models. Also, availability of sufﬁcient training data for teacher and student models can affect to the ﬁnal result. In this regard, the factors that have an affect on the distillation process need to be systematically explored, especially on time-series data from wearable sensors. 2) Data Augmentation: Data augmentation methods have been used to boost the generalizability of models and avoid over-ﬁtting. They have been used in many applications such as time-series forecasting [27], anomaly detection [28], classiﬁcation [8], [29], and so on. There are many data augmentation approaches for time-series data, which can be broadly grouped under two categories [30]. The ﬁrst category consists of transformations in time, frequency, and time-frequency domains [30], [31]. The second group consists of more advanced methods like decomposition [32], model-based [33], and learningbased methods [34], [30]. Time-domain augmentation methods are straightforward and popular. These approaches directly manipulate the original input time-series data. For example, the original data is transformed directly by injecting Gaussian noise or other perturbations such as step-like trend and spikes. Window cropping or sloping also has been used in time domain transformation, which is similar to computer vision method of cropping samples [35]. Other transformations include window warping that compresses or extends a randomly chosen time range and ﬂipping the signal in time-domain. Additionally, one can use blurring and perturbations in the data points, especially for anomaly detection applications [36]. A few approaches have focused on data augmentation in the frequency domain. Gao et al. [36] proposed perturbations for data augmentation in frequency domain, which improves the performance of anomaly detection by convolutional neural networks. The performance of classiﬁcation was found to be improved by amplitude adjusted Fourier transform and iterated amplitude adjusted Fourier transform which are transformation methods in frequency domain [37]. Time-frequency augmentation methods have also been recenlty investigated. SpecAugment is a Fourier-transform based method that transforms in MelFrequency for speech time-series data [31]. The method was found to improve the performance of speech recognition. In [38], a short Fourier transform is proposed to generate a spectrogram for classiﬁcation by LSTM neural network. Decomposition-based, model-based, and learning-based methods are used as advanced data augmentation methods. For decomposition, time-series data are disintegrated to create new data [32]. Kegel et al. ﬁrstly decomposes the time-series based on trend, seasonality, and residual. Then, ﬁnally new timeseries data are generated with a deterministic and a stochastic component. Bootstrapping methods on the decomposed residuals for generating augmented data was found to help the performance of a forecasting model [39]. Model-based approaches are related to modeling the dynamics, using statistical model [33], mixture models [40], and so on. In [33], modelbased method were used to address class imbalance for timeseries classiﬁcation. Learning-based methods are implemented with learning frameworks such as generative adversarial nets (GAN) [34] and reinforcement learning [41]. These methods generate augmented data by pre-trained models and aim to create realistic synthetic data [34], [41]. Finally, augmentation methods can be combined together and applied simultaneously to the data. Combining augmentation methods in time-domain helps to improve performance in classiﬁcation [42]. However, combining various augmentation methods may results in a large amount of augmented data, increasing training-time, and may not always improve the performance [30]. We would like to investigate strategies for training KD with time-series data and identify augmentation methods for teachers and students that can provide better performance. The strategies include two scenarios on KD. Firstly, we apply augmentation methods only when a student model is trained based on KD with a teacher model trained by the original data. Secondly, augmentation methods are applied not only to students, but also to teacher. When a teacher model is trained from scratch, an augmentation method is used, where the model is to be used as a pre-trained model for distillation. And, when a student is trained on KD, the same/different augmentation methods are used. The set of augmentation approaches on KD are illustrated in Fig. 1, and described in further detail later in this section. Also, we explore the effects of ESKD on time-series data – ESKD uses a teacher which is obtained in the early training process. ESKD generates better students rather than using the fully-trained teachers from Full KD [18]. The strategy is derived from the fact that the accuracy is improved initially. However, the accuracy towards the end of training begins to decrease, which is lower than the earlier accuracy. We adopt early stopped teachers with augmentation methods for our experiments presented in Section IV. In order to see effects of augmentation on distillation, we adopt time-domain augmentation methods which are removal, adding noise with Gaussian noise, and shifting. The original pattern, length of the window, and periodical points can be preserved by this transformation. We use transformation methods in time domain so that we can analyze the results from each method, and combinations, more easily. These methods also have been used popularly for training deep learning networks [30]. We apply combinations of augmentation methods, combined with removal and shifting, and with all methods to a data to see the relationships between each property of datasets for teachers and students of KD. An example of different transformation used for data augmentation is shown in Fig. 2. We describe each of the transforms below: quential samples. The values of chosen samples to be erased are transformed to the amplitude of the ﬁrst point. For example, we assume that n samples are chosen as (X, X, · · · , X) and their amplitudes are (A, A, · · · , A) to be erased. Ais the amplitude of the ﬁrst sample Xand is assigned to (A, A, · · · , A). That is, values (A, A, · · · , A) are mapped to (A, A, · · · , A). The ﬁrst point and the number of samples to be erased are chosen randomly. The result of removal is shown in Fig. 2 with a green dashed circle. with mean 0 and a random standard deviation. The result of adding noise is shown in Fig. 2 with yellow dashed circles. such as values of peak points and periodic patterns in the signal, we adopt index shifting and rolling methods to the data for generating new patterns, which means the 100% shifted signal from the original signal by this augmentation corresponds to the original one. For example, assuming the total number of samples are 50 and 10 time-steps (20% of the total number of samples) are chosen to be shifted. The values for amplitude of samples (X, X, · · · , X, · · · X) (X, X, · · · , X, · · · , X, X time-steps to be shifted is chosen randomly. Shifting is shown in Fig. 2 with green dashed arrows. data. shifting simultaneously to the data. In this section, we describe datasets, settings, ablations, and results of our experiments. We perform experiments on two datasets: GENEActiv [43] and PAMAP2 [44], both of which are wearable sensors based activity datasets. We evaluate multiple teachers and students of various capacities for KD with data augmentation methods. 1) GENEactiv: GENEactiv dataset [43] consists of 29 activities over 150 subjects. The dataset was collected with a GENEactiv sensor which is a light-weight, waterproof, and wrist-worn tri-axial accelerometer. The sampling frequency of the sensors is 100Hz. In our experiments, we used 14 activities which can be categorized as daily activities such as walking, sitting, standing, driving, and so on. Each class has over approximately 900 data samples and the distribution and details for activities are illustrated in Fig. 3. We split the dataset for training and testing with no overlap in subjects. The number of subjects for training and testing are over 130 and 43, respectively. A window size for a sliding window is 500 time-steps or 5 seconds and the process for temporal windows is full-non-overlapping sliding windows. The number of windows for training is approximately 16000 and testing is 6000. 2) PAMAP2: PAMAP2 dataset [44] consists of 18 physical activities for 9 subjects. The 18 activities are categorized as 12 daily activities and 6 optional activities. The dataset was obtained by measurements of heart rate, temperature, accelerometers, gyroscopes, and magnetometers. The sensors were placed on hands, chest, and ankles of the subject. The total number of dimensions in the time-series is 54 and the sampling frequency is 100Hz. To compare with previous methods, in experiments on this dataset, we used leaveone-subject-out combination for validation comparing the i subject with the ifold. The input data is in the form of timeseries from 40 channels of 4 IMUs and 12 daily activities. To compare with previous methods, the recordings of 4 IMUs are downsampled to 33.3Hz. The 12 action classes are: lying, sitting, standing, walking, running, cycling, nordic walking, ascending stairs, descending stairs, vacuum cleaning, ironing, and rope jumping. Each class and subject are described in Table I. There is missing data for some subjects and the distribution of the dataset is imbalanced. A window size for a sliding window is 100 time-steps or 3 seconds and step size is 22 time-steps or 660 ms for segmenting the sequences, which allows semi-non-overlapping sliding windows with 78% overlapping [44]. For experiments on GENEactiv, we run 200 epochs for each model using SGD with momentum 0.9 and the initial learning rate lr = 0.1. The lr drops by 0.5 after 10 epochs and drops down by 0.1 every [] where t is the total number of epochs. For experiments on PAMAP2, we run 180 epochs for each model using SGD with momentum 0.9 and the initial learning rate lr = 0.05. The lr drops down by 0.2 after 10 epochs and drops down 0.1 every [] where t is the total number of epochs. The results are averaged over 3 runs for both the datasets. To improve the performance, feature engineering [45], [46], feature selection, and reducing confusion by combining classes [47] can be applied additionally. However, to focus on the effects of KD which is based on featurelearning [46], feature engineering/selection methods to boost performance are not applied and all classes as speciﬁed in Section IV-A are used in the following experiments. 1) Training from scratch to ﬁnd a Teacher: To ﬁnd a teacher for KD, we conducted experiments with training from scratch based on two different network architectures: ResNet [1] and WideResNet [48]. These networks have been popularly used in various state-of-the-art studies for KD [16], [17], [24], [18]. We modiﬁed and compared the structure having the similar number of trainable parameters. As described in Table II, for training from scratch, WideResNet (WRN) tends to show better performance than ResNet18(k) where k is the dimension of output from the ﬁrst layer. The increase in accuracy with the dimension of each block is similar to the basic ResNet. 2) Setting hyperparameters for KD: For setting hyperparameters in KD, we conducted several experiments with different temperature τ as well as lambda λ. We investigated distillation with different hyperparameters as well. We set WRN16-3 as a teacher network [18] and WRN16-1 as a student network, which is shown in Fig. 4. For temperature τ, in general, τ ∈ {3, 4, 5} are used [18]. High temperature mitigated the peakiness of teachers and helped to make the signal to be softened. In our experiments, according to the results from different τ, high temperature did not effectively help to increase the accuracy. When we used τ = 4, the results were better than other choices for both datasets with Full KD and ESKD [18]. For λ = 0.7 and 0.99, we obtained the best results with Full KD and ESKD for GENEactiv and PAMAP2, respectively. 3) Analyzing Distillation with different size of Models: To analyze distillation with different size of models, WRN16-k and WRN28-k were used as teacher networks having different capacity and structures in depth and width k. WRN16-1 and WRN28-1 were used as student networks, respectively. As mentioned in the previous section, in general, a higher capacity network trained from scratch shows better accuracy for WRN16 and WRN28. However, as shown in Fig. 5, in most of the cases, the results from WRN16-k shows better than the results of WRN28-k which has larger width. And the accuracy with teachers of WRN16-3 is higher than the one with teachers having larger width. Therefore, a teacher of higher capacity is not always guaranteed to generate a student whose accuracy is better. 4) Knowledge Distillation based on Fully Iterated and Early Stopped Models: We performed additional experiments with WRN16-k which gives the best results. Table III and Table IV give detailed results for GENEactiv and PAMAP2, respectively. Compared to training from scratch, although the student capacity from KD is much lower, the accuracy is higher. For instance, for the result of GENEactiv with WRN168 by training from scratch, the accuracy is 69.02% and the number of trainable parameters is 3 million in Table III. The number of parameters for WRN16-1 as a student for KD is 61 thousand which is approximately 1.6% of 3 million. However, the accuracy of a student with WRN16-2 teacher from ESKD is 69.34% which is higher than the result of training from scratch with WRN16-8. It shows a model can be compressed with conserved or improved accuracy by KD. Also, we tested with 7 classes on GENEactiv dataset which were used by the method in [50]. This work used over 50 subjects for testing set. Students of KD were WRN16-1 and trained with τ = 4 and λ = 0.7. As shown in Table V where brackets denote the structure of teachers and their accuracy, ESKD from WRN16-3 teacher shows the best accuracy for 7 classes, which is higher than results of models trained from scratch, Full KD, and previous methods [49], [50]. In most of the cases, students are even better than their teacher. In various sets of GENEactiv having different number of classes and window length, ESKD shows better performance than Full KD. In Table IV, the best accuracy on PAMAP2 is 86.38% from ESKD with teacher of WRN16-3, which is higher than results from Full KD. The result is even better than previous methods [57], which are described in Table VI where brackets denote the structure of teachers and their accuracy. Therefore, KD allows model compression and improves the accuracy across datasets. And ESKD tends to show better performance compared to Full KD. Also, the higher capacity models as teachers does not always generate better performing student models. To understand distillation effects based on the various capacity of teachers and augmentation methods, WRN16-1, WRN16-3, and WRN16-8 are selected as “Small”, “Medium”, and “Large” models, respectively. ESKD is used for this experiment which tends to show better performance than the Full KD and requires three-fourths of the total number of epochs for training [18]. In order to ﬁnd augmentation methods impacting KD on students for training, we ﬁrst trained a teacher from scratch with the original datasets. Secondly, we trained students from the pre-trained teacher with augmentation methods which have different properties including removal, adding noise, shifting, Mix1, and Mix2. For experiments on GENEactiv, for removal, the number of samples to be removed is less than 50% of the total number of samples. The ﬁrst point and the exact number of samples to be erased are chosen randomly. To add noise, the value for standard deviation of Gaussian noise is chosen uniformly at random between 0 and 0.2. For shifting, the number of time-steps to be shifted is less than 50% of the total number of samples. For Mix1 and Mix2, the same parameters are applied. For experiments on PAMAP2, the number of samples for removal is less than 10% of the total number of samples and standard deviation of Gaussian noise for adding noise is less than 0.1. The parameter for shifting is less than 50% of the total number of samples. The same parameters of each method are applied for Mix1 and Mix2. The length of the window for PAMAP2 is only 100 which is 3 seconds and downsampled from 100Hz data. Compared to GENEactiv whose window size is 500 time-steps or 5 seconds, for PAMAP2, a small transformation can affect the result very prominently. Therefore, lower values are applied to PAMAP2. The parameters for these augmentation methods and the sensor data for PAMAP2 to be transformed are randomly chosen. These conditions for applying augmentation methods are used in the following experiments as well. 1) Analyzing augmentation methods on training from scratch and KD: The accuracy of training scratch with different augmentation methods on WRN16-1 is presented in Table VII. Most of the accuracies from augmentation methods, except adding noise which can alter peaky points and change gradients, are higher than the accuracy obtained by learning with the original data. Compared to other methods, adding noise may inﬂuence classiﬁcation between similar activities such as walking, which is included in both datasets as detailed sub-categories. The validation accuracy of scratch and Full KD learning on GENEactiv dataset is presented in Fig. 6. Training from scratch with the original data shows higher accuracy than KD with original data in very early stages before 25 epochs. However, KD shows better accuracy than the models trained from scratch after 40 epochs. KD with augmentation tends to perform better in accuracy than models trained from scratch and KD learning with the original data alone. That is, data augmentation can help to boost the generalization ability of student models for KD. Mix1 shows the highest accuracy among the results. The highest accuracies are seen in early stages, which are less than 120 epochs for all methods, where 120 epochs is less than three-fourths of the total number of epochs. On closer inspection, we ﬁnd that the best accuracies are actually seen in less than 20 epochs for training from scratch and Full KD, less than 60 epochs for shifting, Mix1, and Mix2, and less than 120 epochs for adding noise, respectively. This implies that not only early stopped teachers but also early stopped students are able to perform better than fully iterated models. In training based on KD with augmentation methods, the accuracy goes up in early stages, however, the accuracy suffers towards to the end of training. These trends on KD are similar to the previous ESKD study [18]. For the following experiments, we restrict our analyses to ESKD. 2) Analyzing Augmentation Methods on Distillation: The accuracy of each augmentation method with KD is summarized in Table VIII and IX for GENEactiv and Table X and XI for PAMAP2. The results were obtained from small-sized students of ESKD. The gray colored cells of these tables are the best accuracy for the augmentation method among the different capacity teachers of KD. When a higher λ is used, distillation from teachers is improved, and the best results are obtained when the teacher capacity is smaller. Also, the best performance of students, when learning with augmentation methods and the original data, is achieved with similar teacher capacities. For example, for GENEactiv with λ = 0.7, the best results are generated from various capacity of teachers. But, with λ = 0.99, the best results tend to be seen with smaller capacity of teachers. Even though the evaluation protocol for PAMAP2 is leave-one-subject-out with an imbalanced distribution of data, with λ = 0.7, the best results are obtained from larger capacity of teachers as well. Furthermore, results from both datasets verify that larger and more accurate teachers do not always result in better students. Also, the best result from shifting is seen at the same capacity of the teacher with the original data. It might be because shifting includes the same time-series ‘shapes’ as the original data. The method for shifting is simple but is an effectively helpful method for training KD. For all teachers on PAMAP2 with λ = 0.99, the accuracies from training by shifting are even higher than other combinations. Compared to previous methods [57] with PAMAP2, the result by shifting outperforms others. Furthermore, although the student network of KD has the same number of parameters of the network trained from scratch (WRN16-1), the accuracy is much higher than the latter one; the result of Mix1 from GENEactiv and shifting from PAMAP2 by the medium teacher is approximately 2.7% points and 3.8% points better than the result from original data by training from scratch, respectively. These accuracies are even better than the results of their teachers. It also veriﬁes that KD with an augmentation method including shifting has beneﬁts to obtain improved results. To investigate the difference in performance with a model trained from scratch and KD with augmentation methods, statistical analysis was conducted by calculating p-value from a t-test with a conﬁdence level of 95%. Table XII and XIII show averaged accuracy, standard deviation, and calculated pvalue for WRN16-1 trained from scratch with original training set and various student models of WRN16-1 trained with KD and augmentation. That is, student models in KD have the same structure of the model trained from scratch and teachers for KD are WRN16-3 (τ = 4, λ = 0.7). For GENEactiv, in ﬁve out of the seven cases, the calculated p-values are less than 0.05. Thus, the results in the table show statisticallysigniﬁcant difference between training from scratch and KD. For PAMAP2, in all cases, p-values are less than 0.05. This also represents statistically-signiﬁcant difference between training from scratch and KD. Therefore, we can conclude that KD training with augmentation methods, which shows better results in classiﬁcation accuracy, performs signiﬁcantly different from training from scratch, at a conﬁdence level of 95%. Finally, the expected calibration error (ECE) [58] is calculated to measure the conﬁdence of performance for models trained from scratch and KD (τ = 4, λ = 0.7) with augmentation methods. As shown in Table XIV and XV, in all cases, ECE values for KD are lower than when models are trained from scratch, indicating that models trained with KD have higher reliability. Also, results of KD including shifting are lower than results from other augmentation methods. This additionally veriﬁes that KD improves the performance and shifting helps to get improved models. 3) Analyzing training for KD with augmentation methods: The loss values of each method, for the medium-sized teacher, are shown in Table XVI and XVII. The loss values were obtained from the ﬁnal epoch while training student models based on Full KD. As shown in these tables, for both cross entropy and KD loss values, training with shifting-based data augmentation results in lower loss, compared to other augmentation strategies and the original model. The loss value for noise augmentation is higher than the values of shifting. On the other hand, the KD loss value for Mix1 is higher than the values for removal and shifting. However, the training loss is for these two methods and its value of testing is lower. Compared to other methods, Mix2 shows higher loss for training, which may be because this method generates more complicated patterns. However, the testing KD loss value of Mix2 is lower than the value of original and adding noise. These ﬁndings imply that the data of original and shifting have very similar patterns. And data based on Mix1 and Mix2 are not simply trainable data for distillation, however, these methods have an effect of preventing a student from overﬁtting or degradation for classiﬁcation. The contrast of results from GENEactiv between each method is more prominent than the one from PAMAP2. This is due to the fact that smaller parameters for augmentation are applied to PAMAP2. Also, the dataset is more challenging to train on, due to imbalanced data and different channels in sensor data. D. Analysis of Teacher and Student Models with a Variant Properties of Training Set To discuss properties of training set for teacher and student models, we use the same parameter (τ = 4, λ = 0.7) in this experiment on two datasets. In this section, we try to train a medium teacher and a small student by training set having the same or different properties to take into account relationships between teachers and students. Testing set is not transformed or modiﬁed. The medium teacher is chosen because the teacher showed good performance in our prior experiments discussed in previous sections. Further, distillation from a medium model to a small model is an preferable approach [18]. Also, we analyze which augmentation method is effective to achieve higher accuracy. We use adding noise, shifting, and Mix1 methods which transform data differently. To obtain a medium teacher model, the model is trained from scratch with augmentation methods. These results are shown in Table XVIII. For GENEactiv, shifting based data augmentation gives the best performance. However, for PAMAP2, original data achieves the best performance. Mix1 shows slightly lower accuracy than shifting. In these experiments, the student model is trained using the teacher model that achieves best performance over several trials. We also evaluated different combinations of data augmentation strategies for teacher-student network pairs. A pair is obtained by using one or no data augmentation strategy to train the teacher network by training from scratch, and the student network is trained by ESKD under different, same, or no augmentation strategy. The results are shown in Fig. 7. We found that KD with the same data augmentation strategy for training teachers and students may not be the right choice to get the best performance. When a teacher is trained by shifting and a student is trained by Mix1 which showed good performance as a student in the previous sections, the results are better than other combinations for both datasets. Also, when a student is learned by Mix1 including shifting transform, in general, the performance are also good for all teachers. It implies that the method chosen for training a student is more important than choosing a teacher; KD with a medium teacher trained by the original data and a student trained with shift or Mix1 outperforms other combinations. Using the same strategy for training data for teachers and students does not always present the best performance. When the training set for students is more complicated than the set for teachers, the performance in accuracy tends to be better. That is, applying a transformation method to students can help to increase the accuracy. It also veriﬁes that better teachers do not always lead to increased accuracy of students. Even if the accuracies from these combinations of a teacher and student are lower than models trained from scratch by WRN16-3, the number of parameters for the student is only about 11% of the one for WRN16-3. Therefore, the results still are good when considering both performance and computation. E. Analysis of Student Models with Different Data Augmentation Strategies for Training and Testing Set In this section, we study the effect of students on KD from various augmentation methods for training and testing, while a teacher is trained with the original dataset. We use the same parameter (τ = 4, λ = 0.7) and ESKD for this experiment on two datasets. A teacher is selected with a medium model trained by the original data. We use adding noise, shifting and Mix1 methods which transform data differently. After training the teacher network on original data, a student network is trained with different data augmentation strategies and is evaluated on test data transformed with different data augmentation strategies. The results are illustrated in Fig. 8. For GENEactiv, most often, training student networks with Mix1 show better performance on different testing sets. However, if the testing set is affected by adding noise, training students with adding noise and Mix2 shows much better performance than training with shifting and Mix1. From the results on PAMAP2, in most of the cases, training students with Mix1 shows better performance to many different testing set. However, when the testing set is augmented by adding noise, training with original data shows the best performance. This is likely attributable to the window size, which has about a hundred samples, and the dataset includes the information of 4 kinds of IMUs. Therefore, injecting noise, which can affect peaky points and change gradients, creates difﬁculties for classiﬁcation. Also, these issue can affect the both training and testing data. Thus, if the target data includes noise, training set and augmentation methods have to be considered along with the length of the window and intricate signal shapes within the windows. Here, we compare the evaluation time for various models on the GENEactiv dataset. We conducted the test on a desktop with a 3.50 GHz CPU (Intel® Xeon(R) CPU E5-1650 v3), 48 GB memory, and NVIDIA TITAN Xp (3840 NVIDIA® CUDA® cores and 12 GB memory) graphic card. We used a batch size of 1 and approximately 6000 data samples for testing. Four different models were trained from scratch with WRN16-k (k=1, 3, 6, and 8). To test with ESKD and Mix1, WRN16-3 was used as a teacher and WRN16-1 was used for student network. As expected, larger models take more time for testing, as shown in Table XIX. WRN16-1 as a student trained by ESKD with Mix1 augmentation achieves the best accuracy, 71.35%, where the model takes the least amount of time on both GPU and CPU. The results on CPU reiterate the reason why model compression is required for many applications, especially on edge devices, wearables, and mobile devices, which have limited computational and power resources and are generally implemented in real time with only CPU. The gap in performance would be higher if an edge device had lower computational resources. In this paper, we studied many relevant aspects of knowledge distillation (KD) for wearable sensor data as applied to human activity analysis. We conducted experiments with different sizes of teacher networks to evaluate their effect on KD performance. We show that a high capacity teacher network does not necessarily ensure better performance of a student network. We further showed that training with augmentation methods and early stopping for KD (ESKD) is effective when dealing with time-series data. We also establish that the choice of augmentation strategies has more of an impact on the student network training as opposed to the teacher network. In most cases, KD training with the Mix1 (Removal+Shifting) data augmentation strategy for students showed robust performance. Further, we also conclude that a single augmentation strategy is not conclusively better all the time. Therefore, we recommend using a combination of augmentation methods for training KD in general. In summary, our ﬁndings provide a comprehensive understanding of KD and data augmentation strategies for time-series data from wearable devices of human activity. These conclusions can be used as a general set of recommendations to establish a strong baseline performance on new datasets and new applications. This research was funded by NIH R01GM135927, as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences, and by NSF CAREER grant 1452163.