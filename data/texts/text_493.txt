When using Markov chain Monte Carlo (MCMC) algorithms, we can increase the number of samples either by running longer chains or by running more chains. Practitioners often prefer the ﬁrst approach because chains need an initial “warmup” phase to forget their initial states; the number of operations needed for warmup is constant with respect to chain length but increases linearly with the number of chains. However, highly parallel hardware accelerators such as GPUs may allow us to run many chains in parallel almost as quickly as a single chain. This makes it more attractive to run many chains with a short sampling phase. Unfortunately, existing diagnostics are not designed for the “many short chains” regime. This is notably the case for the popularˆR statistic which claims convergence only if the effective sample size per chain is suﬃciently large. We present nˆR, a generalization of ˆR, which does not conﬂate short chains and poor mixing, and oﬀers a useful diagnostic provided we run enough chains and meet certain initialization conditions. We deﬁne what constitutes a proper warmup in the manychains regime and recommend a threshold for nˆR. Furthermore we use nˆR to construct a warmup scheme with an adaptive length, allowing users to avoid running their MCMC algorithms longer than necessary. The growing availability of parallel computation on CPUs, GPUs, and even specialized chips such as TPUs, has generated interest in MCMC strategies Work mostly done while an intern at Google Research. where we run many chains in parallel, using a short sampling phase [Lao et al., 2020, Hoﬀman and Ma, 2020]. New MCMC algorithms, such as ChEES-HMC are particularly well adapted to run many chains, leveraging the parallelization capacities of accelerators [Hoﬀman et al., 2021]. For many Bayesian inference applications, an eﬀective sample size (ESS) of about 100 may be adequate, in which case we might want to run as many as ∼100 chains. Indeed, if the chains are independent and sampling from the stationary distribution, then the number of chains provides a lower bound on the ESS, provided the target distribution has ﬁnite variance. Existing diagnostics are not designed for the regime where we have many chains each made up of only a few iterations. This is notably the case for the widely usedˆR statistic [e.g. Gelman and Rubin, 1992, Vehtari et al., 2020].ˆR checks for consistency between chains and converges to 1 if the chains produce Monte Carlo estimators in suﬃciently good agreement. Convergence to 1 however requires the measured ESS per chain to be large [Vats and Knudson, 2021], a condition which is incompatible with running many short chains; if diagnosing convergence requires us to run long chains, we cannot reap the beneﬁts of running many short chains in parallel. We present a generalization ofˆR, called nestedˆR, and denoted nˆR, which checks for consistency between groups of chains. nˆR is a useful diagnostic for the many-short-chains regime and works under conditions similar to those that the classicˆR assumes. The main additional requirement is to initialize each chain within a group at the same point. Because properly warmed-up MCMC forgets its starting point, this requirement does not aﬀect the quality of our estimates; it does, however, help us diagnose convergence failures. WhileˆR converges to 1 only when the number of post-warmup samples is large, nˆR can go to 1 (once the chains are properly warmed up) when either the number of iterations or the number of chains is large. Hence for a suﬃciently large number of chains we can NestedˆR: Assessing Convergence for Markov chain Monte Carlo when using many short chains diagnose convergence with only a few sampling iterations. If we use a short sampling scheme, the computation of MCMC is dominated by the warmup phase. To reduce this computational burden, we propose an adaptive warmup length scheme, wherein we compute nˆR using a few proposal samples after a short warmup window, and repeat until convergence is diagnosed. Given a random variable, θ ∈ Θ, with a target distribution π, and a scalar function of interest, f : Θ → R, our goal is to estimate the expectation value Ef, where we assume Varf is ﬁnite. We call a Monte Carlo estimator any statistic, T , intended to estimate Ef. This deﬁnition encompasses the traditional Monte Carlo estimator used when doing MCMC sampling but it is deliberately broader. Let Γ be the probability measure which generates T . Any Monte Carlo estimator admits an eﬀective sample size (ESS): Consider an MCMC sampler with stationary distribution π, which generates M chains, each with N iterations. Each chain is initialized at a point θ. We denote the nth sample from chain m as θ. It is common practice to ﬁrst “warm up” the sampler for some number of iterations. During the warmup phase, the Markov chains travel to the region where the probability mass concentrates and begin exploring this region [Betancourt, 2018]; early exploration also allows us to tune the parameters of our sampler [e.g Hoﬀman and Gelman, 2014, Hoﬀman et al., 2021]. Once the chain is warmed up, we generate the samples we use for our inference. For ease of notation, we consider estimation of Eθ and note all our calculations generalize to functions of θ with a ﬁnite variance. The classic MCMC estimator is: A useful perspective is to view this as an average of MP Monte Carlo estimators,¯θ=θ/N, each based on a single chain. TheˆR statistic, in its ﬁrst and simplest iteration as the potential scale reduction factor [Gelman and Rubin, 1992], compares a measure of the total variance across all chains, A = Varθ, to a measure of within- chain variance, W = EVar(θ| m = m). In words, W is the expected variance given that the samples are all in the same chain. We estimate these quantities using the sample variance estimators and The above expression is motivated by the law of total variance.ˆB measures the between-chain variance,q B = Var¯θ. We then deﬁneˆR ,ˆA/ˆW . If the chains are mixing, we expect the total variance and the within-chain variance to converge to Varθ as N grows, meaningˆR converges to 1. A common practice to assess convergence of the chains is to check thatˆR ≤ 1 + , for some  > 0. The choice of  varies across in the literature, and has evolved over time, starting at  = 0.1 and recently using the more conservative value  = 0.01 [Vats and Knudson, 2021, Vehtari et al., 2020]. A second perspective onˆR is due to Vats and Knudson [2021] who observe that when the chains are stationaryˆW /ˆB is a measure of the eﬀective sample size per chain. Observing that we see thatˆR decays to 1 only once we produce chains with a large eﬀective sample size. A third perspective is to viewˆR as a measure of agreement between the Monte Carlo estimators produced by each chain. Each chain is distinguished by its initialization point and its seed, but Ef is independent of both of these factors; if either inﬂuences our estimator too much then that estimator may not be accurate. Equation 5 shows thatˆR converges to 1 ifˆB is small relative toˆW . For a small , such that (1+)≈ 1+2, ˆR ≤ 1 +  is approximately equivalent to This inequality establishes a tolerance value forˆB, determined byˆW and . Once we adopt this perspective two things become clear. First, for small N, the variance of the per-chain The originalˆR uses a slightly diﬀerent estimator for the within-chain variance when computingˆA. ThereˆW is scaled by 1/N, rather than 1/(N − 1). This is of little concern when N is large, but we care about the case where N is small, and therefore adjust theˆR statistic slightly. Figure 1: Accuracy of Monte Carlo for Eθin Banana problem. When using many chains, we only require a single sampling iteration per chain to reach the wanted precision, once the chains are warmed up. Monte Carlo estimator stays relatively large, meaning ˆB will not be small even if Var¯θis. We demonstrate this on the two-dimensional “Banana” distribution, θ| θ∼ Normal(0.03(θ This is a simple transformation of a two-dimensional normal with highly non-convex level sets. After warming up 512 chains for 200 iterations, we only require 1 sampling iteration to achieve a squared error below Varθ/100. Using 4 chains requires a sampling phase with ∼200 iterations (Figure 1). StillˆR is indiﬀerent to the number of chains we run (Figure 2). Second,ˆR focuses on the variance of the per-chain Monte Carlo estimator, not its squared error.ˆR therefore cannot detect when all our Monte Carlo estimators suﬀer from the same bias. A canonical example is a multimodal target π, with all chains getting stuck with high probability in the same mode after N iterations. Changing the MCMC process, for instance by using overdispersed initializations can help ensureˆB stays large until we achieve stationarity. The key idea behind nˆR is to compare Monte Carlo estimators whose variance decreases with both the number of iterations and the number of chains. A natural way to do this is to group chains into super chains. Our MCMC process now has K super chains, each compris- Figure 2: ApplyingˆR and nˆR to the ﬁrst dimension of the Banana problem.ˆR is indiﬀerent to the number of chains we run, despite the increased precision. nˆR on the other hand is sensitive to the number of chains. ing M chains, and we denote θthe nth sample from chain m of super chain k. The Monte Carlo estimators compared by nˆR are the sample means of each super chain, Provided we have enough chains per group, nˆR quickly goes to 1, reﬂecting the high precision of¯θafter a few iterations (Figure 2). The new diagnostic follows the same formula as the originalˆR except thatˆB andˆW are now measures of the between and within-super-chain variances. Overloading our notation, for nˆR we deﬁne and In the special case where N = 1 we take the withinchain variance to be 0, meaning the second term in the above equation vanishes. Note this makesˆW smaller and results in a lower tolerance onˆB. It is possible to use rank-normalization on nˆR, a technique recently employed to improveˆR. One motivation for doing so is to diagnose non-convergence when NestedˆR: Assessing Convergence for Markov chain Monte Carlo when using many short chains π has inﬁnite variance; further beneﬁts are discussed by Vehtari et al. [2020]. We replace each sample, θ, by its rank across all samples, r, and then do an inverse-normal transformation with a fractional oﬀset [Blom, 1958] to obtain nˆR is then computed using zinstead of θ. Taking advantage of the normal distribution of z, we work out the distribution of nˆR under the assumption stationarity. Lemma 1 Suppose (i) the Markov chains are stationary, (ii) N = 1, meaning each chain has a single sample, and (iii)ˆB (Equation 9) andˆW (Equation 10) are computed using rank-normalized samples. Then The proof is in the Supplement. Unfortunately nˆR as deﬁned above is too optimistic, as illustrated by the following thought experiment. Consider a bimodal target distribution π = 0.3 Normal(−10, 1) + 0.7 Normal(10, 1), and suppose each chain is either initialized at -10 or 10 with equal probability. Typical MCMC chains will not be able to mix across the high energy barrier between the modes. As a result, which mode a chain explores depends strongly on initialization, and MCMC incorrectly ascribes (on average) probability mass 0.5 to each mode. The per-chain Monte Carlo estimators are inconsistent, since they estimate the means at diﬀerent modes. This allowsˆR to diagnose the issue. On the other hand, the per-super-chain Monte Carlo estimators may look consistent and nˆR incorrectly claims convergence. To see why, let¯θbe the Monte Carlo estimator we obtain from chain m of super chain k after N iterations. Let σ= Var¯θ. We use a subscript on the variance because we do not assume stationarity. Because the chains are independent, we have Regardless of whether the chain is stationary or not, this value becomes arbitrarily small for large M. This is not unexpected: we obtain good agreement between MCMC estimators so long as they are based on draws from the same distribution, whether or not it is the stationary distribution. One extreme case would be for nˆR to claim convergence after a single iteration, because all the estimators are accurately computing expectation values with respect to the initialization distribution π. To remedy the above issue, we impose the constraint that all chains within a super chain are initialized at the same point, θ. This allows us to track the inﬂuence of the initial point. In our thought experiment, the initial point determines which mode each chain explores. Our proposed initialization scheme therefore produces super chains in disagreement with one another, meaning we can now identify with nˆR that our MCMC estimates are unreliable. We can understand the behavior of nˆR under this initialization condition via a variance decomposition: We denote B := Var¯θ, B:= E[Var(¯θ| θ)], and B:= Var[E(¯θ| θ)]. The above equation is in this notation B = B+ B. Conditional on θ, the chains are independent and B behaves as σ. That is, Bgoes to 0 either as M increases or, once we have reached stationarity, as N increases. B, on the other hand, is indiﬀerent to M. For it to decay, the expected value of¯θmust become independent of θ. To use a common phrase, the chains must “forget” where they started. What we have with this initialization scheme is a useful compromise betweenˆR and nˆR applied to independent chains. We term the latter the naive nˆR, and save the original term, nˆR, devoid of any preﬁx, for the case where we use one intial point per super chain. nˆR ﬁrst behaves likeˆR and, as the correlation to the starting point decays, switches to behaving like the naive nˆR if the chains converge. We demonstrate this transient behavior (or lack thereof) on two examples: (i) a Gaussian distribution where the chains mix and the transient behavior occurs; (ii) a mixture of two Gaussian distributions where the chains fail to mix and nˆR does not transition to the naive behavior (Figure 3). Our chains are properly warmed up if the ﬁrst point of each chain is an independent draw from the stationary Figure 3: Sample variance of Monte Carlo estimators. We compare estimators using a single chain, a naive super chain made of independent chains, and a super chain made of chains initialized at the same point. distribution. Note that this implies the chains have forgotten their starting point (since some of the chains were initialized at the same point), and that therefore B= 0. Unfortunately, we cannot directly measure B. But we can estimate B, and Hence B < 2σimplies B< 2σ. This latter bound becomes sharper as Bdecays to 0, which we can achieve by either increasing N or M. We can therefore set  to construct a scale-free tolerance on B, if perhaps a conservative one, For example, setting  = 0.01 implies we want Bto be at most 2% of the posterior variance. Suppose we run M = 128 chains. Under the assumption of stationarity, σ/B= ESS ≥ M , a lower bound which is attained when N = 1. Then 0.012 = 0.02 − 1/128 ≤ 2 − B/σ< 0.02, meaning that even with N = 1 the bound is relatively sharp. In our experiments, we ﬁnd using N = 5 works well. In the special case where N = 1, we may take advantage of Lemma 1 to obtain quantiles for the distribution of nˆR under the assumption of stationarity. This assumption acts as a null hypothesis, which we reject if nˆR is too large. The threshold we choose lets us control the Type II error (incorrect claims we have not converged). Unfortunately we cannot control the Type I error (incorrect claims we have converged), because the chains can be almost stationary. In fact ﬁnite chains are never stationary and we leave to a future analysis how close to stationarity the chains need to be in order to produce useful Monte Carlo estimators. This can then inform how to calibrate our hypothesis test. A drawback of nˆR, relative toˆR, is that our proposed diagnostic is more sensitive to poor initialization. If the initial distribution πgenerates samples which share the same bias relative to the target distribution, then the per-super-chain Monte Carlo estimators may appear to be in good agreement even though the chains are still transient. In this scenario Var¯θdoes not characterize well the squared error of our Monte Carlo estimator. This can occur if we run a short warmup phase or, somewhat equivalently, if the chains mix slowly.ˆR’s implicit requirement for long chains enforces long MCMC runs which increases our chances of overcoming the transient behavior. Furthermore, comparing diﬀerent sections of a chain, as is done with splitˆR [Gelman et al., 2013], can help identify transient behaviors. With short chains, this approach is not an option and we need additional analysis, e.g. examining traceplots during the warmup phase. Using overdispersed initialization is generally recommended for any MCMC scheme, a point that deserves further emphasis when using nˆR. A related limitation of nˆR is that it cannot detect when the chains have converged to a perturbed stationary distribution. This may happen if we are adapting the parameters of the MCMC kernel during warmup, and do not freeze the parameters for enough iterations before sampling. nˆR will detect that all of the chains are sampling from the same distribution, but not that it diﬀers subtly from the stationary distribution. This can be avoided by freezing adaptation earlier, but in any case if we are averaging adaptation signals across many chains, then this undetected bias is likely to be small, since the inﬂuence of any one chain on the adaptation is diminished. NestedˆR: Assessing Convergence for Markov chain Monte Carlo when using many short chains When using a short sampling phase, the computation is dominated by the warmup. Which warmup length to use depends on the speciﬁc distribution we sample from and the MCMC algorithm we use. Current practice amongst modelers is to prespecify the warmup length and then run a long sampling phase. Only then do we run various diagnostics and if needed adjust the warmup length. This practice means the warmup length is rarely optimal. In this section, we discuss how nˆR can be used to remedy these problems. Zhang et al. [2020] propose a cross-chain warmup scheme for Stan’s dynamic Hamiltonian Monte Carlo [Betancourt, 2018, Hoﬀman and Gelman, 2014]. This warmup strategy shares tuning parameters by pooling information between chains. Note that ChEES-HMC similarly shares information between chains to tune the sampler. Additionally, rather than run a warmup phase with a ﬁxed length, the cross-chain warmup uses multiple warmup windows of length w. Stan’s default warmup length is 1,000 iterations; the proposed window size w = 100 or 200. At the end of each window, we computeˆR, as well as the bulk and tail ESS [Vehtari et al., 2020] for the unnormalized log target density using samples from the most recent warmup window. If ˆR <ˆRand ESS > ESS, for a prespeciﬁedˆRand ESS, we end the warmup phase. We would like to build on this promising approach. Our main concern is that the values we should use for ˆRand ESSdepend on both w and the speciﬁcs of our target distribution. How to pick the “right” threshold remains an open question, even if we can make a sensible guess. nˆR lets us address this challenge. We propose the following modiﬁcation to the adapative warmup scheme. At the end of each warmup window, we generate 5 sampling iterations and compute nˆR based on those samples. If w ≈ 100, computing these additional iterations is relatively cheap. We use the threshold proposed in Section 3.3. This use of nˆR aligns with its natural intent and the window length, w, does not impact our diagnostic. Additionally, rather than examine the unnormalized log target density, we compute nˆR for all quantities of interest. While it is convenient to describe how well the chains are mixing using a single scalar, convergence in one quantity may not indicate convergence in all quantities we may care about. We therefore adopt a more conservative approach, but one that is more aligned with the modeler’s goals. Algorithm 1 summarizes the procedure. We denote the MCMC sampler Γ, which admits two arguments: φ, the tuning parameters of the sampler, and θ, the current state for all chains. If we draw warmup samples, both φ and θ are updated at each iteration. When the number of chains we run matches the wanted ESS, we can use the proposed sample, θ, to construct our Monte Carlo estimators. If the number of chains is smaller than the target ESS, we can run the chains for more iterations until we attain the desired ESS. Input: MCMC sampler, Γ; initial distribution π; initial tuning parameters φ; number of super chains K; number of chains per super chain M ; scalar functions of θ: f, ..., f; warmup window length w; max number of steps T; sampling window length s; threshold for nˆR, 1 + . 2: for k = 1 to K do: Sample θ∼ π. 4: Initialize all chains in kgroup at θ. Set tuning parameters φ = φ. 6: Set chain states θ = θ. for t = 1 to Tdo: 8: Run w warmup iterations from Γ(φ, θ). Update φ and θ. 10: Draw θfrom s sampling iter from Γ(φ, θ). for q = 1 to q = Q do: Check nˆR ≤ 1 + . 14: If nˆR condition met for all q’s, break. Return: θ; f(θ), ..., f(θ); φ. We study the behavior of nˆR on four target distributions, which we describe below. Banana (D = 2). See Section 2, Equation 7. Logistic regression (D = 25). A logistic regression model on the numerical version of the German credit dataset [Dua and Graﬀ, 2017]. There are 24 features and an intercept term. The full model is θ ∼ Normal(0, 1); y∼ Bernoulli11 + e. Hierarchical model (D = 10). A model describing the eﬀect of an SAT training program, as measured by the performance of students across 8 schools [Rubin, 1981]. We estimate the group mean and the population mean and variance. To avoid a funnel shaped Figure 4: Scaled squared error against nˆR. The yellow line occurs at 0.01. The dotted lines form a 0.95 coverage for the distribution of the squared error at stationarity and the solid line is the expectation value (Equation 16). posterior density, we use a non-centered parameterization: µ ∼ Normal(5, 3); σ ∼ Normal(0, 10); η∼ Normal(0, 1); θ, σ = µ + ησ; y= Normal(θ, σ). model describing the diﬀusion of a drug in the body, using data simulated over 100 patients. We use a onecompartment model with ﬁrst-order absorption from the gut, described by the diﬀerential equation: where m is the drug mass in the gut and the central compartment (blood and organs into which the drug diﬀuses profusely). This diﬀerential equation admits an analytical solution. Each patient receives a drug dose at a regular interval. The drug plasma concentration is measured over time. Our measurement model is, for each patient indexed by n, For each patient, we estimate the transmission rates kand k. We use a hierarchical prior on (k, k) and estimate the population means and variances for both kand k. The model is ﬁtted to data sampled from the prior. Further details on the model can be found in the Supplementary Material. We run Hamiltonian Monte Carlo, using the ChEES adaptive scheme [Hoﬀman et al., 2021] with 4 super chains, each composed of 32 chains, for a total of 128 chains. We compute the squared error of our Monte Carlo estimators, using long MCMC runs to calculate with high precision the correct posterior mean and variance. Invoking the Central Limit Theorem, we assume¯θis approximately normally distributed for stationary Markov chains. Then for N = 1, ` = (10, 20, 30, ..., 100, 200, 300, ..., 1000). The warmup lengths are not set ahead of time. Rather we use the adaptive warmup scheme in Section 4, starting with a small warmup window, w = 10, and later expanding this window to w = 100. The initial short windows have no practical use outside this experimental setting, where they help us monitor the behavior of nˆR when the squared error is large. Each adaptive warmup is repeated 10 times. At the end of each window, we record nˆR and the squared error of Monte Carlo estimators using a single sample per chain for each dimension. Figure 4 plots the squared error, scaled by KM/Varθ, against nˆR. As desired, there is good correlation between nˆR and the squared error, and our diagnostic is consistently large over regions that admit an important squared error. When nˆR < 1.01, the squared error mostly falls within the 0.95 coverage area of the χdistribution. However the fraction of points which fall above the 97.5quantile somewhat exceeds 0.025 (Table 1), which suggests our diagnostics could be more conservative. We now use the Eight School model as an illustrative example to highlight further analysis. Using a warmup with length 1,000 is amply suﬃcient, as can be checked by examing the squared error. In this scenario, nˆR falls consistently below the proposed threshold, bearing a few exceptions, whileˆR produces values larger than what would be considered acceptable (Figure 5). NestedˆR: Assessing Convergence for Markov chain Monte Carlo when using many short chains Table 1: Fraction of estimators with a scaled error above the 97.5quantile of a χdistribution with nˆR either below or above 1.01. If the chain is stationary, this number should be close to 0.025. Figure 5: nˆR andˆR across all 10 parameters of the Eight Schools after a long warmup.ˆR is consistently above 1.05, while nˆR is below 1.01. We run the adapative warmup algorithm 50 times, using diﬀerent seeds both for the initialization and the MCMC. Again, we examine the squared error when using one sample per chain and ﬁnd it to agree reasonably well with what we would expect from independent draws from the stationary distribution. For this relatively simple problem, the warmup length varies between 100 and 1,000 iterations (Figure 6), likely because our initialization strongly inﬂuences how quickly the Markov chains reach the stationary distribution. There is no obvious correlation between the observed squared error and the length of the warmup, suggesting the algorithm does a good job of gauging when to stop. nˆR reproduces many of the desirable properties ofˆR and can be applied to the regime where we run many chains with a short sampling phase. This makes it a useful tool to monitor MCMC warmup, without the Figure 6: Squared Error for the Eight Schools over expected squared error across all dimensions. The warmup length is not prespeciﬁed, but obtained using Algorithm 1. The solid line is the expected scaled squared error and the dotted line the 97.5quantile of the χdistribution. requirement to run long chains. The utility of nˆR lies in its ability to check whether non-independent chains behave as though they were independent, per the notion that a proper warmup should “decouple” chains initialized at the same point. We may be able to relax this condition by starting chains in a super chain within the same region, rather than at the same point. The potential beneﬁts of such an approach would be to cover more ground earlier during the warmup phase, which can improve tuning the sampler. We note that there exist many variations onˆR which can also motivate further improvements to nˆR. In the classic MCMC setting, the warmup phase needs to adapt the tuning parameters of the sampler to insure the Markov chains move eﬃciently across the parameter space during the sampling phase. That is we want to reduce the autocorrelation of our Markov chains, in order to generate chains with a large eﬀective sample size. When one sample per chain suﬃces, autocorrelation is no longer a concern, which raises the possibility that our warmup phase can be less sophisticated and ultimately shorter. On the other hand, tuning the sampler well may be a prerequisite to achieve stationarity and obtain the unavoidable ﬁrst sample. We thank the entire TensorFlow Probability team at Google, especially Alexey Radul, as well as Aki Vehtari and Andrew Gelman for many helpful discussions. We thank Aki Vehtari, Rif A. Saurous, Andrew Davison, and Owen Ward for their helpful comments on this manuscript.