We study the problem of recommending relevant products to users in relatively resource-scarce markets by leveraging data from similar, richer in resource auxiliary markets. We hypothesize that data from one market can be used to improve performance in another. Only a few studies have been conducted in this area, partly due to the lack of publicly available experimental data. To this end, we collect and release XMarket, a large dataset covering18local markets on16dierent product categories, featuring52.5million user-item interactions. We introduce and formalize the problem of cross-market product recommendation, i.e., market adaptation. We explore dierent market-adaptation techniques inspired by state-of-the-art domainadaptation and meta-learning approaches and propose a novel neural approach for market adaptation, named FOREC. Our model follows a three-step procedure – pre-training, forking, and ne-tuning – in order to fully utilize the data from an auxiliary market as well as the target market. We conduct extensive experiments studying the impact of market adaptation on dierent pairs of markets. Our proposed approach demonstrates robust eectiveness, consistently improving the performance on target markets compared to competitive baselines selected for our analysis. In particular, FOREC improves on average 24% and up to 50% in terms of nDCG@10, compared to the NMF baseline. Our analysis and experiments suggest specic future directions in this research area. We release our data and codefor academic purposes. • Information systems → Recommender systems; Collaborative search;• Computing methodologies → Transfer learning. Product Recommendation, Meta-Learning, Domain Adaptation, Market Adaptation, Cross-Market Recommendation allan@cs.umass.edu ACM Reference Format: Hamed Bonab, Mohammad Aliannejadi, Ali Vardasbi, Evangelos Kanoulas, and James Allan. 2021. Cross-Market Product Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482493 Nowadays online shopping in many countries is a part of people’s daily lives. While online shopping brings several benets and comfort to both users and vendors [20], it comes at the risk of overwhelming users with virtually unlimited options to choose from. Recommender systems are key in dealing with information overload, helping not only users nding interesting items, but also vendors nding the right customer for their products. E-commerce companies often operate across markets; for instance Amazon has expanded their operations and sales to 18 markets around the globe.This brings both opportunities and challenges. While it is typical that several local e-commerce companies operate in every country, the presence of an international e-commerce company, like Amazon, eBay, and Etsy can benet users even more if these companies can utilize the experience and data gathered across several markets. Using cross-market data however comes at a risk of assuming one-solution-ts-all and applying the same algorithms that are developed for and trained on large and data-rich markets, such as the U.S. [19], to small and data-scarce markets. The key challenge is that data, such as user interaction data with products (clicks, purchases, reviews), convey certain biases of the individual markets [5]. Algorithms that are optimized for a certain market learn various biases and distributions of the data [44]. Therefore, the algorithm trained on a source market, are not necessarily eective in a dierent target market [11], since utilizing the vast amount of data from a large market to improve the performance on low-resource markets comes at the risk of importing the wrong data distributions. For example, assume iPhone is the most popular smartphone in the U.S. (source market), while Samsung is the most popular smartphone brand in Germany (target market). Importing user preference from the U.S. market would yield to recommending iPhone in Germany more often than Samsung, which clearly is a wrong choice. Hence, even though there is a myriad of information to learn from a source market, careful adaptation of data is required. The signicance of Cross-Market Recommendation (CMR) has been pointed out in the literature [43]. However, small progress has been made in this area, mainly due to a lack of experimental data. To this end, we construct a large-scale real-life product recommendation dataset, referred to as XMarket, drawn from18markets in11languages. To develop this dataset we crawled Amazon marketplaces around the globe, locating and including in the dataset the same products within dierent markets. Moreover, we analyze certain statistical properties and trends amongst multiple markets and product categories where we highlight the existence of crucial dierences across dierent markets. In this paper, we focus on the user-item interaction data through ratings and study the problem of recommending relevant products to users in relatively resource-scarce markets by leveraging data from similar, richer in resource auxiliary markets. Our hypothesis is that data from one market can be used to improve performance in another. For this purpose, we rst introduce and formalize the problem of cross-market product recommendation, i.e., market adaptation. In order to solve the problem ofCMR, we explore market-adaptation baselines inspired by domain-adaptation and meta-learning approaches. Then, we propose a novel neural approach, named FOREC, consisted of a three-step procedure – pretraining, forking, and ne-tuning – in order to fully utilize the data from an auxiliary market to boost the product recommendation performance in the target market. More specically, FOREC learns a general recommendation system based on two markets (i.e., source and target) and employs a forking procedure by adding a market specic sub-network to the head of the model and freezing the bottom part in order to adapt the general internal representations to the target market. We conduct extensive experiments studying the impact of market adaptation on dierent pairs of markets. FOREC demonstrates robust eectiveness, consistently improving the performance compared to competitive baselines on7target markets we selected for our study. In particular, FOREC improves on average 24% and up to 50% in terms of nDCG@10, compared to the NMF baseline. Our analysis and experiments provide many insights on theCMRand suggest specic future directions in this research area. In summary, the main contributions of this paper constitute: •Collecting a real-world pragmatic large-scale cross-market and cross-lingual product review dataset. •Performing analysis of cross-market behavioral biases pivoting on the fact that our dataset includes the same items shared across dierent markets. In particular, we study how dierently users in dierent markets interact with the same products. •Proposing a novel neural architecture for market-adaptation and demonstrating the eectiveness of the model through extensive experiments. We adapt various Cross-Domain Recommendation (CDR) and meta-learning approaches for Cross-Market Recommendation (CMR). •Analyzing the performance of the model, considering various setups and conditions to provide further insights. This study is related toCMRandCDR, as well as meta-learning approaches. In this section, we briey review the research done in these domains. Cross-domain and cross-market recommendation.Research onCMRandCDRaims at improving the system’s eectiveness based on the external data that is available from other markets or item categories. While having the same goal, the two tasks dier in various aspects, each bearing their own challenges. Particularly, in CDRthe general assumption is that the model learns from interactions of overlapping usersin dierent domains (e.g. categories in product search) with the aim of improving the recommendation on the target domain items, using help from the source domain items. ForCMRthe situation is reversed: interactions of dierent set of users in the source market are leveraged to boost the recommendation for users in the target market. Here, we assume that the items are shared among dierent markets. Im and Hars[19]conduct an experiment in two product domains, aiming to answer the question “does a one-size recommendation t all?” where they observed that the performance of Collaborative Filtering (CF) is highly aected by the information-seeking mode of the users. Depending on the product domain, users adopt dierent strategies and therefore the system would not t to all domains. Lu et al. [34]later argue that transferring all the knowledge from source domain into the target domain may harm the recommender due to some inconsistencies and propose a criterion for selecting the consistent part of knowledge to be transferred to the target domain. Elkahky et al. [10]apply domain adaptation using user behaviorbased features for learning latent space. Rafailidis and Crestani[40] propose a collaborative ranking model with a weighting strategy that controls the inuence of user preferences from auxiliary domains. Zhao et al. [52]use reviews to transfer user preference at aspect-level as a cross-domain recommendation framework. Krishnan et al. [28]propose using contextual invariances across domains to leverage data from a dense domain to improve learned representations in other sparse domains. Dierent from these works, in this paper we investigate the existence of dierent behavioral biases across dierent markets where unlikeCDR, the items are the same across dierent markets but the users are dierent. Further research has aimed at mitigating these biases and transferring knowledge from one domain to another [18,30]. Unsupervised domain adaptation [14] has also inspired various cross-domain recommender systems in recent years [23,31,47,50].CDRhas been used specically for mitigating the cold-start problem in a number of studies [13, 21, 22, 24, 37, 49, 52]. While there exists much work on domain adaptation, marketadaptation is relatively unstudied.CMRhas attracted attention in music recommendation [11,43] where Ferwerda et al. [11]analyze and study music diversity across countries and propose to use country-based diversity measurements for system evaluation. Roitero et al. [43]studied user behavior in 21 dierent markets on Spotify and highlight the need for market-specic algorithms, as opposed to a global algorithm. We take one step further in this direction by expanding our study to various item categories in e-commerce where users purchase items (rather than having a monthly subscription) and express their opinion and experience with item in the form of ratings and reviews. Meta-learning.The goal of meta-learning is to train a model on multiple tasks, such that it can rapidly adapt to a new task after seeing a small number of new training samples [46]. In the context of recommendation systems, meta-learning has been used for several problems, including but not limitted to recommender algorithm selection [8,9,35], cold-start problem [29,45], and retraining the model [51]. For meta-learning of deep neural networks, a general and powerful technique, Model-Agnostic Meta-Learning (MAML), has been proposed that can be directly applied to any learning problem and model [12].MAMLframework is widely used in recommendation literature. For example, Lu et al. [33]use it on heterogeneous information networks to address the cold-start problem. Others usedMAMLto train a recommender which performs reasonably good enough both for cold- and warm-start users [4, 29]. In this section, we rst describe in detail the data collection process and provide statistics for the collected data. Then, we analyze the data and highlight important characteristics and similarities across dierent markets. We describe our new dataset, called XMarket, and provide details on how we generated it. We constructed the XMarket item and review collection on top of a large-scale publicly-available Amazon dataset [16,36]. The Amazon Product data [36] includes millions of item reviews collected from the Amazon U.S. marketplace in various categories. The dataset was collected in 2014 and later updated in 2018 [38]. We used this dataset as a seed to initiate our crawl. We located the same items that appear on the U.S. market in other markets, by matching the items’ unique identiers (aka. ASIN’s) on all available Amazon markets. In particular we have crawled data from the following markets: Saudi Arabia (sa), Singapore (sg), Australia (au), United Arab Emirates (ae), Turkey (tr), Japan (jp), India (in), Spain (es), U.S. (us), China (cn), Germany (de), Netherlands (nl), France (fr), Brazil (br), Canada (ca), Mexico (mx), Italy (it), United Kingdom (uk). Our main criterion for including an item in the collection process was its popularity on theusmarket. Our decision was motivated by the idea of having a high-resource market (i.e.,us) that would provide a wealth of data to other markets. Therefore, we discarded all items with less than 20 reviews in the past two years, as we did not consider them rich enough to be useful in other markets. In our preliminary analysis, we noticed that in most cases, if an item exists in another market its ASIN is the same. Therefore, we fed our crawler with the ASIN’s that we collected from the U.S. market. In doing so, we collected cross-market metadata information for over one million items and collected about 52 million multilingual reviews. Among the existing multi-lingual review datasets, we nd the Multilingual Amazon Reviews Corpus (MARC) [26] the most similar Table 2: Statistics of dierent markets for Electronics. R 4.1m 0.5m 174k 257k 392k 721k 169k 865k to XMarket. MARC consists of multi-lingual reviews extracted from dierent Amazon marketplaces, however, the scale of the dataset is much smaller. In particular, they do not cover all Amazon marketplaces and categories, whereas XMarket covers a wide range of categories in all 18 Amazon marketplaces. Moreover, XMarket includes rich item and review metadata (e.g., reviewer ID, item description, and related items) that can be utilized to pursue various research directions. We also found another similar dataset named as Amazon Customer Reviews Datasetproviding a collection of reviews from ve marketplaces dated from 1995 to 2015. We notice that a vast majority of the provided data is only from United States and the provided meta-data is limited to only product title and reviews whereas our data covers more number of marketplaces with a full meta-data information (including product text, images, also bought, similar items). In addition, our product reviews are more recent. To the best of our knowledge, no other cross-market multi-lingual recommendation dataset with such a wide coverage of markets and categories exists in the community. Table 1 summarizes some of the main characteristics of XMarket. It provides a cross-lingual e-commerce dataset of 16 shopping categories in 11 languages. We crawled data for∼300K unique items across all markets, which resulted in∼4M cross-market items. Also, Fig. 1a shows the distribution of items in each market. We see that the Canadian (ca) and Mexican (mx) markets have the most items in common with the U.S. market (us), which is expected due to the long-lasting presence of Amazon in these countries and their vicinity tous. Due to space considerations and similarity in results, in the remainder of the paper we analyze and discuss a subset of markets and categories. Our experiments and model evaluations are based on theElectronicscategory with statistics presented in Table 2. We see in the table the main characteristics of the studied marketplaces in terms of recommendation data, such as number of items, users, and ratings. We observe a relatively high number of users as well as items in the U.S. market, making it the most sparse market in our dataset. Our goal in this section is to analyze and demonstrate the similarities and dierences across markets. Having in mind that the markets share the same set of products, we analyze how people in dierent regions interact with these items to uncover behavioral characteristics and biases. Distribution of ratings.Several reasons may inuence users to purchase a product such as users’ nancial status, culture, and companies’ marketing strategies. Therefore, we study the dierence in product rating (as a signal of product purchase) among dierent markets. We plot the distribution of product “purchase” in Fig. 1b. Figure 2: Market similarity based on the cosine distance. The dominance ofusis obvious in this gure, with the highest median in all categories. We see that dierent markets exhibit dierent distributions across categories. In general we see that Electronics is most popular category among dierent markets. We also observe that theHome & Kitchencategory shows a dierent trend compared to other categories, perhaps because such items are more regionally dependent. Distribution of rating starts.We are interested in nding out if the same items are rated dierently in each market. Also, if the dierences happen across categories. We nd signicantly dierent behavior in giving rating stars among markets and categories, as shown in Figure 1c. We see a greater tendency of giving higher rating to items inmxmarket, whereas forinmarket, we see an opposite behavior. Interestingly, we observe a relatively similar trend in all categories where for example the median rating inde market is always higher than that ofusandfrmarket, but slightly lower than theukmarket. This clearly shows a general bias in user rating behavior, which should be taken into account when developing algorithms such as rating prediction. Market similarity.Based on the observations that we had from Figure 1, we estimate the users’ purchase similarity between markets. For a pair of markets in the same category, we build item purchase count vectors and compute cosine similarity between the two vectors. In Figure 2 we plot the similarities of all market pairs for two categories. Interestingly, we observe the highest rate of similarity betweende,fr, anduk, highlighting the similarities in European markets. On the other hand, we see that the American countries do not share much in common. As we seeus,ca,mx exhibit low similarity, which is surprising. Perhaps this is due to strong existence of local vendors in this category. In particular, we see the lowest similarity between de and mx. Remarks.Overall, from our analyses it is evident that users in each market exhibit dierent behavior. These dierences could be due to various reasons such as cultural biases or dierent marketing strategies adopted by companies. Another reason could simply be the popularity of Amazon as an e-commerce marketplace in dierent countries and how long it has been doing business in each country. For instance, we saw a very high similarity between us,ca, andmxin terms of common items that exist in the three markets, however, when it comes to product purchase data, we see very little similarity. This indicates that even though Amazon has a big inuence on the e-commerce market in these countries, users act dierently. With the existence of obvious dierences and similarities, but at the same time having a mix of data-rich and scarce markets, learning from auxiliary markets is not trivial and requires careful development of market adaptation techniques. Assume we are given a set of parallel markets as𝑀 = {M, · · · , M}. LetMbe the base market with the set of itemsI= {𝐼, · · · , 𝐼}. For the base market, one could assume the market with long-lasting existence oering the super-set of items and rich user-item interaction data. For example, with our XMarket settings, theusmarket can be thought of the base market and others such asdeorinare considered the parallel target markets. We assume thatI⊂ Ifor 1 ≤ 𝑙 ≤ 𝑡. Depending on the provided parallel markets,𝑀, the base market could change or there might be no base market. With any of these settings, a union set of items in all the parallel markets could be dened as I, satisfying our assumption. For a given target market,M, let the set of market’s users asU= {𝑈, · · · , 𝑈}. Generally, a user can interact with dierent markets, but for simplicity, we assume that the set of users in each market are mutually disjoint with any other parallel market. The problem of market adaption is to use any of the parallel markets provided, M∈ 𝑀 − {M}as an auxiliary market to improve the quality of items recommended to users of the target market, i.e.U. It is straightforward to use more than single auxiliary market. However, we focus on single auxiliary market and leave the other variations as the future work. For our experiments, we either augment with onlyusmarket or any of the parallel markets and report the results. Automatically selecting the most suitable parallel auxiliary market is another interesting problem that is out of the scope of this study Figure 3: The general schema of our FOREC recommendation system. For a pair of markets, the middle part shows the market-agnostic model that we pre-train, and then fork and ne-tune for each market shown in the left and right. and we plan to explore on that direction as our future work. Here, we explain our proposedCMRsystem, named FOREC. A general schema of our model is presented in Fig. 3. We show an example model architecture for a pair of markets,deandus. The training phase for the FOREC system includes three ordered steps (i.e., pretraining, forking, and ne-tuning) that we explain in the following. Note that FOREC is capable of working with any desired number of target markets. However, for simplicity, we only experiment with pairs of markets for our experimental evaluation. In this step, we aim to train a recommendation model that is marketagnostic, in the sense that all the model parameters𝜃are shared across markets and easily adaptable to every target market. This provides a generalized recommendation performance and a set of internal latent representations that are suitable for each individual market. Having such internal representations maximize the reusability of parameters translating into minimal eort on target market adaptation. To this end, we exploit the Model-Agnostic Meta-Learning (MAML) framework [12] from the few-shot learning literature. The general neural architecture we use for our pre-training step is presented in middle part of Fig. 3. This architecture is rst introduced by He et al. [17]and widely used in the literature. Here, we summarize the Neural Matrix Factorization (NMF) deep network before explaining our learning paradigm across markets. NMF model fuses two sub-networks namely Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP). Both GMF and MLP subnetworks are trained with the data individually and then fused using the NMF model architecture. For each user and item, a onehot vector is constructed and fed to the user and item embedding layers of GMF and MLP networks, respectively. Note that GMF and MLP sub-networks keep their own embeddings space (i.e., no embedding sharing). Let the user and item latent vectors bepand Require: Market set: 𝑀 = {M, M, · · · , M} Require: Step size parameters: 𝛼, 𝛽; Number of shots: 𝐾; q, respectively. The GMF network simply perform an element-wise product between the vectors and uses a linear single-layer feedforward network to calculate the prediction, i.e.h(p⊙ q). Here, hdenotes the weights of the output layer. For individual training of GMF, the prediction^𝑦is calculated using an activation function over the output layer’s output. This implements a generalized matrix factorization network. For the MLP sub-network, the user and item vectors are concatenated and fed to a deep feed-forward network for learning the interactions between user and items, i.e.𝑧= p⊕ q. Every layer of the deep network, takes the output of the previous layer zand calculates the output, i.e.Wz+ b.Wandbdenote the weight matrix and bias vector of each layer. Here, we use ReLU as activation function and calculate^𝑦for individual training. The NMF model simply initialize the network with individual pretrained parameters and for the output layer it concatenates the output layer of GMF and MLP with a hyper-parameter determining the trade-o. For the parameter initialization of our pre-training for market-agnostic NMF model, i.e.𝜃initialization, we simply concatenate the data from all target markets (a pair of markets in our study) and train the model. For the loss function,L, we use the binary cross entropy between the target and the model output. Algorithm 1 lines 2-10 present our market-agnostic NMF pretraining. The general framework for meta-learning considers a probability distribution over tasks. Given the highly imbalanced training data that each market oers and our nal goal to learn a set of generalized network parameters that works for each individual market, we consider equal task sampling across markets. To this end, we simply iterate over each market instead of sampling randomly (line 3). During the meta-training with𝐾-shot setting, for each market the model is trained such that it can adapt itself with only𝐾 samples from the target market. This pushes the model parameters such that they broadly become applicable to each individual market. For this purpose, we sample two𝐾sized batches of the user-item interaction and perform adaptation and evaluation (lines 4 and 7). Considering the NMF model with parameters𝜃, with the adaptation step on marketMthe model parameters become𝜃. With a single gradient update,𝜃= 𝜃 − 𝛼∇LNMF(𝜃 )in which𝛼is the NMF model’s learning rate (line 5-6). The meta-learning optimizes the NMF parameters across markets with the following meta-objective. The meta-optimization across markets are calculated using the 𝐾evaluation user-item interactions on each market with a metalearning step size,𝛽. This updates the original model parameters such that few gradient steps can tune the parameters to a specic target market (line 9). After obtaining the general internal representations using our market-agnostic pre-training step, we conduct a series of operations for preparing a model that is market-specic for the desired target market. We call this step “forking” mainly due to the sharing of bottom parts and initialization of the middle parts of the marketspecic model with the pre-trained model. The general schema of the forking operation is shown in Fig. 3 for our FOREC algorithm. As it can be seen, from the market-agnostic NMF model trained onusandde, we fork market-specic NMF models for each of the models, shown on the right and left sides of the gure. Assuming the MLP part of the NMF model containing𝑚layers, and one additional layer for the NMF model, our objective with forking is to maximize the reusability of the general parameters. To this end, Raghu et al. [41]studies the similarity between an adapted model’s layers and the general model and suggests that the main body of the network barely changes and all the adaptation happens in the head layers of the network. Inspired by this nding, we freeze layers up to layer𝑘of the MLP network(1 ≤ 𝑘 ≤ 𝑚), the only layer of the GMF network, as well as the user and item embeddings learned with each of the sub-networks. Given that freezing some part of the network limits the capacity of the network for learning market-specic parameters, we add𝑛new market-specic layers on top of the original tower-style feed-forward network right after the NMF layer to increase the network’s capacity. We call these new layers as MarketHead layers. We believe that our forking operation provides a network for balancing between the general market and target market-specic parameters after the nal ne-tuning. The𝑘 and𝑛values are experimentally explored on a few pairs and xed for every market-specic forking in our experiments. The forking operation for each target market is shown in line12of Algorithm 1. Further experimental details are given in Section 6.1. Over the forking step, we obtain a new market-specic NMF network for the desired target market that the bottom part of the network is frozen for any update for providing generalized internal features, middle part initialized with the general market that could easily adapted, and the nal part that randomly initialized and needs further training. One advantage of such a design is that it facilitates the maintenance of the entire network with the availability of new interactions on other market(s). Having the new market-specic model forked and initialized as described, we simply ne-tune the model using only the data from the target market. We keep the loss function the same for this part. However, one could easily change the loss function with this step to better adapt the market needs in the target market. The ne-tuning operation for each target market is shown in line 13 of Algorithm 1. Dataset.We use XMarket dataset for our experimental evaluations. We specically focus onElec.category across8markets presented in Table 1. We prepare our data similar to single-market experimental setup in the literature [1–3,42,53]. For the ratings, we ltered items and users that there exist less than ve transactions. We follow a long line of literature and use leave-one-out evaluation [7, 15, 17, 18, 24, 25, 30]. Compared methods.In order to show the eectiveness of our method, we employ the following models on each target market: • GMF, MLP, NMF:The Generalized Matrix Factorization (GMF), Multi-Layer Perceptron (MLP) and Neural Matrix Factorization (NMF) models from [17] trained using only the target market. • GMF++, MLP++, NMF++:The simplest way of leveraging the cross-market data is to train the model on the interactions inside both the source and target markets by sharing the item and user representations. We equally sample from both markets in the training phase—equal number of training interactions from each market is used. We experimentally observed that this training provides higher performance compared to simple concatenation of both markets. • DDTCDR: CDRandCMRhave similarities as discussed in Sec. 2. To test whether high performingCDRmethods can be used to eectively solve theCMRproblem, we include the algorithm proposed in [30], as one of the recent strongCDRmethods in our comparison. As the assumption withCDRis that the set of users are shared across two domains, the original model connects the user features between a pair of MLP networks using an orthogonal transformation matrix. We adapt the model into CMRby connecting the item features between two market’s MLP networks. We performed a similar modications for the CoNet’s network structure proposed by Hu et al. [18]for theCDRproblem. We only report DDTCDR due to its consistent superiority compared to CoNet and the space limitations. • MAML:Meta-learning is widely used in the recommendation literature for variety of problem settings—see Section 2. Here, we adapt the learning paradigm to theCMRby employing theMAML framework providing model-agnostic solution for meta-learning. OurMAMLtraining phase is described in Algorithm 1 lines 1-10. Here, after the training phase, we perform a single pass with𝐾shots of sampled interactions from the validation split of the target market and fast adapt the pre-trained model parameters to the target market. We observed no further improvements with more number of passes on the adaptations. This baseline provides the sole importance of our MAML adaptation to the CMR problem. • NMF-FOREC:In order to show the impact of our MAML-based pre-training with the FOREC, we pre-train our market-agnostic Table 3: Performance comparison of dierent CMR methods. Best performing method in each source selection scenario is shown in bold fonts. Signicance (Student’s t-test) with 𝑝 < 0.05 compared to MAML and NMF++ is indicated by respectively. model with only NMF++ method described above and perform forking and ne-tuning for each specic market. This baseline provides evidences in two ways; (1) The importance of MAML pre-training on the performance of the FOREC model, (2) The sole impact of forking and ne-tuning operations over a weak pre-training of internal features. Hyper-parameters. For GMF, MLP and NMF we follow [17] and set all the network structure and the latent factor dimension as suggested, i.e.[16, 64, 32, 16, 8]with8as embedding dimensions. For the optimizer, we use Adam [27] and select the learning rate and𝑙-regularization coecient hyper-parameters using the validation data of a subset of our markets. For the learning rate we considered{0.1, 0.05, 0.01, 0.005, 0 .001, 0.0001}and selected0.01 for MLP and NMF and0.005for the GMF model. For regularization we observed that1𝑒is the best among our consideration set. We use4negative training samples for each user re-sampled with each epoch. ReLU is used as the activation function. For DDTCDR we use the same hyper-parameters provided in the original implementation of [30]. As the model uses a preset embeddings, we employ the GMF model for the initialization. For MAML training, we set the fast learning rate𝛽 = 0.1selected from{0.5, 0.3, 0.1, 0.01, 0 .001} and the number of shots as20selected from{5, 20, 50, 100, 200}. For our FOREC’s market-agnostic part we use the same architecture as of NMF and employ the last three layers of the NMF network for forking and freeze the remaining bottom layers as well as the embeddings. For the market head layers, we considered three dierent layer sizes; (a) no new layers, (b) two16layers, and (c) 3 layers with [16, 32, 16], and evaluated experimentally using the validation data on a subset of markets. We selected (c) as our model’s market head layers for all experiments. In addition, we observed that setting higher𝑙-regularization with the ne-tuning step helps the overall performance, especially with lower resourced markets—we set it to 0.001 for all ne-tuning steps. Evaluation Metrics.We use Hit-Rate (HR) and nDCG as our evaluation metrics, commonly used in the literature. We report these metrics for a cut-o of 10. Similar to other works, we constructed the ground truth using the buying behavior by considering an item as relevant if the user gave a rating. In addition, we follow the literature and sample 99 negative items for each user in our evaluations. We compare our FOREC model to several baseline techniques discussed in Sec. 6.1 in terms of recommendation performance. In theory, each target market can be paired with each auxiliary source market. However, for the7markets that we consider, all possible pairings of source and target markets leads to49dierent settings. For better readability and due to space limitation, we report our results in the following scenarios for every target market: Figure 4: Impact of choosing dierent source markets on different target markets for dierent models. • Best-Src:Each of the parallel markets are once considered as source and the model is evaluated using the nDCG@10 on the validation set. For each method, the source market with the highest improvement on the validation set is selected. As such, for a single target market, the best source market for dierentCMR methods may be dierent. • Ave-Src:In order to provide a rough indicator of the safer choice of theCMRmethod on each target market along with an overall insight on dierent source selections, we report the average performance of each model using dierent source markets. • Fix-Src:We report the results when a xed base market is available (i.e.usmarket) and allCMRmethods can only use that market to improve the target market recommendation. Table 3 summarizes the evaluation results with and without crossmarket data. All three aforementioned source selection scenarios are reported respectively in the Table. The best performing method for each target market and each source selection scenario is shown by bold fonts. We observe that FOREC is the winner in almost all target markets and across all the source selection scenarios, except forjpandukin Best-Src scenario, andjpandinfor Fix-Src. In addition, we observe that even our simple baselines are able to utilize to some degree the cross-market data provided and improve the target market’s recommendation performance. This suggests the importance of cross-market training, even with a xed base market, for better recommendation systems across dierent local markets. It also suggests the importance of the model and source market selection for deployment purposes. When the source market is selected by the validation performance (i.e. Best-Src scenario), FOREC andMAMLnoticeably outperform otherCMRmethods. Foruk,MAMLis slightly better than FOREC, though the dierence is not substantial. Forjp,MAMLhas a higher nDCG, but FOREC is better in terms ofHR, suggesting that neither can be picked as the winner. For the other5markets, FOREC outperforms MAML both in terms of nDCG and HR. Looking at the results in Ave-Src scenario, FOREC outperforms other powerfulCMRmethods on average, meaning that given a source and target market, FOREC is a safer choice forCMR. This is further illustrated in Table 4 where we report for each target market the number of source markets for which each of FOREC orMAML is the winner along with the relative percentage improvement of FOREC compared toMAML. As it can be seen, over all49possible Table 4: nDCG Comparison of FOREC vs. MAML with dierent source markets. Values denote the percentage of relative improvement of FOREC compared with MAML. Positive values indicate FOREC superiority. Source Markets combinations of source-target pairs, only in5of themMAMLis slightly better than FOREC. As source markets,mxandukare the ones with the highest average improvement of FOREC over MAML, whiledemarket is the one where MAML performs closest to FOREC and even surpasses it in two target markets. Comparing the Fix-Src results, suggests that even thoughusis not the best source market for all target markets in terms of performance boost (see Best-Src scenario), comparing with the single market results, it can be observed that us helps all of the tested target markets and theCMRcomes to its highest performance when used with our FOREC model. In addition, we notice that NMF-FOREC is performing better in two markets compared to FOREC. We hypothesize that this might be due to the relative data size dierences between theusand each of these markets (see Table 2), performing the pre-training using MAML provides a limited added value. Comparing NMF-FOREC with FOREC, we observe that in most of the target markets, FOREC performs better. Based on the observations from Table 3, in most of the cases, MAML performs better than NMF++ and FOREC performs better than MAML. This shows the signicance of the MAML-based pre-training as well as forking and ne-tuning, as proposed by our model. Fig. 4 compares these four models more deeply. In this gure, the nDCG@10 improvement of theseCMRmodels over the NMF on single market are shown for dierent target markets. For each method in each target market, the distribution of nDCG@10 improvements based on dierent source markets is given as a box plot. This gure provides a better illustration of the trend between these four models described earlier. Finally, we observe that the adopted DDTCDR, as one of the state-of-the-artCDRmethods, does not perform well forCMR. In order to adopt aCDRmethod toCMRscenario, as described before, the users and items should be interchanged: the shared users in CDRare analogous to the shared items inCMR. This change of perspective looks natural at rst sight, but it introduces some issues. Here we discuss the issue. In the item recommendation problem, a number of past interactions of a user with items are used to predict her future interactions. This means that the evaluation is based on the accuracy of the predicted items for users. InCDR, the users are shared across domains and the interacted items from the source domain add to the per-user information of the target domain. Dierently, inCMR, the items are shared across markets Figure 5: Performance comparison of FOREC on (a) dierent user groups; and (b) dierent training data size. and the users are separate. Changing the perspective of users/items during training but having a xed evaluation based on the per-user predictions is the issue of naïvely adopting aCDRtoCMR. An interesting future direction would be to analyze the impact of such adoption for user recommendation in CMR scenarios. Here we study the impact of dierent cross-market training approaches on user groups in terms of their training data size. Following the work of Liu et al. [32]we split the users based on the interactions into ve groups. The users with the least number of interactions are named cold (average of 5 interactions), and the ones with the highest number of interactions are called warm (average of 13.3 interactions). We create ve equally-sized user splits and report the average nDCG@10 for each group in Fig. 5a. This gure contains the performance of four models on these ve user groups in thecamarket: single market NMF, NMF++, MAML and FOREC. Here, we useddeas the source market.We observe that MAML and FOREC almost uniformly improve the performance over the single market NMF in all ve user groups. Comparing FOREC with MAML, again we see a consistent improvement in all ve user groups with a slightly bigger gap toward the cold user group. This observation, together with other similar observations on our tests over other source-target pairs, provide experimental evidence that FOREC is suitable both for cold- and warm-start situations in the target market. NMF++, on the other hand, only helps the cold user groups, as can be seen in the gure. In order to study the impact of target training data size, for a given market pair, we gradually decrease the number of training interactions for each user. Starting from the full target market user-item interactions, each time we halve the training data for the target market until only10%of the target market’s training interactions remains for each user. We train four of our models (NMF, NMF++, MAML, FOREC) on each of these settings and test on the target market. Fig. 5b presents the resulting nDCG@10 performance on cawithukas the source market—similar observations made for a many pairs. As it can be seen, among the cross-market methods we see that FOREC and MAML are performing similarly especially when the target market’s size is extremely small. On the extremely small target market size (10%of the data), we see that single market NMF model as well as NMF++ are performing better. As more data becomes available, a signicant boost among cross-market methods are observed, especially with FOREC—as opposed to NMF with lazy reaction to the new data availability. With further data availability, we see the full superiority of MAML as well as FOREC. This analysis suggests that some minimum amount of training data in the target market is essential for the cross-market models that we examined in order to be able to make use of the auxiliary source market. We hypothesize that when the target market provides limited amount of training data the pre-training through the MAML approach shifts parameters more toward the source market compared to the NMF++ training approach. However, the general observation from this analysis could be that FOREC and MAML are relatively resilient to the amount of the target training data. We note that this requires further analysis which is out of the scope of our study. We studied the problem of recommending relevant products to users in relatively resource-scarce markets by leveraging data from similar or richer-in-resource auxiliary markets. To this aim, we introduced a large-scale real-life dataset, named as XMarket, providing product information and reviews on18Amazon marketplaces featuring52.5million user-item interactions. We hypothesized and showed through extensive experiments on7target markets that data from one market can actually be used to improve the performance in another. Our model, named as FOREC, demonstrates robust eectiveness, consistently improving the performance on target markets compared to competitive baselines selected for our analysis. In particular, FOREC improves on average 24% and up to 50% in terms of nDCG@10, compared to the NMF baseline. Our analysis and experiments suggest specic future directions in this research area. We show that models that are designed for CDRare not necessarily suitable for the market adaptation problem setting. One interesting extension of our study could be designing models that are domain and market agnostic in the sense that they can consume the data across dierent markets and domains and leverage that for the improved recommendation on a target market’s specic domain. In addition, we believe that data ltering or selection across markets could potentially be helpful for the models we discussed in our study. Moreover, using data augmentation techniques to generate synthetic ratings for target markets [6,48] could be a potential solution for the extreme low-resource markets, i.e. cold-start markets. We believe that many potentially interesting problems are yet to be explored in the CMR area. Acknowledgments.This work was supported in part by the Center for Intelligent Information Retrieval and in part by the NWO Innovational Research Incentives Scheme Vidi (016.Vidi.189.039), the NWO Smart Culture - Big Data / Digital Humanities (314-99301), the Elsevier and NWO (612.001.551), and the H2020-EU.3.4. SOCIETAL CHALLENGES - Smart, Green And Integrated Transport (814961). Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reect those of the sponsors.