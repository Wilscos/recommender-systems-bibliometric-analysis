 properties, which has led to proposals that signiﬁcantly improve overall system performance by reducing DRAM access latency and power consumption. In addition to improving system performance, a deeper understanding of DRAM technology via characterization can also improve device reliability and security. These can be seen with the recent discoveries of 1) DRAM-based true random number generators (TRNGs), a method for generating true random numbers using DRAM devices which can be used in many applications, 2) DRAM-based physical unclonable functions (PUFs), a method for generating unique device-dependent keys for identiﬁcation and authentication, and 3) the RowHammer vulnerability, a phenomenon where repeatedly accessing a DRAM row can cause failures in unaccessed neighboring DRAM rows. characterizes many modern commodity DRAM devices and shows that by exploiting DRAM access timing margins within manufacturer-recommended DRAM timing speciﬁcations, we can signiﬁcantly improve system performance, reduce power consumption, and improve device reliability and security. First, we characterize DRAM timing parameter margins and ﬁnd that certain regions of DRAM can be accessed faster than other regions due to DRAM cell process manufacturing variation. We exploit this by enabling variable access times depending on the DRAM cells being accessed, which not only improves overall system performance, but also decreases power consumption. Second, we ﬁnd that we can uniquely identify DRAM devices by the locations of failures that result when we access DRAM with timing parameters reduced below speciﬁcation values. Because we induce these failures with DRAM accesses, we can generate these unique identiﬁers signiﬁcantly more quickly than prior work. Third, we propose a random number generator that is based on our observation that timing failures in certain DRAM cells are randomly induced and can thus be repeatedly polled to very quickly generate true random values. Finally, we characterize the RowHammer security vulnerability on a wide range of modern DRAM chips while violating the DRAM refresh requirement in order to directly characterize the underlying DRAM technology without the interference of refresh commands. We demonstrate with our characterization of real chips, that existing RowHammer mitigation mechanisms either are not scalable or suﬀer from prohibitively large performance overheads in projected future devices and it is critical to research more eﬀective solutions to RowHammer. Overall, our studies build a new understanding of modern DRAM devices to improve computing system performance, reliability and security all at the same time. Characterization of real DRAM devices has enabled ﬁndings in DRAM device To advance DRAM-based discoveries and mechanisms, this dissertation rigorously foremost, I am extremely grateful to my advisor, Onur Mutlu, who has generously mentored and guided me since my sophomore year of college. His passion for Computer Architecture and his research initially caught my interest during a seminar course lecture, and his exemplary feedback, encouragement, and ongoing support helped me to adopt his passion into my own work as well. I am grateful to have experienced, ﬁrst-hand, the thought and care Onur puts into both his teaching and his research, and I have learned greatly through many iterations of paper submissions, conference talks, course exams, and lectures with him. Onur has also provided countless opportunities for collaboration within the SAFARI research group and industrial collaborators, which provided me with many new experiences during my exciting and unique PhD experience, as well as made this thesis possible. and Saugata Ghose, for their valuable feedback and stimulating discussions. group. I could not have done it without Minesh Patel, a great friend that truly made my PhD experience enjoyable. He brought an immense wealth of knowledge, enforced high research standards, and fostered my enjoyment of hard liquors. I am very thankful for Can Firtina, an endless source of entertainment, unforgettable experiences, and shelter. I am grateful for Hasan Hassan who on many occasions kept me company in the oﬃce late into the night. I am thankful for Damla Senol Cali, my ﬁrst friend in SAFARI with whom I had to navigate the new ﬁeld of bioinformatics and learn Onur’s research process. I also want to thank Giray Yaglikci for his friendship and company on hikes around the world. for being both great friends and colleagues. I want to especially thank Yoongu Kim, Hongyi Xin, Donghyuk Lee, Rachata Ausavarungnirun, Yixin Luo, Saugata Ghose, Vivek Seshadri, and Kevin Chang, for their mentorshop during my formative years I have many people to thank for their support during my PhD journey. First and I am grateful to the members of my PhD committee, James Hoe, Derek Chiou, I am immensely grateful to have found many great friends in the SAFARI research I would like to acknowledge all past and current members of our research group in SAFARI. I thank Can Firtina, Giray Yaglikci, Nastaran Hajinazar, Geraldo De Oliveira, and Ivan Fernandez-Vega for gracefully welcoming my invasion of their oﬃce space and providing a fun working environment. I also thank all others for their discussions, feedback, collaboration, and support: Arash Tavakkol, Jawad Haj-Yahya, Mohammed Alser, Roknoddin Azizibarzoki, Nika Mansourighiasi, Lois Orosa, Juan Gomez Luna, Amirali Boroumand, Jisung Park, Nandita Vijaykumar, Ivan Puddu, Ataberk Olgun, Nisa Bostanci, Rahul Bera, Konstantinos Kanellopoulos, and Taha Shahroodi. Meier, Jared Zerbe, Jung-Sik Kim, Heonjae Ha, Seung Lee, Gihong Kim, Taehyun Kim, Augustin Hong, Can Alkan, and countless others during my PhD journey. They have all provided great insight, mentorship, and support. during my time at Microsoft Research, who provided me with a stimulating environment and an industrial perspective on system security. I sincerely thank Microsoft for this opportunity. the National Institutes of Health (grant HG006004) and SAFARI Research Group’s industrial partners for respectively the ﬁnancial support and the gift funding they have provided that have contributed to works during my PhD. journey were worth more than I can express on paper. I want to particularly thank Jimmy Lee, Ho-Gyun Choi, Noelle Jung, Stephanie Chen, Justine Kim, and Matt Yin for always being there. encouragement, and love. I am especially grateful to have met and worked with Tyler Huberty, Stephan I would also like to thank my internship mentors, Stefan Saroiu and Alec Wolman, I would like to thank the National Science Foundation (grants 1212962 and 1320531), To my many friends and family, your support and encouragement throughout my Finally, I want to thank my parents Hyong and Anita for their unwavering support, 3 Solar-DRAM: Reducing DRAM Access Latency by Exploiting the 4 The DRAM Latency PUF: Quickly Evaluating Physical Unclonable Functions by Exploiting the Latency-Reliability Tradeoﬀ in Modern 5.5 D-RaNGe: A DRAM-based TRNG . . . . . . . . . . . . . . . . . . . 103 5.6 D-RaNGe Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 5.7 Comparison with Prior DRAM TRNGs . . . . . . . . . . . . . . . . . 115 6 Revisiting RowHammer: An Experimental Analysis of Modern De- 5.8 Other Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 5.9 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 5.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 6.1 RowHammer: DRAM Disturbance Errors . . . . . . . . . . . . . . . . 123 6.2 Motivation and Goal . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 6.3 Experimental Methodology . . . . . . . . . . . . . . . . . . . . . . . . 125 6.4 RowHammer Characterization . . . . . . . . . . . . . . . . . . . . . . 132 6.5 Implications for Future Systems . . . . . . . . . . . . . . . . . . . . . 145 6.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 6.7 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 6.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 7.1 Implementing All Proposed Techniques on the Same System . . . . . 159 7.2 Cost-Beneﬁt Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 8.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 8.2 Future Research Directions . . . . . . . . . . . . . . . . . . . . . . . . 164 8.2.1Reducing DRAM Latency by Exploiting Diﬀerent Timing Pa- 8.3 Final Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . 169 2-3Command sequence for reading data from DRAM and the state of a 3-2Probability of the ﬁrst access to a newly-activated row going to a 3-3Data pattern dependence of the proportion of local bitlines with activa- 3-4 Temperature eﬀects on a local bitline’s F 3-5 F 3-6Weighted speedup improvements of Solar-DRAM, its three individual components, and FLY-DRAM over baseline LPDDR4 DRAM, evaluated over various 4-core workload mixes from the SPEC CPU2006 benchmark 4-1Average DRAM retention PUF evaluation time vs. temperature shown for three selected memory segment sizes for each manufacturer. Average DRAM latency PUF evaluation time (Section 4.6.2) is shown as a 4-2Distributions of Jaccard indices calculated across every possible pair of PUF responses across all tested PUF memory segments from each of 4-3Distributions of Jaccard indices calculated between PUF responses of 4-4Distribution of the Intra-Jaccard index range values calculated between many PUF responses that a PUF memory segment generates over a 5-2Data pattern dependence of DRAM cells prone to activation failure 5-3 Eﬀect of temperature variation on failure probability . . . . . . . . . 101 5-4 Density of RNG cells in DRAM words per bank. . . . . . . . . . . . . 110 5-5 Distribution of TRNG throughput across chips. . . . . . . . . . . . . 113 6-2RowHammer bit ﬂip coverage of diﬀerent data patterns (described in Section 6.3.3) for a single representative DRAM chip of each type-node conﬁguration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 6-3Hammer count (HC ) vs. RowHammer bit ﬂip rate across DRAM type-node conﬁgurations. . . . . . . . . . . . . . . . . . . . . . . . . . 135 6-4Distribution of RowHammer bit ﬂips across row oﬀsets from the victim row. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 6-5Distribution of the number of RowHammer bit ﬂips per 64-bit word for each DRAM type-node conﬁguration. . . . . . . . . . . . . . . . . . . 139 6-6Number of hammers required to cause the ﬁrst RowHammer bit ﬂip (HC) per chip across DRAM type-node conﬁgurations. . . . . . . 141 6-7Hammer Count (left y-axis) required to ﬁnd the ﬁrst 64-bit word containing one, two, and three RowHammer bit ﬂips. Hammer Count Multiplier (right y-axis) quantiﬁes the HC diﬀerence between every two points on the x-axis (as a multiplication factor of the left point to the right point). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 6-8Eﬀect of RowHammer mitigation mechanisms on a) DRAM bandwidth overhead (note the inverted log-scale y-axis) and b) system performance, as DRAM chips become more vulnerable to RowHammer (from left to right). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 4.1The number of tested PUF memory segments across the tested chips 4.3Percentage of PUF memory segments per chip with Intra-Jaccard index ranges <0.1 or 0.2 over a 30-day period. Median [minimum, maximum] 4.4Percentage of good memory segments per chip across manufacturers. 5.2 Comparison to previous DRAM-based TRNG proposals. . . . . . . . 115 6.1 Summary of DRAM chips tested. . . . . . . . . . . . . . . . . . . . . 127 6.2Fraction of DDR3 DRAM chips vulnerable to RowHammer when H C < 150k. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 6.3Worst-case data pattern for each DRAM type-node conﬁguration at 50C split into diﬀerent manufacturers. . . . . . . . . . . . . . . . . . 134 6.4LowestH Cvalues (×1000) across all chips of each DRAM type-node conﬁguration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 6.5Percentage of cells with monotonically increasing RowHammer bit ﬂip probabilities as H C increases. . . . . . . . . . . . . . . . . . . . . . . 144 6.6 System conﬁguration for simulations. . . . . . . . . . . . . . . . . . . 151 properties, which has lead to proposals that signiﬁcantly improve overall system performance by reducing DRAM access latency and power consumption. In addition to improving system performance, a deeper understanding of DRAM technology via characterization can also improve device reliability and security. These can be seen with the recent discoveries of 1) DRAM-based true random number generators (TRNGs) [ numbers using DRAM devices which can be used in many applications, 2) DRAMbased physical unclonable functions (PUFs) [ 235 and authentication, and 3) the RowHammer vulnerability [ 265 accessing a DRAM row can cause failures in unaccessed neighboring DRAM rows. To advance this collection of discoveries and mechanisms, we rigorously characterize many modern commodity DRAM devices and show that by exploiting DRAM access timing margins and speciﬁcations, we can signiﬁcantly improve system performance, reduce power consumption, and improve device reliability and security. First, we Characterization of real DRAM devices has enabled ﬁndings in DRAM device ,321,250], a method for generating unique device-dependent keys for identiﬁcation ,270,285,324,340,360,87,145,184,239,341], a phenomenon where repeatedly characterize DRAM timing parameter margins and ﬁnd that certain regions of DRAM can be accessed faster than other regions due to DRAM cell process manufacturing variation. We exploit this by enabling variable access times depending on the DRAM cells that are being accessed, which not only improves overall system performance, but also decreases power consumption. Second, with further characterization, we ﬁnd that we can uniquely identify DRAM devices by the locations of failures that result when we access DRAM with timing parameters reduced below speciﬁcation values. Because we induce these failures with DRAM accesses, we can generate these unique identiﬁers signiﬁcantly quicker than prior work. Third, we propose a random number generator that is based on our observation that timing failures in certain DRAM cells are randomly induced and can thus be repeatedly polled to very quickly generate true random values. Finally, we characterize the RowHammer security vulnerability on a wide range of modern DRAM devices while violating the DRAM refresh requirement in order to directly characterize the underlying DRAM technology without the interference of refresh commands. ing DRAM device characteristics, we can signiﬁcantly improve system performance and enhance system security and reliability. chips to make novel observations on chip properties and use these observations to improve system performance and enhance system security and reliability. We demonstrate across four works, that by understanding per-chip error characteristics using a proﬁling mechanism, we can develop mechanisms that exploit chip-dependent error proﬁles to improve system performance or enhance system security and reliability. Access characterization and exploits each of our novel observations to signiﬁcantly and robustly Our thesis statement is as follows:By rigorously understanding and exploit- In line with our thesis statement, we use rigorous characterization of real DRAM The ﬁrst mechanism that we develop based on our observations,Subarray-optimized LatencyReduction DRAM (Solar-DRAM), builds on our detailed experimental reduce DRAM access latency. The key ideas of Solar-DRAM are to issue 1) DRAM reads with reduced t weak DRAM cells that are likely to fail under reduced t with reduced t is weak using a per-chip static proﬁle of local bitlines, which we experimentally ﬁnd to be reliable across time. Compared to state-of-the-art LPDDR4 DRAM, SolarDRAM provides signiﬁcant system performance improvement while maintaining data correctness. that reducing DRAM read access latency below the reliable datasheet speciﬁcations using software-only system calls results in error patterns that can be used as unique identiﬁers. We demonstrate that users can further enhance the reliability of the unique identiﬁers using an error proﬁle of the DRAM chip, which enables users to select regions of DRAM that are better suited for evaluating PUFs. We experimentally demonstrate, using 223 modern LPDDR4 DRAM chips, that the DRAM latency PUF satisﬁes all of the requirements of an eﬀective runtime-accessible PUF. In particular, a DRAM latency PUF can be evaluated in 88.2ms on average across all devices at all operating temperatures. We show that, for a constant DRAM capacity overhead of 64KiB, the DRAM latency PUF’s average (minimum, maximum) evaluation time speedup over the DRAM retention PUF [ even lower temperatures. acterization results that true random numbers can be extracted from access latency failures with high throughput. D-RaNGe consists of two steps: 1) identifying speciﬁc DRAM cells that are vulnerable to activation failures using a low-latency proﬁling step and 2) generating a continuous stream (i.e., constant rate) of random numbers by repeatedly inducing activation failures in the previously-identiﬁed vulnerable cells. D-RaNGe runs entirely in software and is capable of immediately running on any commodity system that provides the ability to manipulate DRAM timing parameters The second mechanism, the DRAM latency PUF, exploits our novel observation C and 1426x (868x, 1783x) at 55C, with exponentially increasing speedups at The third mechanism, D-RaNGe, exploits the novel observation from our charwithin the memory controller [ must be exposed without any hardware changes to the commodity DRAM device (e.g., similarly to SoftMC [ on most existing systems today. node generations, that the DRAM-based security vulnerability, RowHammer, is getting worse as device feature size reduces. This means that the number of activations needed to induce a RowHammer bit ﬂip also reduces, to as few as 9.6k in the most vulnerable chip we tested. We then use our characterization results to demonstrate how ﬁve state-of-the-art RowHammer mitigation mechanisms do not scale to support the degrees of RowHammer vulnerability that we expect to see in future devices. We conclude by discussing various methods for improving RowHammer mitigation for future DRAM devices. Finally, we demonstrate via characterization of many DRAM chips and technology This dissertation makes the following key contributions: 1.Using 282 LPDDR4 DRAM modules from three major DRAM manufacturers, we extensively characterize the eﬀects of multiple testing conditions (e.g., DRAM temperature, DRAM access latency parameters, data patterns written in DRAM) on activation failures. We demonstrate the viability of mechanisms that exploit variation in access latency of DRAM cells by showing that cells that operate correctly at reduced latency continue to operate correctly at the same latency over time. That is, a DRAM cell’s activation failure probability is not vulnerable to signiﬁcant variation over short time intervals. (a)We present data across our DRAM modules, that activation failures exhibit (b)We demonstrate that tcan be greatly reduced (i.e., by 77%) for DRAM (c)We ﬁnd that across SPEC CPU2006 benchmarks, DRAM accesses to closed (d)We propose Solar-DRAM, a mechanism that exploits our three key ob- 2.We introduce the DRAM latency PUF, a new class of DRAM PUFs, that is based on the deliberate violation of manufacturer-speciﬁed DRAM latency parameters. DRAM latency PUFs can be implemented with no additional hardware overhead on any commodity oﬀ-the-shelf (COTS) system that permits software-controlled manipulation of DRAM access latencies at the memory controller (e.g., [191, 1, 12]). (a)Using experimental data from 223 real LPDDR4 DRAM chips, we exten- (b)We experimentally show that the DRAM latency PUF signiﬁcantly outper- 3.We introduce D-RaNGe, a new methodology for extracting true random numbers from a commodity DRAM device at high throughput and low latency. The key idea of D-RaNGe is to use DRAM cells as entropy sources to generate true random numbers by accessing them with a latency that is lower than manufacturer-recommended speciﬁcations. (a)Using experimental data from 282 state-of-the-art LPDDR4 DRAM devices (b)We evaluate the quality of D-RaNGe’s output bitstream using the standard 4.We provide the ﬁrst rigorous RowHammer failure characterization study of a broad range of real modern DRAM chips across diﬀerent DRAM types, technology node generations, and manufacturers. We experimentally study 1580 DRAM chips (408×DDR3, 652×DDR4, and 520×LPDDR4) from 300 DRAM modules (60×DDR3, 110×DDR4, and 130×LPDDR4) and present our RowHammer characterization results for both aggregate RowHammer failure rates and the behavior of individual cells while sweeping the hammer count (HC ) and stored data pattern. (a)Via our rigorous characterization studies, we deﬁnitively demonstrate that (b)We demonstrate, based on our rigorous evaluation of ﬁve state-of-the-art (c)We evaluate an ideal refresh-based mitigation mechanism that selectively on DRAM organization, operations, and failure mechanisms. Chapter 3 presents solar-DRAM, a mechanism for reducing DRAM access latency. Chapter 4 presents the DRAM Latency PUF, a fast and eﬃcient method for generating unique identiﬁers in commodity DRAM. Chapter 5 presents D-RaNGe, a method for quickly and eﬃciently generating true random numbers in DRAM. Chapter 6 presents our experimental study of RowHammer on several technology node generations of DRAM chips. Finally, Chapter 7 presents conclusions and future research directions that are enabled by this dissertation. This thesis is organized into 7 chapters. Chapter 2 describes necessary background observations and mechanism for reducing DRAM access latencies. We refer the reader to prior works [ 193 for more detail. where each memory controller interfaces with a DRAM channel to perform read and write operations. As we show in Figure 2-1 (left), a DRAM channel has its own I/O bus and operates independently of other channels in the system. To achieve high memory capacity, a channel can host multiple DRAM modules by sharing the I/O bus between the modules. A DRAM module implements a single or multiple DRAM ranks. Command and data transfers are serialized between ranks in the same channel due to the shared I/O bus. A DRAM rank consists of multiple DRAM chips that operate in lock-step, i.e., all chips simultaneously perform the same operation, but they do so on diﬀerent bits. The number of DRAM chips per rank depends on the data bus width of the DRAM chips and the channel width. For example, a typical We describe the DRAM organization and operation necessary for understanding our In a typical system conﬁguration, a CPU chip includes a set of memory controllers, system has a 64-bit wide DRAM channel. Thus, four 16-bit or eight 8-bit DRAM chips are needed to build a DRAM rank. chically organized to maximize storage density and performance. We describe each level of the hierarchy of a modern DRAM chip. right). The chip communicates with the memory controller through the I/O circuitry. The I/O circuitry is connected to the internal command and data bus that is shared among all banks in the chip. row decoder partially decodes the address of the accessed DRAM row to select the corresponding DRAM subarray. A DRAM subarray is a 2D array of DRAM cells, where cells are horizontally organized into multiple DRAM rows. A DRAM row is a set of DRAM cells that share a wire called the wordline, which the local row decoder of the subarray drives after fully decoding the row address. In a subarray, a column of cells shares a wire, referred to as the bitline, that connects the column of cells to a sense ampliﬁer. The sense ampliﬁer is the circuitry used to read and modify the data of a DRAM cell. The row of sense ampliﬁers in the subarray is referred to as the local row-buﬀer. To access a DRAM cell, the corresponding DRAM row ﬁrst needs to be copied into the local row-buﬀer, which connects to the internal I/O bus via the global row-buﬀer. At a high-level, a DRAM chip consists of billions of DRAM cells that are hierar- A modern DRAM chip is composed of multiple DRAM banks (shown in Figure 2-1, Figure 2-2a illustrates the organization of a DRAM bank. In a bank, the global and access transistor. A DRAM cell stores a single bit of information based on the charge level of the capacitor. The data stored in the cell is interpreted as a “1” or “0” depending on whether the charge stored in the cell is above or below a certain threshold. Unfortunately, the capacitor and the access transistor are not ideal circuit components and have charge leakage paths. Thus, to ensure that the cell does not leak charge to the point where the bit stored in the cell ﬂips, the cell needs to be periodically refreshed to fully restore its original charge. DRAM chip. To perform a read or write operation, the memory controller ﬁrst needs to open a row, i.e., copy the data of the cells in the row to the row-buﬀer. To open a row, the memory controller issues an activate (ACT) command to a bank by specifying the address of the row to open. The memory controller can issue ACT commands to diﬀerent banks in consecutive DRAM bus cycles to operate on multiple banks in parallel. After opening a row in a bank, the memory controller issues either a READ or a WRITE command to read or write a DRAM word (which is typically equal to 64 bytes) within the open row. An open row can serve multiple READ and WRITE requests without incurring precharge and activation delays. A DRAM row typically contains 4-8 KiBs of data. To access data from another DRAM row in the same bank, the memory controller must ﬁrst close the currently open row by issuing a Figure 2-2b illustrates a DRAM cell, which is composed of a storage capacitor The memory controller issues a set of DRAM commands to access data in the precharge (PRE) command. The memory controller also periodically issues refresh (REF) commands to prevent data loss due to charge leakage. from a DRAM cell. command. Each step takes a certain amount of time to complete, and thus, a DRAM command is typically associated with one or more timing constraints known as timing parameters. It is the responsibility of the memory controller to satisfy these timing parameters in order to ensure correct DRAM operation. involved in a read operation. Each DRAM cell diagram corresponds to the state of the cell at exactly the tick mark on the time axis. Each command (shown in purple boxes below the time axis) is issued by the memory controller at the corresponding tick mark. Initially, the cell is in a precharged state the cell is disconnected from the bitline since the wordline is not asserted and thus the access transistor is oﬀ. The bitline voltage is stable at perturbed towards the voltage level of the cell capacitor upon enabling the access transistor. Figure 2-3: Command sequence for reading data from DRAM and the state of a DRAM cell during each related step. We describe DRAM operation by explaining the steps involved in reading data In Figure 2-3, we show how the state of a DRAM cell changes during the steps To read data from a cell, the memory controller ﬁrst needs to perform row activation by issuing an ACT command. During row activation ( the wordline that connects the storage capacitor of the cell to the bitline by enabling the access transistor. At this point, the capacitor charge perturbs the bitline via the charge sharing process. Charge sharing continues until the capacitor and bitline voltages reach an equal value of begins driving the bitline towards either V the perturbation in the charge sharing step. This step, which ampliﬁes the voltage level on the bitline as well as the cell is called charge restoration. Although charge restoration continues until the original capacitor charge is fully replenished ( memory controller can issue a READ command to safely read data from the activated row before the capacitor charge is fully replenished. A READ command can reliably be issued when the bitline voltage reaches the voltage level V the read occurs after the bitline reaches V interval t the DRAM manufacturer to ensure that their DRAM chip operates safely as long as the memory controller obeys the t DRAM standard [ of a wrong value. restored. A cell is expected to be fully restored when the memory controller satisﬁes a time interval dictated by t accessed row. A subsequent activation of the row can then result in the reading of incorrect data from the cells. PRE command to close the currently-open row to prepare the bank for an access to another row. The cell returns to the precharged state ( parameter t ampliﬁers from fully driving the bitline back to elapses, the bitline voltage may be below V, which can lead to the reading To return a cell to its precharged state, the voltage in the cell must ﬁrst be fully may lead to insuﬃcient amount of charge to be restored in the cells of the Once the cell is successfully restored (4○), the memory controller can issue a to be activated with too small amount of charge in its cells, potentially preventing the sense ampliﬁers to read the data correctly. the DRAM timing parameters deﬁned in the DRAM speciﬁcation are not violated. Violation of the timing parameters may lead to incorrect data to be read from the DRAM, and thus cause unexpected program behavior [ In this work, we study the failure modes due to violating DRAM timing parameters and explore their application to reliably generating true random numbers. associated with DRAM commands for correct operation. We deﬁne access latency failures as failures that occur due to accessing a DRAM module with any reduced timing parameter. In this dissertation, we focus on activation failures, which is a special case of access latency failures, caused by reducing the t the bitline to V two modes of activation failure. The ﬁrst mode of activation failure results in transient failures in the returned data, but no failures in the data stored in the DRAM cells. In this case, the next access to the same row that satisﬁes the timing parameters would return correct data. Such a failure may happen when the bitline does not reach towards the same direction (i.e., full 0 or 1) as the charge-sharing phase has already started. such that future accesses (with default timing parameters) return failures. Such a failure may happen when, at the time the suﬃciently low. In this case, the read operation could signiﬁcantly disturb the bitline such that the sense ampliﬁer starts driving the bitline towards the opposite of the For correct DRAM operation, it is critical for the memory controller to ensure that As we describe in Section 2.4, the memory controller must satisfy timing parameters An activation failure occurs due to insuﬃcient time for the sense ampliﬁer to drive prior to the read operation but the sense ampliﬁer continues to drive the bitline The second mode of activation failure destroys the data stored in a DRAM cell original direction. We observe both of the activation failure modes in our experiments with real LPDDR4 DRAM modules. (for each timing parameter) due to two major reasons: 1) design (architectural) diﬀerences [ to the sense ampliﬁers than an otherwise-equivalent cell can operate correctly with a lower t the sense ampliﬁers is lower. Similarly, a cell that happens to have a larger capacitor (due to manufacturing process variation) can operate reliably with tighter timing constraints than a smaller cell elsewhere in the same chip [191]. locations within and across chips [ the manufacturer-published timing parameters are chosen to ensure reliable operation of the worst-case cell in any acceptable device at the worst-case operating conditions (e.g., highest supported temperature, lowest supported voltage). This results in a large safety margin (or, guardband) for each timing parameter, which prior work shows can often be reliably reduced at typical operating conditions [55, 191, 53]. results in failures, with increasing error rates observed for larger reductions in timing parameter values [ occur because, with reduced timing parameters, the internal DRAM circuitry is not allowed enough time to properly perform its functions and stabilize outputs before the memory controller issues the next command (Section 2.4). Diﬀerent cells in the same DRAM chip have diﬀerent reliable operation latencies Because manufacturing process variation occurs in random and unpredictable Prior work also shows that decreasing the timing parameters too aggressively systems. In this chapter, we rigorously characterize the eﬀects of reducing DRAM access latency on 282 state-of-the-art LPDDR4 DRAM modules. As found in prior work on older DRAM generations (DDR3), we show that regions of LPDDR4 DRAM modules can be accessed with latencies that are signiﬁcantly lower than manufacturerspeciﬁed values without causing failures. We present novel data that 1) further supports the viability of such latency reduction mechanisms and 2) exposes a variety of new cases in which access latencies can be eﬀectively reduced. Using our observations, we propose a new low-cost mechanism, Solar-DRAM, that 1) identiﬁes failure-prone regions of DRAM at reduced latency and 2) robustly reduces average DRAM access latency while maintaining data correctness, by issuing DRAM requests with reduced access latencies to non-failure-prone DRAM regions. We evaluate Solar-DRAM on a DRAM latency is a major bottleneck for many applications in modern computing wide variety of multi-core workloads and show that for 4-core homogeneous workloads, Solar-DRAM provides an average (maximum) system performance improvement of 4.31% (10.87%) compared to using the default ﬁxed DRAM access latency. important workloads exhibit low access locality and thus are unable to eﬀectively exploit row-buﬀer locality. In other words, these workloads issue a signiﬁcant number of DRAM accesses that result in bank (i.e., row buﬀer) conﬂicts, which negatively impact overall system performance. Each access that causes a bank conﬂict requires activating a closed row, a process whose latency is dictated by the t memory controller must wait for t bank. To reduce the overhead of bank conﬂicts, we aim to reduce the t parameter while maintaining data correctness. activation failures 1) are highly constrained to speciﬁc columns of DRAM cells across an entire DRAM bank, i.e., global bitlines, and regions of memory that are closer to the row decoders, 2) can only aﬀect cells within the cache line granularity of bits that is ﬁrst requested in a closed row, and 3) propagate back into DRAM cells and become permanent failures in the stored data. proﬁles DRAM global bitlines as weak or strong using a one-time proﬁling step. During execution, FLY-DRAM relies on this static proﬁle to access weak or strong global bitlines with default or reduced t whether a strong bitline will ever become a weak bitline or vice versa. This analysis is necessary to demonstrate the viability of relying on a static proﬁle of global bitlines to guarantee data integrity. Second, the authors present a characterization of activation failures on an older generation of DRAM (DDR3). Third, the proposed mechanism, Many prior works [119,172,187,237,242,243,381,173,192,190] show that various Prior Observations.In a recent publication, Chang et al. [55] observe that Based on these observations, Chang et al. propose FLY-DRAM, which statically Unfortunately, [55] falls short in three aspects. First, the paper lacks analysis of FLY-DRAM, does not fully take advantage of all opportunities to reduce t modern DRAM modules (as we show in Section 3.3). rigorous characterization of activation failures on state-of-the-art LPDDR4 DRAM modules, 2) demonstrate the viability of mechanisms that rely on a static proﬁle of weak cells to reduce DRAM access latency, and 3) devise new mechanisms that exploit more activation failure characteristics on state-of-the-art LPDDR4 DRAM modules to further reduce DRAM latency. tructure to characterize state-of-the-art LPDDR4 DRAM chips [ controlled chamber. Our testing environment gives us precise control over DRAM commands and t addition, we determined the address mapping for internal DRAM row scrambling so that we could study the spatial locality of activation failures in the physical DRAM chip. We test for activation failures across a DRAM module using Algorithm 1. The key idea is to access every cache line across DRAM, and open a closed row on each access. This guarantees that we test every DRAM cell’s propensity for activation failure. Algorithm 1: DRAM Activation Failure Testing conditions. The for loops (Lines 3-4) ensure that we test all DRAM cache lines. For Given the shortcomings of prior work [55],our goalis to 1) present a more To analyze DRAM behavior under reduced tvalues, we developed an infras- We ﬁrst write a known data pattern to DRAM (Line 2) for consistent testing each cache line, we 1) refresh the row containing it (Line 5) to induce activation failures in cells with similar levels of charge, 2) precharge the row (Line 6), and 3) activate the row again with a reduced t then ﬁnd and record the activation failures in the row (Line 8), by comparing the read data to the data pattern the row was initialized with. We experimentally determine that Algorithm 1 takes approximately 200ms to test a single bank. from three major manufacturers in a thermally-controlled chamber held at 55 control the ambient temperature precisely using heaters and fans. A microcontrollerbased PID loop controls the heaters and fans to within an accuracy of 0.25 a reliable range of 40 ambient temperature using a separate local heating source. This local heating source probes local on-chip temperature sensors to smooth out temperature variations due to self-induced heating. DRAM modules from three major DRAM manufacturers. We make a number of key observations that 1) support the viability of a mechanism that uses a static proﬁle of weak cells to exploit variation in access latencies of DRAM cells, and 2) enable us to devise new mechanisms that exploit more activation failure characteristics to further reduce DRAM latency. by visually inspecting bitmaps of activation failures across many DRAM banks. A representative 1024x1024 array of DRAM cells with a signiﬁcant number of activation failures is shown in Figure 3-1. Using these bitmaps, we make three key observations. Observation 1: Unless otherwise speciﬁed, we perform all tests using 2y-nm LPDDR4 DRAM chips We present our extensive characterization of activation failures in modern LPDDR4 We ﬁrst analyze the spatial distribution of activation failures across DRAM modules that the granularity at which we see bitline-wide activation failures is a subarray. This is because the number of consecutive rows with activation failures on the same bitline falls within the range of expected modern subarray sizes of 512 to 1024 [ We hypothesize that this occurs as a result of process manufacturing variation at the level of the local sense ampliﬁers. Some sense ampliﬁers are manufactured “weaker” and cannot amplify data on the local bitline as quickly. This results in a higher probability of activation failures in DRAM cells attached to the same “weak” local bitline. While manufacturing process variation dictates the local bitlines that contain errors, the manufacturer design decisions for subarray size dictates the number of cells attached to the same local bitline, and thus, the number of consecutive rows that contain activation failures in the same local bitline. Vendor B and C’s DRAM modules consist of 512 DRAM rows, while subarrays from Vendor A’s DRAM modules consist of 1024 DRAM rows. that within a set of subarray rows, very few rows (<0.001%) exhibit a signiﬁcantly diﬀerent set of cells that experience activation failures compared to the expected set of cells. We hypothesize that the rows with signiﬁcantly diﬀerent failures are rows that are remapped to redundant rows (see [ manufactured (indicated in Figure 3-1). accessing a row. We make two observations (also seen in prior work [ We next study the granularity at which activation failures can be induced when When accessing a row with low t, the errors in the row are constrained to the DRAM cache line granularity (typically 32 or 64 bytes), and only occur in the aligned 32 bytes that is ﬁrst accessed in a closed row (i.e., up to 32 bytes are aﬀected by a single low t to cache lines on a system with 64 byte cache lines. cache line accessed in a closed DRAM row is the only cache line in the row that we observe to exhibit activation failures. We hypothesize that DRAM cells that are subsequently accessed in the same row have enough time to have their charge ampliﬁed and completely restored for correct sensing. DRAM banks from all 282 of our DRAM modules. We collect the proportion of weak subarray columns per bank across two banks from each of our DRAM modules across all three manufacturers. For a given bank, we aggregate the subarray columns that contain activation failures when accessed with reduced t temperatures. and C have an average/maximum (standard deviation) proportion of weak subarray columns of 3.7%/96% (12%), 2.5%/100% (6.5%), and 2.2%/37% (4.3%), respectively. We ﬁnd that on average, banks have a very low proportion of weak subarray columns, which means that the memory controller can issue DRAM accesses to most subarray columns with reduced t Ramulator [ from the SPEC CPU2006 benchmark suite [ the ﬁrst access to a newly-activated row is to a particular cache line oﬀset within the row. For a given cache line oﬀset (x-axis value), the probability is presented as a distribution of probabilities, found across the SPEC CPU2006 workloads. Each distribution of probabilities is shown as a box-and-whisker plot We next study the proportion of weak subarray columns per bank across many We next study how a real workload might be aﬀected by reducing t. We use ACTIVATE(i.e., accesses that can induce activation failures) across 20 workloads Figure 3-2: Probability of the ﬁrst access to a newly-activated row going to a particular cache line oﬀset within the row. (y-axis) is logarithmically scaled. accesses to a newly-activated DRAM row requests the 0 with a maximum (average) proportion of 22.2% (6.6%). This indicates that simply reducing t signiﬁcantly improve overall system performance. We hypothesize that the 0 line is accessed with a signiﬁcantly higher probability due to a signiﬁcant number of streaming accesses to DRAM rows in our evaluated workloads. Streaming accesses would result in accesses ﬁrst to the 0 accesses to the remaining cache lines in the row in a consecutive manner. bitlines, we analyze the number of local bitlines containing activation failures with diﬀerent data patterns written to the DRAM array. Similar to prior works [ that extensively describe DRAM data patterns, we study a total of 40 unique data patterns: solid 1s, checkered, row stripe, column stripe, 16 walking 1s, and the inverses of all 20 aforementioned data patterns. vation failures over 16 iterations with diﬀerent data patterns across representative DRAM modules from three DRAM manufacturers. This data was gathered with 100 iterations of Algorithm 1 per data pattern, but we present only the ﬁrst 16 iterations To understand the eﬀects of DRAM data patterns on activation failures in local Figure 3-3 shows the cumulative number of unique local bitlines containing acti- Figure 3-3: Data pattern dependence of the proportion of local bitlines with activation failures found over 16 iterations. to highlight the accumulation rate of local bitlines with failures in earlier iterations. For a given iteration, we calculate the coverage of each data pattern as: where unique failures in a given iteration but not observed to contain failures in any prior iteration when using a speciﬁc data pattern, and total number of unique local bitlines observed to contain failures at any iteration, with any data pattern. The coverage of a single data pattern indicates the eﬀectiveness of that data pattern to identify the full set of local bitlines containing activation-failure-prone DRAM cells. (i.e., 16 walking 1 patterns and their inverses) ﬁnds a similar coverage of local bitlines over many iterations. Given Observation 8, we have already simpliﬁed Figure 3-3 by grouping the set of 16 walking 1 patterns and plotting the distribution of coverages of the patterns as a box-and-whisker-plot (WALK1). We have done the same for the set of 16 walking 0 patterns (WALK0). exhibits the highest coverage of activation-failure-prone local bitlines across all three DRAM manufacturers. We hypothesize that the random data results in, on average across DRAM cells, the worst-case coupling noise of a DRAM cell and its neighbors. This is consistent with prior works’ experimental observations that the random data pattern causes the highest rate of charge leakage in cells [260, 206, 155]. number of activation failures across a DRAM module (at reduced t similar observations as prior work [ number of activation failures across a DRAM device and DRAM temperature. However, when we analyze the activation failure rates at the granularity of a local bitline, we observe correlations between DRAM temperature and the number of activation failures in a local bitline. cells with activation failures, we study activation failures on a local bitline granularity with a range of temperatures. For a set of 5 between 55 probability of failure across all our DRAM modules. We indicate a local bitline’s probability of failure (F where cells indicates the number of iterations out of the 100 tested iterations in which cell and num facturer. Each point in the ﬁgure represents the F T on the x-axis (i.e., the baseline temperature) and the F at temperature T + 5 on the y-axis (i.e., 5 represent the range of F T + 5 for every local bitline whose F We next study the eﬀect of DRAM temperature (at the granularity of 5C ) on the To determine the eﬀect of temperature on a local bitline’s probability to contain _iters is the total number of iterations that the DRAM module is tested for. Figure 3-4 aggregates our data across 30 DRAM modules from each DRAM manuvalues at the baseline temperature are binned at the granularity of 1% and on the x-axis. We aggregate each set of F show how the F box-and-whisker plot with a blue box, orange whiskers, black whisker ends, and red medians. higher than F x = y line). Thus, F are cases (i.e., <25% of all data points) where the F temperature. We conclude that in order to ﬁnd a comprehensive set of weak subarray columns, we must proﬁle for activation failures with a range (e.g., 40 DRAM temperatures. We sweep t we study the correlation of t make two observations analogous to those made by Chang et al. [ 11: We observe no activation failures when using t of the temperature. The ﬁrst t manufacturer-recommended values. This demonstrates the additional guardband that manufacturers place to account for process variation. Observation 12: We observe that a small reduction (i.e., by 2ns) in t the number of activation failures. Observation 10:We observe that Fat temperature T + 5 tends to be We next study the eﬀects of changing the value of ton activation failures. ﬁrst to study the eﬀects of changing the t individual cell. time when accessed with a reduced t 0% and 100% when t n + 4. We hypothesize that the large changes in activation failure probability is due to the coarse granularity with which we can change t infrastructure limitations). For this very reason, we cannot observe gradual changes in the activation failure probability that we expect would occur at smaller intervals of the probability of activation failure of a DRAM cell to future work. 120 that there is a well-known phenomenon called variable retention time (VRT), where variation occurs over time in DRAM circuit elements that results in signiﬁcant and sudden changes in the leakage rates of charge from a DRAM cell. This aﬀects the retention time of a DRAM cell over short-term intervals, resulting in varying retention failure probabilities for a given DRAM cell over the span of minutes or hours. To see if a similar time-based variation phenomenon aﬀects the probability of an activation failure, we sample the F study how F the change in F given local bitline, every pair of sample F plotted as (x,y) pairs. We collect these data points across all local bitlines in 30 DRAM modules (10 of each DRAM manufacturer) and plot the points. All points sharing the same F 14: diagonal axis (where x equals y). This indicates that the F In addition to repeating analyses on older generation modules [55], we are the . We leave the exploration of correlating ﬁner granularity changes of twith Many previous DRAM retention characterization works [21,155,206,266,344, We ﬁnd that the box-and-whisker plots show a tight distribution around the remains highly similar (correlation r = 0.94) across time. This means that a weak local bitline is very likely to remain weak and a strong local bitline is very likely to remain strong across time. Thus, we can identify the set of weak local bitlines once and that set would remain constant across time. To determine the number of iterations we expect to proﬁle for to ﬁnd a comprehensive set of weak local bitlines, we run iterations of Algorithm 1 for each bank until we only observe either zero or one failing bit in a local bitline that has never been observed to fail before in the tested bank. At this point, we say that we have found the entire set of local bitlines containing activation failures. to ﬁnd the entire set of local bitlines containing activation failures diﬀers signiﬁcantly across chips and manufacturers. The average/maximum (standard deviation) number of iterations required to ﬁnd the entire set of local bitlines for manufacturers A, B, and C is 843/1411 (284.28), 162/441 (174.86), and 1914/1944 (26.28), respectively. that t the time required for the sense ampliﬁers to amplify the data in DRAM cells to an I/O readable value (V We next study the eﬀects of reduced ton write operations. We hypothesize eﬀects of reducing t DRAM modules. First, we sweep the value of t known data pattern across DRAM. We then read every value in the DRAM array with the default t this process 100 times using the random data pattern for each of our DRAM modules. We observe activation failures only when t can reliably issue DRAM write operations to our LPDDR4 DRAM modules with a signiﬁcantly reduced t latency failures in DRAM (Section 3.3), we propose Reduction DRAM (Solar-DRAM), a mechanism that robustly reduces t DRAM read and write requests. activation failures and memory access patterns. These three components are pure hardware approaches implemented within the memory controller without any DRAM changes and are invisible to applications. that we exploit is that activation failures are highly constrained to some (or few) local bitlines (i.e., only 3.7%/2.5%/2.2% of subarray columns per bank are weak on average for DRAM manufacturers A/B/C respectively. See Section 3.3.1), and the local bitlines with activation-failure-prone cells are randomly distributed across the chip (not shown). Given the known spatial distribution of activation failures, the memory controller can issue memory requests with varying activation latency depending on whether or not the access is to data contained in a “weak” local bitline. To enable such a mechanism, Solar-DRAM requires the use of a weak subarray column proﬁle Based on our key observations from our extensive characterization of activation Solar-DRAM consists of three components that exploit various observations on Component I: Variable-latency cache lines (VLC).The ﬁrst key observation that identiﬁes local bitlines as either weak or strong. However, since activation failures aﬀect DRAM only at the granularity of a cache line (Section 3.3.1), Solar-DRAM needs to only store whether or not a column of cache-line-aligned DRAM cells within a subarray, i.e., a subarray column, contains a weak local bitline. when accessed with a reduced t i.e., we did not observe signiﬁcant changes within 14 days of testing (Section 3.3.5). This novel observation is necessary to ensure that a proﬁle of weak local bitlines will not change over time and thus allows Solar-DRAM to rely on a static proﬁle. subarray columns with the default t reduced t still a very low probability (i.e., error. Fortunately, we ﬁnd this probability to be low enough such that employing error correction codes (ECC) [ DRAM chips, would transparently mitigate low-probability activation failures in strong columns. tion 3.3.1, that the memory controller accesses the 0 DRAM row with the highest probability compared to the rest of the cache lines. Thus, we would like to devise a mechanism that reduces access latency (i.e., t speciﬁcally to the 0 newly-activated row is most aﬀected by t that scrambles column addresses such that the 0 get mapped to weak subarray columns. Given a weak subarray column proﬁle, we identify the global column (i.e., the column of cache-line-aligned DRAM cells across a full DRAM bank) containing the fewest weak subarray columns, called the strongest The second key observation that we exploit is that the failure probability of a cell, Given a static proﬁle of weak subarray columns, we can safely access the weak Component II: Reordered subarray columns (RSC).We observe in Secglobal column. We then scramble the column address bits such that the 0 line for each bank maps to the strongest global column in the bank. We perform this scrambling by changing the DRAM address mapping at the granularity of the global column, in order to to reduce the overhead in address scrambling. that we exploit in Solar-DRAM is that write operations do not require the default (i.e., 4ns, as measured with our experimental infrastructure) for all write operations to DRAM. of Algorithm 1, recording all subarray columns containing observed activation failures. As we observe in Section 3.3, there are various factors that aﬀect a local bitline’s probability of failure (F fying a comprehensive proﬁle of weak subarray columns for a given DRAM module. First, we use our observation on the accumulation rate of ﬁnding weak local bitlines (Section 3.3.5) to determine the number of iterations we expect to test each DRAM module. However, since there is such high variation across each DRAM module (as seen in the standard deviations of the distributions in Observation 11), we can only provide the expected number of iterations needed to ﬁnd a comprehensive proﬁle for DRAM modules of a manufacturer, and the time to proﬁle depends on the module. We show in Section 3.3.2 that no single data pattern alone ﬁnds a high coverage of weak local bitlines. This indicates that we must test each data pattern (40 data patterns) for the expected number of iterations needed to ﬁnd a comprehensive proﬁle of a DRAM module for a range of temperatures (Section 3.3.3). While this could result in many iterations of testing (on the order of a few thousands; see Section 3.3.5), this is a one-time process on the order of half a day per bank that results in a reliable proﬁle of weak subarray columns. The required one-time proﬁling can be performed in two ways: 1) the system running Solar-DRAM can proﬁle a DRAM module when Component III: Reduced latency for writes (RLW).The ﬁnal observation value (Section 3.3.6). To exploit this observation, we use a reliable, reduced t To obtain the static proﬁle of weak subarray columns, we run multiple iterations the memory controller detects a new DRAM module at bootup, or 2) the DRAM manufacturer can proﬁle each DRAM module and provide the proﬁle within the Serial Presence Detect (SPD) circuitry (a Read-Only Memory present in each DIMM) [ memory controller, we encode each subarray column with a bit indicating whether or not to issue accesses to it with a reduced t controller loads the weak subarray column proﬁle once into a small lookup table in the DRAM channel’s memory controller. references the lookup table with the subarray column that is being accessed. The memory controller determines the t the bit found in the lookup table. We then present our multi-core simulation results for our chosen system conﬁgurations. System Conﬁgurations. system using Ramulator [ in CPU-trace-driven mode. We analyze various real workloads with traces from the SPEC CPU2006 benchmark [ the conﬁguration of our evaluated system. We use the standard LPDDR4-3200 [ timing parameters as our baseline. To give a conservative estimate of Solar-DRAM’s performance improvement, we simulate with a 64B cache line and a subarray size of To minimize the storage overhead of the weak subarray column proﬁle in the We ﬁrst discuss our evaluation methodology and evaluated system conﬁgurations. 1024 rows. Solar-DRAM Conﬁguration. a variety of diﬀerent DRAM modules with unique properties, we simulate varying 1) the number of weak subarray columns per bank between n = 1 to 512, and 2) the chosen weak subarray columns in each bank. For a given n, i.e., weak subarray column count, we generate 10 unique proﬁles with n randomly chosen weak subarray columns per bank. The proﬁle indicates whether a subarray column should be accessed with the default t ns). We use these proﬁles to evaluate 1) Solar-DRAM’s three components (described in Section 3.4.1) independently, 2) Solar-DRAM with all its three components, 3) FLY-DRAM [55], and 4) our baseline LPDDR4 DRAM. to determine whether an access should be issued with a reduced or default t value. Reordered subarray columns (RSC) takes a proﬁle and maps the 0 line to the strongest global column in each bank. For a given proﬁle, this maximizes the probability that any access to the 0 a reduced t Variable latency cache lines (VLC), directly uses a weak subarray column proﬁle ns) (Section 3.3.6) for all write operations to DRAM. Solar-DRAM (Section 3.4.1) combines all three components (VLC, RSC, and RLW ). Since FLY-DRAM [ read requests at the granularity of the global column depending on whether a global column contains weak bits, we evaluate FLY-DRAM by taking a weak subarray column proﬁle and extending each weak subarray column to the global column containing it. Baseline LPDDR4 uses a ﬁxed t present performance improvement of the diﬀerent mechanisms over this LPDDR4 baseline. system throughput [ and 20 heterogeneous mixes of 4-core workloads randomly combined from the set of workloads in the SPEC CPU2006 benchmark suite [ <weak subarray column count, weak subarray column proﬁle, mechanism, workload mix>, we aggregate all weighted speedup improvement results into a box-and-whisker plot. speedup improvement. Even when half of the subarray columns are classiﬁed as weak (which is very unrealistic and conservative, as our experiments on real DRAM modules show), Solar-DRAM improves performance by 4.03% (7.71%) for heterogeneous and 3.36% (8.80%) for homogeneous workloads. In the ideal case, where there are 0 weak subarray columns per bank and thus, the memory controller issues all memory Figure 3-6: Weighted speedup improvements of Solar-DRAM, its three individual components, and FLY-DRAM over baseline LPDDR4 DRAM, evaluated over various 4-core workload mixes from the SPEC CPU2006 benchmark suite. Figure 3-6 plots the improvement in weighted speedup[301], which corresponds to We make four key observations. First, Solar-DRAM provides signiﬁcant weighted accesses with a reduced t for heterogeneous and 4.31% (10.87%) for homogeneous workloads. Second, each individual component of Solar-DRAM improves system performance. RLW is the best alone: it improves performance by 2.92% (5.90%) for heterogeneous and 2.25% (6.59%) for homogeneous workloads. Because RLW is independent of the number of weak subarray columns in a bank, its weighted speedup improvement is constant regardless of the number of weak subarray columns per bank. Third, Solar-DRAM provides higher performance improvement than each of its components, demonstrating that the combination of VLC, RSC, and RLW is synergistic. Fourth, Solar-DRAM provides much higher performance improvement than FLY-DRAM. This is because Solar-DRAM 1) exploits the observation that all write requests can be issued with a greatly reduced t the granularity of the local bitline rather than the global bitline. This means that for a single weak cache line in a subarray, Solar-DRAM issues read requests with default FLY-DRAM would issue read requests with default t column across the full bank. For this very same reason, we also observe that VLC alone outperforms FLY-DRAM. Fourth, Solar-DRAM enables signiﬁcantly higher performance improvement on DRAM modules with a high rate of activation failures, where FLY-DRAM provides no beneﬁt. Because FLY-DRAM categorizes columns across the entire bank as strong or weak, even a low activation failure rate across the DRAM chip results in a high number of cache lines requiring the default t timing parameter in FLY-DRAM. We experimentally observe the average proportion of weak subarray columns per bank to be 3.7%/2.5%/2.2% for DRAM manufacturers A/B/C (Section 3.3.1). Even at such a low proportion of weak subarray columns (i.e., 38/26/23 subarray columns out of 1024 subarray columns in our evaluated DRAM conﬁguration), we expect the performance beneﬁt of FLY-DRAM to be well below 1.6% (i.e., the median performance beneﬁt when we evaluate FLY-DRAM with 16 weak subarray columns in Figure 3-6 across all workload mixes) for DRAM manufacturers B and C, and 0% for DRAM manufacturer A. We conclude that Solar-DRAM’s three only to cache lines in the subarray column containing the weak cache line, while components provide signiﬁcant performance improvement on modern LPDDR4 DRAM modules over LPDDR4 DRAM and FLY-DRAM. to the mechanisms they take advantage of, as follows. Static Variation. and compared to FLY-DRAM [ DRAM. Das et al. [ to Solar-DRAM. Operational Factors. advantage of changes in operational factors such as temperature [ These works are orthogonal to Solar-DRAM since they reduce latency in response to changes in factors that are independent of latency variations inherent to the DRAM module. Access Locality. and reorganizes DRAM accesses to allow for higher locality [ to reduce average DRAM access latency. These can be combined with Solar-DRAM for further latency reduction. Modiﬁcations to DRAM Architecture. 213 structure of DRAM to reduce latency. Solar-DRAM requires no changes to the DRAM chip. Software Support. optimizations to improve DRAM access locality and thus, decrease overall DRAM access latency. Solar-DRAM reduces the latency of the average memory access and would provide added beneﬁts to software optimizations. If the proﬁle of weak subarray columns is exposed to the compiler or the system software, the software could potentially use this device-level information to allocate latency-critical data at Many works seek to improve DRAM access latency. They can be classiﬁed according ,288,292,293,294,291,305,376,216,349] propose mechanisms that change the stronger locations in DRAM, while decreasing the hardware overhead of storing weak subarray column proﬁles in the memory controller. viability of such a mechanism on a real system. low-latency accesses are generally localized to speciﬁc local bitlines. However, there are low probability bit ﬂips that may occur outside of the "weak" local bitlines less predictably. Solar-DRAM relies on Error Correcting Code (ECC) hardware to handle such low probability bit ﬂips that are not encapsulated by the Solar-DRAM proﬁle. In DRAM devices without ECC hardware or margin in their ECC capability for additional bit ﬂips, applications using Solar-DRAM on such devices will likely see failures in their data. We expect future work to enhance Solar-DRAM by enabling the ability to either 1) comprehensively predict all weak subarray cache lines with a better proﬁling methodology, 2) predict low probability bit ﬂips and mitigate them before they occur, or 3) understand why these low probability bit ﬂips occur and eliminate them completely. for our data. These unknowns include 1) the percentage of DRAM modules that we characterize compared to the total number of available modules (billions), 2) the total number of DRAM chips available for a given DRAM type, technology, and process size, and 3) the inability to randomly sample DRAM modules. Without this knowledge, we are unable to provide conﬁdence intervals, or margins of sampling error. Therefore, the proﬁle storage sizes and performance beneﬁt distributions may not be representative of the average DRAM device. However, given that we were able to see the overarching trend of weak subarray bitlines in each DRAM chip that we tested, we are conﬁdent that this eﬀect is prevalent across chips of the DRAM types and process nodes that we tested and all chips should be able to beneﬁt from Solar-DRAM with low storage Solar-DRAM has a few limitations that must be considered when analyzing the First, our characterization in Solar-DRAM demonstrates that failures due to Second, a number of unknowns prevents us from providing sampling statistics overhead. tion. Thus, we are not able to conﬁdently determine how long a proﬁle will remain viable, and how often re-proﬁling will need to occur in order to reliably issue low latency accesses according to the Solar-DRAM proﬁle. However, we do show that a proﬁle will likely remain viable after 14 days of usage. In the worst case, the system may require proﬁling the DRAM chip every 14 days in order to create a reliable proﬁle. However, we believe this is a conservative estimate, and there may be more intelligent ways for identifying gradual changes to the proﬁle during normal execution. state-of-the-art LPDDR4 DRAM modules, 2) Solar-DRAM, whose key idea is to exploit our observations and issue DRAM accesses with variable latency depending on the target DRAM location’s propensity to fail with reduced access latency, and 3) an evaluation of Solar-DRAM and its three individual components, with comparisons to the state-of-the-art [ improvement over the state-of-the-art DRAM latency reduction mechanism across a wide variety of workloads, without requiring any changes to DRAM chips or software. Third, we did not account for long-term DRAM aging eﬀects during characteriza- We introduced 1) a rigorous characterization of activation failures across 282 real identify devices based on the uniqueness of their physical microstructures. DRAMbased PUFs have numerous advantages over PUF designs that exploit alternative substrates: DRAM is a major component of many modern systems, and a DRAM-based Physically Unclonable Functions (PUFs) are commonly used in cryptography to PUF can generate many unique identiﬁers. However, none of the prior DRAM PUF proposals provide implementations suitable for runtime-accessible PUF evaluation on commodity DRAM devices. Prior DRAM PUFs exhibit unacceptably high latencies, especially at low temperatures (e.g., >125.8s on average for a 64KiB memory segment below 55 unavailable during PUF evaluation. DRAM PUFs. The key idea is to reduce DRAM read access latency below the reliable datasheet speciﬁcations using software-only system calls. Doing so results in error patterns that reﬂect the compound eﬀects of manufacturing variations in various DRAM structures (e.g., capacitors, wires, sense ampliﬁers). Based on a rigorous experimental characterization of 223 modern LPDDR4 DRAM chips, we demonstrate that these error patterns 1) satisfy runtime-accessible PUF requirements, and 2) are quickly generated (i.e., at 88.2ms) irrespective of operating temperature using a real system with no additional hardware modiﬁcations. We show that, for a constant DRAM capacity overhead of 64KiB, our implementation of the DRAM latency PUF enables an average (minimum, maximum) PUF evaluation time speedup of 152x (109x, 181x) at 70 PUF and achieves greater speedups at even lower temperatures. device-speciﬁc signatures that can be generated repeatably and reliably. We refer to the process of generating a signature using a given set of input parameters as the evaluation of a PUF. The resulting signature reﬂects a device’s inherent, random physical variations introduced during manufacturing. This property guarantees that the signature is practically impossible to predict or replicate without access to the device itself [ security applications such as low-cost authentication mechanisms against system C), and they cause high system interference by keeping part of DRAM In this chapter, we introduce the DRAM latency PUF, a new class of fast, reliable A Physically Unclonable Function (PUF) maps a set of input parameters to unique, security attacks and prevention of integrated circuit (IC) counterfeiting [318, 364]. trusted server gives a device a challenge (i.e., a set of input parameters and conditions with which to evaluate a PUF), and veriﬁes the device’s PUF response (i.e., the signature generated by the PUF). A CR protocol generally consists of two phases: enrollment and authentication. Enrollment is a one-time setup phase in which a given device is analyzed, and all possible PUF responses are stored in the trusted server. Authentication occurs when an application running on the enrolled device requests escalated permissions from the trusted server to perform a secure action. The server provides a challenge to the application, which then evaluates the PUF with the requested parameters and returns the PUF response. If the response matches with the previously-enrolled response for the challenge, i.e., the golden key, authentication is successful. The CR can be done statically, where the PUF is evaluated only once before runtime (e.g., at bootup) or at runtime, where an application running on the enrolled device can evaluate a PUF on-demand [364]. (IC) identiﬁcation, exploiting manufacturing process variation among devices for disambiguating diﬀerent devices [ PUF evaluation techniques for diﬀerent substrates (e.g., ASICs, FPGAs, memories), exploiting manufacturing variation in diﬀerent components such as emerging memory technologies [ Circuit (ASIC) logic [ 309 382 329, 268, 322, 319]. 364 satisfying these characteristics 1) guarantee a level of robustness for disambiguating many devices and 2) are practically impossible for an attacker to duplicate without access to the physical device itself. In addition to these properties, a runtime-accessible PUFs are generally used in a challenge-response (CR) protocol [318], in which a PUFs for silicon devices were ﬁrst introduced as a method for integrated circuit ,221,338,371], Static Random Access Memory (SRAM) [107,126,127,67,32, ,359,19], and Dynamic Random Access Memory (DRAM) [318,154,117,328, PUFs must satisfy ﬁve key characteristics to be eﬀective in security applications [328, ,117,318,222]. We describe these characteristics in detail in Section 4.3.1. PUFs PUF, i.e., a PUF that is accessible online to an application running on an enrolled device, must 1) be easily evaluated with low latency to prevent unnecessary slowdown of the application requesting authentication, and 2) provide low system interference, i.e., minimize the disturbance PUF evaluation causes to other applications running on the same system. Section 4.3.2 describes the characteristics of ideal runtime-accessible PUFs. signiﬁcant interest for two key reasons: 1) DRAM is already widely used in a wide variety of modern systems [ large address space, which is on the order of Giga- or Tera-bytes, makes it naturally suitable for CR applications by providing a greater CR space relative to smaller components (e.g., SRAMs) [ PUF proposals exploit variations in DRAM start-up values [ latencies [ reliable PUF responses. make them unsuitable as runtime-accessible PUFs. PUFs that use DRAM start-up values [ cycle for every authentication. This requires either interrupting other applications using DRAM or restarting the entire system, which is likely infeasible at runtime. On the other hand, PUFs that exploit variation in write access latencies [ be evaluated at runtime. However, [ DRAM chip to allow ﬁne-grained manipulation of write latency [ changes to DRAM chips, rendering such proposals inapplicable to devices used in the ﬁeld today. In this chapter, we would like to design a new runtime-accessible PUF without modifying commodity DRAM chips. DRAM-based PUFs, henceforth called DRAM PUFs, have recently attracted 117], and DRAM cell retention failures [318,154,210,364] to generate Unfortunately, these prior DRAM PUF proposals have signiﬁcant drawbacks that 328] preclude runtime-accessible PUF evaluation by requiring a DRAM power Using cell charge retention failures and their resulting error patterns [207,113,206, 155 commodity devices today, since it does not require a power cycle or any modiﬁcations to DRAM chips. Unfortunately, such DRAM retention PUFs impose two major overheads. First, due to the 1) wide distribution of charge retention times across DRAM cells [ retention failures across a chip [ retention PUF takes on the order of minutes at 55 failures. The evaluation time increases exponentially as temperature decreases. Second, this means that DRAM refresh must be disabled for long periods of time. Because DRAM refresh can only be disabled for large regions of DRAM [ DRAM retention PUF on a small region of memory, i.e., a PUF memory segment, requires disabling refresh on the entire large memory region containing the PUF memory segment. However, to maintain the integrity of data inside the large region but outside of the PUF memory segment, all such data must be continuously refreshed with additional DRAM commands, which results in signiﬁcant system interference [ Based on extensive experimental analysis using 223 state-of-the-art LPDDR4 DRAM devices, we ﬁnd that DRAM retention PUFs are too slow for reasonable runtime operation, e.g., they have average evaluation times of 125.8s at 55 existing commodity DRAM devices, 2) satisﬁes all characteristics of an eﬀective runtime-accessible PUF, and 3) provides low-latency evaluation with low system interference across all operating conditions. PUF. Prior works present various diﬀerent metrics for deﬁning the eﬀectiveness of a PUF [ below. We then discuss two properties that we consider necessary for an eﬀective ,260,266] is the best candidate for runtime-accessible DRAM PUF evaluation in C using a 64KiB memory segment (Section 4.5). Our goalin this work is to develop a new runtime-accessible PUF that 1) uses In this section, we examine the desired properties of an eﬀective runtime-accessible 328,364,117,318,222]. We consolidate these metrics into ﬁve key properties runtime-accessible PUF. We refer to these seven properties when analyzing DRAM PUFs (Section 4.5 and 4.6). In Section 4.6, we show how DRAM latency PUFs overcome the weaknesses of DRAM retention PUFs based on a comparison of these properties between the two types of PUFs. be evaluated across a set of devices: response authentication. 1) systems that employ remote communication protocols to access devices via remote direct memory access (RDMA [ remote servers), 2) systems that have interchangeable/broken system components The following ﬁve key properties must be provided by any eﬀective PUF that can 1.Diﬀuseness: a single device is able to generate many unique and independent responses to diﬀerent input parameters [19, 90, 69, 129]. 2.Uniqueness: a single device can be uniquely identiﬁed among the set of devices [328, 364, 117, 355, 205, 129]. 3.Uniform Randomness: all possible PUF responses must be equally diﬀerent from each other [328, 364, 205, 129]. 4.Unclonability: it should be practically impossible for an adversary to construct a device that exhibits the same properties as another [222, 153, 355]. 5.Repeatability: given a set of input parameters, PUF evaluation results in the same PUF regardless of internal and external conditions (e.g., temperature, aging) [328, 364, 117, 355, 205, 129]. These ﬁve properties ensure that a PUF can be used eﬀectively for challenge- There are many important use cases for runtime-accessible PUFs. Examples include (e.g., SSD drives, external sensors, peripheral devices). In each of these systems, a connection/component can be maliciously swapped out during runtime so that a malicious device can be swapped in. One way to avoid such an attack is to utilize a low-overhead runtime-accessible PUF-based challenge-response mechanism that frequently authenticates the communicating devices. This enables re-authentication of the system components during each step of communication rather than just once at bootup time. More generally, a fast runtime-accessible PUF enables the protection of a system from attacks that exploit the fact that the time of check is diﬀerent from the time of use [364]. while the system is running without signiﬁcantly interfering with application execution and system operation. Thus, a runtime-accessible PUF must possess the following two key properties: parameters, we developed an infrastructure to characterize modern LPDDR4 [ DRAM chips. Our testing environment gives us precise control over the DRAM commands and DRAM timing parameters as veriﬁed with a logic analyzer probing the command bus. DRAM chips from three major manufacturers in a thermally-controlled chamber held at 45 using heaters and fans controlled via a microcontroller-based proportional-integralderivative (PID) loop to within an accuracy of 0.25 In order to be useful for runtime authentication, a PUF must be eﬀectively usable 1.Low Latency: PUF evaluation must be fast so that the application requesting authentication stalls for the smallest possible amount of time. 2.Low System Interference: PUF evaluation must not signiﬁcantly slow down concurrently-running applications. To analyze DRAM behavior with both reduced refresh rates and reduced timing We perfom all tests, unless otherwise speciﬁed, using a total of 223 2y-nm LPDDR4 C. For consistency across results, we stabilize the ambient temperature precisely to 55 using a separate local heating source. We utilize temperature sensors to smooth out temperature variations caused by self-induced heating. require no modiﬁcations to commodity DRAM chips. These works evaluate their proposals using DDR3 DRAM modules and ﬁnd that while the use of charge retention times in DRAM cells can result in repeatable PUFs, delays on the order of minutes are required to produce enough failures for uniquely identifying many devices. modern LPDDR4 DRAM modules. Our experimental results (Section 4.5.2) conﬁrm that DRAM retention PUFs can be eﬀectively implemented with commodity LPDDR4 DRAM devices. However, similarly to prior work [ required to evaluate retention PUFs is prohibitively long (e.g., on the order of minutes) at temperatures that are likely encountered under common-case operating conditions (e.g., 35 Algorithm 2. The DRAM retention PUF disables refresh for a period indicated by the wait (seg memory segment, the user must refresh the rows contained in the DRAM rank, but not in the PUF memory segment during the wait data in the memory segment after the wait returned for authentication (line 10). This PUF response is uniquely represented by the pattern of DRAM cells that fail in the memory segment after not being refreshed during the wait_time interval. C. We maintain DRAM temperature at 15C above ambient temperature Recent works [318,364,268,319,329] propose DRAM retention PUFs, which In this section, we evaluate prior proposals using our own infrastructure with 223 C-55C) [62, 191, 78]. We evaluate DRAM retention PUFs on our modern LPDDR4 devices, as shown in _time input parameter on a memory segment indicated by the segment ID _id) input parameter (line 3). In order to constrain retention failures to the PUF or banks [ runtime-accessible DRAM retention PUF using a given DRAM memory segment requires continuous refreshing of all rows that are within the same rank or bank but outside of the PUF memory segment. Doing so results in high system interference (see, e.g., [ the standard 64ms) required for repeatable retention PUF evaluation at common-case temperatures (e.g. 35 Algorithm 2 retention PUF evaluation on the DRAM retention PUF evaluation time. Based on extensive experimental data from 223 LPDDR4 DRAM chips, we ﬁnd that the evaluation time of a DRAM retention PUF exhibits a strong dependence on DRAM temperature during evaluation. With even just a 10 the evaluation time for the same PUF memory segment increases by 10x [ This is due to the direct correlation between retention failure rate and temperature. We reproduce the bit error rate (BER) vs. temperature relationship studied for DDR3 [ and LPDDR4 [ intervals of 30s, there is an exponential dependence of BER on temperature with an average exponential growth factor of 0.23 per 10 10x decrease in the retention failure rate with every 10 The memory controller can disable refresh only at the granularity of DRAM ranks 56,207]) that is greatly exacerbated by the long refresh intervals (e.g., 60s vs. In this section, we explore the eﬀects of DRAM temperature during DRAM and is consistent with prior work’s ﬁndings with older DRAM chips [ Due to the sensitivity of DRAM retention PUFs to temperature, a stable temperature is required to generate a repeatable PUF response. to prior works on DRAM retention PUFs, which disable DRAM refresh and wait for at least 512 retention failures to accumulate across a memory segment [ 1 shows the results of DRAM retention PUF evaluation times for three diﬀerent memory segment sizes (8KiB, 64KiB, 64MiB) across our testable DRAM temperature range (i.e., 55 manufacturer in order to isolate manufacturer-speciﬁc variation [ Figure 4-1 also shows, for comparison, the DRAM latency PUF evaluation time, which is experimentally determined to be 88.2ms on average for any DRAM device at all operating temperatures (see Section 4.6.2). Figure 4-1: Average DRAM retention PUF evaluation time vs. temperature shown for three selected memory segment sizes for each manufacturer. Average DRAM latency PUF evaluation time (Section 4.6.2) is shown as a comparison point. retention PUF across all manufacturers can be evaluated on average (minimum, maximum) in 40.6s (28.1s, 58.6s) using an 8KiB segment size. By increasing the memory segment size from 8KiB to 64KiB, we can evaluate a DRAM retention PUF in 13.4s (9.6s, 16.0s), and at 64MiB, in 1.05s (1.01s, 1.09s). However, at our lowest testable temperature (i.e., 55 To ﬁnd the evaluation time of DRAM retention PUFs, we use a similar methodology C-70C). Results are shown for the average across all tested chips from each We ﬁnd that at our maximum testing temperature of 70C, the average DRAM to 2.9 hours (49.7 minutes, 5.6 hours) using an 8KiB segment, 125.8s (76.6s, 157.3s) using a 64KiB segment, and 3.0s (1.5s, 5.3s) using a 64MiB segment. is prohibitively high for at least three reasons: 1) such high latency leads to very long application stall times and very high system interference, 2) since DRAM refresh intervals can be modiﬁed only at a rank/bank granularity, the memory controller must continuously issue extra accesses, during PUF evaluation, to each row inside the rank/bank but outside of the PUF memory segment, which causes signiﬁcant bandwidth performance and energy overhead, and 3) such a long evaluation time allows ample opportunity for temperature to ﬂuctuate, which would result in a PUF response with low similarity to the golden key, and thus, an unreliable PUF. perature. This is due to the temperature dependence of charge leakage in DRAM cell capacitors, and is a fundamental limitation of using DRAM retention failures as a PUF mechanism. Therefore, any devices operating at common-case operating temperatures (35 PUFs for runtime accessibility. In Sections 4.6.1 and 4.7.2, we describe the DRAM latency PUF in detail and show how it 1) provides a much lower evaluation time than the DRAM retention PUF, and 2) enables a reliably short evaluation time across all operating temperatures. (i.e., signiﬁcantly faster) at common-case operating temperatures by increasing the rate at which retention failures are induced. Given that ambient (i.e., environmental) temperature is ﬁxed, we can increase the rate of induced retention failures in two ways: 1) using a larger PUF memory segment in DRAM, or 2) accelerating the rate A DRAM retention PUF evaluation time on the order of even seconds or minutes In general, DRAM retention PUF evaluation time increases with decreasing tem- C-55C) [191,78,209] or below will have great diﬃculty adopting DRAM retention We explore if it is possible to make DRAM retention PUFs runtime-accessible of charge leakage using means other than increasing ambient temperature. in additional DRAM capacity overhead that does not scale favorably with decreasing temperatures. As shown in Section 4.5.2, the number of retention failures drops exponentially with temperature, so the PUF memory segment size required to compensate for the decreasing retention failure rate increases exponentially. Our experimental analysis in Figure 4-1 shows that at 55 on the order of tens of megabytes, a DRAM retention PUF cannot be evaluated in under 1 second. Assuming the exponential growth factor of 0.23 for DRAM BER as a function of temperature (found in Section 4.5.2), a corresponding PUF evaluation time of ~1s at 20 larger (i.e., hundreds of gigabytes). Thus, it is not cost-eﬀective (i.e., scalable) to naïvely increase the PUF memory segment size. ature can be done by either 1) making hardware modiﬁcations or 2) exploiting factors other than temperature that aﬀect charge leakage. Unfortunately, as we discuss in this section, there is no easy way to achieve these using commodity oﬀ-the-shelf (COTS) systems. increase the number of retention failures observed at a ﬁxed ambient temperature. For example, partial restoration of DRAM cells [ the PUF memory segment with reduced charge levels in order to exacerbate the number of retention failures observed with a given refresh interval. Similarly, other mechanisms in prior work (e.g., [ PUF evaluation time at common-case temperatures where DRAM retention PUFs are otherwise infeasible. However, these approaches require modiﬁcations in DRAM or the memory controller, and thus, cannot be used in COTS DRAM. DRAM chip [ at low ambient temperatures. However, these approaches require custom system Larger PUF memory segments.Using a larger PUF memory segment results Accelerating charge leakage.Accelerating charge leakage given a ﬁxed temper- In-DRAM hardware modiﬁcations proposed in prior work can be leveraged to System-level hardware modiﬁcations, such as adding a heating source local to the architectures, which contradicts our goal of designing a PUF for COTS systems. They may also open up system security and reliability concerns. on factors such as supply voltage [ 155 206 be intelligently manipulated to exacerbate the number of retention failures observed. Unfortunately, these eﬀects are either relatively weak to signiﬁcantly increase the number of observed retention failures (e.g., data pattern dependence), require system modiﬁcations to implement (e.g., voltage control [ control (e.g., VRT eﬀects). data loss throughout retention PUF evaluation (Section 4.5.1), DRAM refresh optimizations proposed in prior work [ can be used to increase the granularity of the refresh operation. While this approach could potentially eliminate the extra refresh operations altogether, these mechanisms come with their own hardware and runtime overheads that may diminish the beneﬁts of not having to issue the extra refresh commands during PUF evaluation. Many such mechanisms also require hardware modiﬁcations to either DRAM chips or memory controllers or both. PUF evaluation time for COTS DRAM devices today. While many approaches to improve evaluation time exist, they are all impractical in COTS systems due to 1) lack of applicability and scalability to common-case temperatures, 2) need for DRAM modiﬁcation, or 3) inherent diﬃculties in control. This motivates the need for a runtime-accessible PUF that is suitable across all temperature conditions and can be implemented on COTS DRAM devices today. Experimental studies on DRAM have shown that charge leakage rates are dependent ], and random charge ﬂuctuations known as variable retention time (VRT ) [366,271, ,266,155,260]. Analogously to temperature control, any of these quantities could In order to reduce the number of extra row refresh operations necessary to prevent We conclude that there is no good known way to optimize DRAM retention and low system interference across all operating temperatures, and 2) without any modiﬁcation to DRAM chips. To this end, we present the DRAM latency PUF, a new class of DRAM PUFs with these characteristics. In particular, a DRAM latency PUF provides low evaluation time at a wide range of operating temperatures (0 which includes common-case temperatures (35 signatures using the error pattern resulting from accessing DRAM with reduced timing parameters. These latency failures are inherently related to chip-speciﬁc random process variation introduced during manufacturing (Section 2.6), which allows us to use the failures as unique identiﬁers for each DRAM chip. To evaluate a DRAM latency PUF, we write known data into a ﬁxed-size memory segment (e.g., 4 DRAM rows parameters. The resulting failures form a pattern of bits unique to the tested device. the probability of cell failure is based on random variations in both the cell itself and any peripheral circuitry used to access the cell. This is due to the probabilistic behavior of circuit elements when timing requirements are violated. To ﬁnd a repeatable set of latency-failure-prone DRAM cells, each cell should be accessed multiple times with reduced timing parameters. In the case of reduced t of reading each cell to accumulate a reliable set of latency failures. Fortunately, as we show in Section 4.6.2, ﬁnding a reliable set of latency failures is a relatively fast process (i.e., it takes 88.2ms on average). the DRAM latency PUF. These variables deﬁne the tradeoﬀs between the DRAM latency PUF’s evaluation time and its eﬀectiveness. from diﬀerent parts of DRAM. Each segment results in unique error patterns and can Our goal is to develop a DRAM PUF that can be evaluated 1) with low latency Key Idea.The key idea of the DRAM latency PUF is to provide unique device ≈8KiB in our LPDDR4 DRAM chips) and read it back with reduced timing Probabilistic Nature.Inducing latency failures is a stochastic process in which Key Variables.We identify three key variables to optimize for when designing 1) Memory segment ID. DRAM PUFs can be evaluated using memory segments therefore be used for diﬀerent challenge-response pairs. In Section 4.7.3, we discuss how variation in process manufacturing causes some chips to have fewer memory segments (where fewer is worse) that are viable for DRAM latency PUF evaluation than others. uniquely identiﬁed at the cost of higher PUF evaluation time because more memory accesses are required to induce latency failures across the memory segment. With an experimental analysis of memory segment size based on data from 223 real DRAM chips (Section 4.6.1), we ﬁnd that a memory segment size of 8KiB is suﬃcient to ﬁnd enough latency failures for an eﬀective DRAM latency PUF. the amount of reduction in the chosen timing parameter result in diﬀerent error patterns (Section 2.6). This is because 1) diﬀerent timing parameters guard against diﬀerent underlying error mechanisms [ latency reduction exercise diﬀerent failure-prone bits [ control add more degrees of freedom to the DRAM latency PUF, further increasing its diﬀuseness (Section 4.6.1). PUF satisﬁes all requirements for 1) a reliable PUF (Section 4.3.1) and 2) runtimeaccessible PUF evaluation (Section 4.3.2) across all temperatures. We focus on with any other timing parameter whose timing violation results in failures (e.g., obtained by using a single timing parameter alone. DRAM chips, that the DRAM latency PUF satisﬁes each of the ﬁve characteristics of a desirable PUF discussed in Section 4.3.1. 2) Memory segment size. Larger memory segments allow more devices to be 3) DRAM timing parameters. Both using diﬀerent timing parameters and changing Throughout the rest of this section, we ﬁrst demonstrate that the DRAM latency -induced DRAM read errors in this work, but DRAM latency PUFs also work , t, t), thereby enabling a potentially larger challenge-response space than This section shows, with experimental results from 223 state-of-the-art LPDDR4 Diﬀuseness patterns [ diﬀerent memory segments provide diﬀerent challenge-response pairs. For example, our selected segment size of 8KiB (Section 4.7.3) in a 2GiB DRAM, oﬀers up to 256K prior DRAM PUFs [364, 318]. Uniqueness and Uniform Randomness across diﬀerent memory segments, we study a large number of diﬀerent memory segments from each of our 223 LPDDR4 DRAM chips (as speciﬁed in Table 4.1). Table 4.1: The number of tested PUF memory segments across the tested chips from each of the three manufacturers. uniqueness of a PUF, we use the notion of a Jaccard index [ work [ responses. The Jaccard index is determined by taking the two sets of latency failures of failures over the total number of unique errors in the two sets index value closer to 1 indicates a high similarity between the two PUF responses, and a value closer to 0 indicates uniqueness of the two. Thus, a unique PUF should have Jaccard index values close to 0 across all pairs of distinct memory segments. our metric for evaluating the similarity between PUF responses because the Jaccard index places a heavier emphasis on the diﬀerences between two large bitﬁelds. This is Diﬀerent memory segments within the same device result in diﬀerent error 191,190,58,55]. Given the large address space provided by modern DRAM, ) diﬀerent challenge-response pairs, which is on the same order of magnitude as To show the uniqueness and uniform randomness of DRAM latency PUFs evaluated For each memory segment, we evaluate the PUF 50 times at 70C. To measure the 364,282,17]. We use the Jaccard index to measure the similarity of two PUF ) from two PUF responses and computing the ratio of the size of the shared set We choose to employ the Jaccard index instead of the Hamming distance [115] as especially true in the case of devices that exhibit inherently lower failure rates. In the case of Hamming distance, calculating similarity between two PUF responses depends heavily on the number of failures found, and we ﬁnd this to be an unfair comparison due to the large variance in the number of failures across distinct memory segments. For example, consider the case where two memory segments each generate PUF responses consisting of a single failure in diﬀerent locations of a bitﬁeld comprised of 100 cells. The Hamming distance between these PUF responses would be 1, which could be mistaken for a match, but the Jaccard index would be calculated as a 0, which would guarantee a mismatch. Because we are more interested in the locations with failures than without, we use the Jaccard index, which discounts locations without failures. Throughout the rest of this chapter, we use the terms 1) Intra-Jaccard [ refer to the Jaccard index of two PUF responses from the same memory segment and 2) Inter-Jaccard [ diﬀerent memory segments. segment from any device from any manufacturer. To show that these characteristics hold for the DRAM latency PUF, we ensure that the distribution of Inter-Jaccard indices are distributed near 0. This demonstrates that 1) the error patterns are unique such that no two distinct memory segments would generate PUF responses with high similarity, and 2) the error patterns are distributed uniform randomly across the DRAM chip(s) such that the likelihood of two chips (or two memory segments) generating the same error pattern is exceedingly low. all possible pairs of PUF responses generated at the same operating temperature (70 from all tested memory segments across all chips from three manufacturers. The distribution of the Intra-Jaccard indices are also shown in red (discussed later in this section). The x-axis shows the Jaccard indices and the y-axis marks the probability of any pair of memory segments (either within the same device or across two diﬀerent devices) resulting in the Jaccard index indicated by the x-axis. We observe that the distribution of the Inter-Jaccard indices is multimodal, but the Inter-Jaccard index A PUF must exhibit uniqueness and uniform randomness across any memory Figure 4-2 plots, in blue, the distribution of Inter-Jaccard indices calculated between always remains below 0.25 for any pair of distinct memory segments. This means that PUFs from diﬀerent memory segments have low similarity. Thus, we conclude that latency-related error patterns approximate the behavior of a desirable PUF with regard to both uniqueness and uniform randomness. Figure 4-2: Distributions of Jaccard indices calculated across every possible pair of PUF responses across all tested PUF memory segments from each of 223 LPDDR4 DRAM chips. and Inter-Jaccard distributions of PUF responses from chips of a single manufacturer in subplots. Each subplot indicates the manufacturer encoding in the top left corner (A, B, C). From these per-manufacturer distributions, we make three major observations: 1) Inter-Jaccard values are quite low, per-manufacturer, which shows uniqueness and uniform randomness, 2) there is variation across manufacturers, as expected, and 3) Figure 4-2’s multimodal behavior for Inter- and Intra-Jaccard index distributions can be explained by the mixture of per-manufacturer distributions. We also ﬁnd that the distribution of Inter-Jaccard indices calculated between two PUF responses from chips of distinct manufacturers are tightly distributed close to 0 (not shown). Unclonability inherent to the chip (discussed in Section 2.6). Chips of the same design contain physical diﬀerences due to manufacturing process variation which occurs as a result of imperfections in manufacturing [ variations are inherent to each individual chip, as shown by previous work [ To understand manufacturer-related eﬀects, Figure 4-3 separately plots the Intra- We attribute the probabilistic behavior of latency failures to physical variation 55,54,188,170] and conﬁrmed by our experiments (not shown), and the pattern Figure 4-3: Distributions of Jaccard indices calculated between PUF responses of DRAM chips from a single manufacturer. of variations is very diﬃcult to replicate as it is created entirely unintentionally. Repeatability well a PUF memory segment can result in the same PUF response 1) at diﬀerent times or 2) under diﬀerent operating temperatures. For each of many diﬀerent memory segments, we evaluate a PUF multiple times and calculate all possible Intra-Jaccard indices (i.e., Jaccard indices between two PUF responses generated from the same exact memory segment). Because a highly-repeatable PUF generates very similar PUF responses during each evaluation, we expect the Intra-Jaccard indices between PUF responses of a highly-repeatable PUF to be tightly distributed near a value of 1. Figure 4-2 plots the distribution of Intra-Jaccard indices across every PUF memory segment we tested in red. We observe that while the distribution is multimodal, the Intra-Jaccard indices are clustered very close to 1.0 and never drop below 0.65. we ﬁnd that the diﬀerent modes of the Intra-Jaccard index distribution shown in Figure 4-2 arise from combining the Intra-Jaccard index distributions from all three manufacturers. We plot the Intra-Jaccard index distributions for each manufacturer To demonstrate that the DRAM latency PUF exhibits repeatability, we show how Similarly to the Inter-Jaccard index distributions (discussed in Section 4.6.1), alone in Figure 4-3 as indicated by (A),(B), and (C). We observe from the higher distribution mean of Intra-Jaccard indices in Figure 4-3 for manufacturer B that DRAM latency PUFs evaluated on chips from manufacturer B exhibit higher repeatability than those from manufacturers A or C. We conclude from the high Intra-Jaccard indices in Figures 4-2 and 4-3, that DRAM latency PUFs exhibit high repeatability. PUFs on a subset of chips over a 30-day period to show that the repeatability property holds for longer periods of time (i.e., a memory segment generates a PUF response similar to its previously-enrolled golden key irrespective of the time since its enrollment). We examine a total of more than a million 8KiB memory segments across many chips from each of the three manufacturers as shown in Table 4.2. The right column indicates the number of memory segments across n devices, where n is indicated in the left column, and the rows indicate the diﬀerent manufacturers of the chips containing the memory segments. over long periods of time, we continuously evaluate our DRAM latency PUF across a 30-day period using each of our chosen memory segments. For each memory segment, we calculate the Intra-Jaccard index between the ﬁrst PUF response and each subsequent PUF response. We ﬁnd the Intra-Jaccard index range, or the range of values (max every pair of PUF responses from a memory segment. If a memory segment exhibits a low Intra-Jaccard index range, the memory segment generates highly-similar PUF responses during each evaluation over our testing period. Thus, memory segments that exhibit low Intra-Jaccard index ranges demonstrate high repeatability. Long-term Repeatability.We next study the repeatability of DRAM latency Table 4.2: Number of PUF memory segments tested for 30 days. In order to demonstrate the repeatability of evaluating a DRAM latency PUF Figure 4-4 shows the distribution of Intra-Jaccard index ranges across our memory segments with box-and-whisker plots that the Intra-Jaccard index ranges are quite low, i.e., less than 0.1 on average for all manufacturers. Thus, we conclude that the vast majority of memory segments across all manufacturers exhibit very high repeatability over long periods of time. Figure 4-4: Distribution of the Intra-Jaccard index range values calculated between many PUF responses that a PUF memory segment generates over a 30-day period. that exhibit high reliability over time, we analyze per-chip Intra-Jaccard index range properties. Table 4.3 shows the median [minimum, maximum] of the fraction of memory segments per chip that are observed to have Intra-Jaccard index ranges below 0.1 and 0.2. Over 90% of all segments in each chip are suitable for PUF evaluation for Intra-Jaccard index ranges below 0.1, and over 97% for Intra-Jaccard index ranges below 0.2. This means that each chip has a signiﬁcant number of memory segments that are viable for DRAM latency PUF evaluation. Furthermore, the distributions are very narrow, which indicates that diﬀerent chips show similar behavior. We conclude that every chip has a signiﬁcant number of PUF memory segments that exhibit high repeatability across time. We show in Section 4.7.5 how we can use a simple characterization step to identify these viable memory segments quickly and reliably. evaluation, we evaluate the DRAM latency PUF 10 times for each of the memory In order to show that every chip has a signiﬁcant proportion of memory segments Temperature Eﬀects.To demonstrate how changes in temperature aﬀect PUF Table 4.3: Percentage of PUF memory segments per chip with Intra-Jaccard index ranges <0.1 or 0.2 over a 30-day period. Median [minimum, maximum] values are shown. segments in Table 4.2 at each 5 range (55 lated between every possible pair of PUF responses generated by the same memory segment. The deltas between the operating temperatures at the time of PUF evaluation are denoted in the x-axis (temperature delta). Since we test at four evenly-spaced temperatures, we have four distinct temperature deltas. The y-axis marks the Jaccard indices calculated between the PUF responses. The distribution of Intra-Jaccard indices found for a given temperature delta is shown using a box-and-whisker plot. indicated by A, B, and C. Two observations are in order. 1) Across all three manufacturers, the distribution of Intra-Jaccard indices strictly shifts towards zero as the temperature delta increases. 2) The Intra-Jaccard distribution of PUF responses from chips of manufacturer C are the most sensitive to changes in temperature as reﬂected in the large distribution shift in Figure 4-5(C). Both observations show that evaluating a PUF at a temperature diﬀerent from the temperature during enrollment aﬀects the quality of the PUF response and reduces repeatability. However, 1) for small temperature deltas (e.g., 5 we discuss in Section 4.7.5 how we can ameliorate this eﬀect during device enrollment. PUF satisﬁes the characteristics of a runtime-accessible PUF (i.e., low latency and low system interference) discussed in Section 4.3.2, and 2) that the DRAM latency Figure 4-5 subdivides the distributions for each of the three manufacturers as Throughout the remainder of this section, we show 1) how the DRAM latency PUF signiﬁcantly outperforms the DRAM retention PUF in terms of both evaluation time and system interference. Low Latency and 2) ﬁltering the PUF segment, which improves PUF repeatability (to be discussed in Section 4.7.1). During Phase 1, we induce latency failures multiple times (i.e., for multiple iterations) over the PUF memory segment and count the failures in a separate buﬀer for additional bookkeeping (we discuss this in further detail in Section 4.7.2). The execution time of this phase depends directly on three factors: The DRAM latency PUF consists of two key phases: 1) inducing latency failures, 1.The value of the ttiming parameter. A smaller tvalue causes each read to have a shorter latency. 2.The size of the PUF memory segment. A larger memory segment requires more DRAM read requests per iteration. In our devices, we observe that latency failures are induced at a granularity of 32 bytes with each read request, so we can ﬁnd the total number of required DRAM reads by dividing the size of the memory segment by 32 bytes. 3.The number of iterations used to induce latency failures. More iterations lead to a longer evaluation time. Increasing any one of these factors independently of the others directly results in an increase in PUF evaluation time. We experimentally ﬁnd that a single low-t access to DRAM, along with its associated bookkeeping and memory barrier, takes 3.4 its value negligibly aﬀects the time for each low-t 3.4 evaluation time in Equation 4.1. We experimentally show that Phase 2 has negligible runtime ( so we omit Phase 2 in our PUF evaluation time estimation.We express PUF evaluation time estimation as: where N block of the memory segment, and size used to evaluate the PUF. For our ﬁnal chosen conﬁguration (discussed in detail in Section 4.7), we use the parameters size (Section 4.7.4), and N conﬁguration to result in an evaluation time of approximately 87ms. the DRAM latency PUF for 10000 evaluations across chips from all three manufacturers at 55 according to times have very similar means and are extremely tightly distributed (i.e., relative standard deviation). This is expected because, for any particular conﬁguration, DRAM latency PUF evaluation essentially requires a constant number of DRAM accesses. Therefore, any variation in PUF evaluation time comes from variations in code execution (e.g., multitasking, interrupts, DRAM refresh, etc.) rather than any characteristics of the PUF itself. In order to compare these runtime distributions with the result of Equation 4.1, we take take the mean of the mixture distribution of the 𝜇s. Because the value of tis on the scale of tens of nanoseconds [141], changing 𝜇s for each read regardless of the tvalue to ﬁnd a good estimate of the PUF <0.1% of total DRAM latency PUF evaluation time) compared with Phase 1, In order to experimentally verify Equation 4.1, we measure the evaluation time of C. We ﬁnd that evaluation times are normally distributed per-manufacturer (𝜇= 87.2ms,𝜎= 0.0102ms). These distribution parameters show that evaluation three per-manufacturer distributions (i.e., ﬁnd that the 87ms estimate from Equation 4.1 results in only 1.4% error. retention PUF evaluation time across our testable temperature range (i.e., 55 We ﬁnd that the DRAM latency PUF signiﬁcantly outperforms the DRAM retention PUF for an equivalent DRAM capacity overhead of 64KiB (i.e., 8KiB latency PUF memory segment + 56KiB counter buﬀer), providing an average (minimum, maximum) speedup of 152x (109x, 181x) at 70 the memory segment size from 64KiB to 64MiB, we can evaluate a DRAM retention PUF in 1.05s (1.01s, 1.09s) at 70 still outperforms this conﬁguration without an increase in DRAM capacity overhead (i.e., still with an 8KiB memory segment), providing a speedup of 12.1x (11.6x, 12.5x). ﬁnd that inducing latency failures is minimally aﬀected by changes in temperature. Importantly, since our method of inducing latency failures does not change with temperature (Section 4.7.2), DRAM latency PUF evaluation time remains reliably short across all operating temperatures. We conclude that the DRAM latency PUF 1) can be evaluated at speeds that are orders of magnitude faster than the DRAM retention PUF, and 2) overcomes the temperature dependence of the DRAM retention PUF and maintains a low evaluation time across all temperatures. Low System Interference requiring exclusive DRAM rank/bank access throughout PUF evaluation, and 2) using a region in a separate DRAM rank to count latency failures (Section 4.7.2). granularity of a DRAM rank, any other access to the same rank containing the PUF memory segment must be blocked during PUF evaluation. Such blocking prevents other accesses from obeying the same reduced timing parameters and corrupting the data. For this reason, DRAM latency PUF evaluation requires exclusive access to a Figure 4-1 provides a comparison of DRAM latency PUF evaluation time with Similarly to prior work on DRAM latency reduction [55,191], we experimentally The DRAM latency PUF exhibits two major sources of system interference: 1) First, because DRAM timing parameters can only be manipulated for the coarse full DRAM rank for the entire duration of PUF evaluation. Fortunately, the DRAM latency PUF’s quick evaluation time (i.e., 88.2ms on average) guarantees that the DRAM rank will be unavailable only for a short period of time. This is in stark contrast with the DRAM retention PUF, which 1) blocks rank/bank access for much longer periods of time (e.g., on the order of minutes or seconds), and 2) requires the memory controller to issue a large number of refresh operations to rows in the rank/bank outside of the PUF memory segment for the same period of time [364]. requires a small counter buﬀer (e.g., a 56KiB buﬀer for an 8KiB PUF memory segment) which stores counters for each bit of the PUF memory segment. This comes at the cost of both DRAM capacity overhead and additional memory traﬃc penalty. However, given that the DRAM capacity overhead is small (e.g., <0.003% for a 2GB DRAM using an 8KiB memory segment) and the additional bandwidth consumed is extremely low (e.g., on the order of 100MB/s using an 8KiB memory segment) in the context of total DRAM bandwidth (e.g., 8GB/s), we conclude that the additional system interference induced by the counter buﬀer is insigniﬁcant. In practice, we expect system caches to (fully) hold the counter buﬀer, further reducing the required DRAM bandwidth. a viable method for evaluating runtime-accessible PUFs in commodity, unmodiﬁed DRAM chips. However, due to variation across DRAM cells and chips, there are various important design considerations that must be made in the implementation of the DRAM latency PUF. In this section, we discuss these considerations for implementing the DRAM latency PUF. Second, the DRAM latency PUF algorithm (described in detail in Section 4.7.2) As we experimentally showed in Section 4.6, utilizing DRAM latency failures is cell fails with a diﬀerent probability when read with a timing parameter reduced beyond the manufacturer speciﬁcation [ cell as a cell that has a signiﬁcant probability of failure when read with a reduced timing parameter. Our DRAM latency PUFs are comprised of the locations of latencyweak cells because such cells can be repeatably found. In order to repeatably ﬁnd the set of latency-weak cells, we employ many iterations (e.g., on the order of 100) of inducing latency failures at the PUF memory segment. This improves the chances of a PUF evaluation to ﬁnd a signiﬁcant proportion of the latency-weak cells. Because we assume that any given cell has a static probability (p) to fail when accessed with reduced latency, we can model the number of times that a cell must be accessed before observing a latency failure as a geometric random variable with a success probability of p. The geometric distribution with parameter p has a mean value of cells over x iterations during DRAM latency PUF evaluation, we expect to ﬁnd all cells that fail with a probability greater than or equal to cell with a failure probability below the threshold fails during an instance of the PUF evaluation, reducing the similarity of the PUF responses across evaluations and thus the repeatability of the PUF. To mitigate this issue, we apply a ﬁlter (see Section 4.7.2) that removes cells that we observe to fail in only a small proportion of the x iterations. We empirically ﬁnd that removing cells that fail in less than 10% of the iterations results in the highest Intra-Jaccard indices across PUF responses. latency PUF evaluation, we generate PUF responses across our devices using a varying number of iterations between 1 and 1024. For each set of PUF responses generated with a given number of iterations, we calculate the box-and-whisker plots for both Inter- and Intra-Jaccard distributions (not shown). We ﬁnd that for PUF responses from chips across all manufacturers, the Inter-Jaccard and Intra-Jaccard distributions have strictly the same or increasing medians, ﬁrst and third quartiles, and whiskers, Due to many underlying factors (e.g., process variation, temperature), each DRAM In order to determine how many iterations to induce latency failures for during for an increasing number of iterations. since the distribution directly reﬂects the similarities of PUF responses from the same memory segment. We ﬁnd that the Intra-Jaccard index distribution’s median and bottom whiskers increase by 0.0025 and 0.0054, respectively, for every doubling of the number of iterations. On the other hand, higher Inter-Jaccard index distribution values represent higher similarity across distinct memory segments. Such higher values would limit the PUF’s ability to identify many unique devices. We ﬁnd that the Inter-Jaccard index distribution’s median and top whiskers increase by 0.0012 and 0.0011, respectively, for every doubling of the number of iterations. Based on our experimental analyses of these tradeoﬀs, we choose to induce latency failures for 100 iterations during each DRAM latency PUF evaluation. We next discuss in detail our algorithm for evaluating DRAM latency PUFs with high repeatability. PUF at a given memory segment. While we focus on evaluating DRAM latency PUFs with t capable of inducing failures. We ﬁrst initialize the PUF memory segment indicated by Segment[seg then attempt to ﬁnd the reliable set of failures as fast as possible in the memory segment (lines 4-11). Because DRAM RD commands require a t the activation of a previously closed DRAM row, t when issuing a read request to a closed DRAM row. The key idea is to iterate over each row sequentially such that each read request goes to a diﬀerent row (i.e., perform column order accesses through the memory segment of interest) as shown in lines 7-9. Before inducing failures across the PUF memory segment, we must ﬁrst obtain exclusive access to the rank containing the PUF memory segment (line 4), due to the rank-level granularity of changing DRAM timing parameters (Section 2.6). We then reduce the value of t Higher Intra-Jaccard index distribution values represent a more repeatable PUF We provide an implementable algorithm for evaluating a repeatable DRAM latency (line 5). During the iterations of inducing t memory barrier (line 10) after each read. This ensures that 1) only one memory instruction is in ﬂight at a given time and, thus, improves repeatability by simplifying the logic required by the memory controller when issuing memory accesses, and 2) read requests do not get reordered by the memory controller to exploit row buﬀer locality [ row, while obeying the t indicated by lines 6-11 is the fastest method for ﬁnding a reliable set of latency failures in a memory segment. For every read, the t and their failures are counted in a separate rank for bookkeeping (line 11). After all iterations of inducing t (line 12), ﬁlter the PUF segment (line 13; see Filtering Mechanism), release exclusive access to the rank containing the PUF memory segment (line 14), and ﬁnally return the PUF response, i.e., the resulting error pattern from the PUF evaluation at the PUF memory segment (line 15). Algorithm 3: Evaluate DRAM latency PUF latency PUF, we employ a ﬁltering mechanism which removes the cells with low failure probability from the PUF response (as shown on line 13 in Algorithm 3). The key idea is to count, for each bit location in the PUF memory segment, the number of 274,273,242,171,243,311,337,172]. Instead, each access activates a new Filtering Mechanism.In order to improve the repeatability of the DRAM iterations in which the location fails and then use that count to determine whether the bit location should be set (“1”) or cleared (“0”) in the ﬁnal PUF response. Every bit in the DRAM PUF memory segment has a corresponding counter that we store in the counter buﬀer, a data structure we allocate in a DRAM rank separate from the one containing the PUF memory segment. This is to ensure that read/write requests to the counter buﬀer follow manufacturer-speciﬁed timing parameters and do not induce latency failures. all bit locations in the read data that resulted in a latency failure, and increment their corresponding counters in the counter buﬀer. After all iterations of inducing latency failures are completed, we compare every counter of each bit location in the PUF memory segment against a threshold. If a counter holds a value greater than the threshold (i.e., the counter’s corresponding bit location failed more than n times, where n is the threshold), we set the corresponding bit location. Otherwise, we clear it. PUF evaluation: where size the size of the counter buﬀer. The size of the counter buﬀer can be calculated using Equation 4.3: where size of iterations that we want to induce latency failures for. Since we require one counter per bit in the memory segment, we must multiply this quantity by the size of each counter. Since the counter must be able to store up to the value of N case of a cell that fails every iteration), each counter must be After each reduced-latency read request in the PUF memory segment, we ﬁnd Memory Footprint.Equation 4.2 provides the memory footprint required by For a memory segment size of 8KiB, we ﬁnd that the DRAM latency PUF’s total memory footprint is 64KiB. From this, we conclude that DRAM latency PUFs have insigniﬁcant DRAM capacity overhead. which make some DRAM memory segments more desirable to evaluate DRAM latency PUFs with than others. Because we want to ﬁnd 512 bits that fail per PUF memory segment (Section 4.5.2), we consider only those memory segments that have at least 512 failing bits as good memory segments. In order to determine the best size of the memory segment to evaluate the DRAM latency PUF on, we study the eﬀect of varying memory segment size on 1) DRAM capacity overhead, 2) PUF evaluation time, and 3) fraction of good memory segments per device. As the memory segment size increases, both the DRAM capacity overhead and the PUF evaluation time increase linearly. The number of possible PUF memory segments for a DRAM device with a DRAM latency PUF is obtained by counting the number of contiguous PUF memory segments across all of DRAM (i.e., dividing the DRAM size by the PUF memory segment size). Thus, larger PUF memory segments result in fewer possible PUF memory segments for a DRAM device. From an experimental analysis of the associated tradeoﬀs of varying the PUF memory segment size (not shown), we choose a PUF memory segment size of 8KiB. segments per chip with a median [minimum, maximum] across each of the three manufacturers. The left column shows the number of chips tested, the right column shows the representation of the distribution, and the rows indicate the diﬀerent manufacturers of the chips. We see that an overwhelming majority of memory segments from manufacturers A and B are good for PUF evaluation. Memory segments from chips of manufacturer C were observed to exhibit less latency failures, but across each We observe a variation in latency failure rates across diﬀerent memory segments, In Table 4.4, we represent the distribution of the percentage of good memory of our chips we could ﬁnd at least 19.4% of the memory segments to be good for PUF evaluation. Of the total number of PUF memory segments tested (shown in Table 4.2), we experimentally ﬁnd that 100%, 64.06%, and 19.37% of memory segments are good (i.e., contain enough failures to be considered for PUF evaluation) in the worst-case chips from manufacturers A, B, and C. We conclude that there are plenty of PUF memory segments that are good enough for DRAM latency PUF evaluation. Table 4.4: Percentage of good memory segments per chip across manufacturers. Median [min, max] values are shown. parameters must be changed. Some existing processors [ to directly manipulate DRAM timing parameters. These processors can trivially implement and evaluate a DRAM latency PUF with minimal changes to the software and no changes to hardware. However, for other processors that cannot directly manipulate DRAM timing parameters, we would need to simply enable software to programmatically modify memory controller registers which indicate the DRAM timing parameters that a memory access must observe. failure-inducing t requests with a t from across the entire challenge-response space and securely storing the evaluated In order to induce latency failures, the manufacturer-speciﬁed DRAM timing We ﬁnd that we can reliably induce latency failures when we reduce the value of from a default value of 18ns to between 6ns and 13ns. Given this wide range of Device enrollment is a one-time process consisting of evaluating all possible PUFs PUFs in a trusted database such that they can be later queried for authentication [ 318 is diﬃcult to replicate without access to the original device, enrollment must be done securely so that the full set of all possible challenge-response pairs is known only to the trusted database and can be created only by the device owner. a trusted third party (e.g., the DRAM manufacturer) performs device enrollment prior to making the system available to the end consumer. This ensures that the complete set of all possible challenge responses are known only by the trusted third party. After the device is in the ﬁeld, even the trusted third party cannot regenerate the enrollment data. Thus, allowing the trusted third party to both characterize and enroll the responses makes it extremely diﬃcult for a malicious attacker to obtain the full set of possible response pairs without ﬁrst compromising the trusted third party. DRAM during evaluation time (see Section 4.6.1), we must enroll multiple golden keys at varying temperature intervals. This enables a PUF response to match at least one golden key during authentication regardless of the temperature during evaluation time. We ﬁnd that some chips generate PUF responses with less variation across a range of temperatures than other chips. Chips with less variation can enroll golden keys for temperatures at larger intervals than chips with more variation. perform single-bit error correction per word (i.e., typically 64 data bits) invisibly to the system [ scaling [ latency PUFs, ECC words with only one error appear to be error-free to the memory controller, leaving fewer total errors available for PUF response. We note that error correction deterministically transforms DRAM error patterns. Thus, a DRAM PUF (on a chip with in-DRAM ECC) that repeatably induces the same error pattern prior ,364]. Since the goal of PUF authentication is to ensure that a challenge-response Similar to prior works’ approach to DRAM PUFs [364,328,318], we assume that Because DRAM latency PUF responses vary depending on the temperature of Some new DRAM chips utilize in-DRAM error-correcting codes (ECC), which 238,239,170,207,244,230,284]. When such chips are used for DRAM to ECC correction, would repeatably result in a diﬀerent but consistent error pattern after ECC correction. ECC, we would need to evaluate PUFs with a higher raw bit error rate (i.e., the error rate before ECC is performed) relative to non-ECC DRAMs. A higher raw bit error rate would produce enough observable failures (after ECC is performed) for a PUF. The DRAM latency PUF can achieve this higher raw bit error rate by simply reducing the latency parameter value further. Such reduction would also ideally reduce PUF evaluation time and system interference. Therefore, we expect the DRAM latency PUF to be evaluated even faster in chips with built-in ECC. are used, i.e., ECC can correct DRAM words containing more than 1 error, we expect even lower evaluation latencies and lower system interference with the DRAM latency PUF. maximum DRAM temperature. We clearly show in Section 4.5.2 that DRAM latency PUFs are much faster than DRAM retention PUFs at 70 However, at higher temperatures (e.g., faster than DRAM latency PUFs. temperature, it is easy to envision a mechanism that dynamically switches between DRAM latency PUFs and DRAM retention PUFs based on the device operating temperature at the time of evaluation. This mechanism could exploit the strengths of each type of DRAM PUF in order to allow the fastest possible PUF evaluation time. By exploiting in-DRAM temperature sensors that already exist in modern DRAM In order to support PUF evaluation in a system using DRAM chips with in-DRAM Our evaluation of the DRAM latency and retention PUFs is limited to the 70C If a DRAM retention PUF is faster than the DRAM latency PUF at a very high chips [ additional hardware overhead beyond what is already required for DRAM retention PUFs and DRAM latency PUFs individually. retention PUF and DRAM latency PUF to be enrolled. Furthermore, since retention failure rates vary signiﬁcantly across diﬀerent chips [ a diﬀerent temperature at which the mechanism switches between the two DRAM PUFs. This could considerably impact enrollment time and complicate the device authentication process. Ultimately, it is up to the system architect to decide whether such a mechanism is worth the evaluation runtime beneﬁts at very high temperatures (which is likely to be on the order of tens of milliseconds). We leave a full exploration of this hybrid DRAM latency-retention PUF mechanism to future work. DRAM read latency parameters to create a fast, runtime-accessible DRAM PUF without modifying commodity DRAM devices, 2) introduce an eﬀective DRAM PUF that is runtime-accessible at all operating temperatures, 3) demonstrate a wide variety of tradeoﬀs in DRAM PUFs, based on extensive new experimental data from 223 stateof-the-art LPDDR4 DRAM chips, 4) demonstrate the prohibitively slow evaluation times of the DRAM retention PUF, the previously fastest DRAM PUF suitable for commodity devices. on other substrates. The proliferation of recent works on DRAM PUFs reﬂects the growing importance of DRAM PUFs given DRAM’s near ubiquity in modern systems and large address space. DRAM Retention PUFs. PUFs in Section 4.5 and extensively evaluated them in Section 4.5. We brieﬂy explain the diﬀerences between prior proposals. Keller et al. [ 141,140,191,139], this mechanism could potentially be implemented with no Such a mechanism would require challenge-response pairs from both the DRAM To our knowledge, this is the ﬁrst work to: 1) introduce the idea of violating In this section, we discuss prior works that propose DRAM PUFs and PUFs based using DRAM retention failures as unique identiﬁers, shortly followed by Xiong et al. [ PUFs. Other works propose further optimizations for improving the quality of DRAM retention PUFs [ DRAM chips show, DRAM retention PUFs take very long to evaluate at common-case operating temperatures; they are orders of magnitude slower than our proposal (see Section 4.5). A work [ highly reliable debiasing techniques for PUFs generated with retention failures. Other DRAM PUFs. DRAM write-circuitry to induce failures. However, this requires additional hardware and cannot be applied to existing DRAM designs. Tehranipoor et al. [ suggest using DRAM start-up values for PUFs, but this precludes runtime evaluation by requiring a DRAM power cycle for every authentication. In a work published after this work, Talukder et al., [ latency failures and show that they can provide PUFs orders of magnitude faster, since precharge latency failures occur at larger granularities than activation latency failures. PUFs Based on Other Substrates. other substrates, including other memory technologies and customized hardware designs. We categorize them into delay-based and memory-based PUFs. 370 competing circuit paths, 2) ring oscillator PUFs [ frequencies of oscillating signals from chained inverters, and 3) Current Mirror Array (CMA) PUFs [ circuit typically used for machine learning tasks. These works rely on customized hardware not present in commodity systems. FPGA-based PUFs [ 107 common as DRAM in computer systems. values [ 364] and D-PUF [318], both of which enable runtime-accessible DRAM retention Delay-based PUFsinclude 1) arbiter PUFs [195,200,226,278,251,252,116, ], which rely on process variation to extract the unique behavior of two identical ,106,331,93] overcome the need for hardware changes. However, they are not as Memory-based PUFsinclude SRAM PUFs, which rely on SRAM start up 107,126,127,67,32,382,359,319] and voltage reduction induced failures [19]; butterﬂy PUFs, which mimic the behavior of SRAM cells with cross-coupled data latches [ which exploit the power up behavior of regular ﬂip-ﬂops [ emerging memory technologies [ 254 of SRAM, which has a small address space compared to DRAM, and thus cannot accommodate a large number of challenge-response pairs. the DRAM Latency PUF has a few limitations that prevent its immediate deployment in all real systems at its full potential. in its categorization as a weak PUF. While weak PUFs have many use cases in the ﬁeld, a strong PUF (i.e., a PUF with an exponential challenge-response space) is more practical and provides added value to a system. We believe that studying the interactions between many DRAM timing parameters may help to identify eﬀects of parameters that can be combined to build a strong PUF with an exponential challenge-response space. interleave DRAM accesses with varying timing parameters in order to minimize the overhead that evaluating this PUF would cause on a standard system that likely has higher overhead in changing timing parameters. current interface for evaluating the PUF over the DRAM bus is prone to interference and eavesdropping. As the DRAM Latency PUF can be evaluated purely on the DRAM chip itself, we believe that developing simple logic nearby memory to facilitate the evaluation of the DRAM latency PUF on DRAM without relying on an insecure channel may help to further improve the security of the PUF. 179]; latch PUFs, which cross-couple two NOR-gates [309]; ﬂip-ﬂop PUFs, ,149]. These prior works either require additional customized hardware or usage Although the DRAM Latency PUF can be used immediately in certain systems, First, the DRAM Latency PUF has a linear challenge-response space which results Second, the DRAM Latency PUF requires a ﬂexible memory controller that can Third, as the DRAM bus is considered an insecure communication channel, the that PUF responses do not change signiﬁcantly over 30 days, long-term aging may have stronger eﬀects. If long-term eﬀects change PUF responses such that they are incomparable to the golden PUF responses, occasional re-proﬁling may be required to reliably employ the DRAM Latency PUF. runtime authentication. The DRAM latency PUF intentionally violates manufacturerspeciﬁed DRAM timing parameters in order to provide many highly repeatable, unique, and unclonable PUF responses with low latency. Through experimental evaluation using 223 state-of-the-art LPDDR4 DRAM devices, we show that the DRAM latency PUF reliably generates PUF responses at runtime-accessible speeds (i.e., 88.2ms on average) at all operating temperatures. We show that the DRAM latency PUF achieves an average speedup of 152x/1426x at 70 DRAM retention PUF of the same DRAM capacity overhead, and it achieves even greater speedups at lower temperatures. We conclude that the DRAM latency PUF enables a fast and eﬀective substrate for runtime device authentication across all operating temperatures, and we hope that the advent of runtime-accessible PUFs like the DRAM latency PUF and the detailed experimental characterization data we provide on modern DRAM devices will enable security architects to develop even more secure systems for future devices. Fourth, our aging studies are limited to a 30-day period. While we demonstrate We introduce and analyze the DRAM latency PUF, a new DRAM PUF suitable for leverages DRAM cells as an entropy source. The key idea is to intentionally violate the DRAM access timing parameters and use the resulting errors as the source of randomness. Our technique speciﬁcally decreases the DRAM row activation latency (timing parameter t read errors, or activation failures, that exhibit true random behavior. We then aggregate the resulting data from multiple cells to obtain a TRNG capable of providing a high throughput of random numbers at low latency. We propose a new DRAM-based true random number generator (TRNG) that To demonstrate that our TRNG design is viable using commodity DRAM chips, we rigorously characterize the behavior of activation failures in 282 state-of-the-art LPDDR4 devices from three major DRAM manufacturers. We verify our observations using four additional DDR3 DRAM devices from the same manufacturers. Our results show that many cells in each device produce random data that remains robust over both time and temperature variation. We use our observations to develop D-RaNGe, a methodology for extracting true random numbers from commodity DRAM devices with high throughput and low latency by deliberately violating the read access timing parameters. We evaluate the quality of our TRNG using the commonly-used NIST statistical test suite for randomness and ﬁnd that D-RaNGe: 1) successfully passes each test, and 2) generates true random numbers with over two orders of magnitude higher throughput than the previous highest-throughput DRAM-based TRNG. dioactive decay, thermal noise, Poisson noise) to construct a bitstream of random data. Unlike pseudo-random number generators, the random numbers generated by a TRNG do not depend on the previously-generated numbers and only depend on the random noise obtained from physical processes. TRNGs are usually validated using statistical tests such as NIST [ of 1) an entropy source, 2) a randomness extraction technique, and sometimes 3) a post-processor, which improves the randomness of the extracted data often at the expense of throughput. These three components are typically used to reliably generate true random numbers [308, 315]. number generator, as its amount of entropy aﬀects the unpredictability and the throughput of the generated random data. Various physical phenomena can be used as entropy sources. In the domain of electrical circuits, thermal and Poisson noise, jitter, and circuit metastability have been proposed as processes that have high entropy [ A true random number generator (TRNG) requires physical processes (e.g., ra- Entropy Source.The entropy source is a critical component of a random 353,269,126,127,339,229,42,332,33]. To ensure robustness, the entropy source should not be visible or modiﬁable by an adversary. Failing to satisfy that requirement would result in generating predictable data, and thus put the system into a state susceptible to security attacks. harvests random data from an entropy source. A good randomness extraction technique should have two key properties. First, it should have high throughput, i.e., extract as much as randomness possible in a short amount of time [ important for applications that require high-throughput random number generation (e.g., security applications [ the physical process [ extraction process would make the harvested data predictable, lowering the reliability of the TRNG. produce bits that are biased or correlated [ step, which is also known as de-biasing, is applied to eliminate the bias and correlation. The post-processing step also provides protection against environmental changes and adversary tampering [ von Neumann corrector [ MD5 [ throughput (e.g., up to 80% [182]). real-world applications from system security [ tainment [ of-Things (IoT) and mobile devices, enabling primitives that provide security on such systems becomes critically important [ typical method for securing systems against various attacks by encrypting the sys- Randomness Extraction Technique.The randomness extraction technique 323,40,229,368], scientiﬁc simulation [220,40]). Second, it should not disturb Post-processing.Harvesting randomness from a physical phenomenon may 272]. These post-processing steps work well, but generally result in decreased True random numbers sampled from physical phenomena have a number of 308]. As user data privacy becomes a highly-sought commodity in Internettem’s data with keys generated with true random values. Many cryptographic algorithms require random values to generate keys in many standard protocols (e.g., TLS/SSL/RSA/VPN keys) to either 1) encrypt network packets, ﬁle systems, and data, 2) select internet protocol sequence numbers (TCP), or 3) generate data padding values [ thentication protocols and in countermeasures against hardware attacks [ psuedo-random number generators (PRNGs) are shown to be insecure [ keep up with the ever-increasing rate of secure data creation, especially with the growing number of commodity data-harvesting devices (e.g., IoT and mobile devices), the ability to generate true random numbers with high throughput and low latency becomes ever more relevant to maintain user data privacy. In addition, high-throughput TRNGs are already essential components of various important applications such as scientiﬁc simulation [ and recreational entertainment [22, 276, 220, 308, 30, 323, 40, 377, 229, 368]. mentioned applications that rely on TRNGs, including improved security and privacy in most systems that are known to be vulnerable to attacks [ as enable research that we may not anticipate at the moment. One such direction is using a one-time pad (i.e., a private key used to encode and decode only a single message) with quantum key distribution, which requires at least 4Gb/s of true random number generation throughput [ been recently proposed [ and the availability of these high-throughput TRNGs can enable a wide range of new applications with improved security and privacy. TRNG due to the prevalence of DRAM throughout all modern computing systems ranging from microcontrollers to supercomputers. A high-throughput DRAM-based TRNG would help enable widespread adoption of applications that are today limited to only select architectures equipped with dedicated high-performance TRNG engines. Examples of such applications include high-performance scientiﬁc simulations and 109,346,160,75,181,61,377,185]. TRNGs are also commonly used in au- A widely-available, high-throughput, low-latency TRNG will enable all previously DRAM oﬀers a promising substrate for developing an eﬀective and widely-available cryptographic applications for securing devices and communication protocols, both of which would run much more eﬃciently on mobile devices, embedded devices, or microcontrollers with the availability of higher-throughput TRNGs in the system. could help the memory controller to improve scheduling decisions [ 310 PARA [ 239 and software applications as system designs become more capable and increasingly security-critical. processing-in-memory (PIM) architectures [ or near memory to overcome the large bandwidth and energy bottleneck caused by the memory bus and leverage the signiﬁcant data parallelism available within the DRAM chip itself. Many prior works provide primitives for PIM or exploit PIM-enabled systems for workload acceleration [ high-throughput DRAM-based TRNG can enable PIM applications to source random values directly within the memory itself, thereby enhancing the overall potential, security, and privacy, of PIM-enabled architectures. For example, in applications that require true random numbers, a DRAM-based TRNG can enable large contiguous code segments to execute in memory, which would reduce communication with the CPU, and thus improve system eﬃciency. A DRAM-based TRNG can also enable security tasks to run completely in memory. This would remove the dependence of PIM-based security tasks on an I/O channel and would increase overall system security. TRNG must satisfy six key properties: it must 1) have low implementation cost, 2) be fully non-deterministic such that it is impossible to predict the next output given complete information about how the mechanism operates, 3) provide a continuous stream of true random numbers with high throughput, 4) provide true random numbers In terms of the CPU architecture itself, a high-throughput DRAM-based TRNG ,172,242,313,312] and enable the implementation a truly-randomized version of 170] (i.e., a protection mechanism against the RowHammer vulnerability [170, ]). Furthermore, a DRAM-based TRNG would likely have additional hardware In addition to traditional computing paradigms, DRAM-based TRNGs can beneﬁt We posit, based on analysis done in prior works [176,146,283], that an eﬀective with low latency, 5) exhibit low system interference, i.e., not signiﬁcantly slow down concurrently-running applications, and 6) generate random values with low energy overhead. DRAM devices that satisﬁes all six key properties of an eﬀective TRNG. when accessed with reduced DRAM timing parameters, we developed an infrastructure to characterize modern LPDDR4 DRAM chips. We also use an infrastructure for DDR3 DRAM chips, SoftMC [ is applicable beyond the LPDDR4 technology. Both testing environments give us precise control over DRAM commands and DRAM timing parameters as veriﬁed with a logic analyzer probing the command bus. DRAM chips from three major manufacturers in a thermally-controlled chamber held at 45 using heaters and fans controlled via a microcontroller-based proportional-integralderivative (PID) loop to within an accuracy of 0.25 separate local heating source. We use temperature sensors to smooth out temperature variations caused by self-induced heating. validate our mechanism on 4 DDR3 DRAM chips from a single manufacturer. SoftMC enables precise control over timing parameters, and we house the DRAM chips inside another temperature chamber to maintain a stable ambient testing temperature (with the same temperature range as the temperature chamber used for the LPDDR4 devices). To this end, ourgoalin this work, is to provide a widely-available TRNG for In order to test our hypothesis that DRAM cells are an eﬀective source of entropy We perform all tests, unless otherwise speciﬁed, using a total of 282 2y-nm LPDDR4 C. For consistency across results, we precisely stabilize the ambient temperature C. We maintain DRAM temperature at 15C above ambient temperature using a We also use a separate infrastructure, based on open-source SoftMC [120,302], to To explore the various eﬀects of temperature, short-term aging, and circuit-level interference (in Section 5.4) on activation failures, we reduce the t the default 18ns to 10ns for all experiments, unless otherwise stated. Algorithm 4 explains the general testing methodology we use to induce activation failures. First, we write a data pattern to the region of DRAM under test (Line 2). Next, we reduce Algorithm 4: DRAM Activation Failure Testing the t the DRAM region in column order (Lines 4-5) in order to ensure that each DRAM access is to a closed DRAM row and thus requires an activation. This enables each access to induce activation failures in DRAM. Prior to each reduced-latency read, we ﬁrst refresh the target row such that each cell has the same amount of charge each time it is accessed with a reduced-latency read. We eﬀectively refresh a row by issuing an activate (Line 6) followed by a precharge (Line 7) to that row. We then induce the activation failures by issuing consecutive activate (Line 8), read (Line 9), and precharge (Line 10) commands. Afterwards, we record any activation failures that we observe (Line 11). We ﬁnd that this methodology enables us to quickly induce activation failures across all of DRAM, and minimizes testing time. data, we explore and characterize DRAM failures when employing a reduced DRAM activation latency (t parameter to begin inducing activation failures (Line 3). We then access To demonstrate the viability of using DRAM cells as an entropy source for random ﬁndings against those of prior works that study an older generation of DDR3 DRAM chips [ of changing environmental conditions on a DRAM cell that is used as a source of entropy, we rigorously characterize DRAM cell behavior as we vary four environmental conditions. First, we study the eﬀects of DRAM array design-induced variation (i.e., the spatial distribution of activation failures in DRAM). Second, we study data pattern dependence (DPD) eﬀects on DRAM cells. Third, we study the eﬀects of temperature variation on DRAM cells. Fourth, we study a DRAM cell’s activation failure probability over time. We present several key observations that support the viability of a mechanism that generates random numbers by accessing DRAM cells with a reduced t cells to extract true random numbers while minimizing the eﬀects of environmental condition variation (presented in this section) on the DRAM cells. ﬁrst visually inspect the spatial distributions of activation failures both across DRAM chips and within each chip individually. Figure 5-1 plots the spatial distribution of activation failures in a representative 1024 single DRAM chip. Every observed activation failure is marked in black. We make two observations. First, we observe that each contiguous region of 512 DRAM rows consists of repeating rows with the same set (or subset) of column bits that are prone to activation failures. As shown in the ﬁgure, rows 0 to 511 have the same 8 (or a subset of the 8) column bits failing in the row, and rows 512 to 1023 have the same 4 (or a subset of the 4) column bits failing in the row. We hypothesize that these contiguous regions reveal the DRAM subarray architecture as a result of variation across the local sense ampliﬁers in the subarray. We indicate the two subarrays in Figure 5-1 as Subarray A and Subarray B. A “weaker” local sense ampliﬁer results 55,190,191,163] to cross-validate our infrastructure. To understand the eﬀects To study which regions of DRAM are better suited to generating random data, we in cells that share its respective local bitline in the subarray having an increased probability of failure. For this reason, we observe that activation failures are localized to a few columns within a DRAM subarray as shown in Figure 5-1. Second, we observe that within a subarray, the activation failure probability increases across rows (i.e., activation failures are more likely to occur in higher-numbered rows in the subarray and are less likely in lower-numbered rows in the subarray). This can be seen from the fact that more cells fail in higher-numbered rows in the subarray (i.e., there are more black marks higher in each subarray). We hypothesize that the failure probability of a cell attached to a local bitline correlates with the distance between the row and the local sense ampliﬁers, and further rows have less time to amplify their data due to the signal propagation delay in a bitline. These observations are similar to those made in prior studies [190, 55, 191, 163] on DDR3 devices. accessing a row. We observe (not shown) that activation failures occur only within the ﬁrst cache line that is accessed immediately following an activation. No subsequent access to an already open row results in activation failures. This is because cells within the same row have a longer time to restore their cell charge (Figure 2-3) when they are accessed after the row has already been opened. We draw two key conclusions: 1) the region and bitline of DRAM being accessed aﬀect the number of observable activation failures, and 2) diﬀerent DRAM subarrays and diﬀerent local bitlines exhibit varying levels of entropy. We next study the granularity at which we can induce activation failures when entropy, we study how eﬀectively we can discover failures using diﬀerent data patterns across multiple rounds of testing. Our goal in this experiment is to determine which data pattern results in the highest entropy such that we can generate random values with high throughput. Similar to prior works [ data patterns, we analyze a total of 40 unique data patterns: solid 1s, checkered, row stripe, column stripe, 16 walking 1s, and the inverses of all 20 aforementioned data patterns. pattern after 100 iterations of Algorithm 4 relative to the total number of failures discovered by all patterns for a representative chip from each manufacturer. We call this metric coverage because it indicates the eﬀectiveness of a single data pattern to identify all possible DRAM cells that are prone to activation failure. We show results for each pattern individually except for the WALK1 and WALK0 patterns, for which we show the mean (bar) and minimum/maximum (error bars) coverage across all 16 iterations of each walking pattern. Figure 5-2: Data pattern dependence of DRAM cells prone to activation failure over 100 iterations with diﬀerent data patterns identiﬁes diﬀerent subsets of the total set of possible activation failures. This indicates that 1) diﬀerent data patterns cause diﬀerent DRAM cells to fail and 2) speciﬁc data patterns induce more activation failures than others. To understand the data pattern dependence of activation failures and DRAM cell Figure 5-2 plots the ratio of activation failures discovered by a particular data We make three key observations from this experiment. First, we ﬁnd that testing Thus, certain data patterns may extract more entropy from a DRAM cell array than other data patterns. Second, we ﬁnd that, of all 40 tested data patterns, each of the 16 walking 1s, for a given device, provides a similarly high coverage, regardless of the manufacturer. This high coverage is similarly provided by only one other data pattern per manufacturer: solid 0s for manufacturers A and B, and walking 0s for manufacturer C. Third, if we repeat this experiment (i.e., Figure 5-2) while varying the number of iterations of Algorithm 4, the total failure count across all data patterns increases as we increase the number of iterations of Algorithm 4. This indicates that not all DRAM cells fail deterministically when accessed with a reduced t providing a potential source of entropy for random number generation. eﬀectively identiﬁes cells that provide high entropy. We note that DRAM cells with an activation failure probability F many times. With the same data used to produce Figure 5-2, we study the diﬀerent data patterns with regard to the number of cells they cause to fail 50% of the time. Interestingly, we ﬁnd that the data pattern that induces the most failures overall does not necessarily ﬁnd the most number of cells that fail 50% of the time. In fact, when searching for cells with an F patterns that ﬁnd the highest number of cells are solid 0s, checkered 0s, and solid 0s for manufacturers A, B, and C, respectively. We conclude that: 1) due to manufacturing and design variation across DRAM devices from diﬀerent manufacturers, diﬀerent data patterns result in diﬀerent failure probabilities in our DRAM devices, and 2) to provide high entropy when accessing DRAM cells with a reduced t the respective data pattern that ﬁnds the most number of cells with an F for DRAM devices from a given manufacturer. 0s, and solid 0s data patterns for manufacturers A, B, and C, respectively, to analyze time on our sources of entropy. We next analyze each cell’s probability of failing when accessed with a reduced (i.e., its activation failure probability) to determine which data pattern most Unless otherwise stated, in the rest of this chapter, we use the solid 0s, checkered at the granularity of a single cell and to study the eﬀects of temperature and activation failure probability and thus the entropy that can be extracted from the DRAM cell. To analyze temperature eﬀects, we record the F our DRAM devices across 100 iterations of Algorithm 4 at 5 DRAM manufacturer. Each point in the ﬁgure represents how the F cell changes as the temperature changes (i.e., ΔF single cell at temperature T (i.e., the baseline temperature), and the y-axis shows the Because we test each cell at each temperature across 100 iterations, the granularity of the x-axis), we aggregate all respective F the y-axis) with box-and-whiskers plots the increased DRAM temperature. The box is drawn in blue and contains the median drawn in red. The whiskers are drawn in gray, and the outliers are indicated with orange pluses. In this section, we study whether temperature ﬂuctuations aﬀect a DRAM cell’s C and 70). Figure 5-3 aggregates results across 30 DRAM modules from each of the same cell at temperature T + 5 (i.e., 5C above the baseline temperature). on both the x- and y-axes is 1%. For a given Fat temperature T (x% on Figure 5-3: Eﬀect of temperature variation on failure probability We observe that Fat temperature T + 5 tends to be higher than Fat temperature T , as shown by the blue region of the ﬁgure (i.e., the boxes of the box-and-whiskers plots) lying above the x = y line. However, fewer than 25% of all data points fall below the x = y line, indicating that a portion of cells have a lower temperature diﬀerently. DRAM cells of manufacturer A have the least variation of are strongly correlated with the x = y line. How a DRAM cell’s activation failure probability changes in DRAM devices from other manufacturers is unfortunately less predictable under temperature change (i.e., a DRAM cell from manufacturers B or C has higher variation in F correlation between temperature and F failure probability (F DRAM device, but increasing temperature generally increases the activation failure probability. we complete 250 rounds of recording the activation failure probability of DRAM cells over the span of 15 days. Each round consists of accessing every cell in DRAM 100 times with a reduced t individual cell (out of 100 iterations). We ﬁnd that a DRAM cell’s activation failure probability does not change signiﬁcantly over time. This means that, once we identify a DRAM cell that exhibits high entropy, we can rely on the cell to maintain its high entropy over time. We hypothesize that this is because a DRAM cell fails with high entropy when process manufacturing variation in peripheral and DRAM cell circuit elements combine such that, when we read the cell using a reduced t induce a metastable state resulting from the cell voltage falling between the reliable sensing margins (i.e., falling close to determined at manufacturing time, a DRAM cell’s activation failure probability is as temperature is increased. We observe that DRAM devices from diﬀerent manufacturers are aﬀected by when temperature is increased since the boxes of the box-and-whiskers plots To determine whether the failure probability of a DRAM cell changes over time, stable over time given the same experimental conditions. In Section 5.5.1, we discuss our methodology for selecting DRAM cells for extracting stable entropy, such that we can preemptively avoid longer-term aging eﬀects that we do not study in this chapter. tion 5.4), we propose D-RaNGe, a ﬂexible mechanism that provides high-throughput DRAM-based true random number generation (TRNG) by sourcing entropy from a subset of DRAM cells and is built fully within the memory controller. D-RaNGe is based on the with reduced DRAM timing parameters, and this probabilistic failure mechanism can be used as a source of true random numbers. While there are many other timing parameters that we could reduce to induce failures in DRAM [ we focus speciﬁcally on reducing t study the resulting activation failures. soon after sense ampliﬁcation. This results in reading the value at the sense ampliﬁers before the bitline voltage is ampliﬁed to an I/O-readable voltage level. The probability of reading incorrect data from the DRAM cell therefore depends largely on the bitline’s voltage at the time of reading the sense ampliﬁers. Because there is signiﬁcant process variation across the DRAM cells and I/O circuitry [ variety of failure probabilities for diﬀerent DRAM cells (as discussed in Section 5.4) for a given t probability, and a subset of these cells fail randomly with high entropy (shown in Section 5.6.2). Based on our rigorous analysis of DRAM activation failures (presented in Sec- Activation failures occur as a result of reading the value from a DRAM cell too We discover that in each DRAM chip, a subset of cells fail at ∼50% such cells, which we refer to as RNG cells (in Section 5.5.1). Second, we describe the mechanism with which D-RaNGe samples RNG cells to extract random data (Section 5.5.2). Finally, we discuss a potential design for integrating D-RaNGe in a full system (Section 5.5.3). producing truly random output (i.e., RNG cells). Our process of identifying RNG cells involves reading every cell in the DRAM array 1000 times with a reduced t approximating each cell’s Shannon entropy [ symbols across its 1000-bit stream. We identify cells that generate an approximately equal number of every possible 3-bit symbol ( as RNG cells. step (described in Section 5.1) is not necessary to provide suﬃciently high entropy for random number generation. We also ﬁnd that RNG cells maintain high entropy across system reboots. In order to account for our observation that entropy from an RNG cell changes depending on the DRAM temperature (Section 5.4.3), we identify reliable RNG cells at each temperature and store their locations in the memory controller. Depending on the DRAM temperature at the time an application requests random values, D-RaNGe samples the appropriate RNG cells. To ensure that DRAM aging does not negatively impact the reliability of RNG cells, we require re-identifying the set of RNG cells at regular intervals. From our observation that entropy does not change signiﬁcantly over a tested 15 day period of sampling RNG cells (Section 5.4.4), we expect the interval of re-identifying RNG cells to be at least 15 days long. Our RNG cell identiﬁcation process is eﬀective at identifying cells that are reliable entropy sources for random number generation, and we quantify their randomness using the NIST test suite for randomness [279] in Section 5.6.1. Prior to generating random data, we must ﬁrst identify cells that are capable of We ﬁnd that RNG cells provide unbiased output, meaning that a post-processing design a high-throughput TRNG that quickly and repeatedly samples RNG cells with reduced DRAM timing parameters. Algorithm 5 demonstrates the key components of D-RaNGe that enable us to generate random numbers with high throughput. D-RaNGe takes in num Algorithm 5: D-RaNGe: A DRAM-based TRNG random bits desired (Line 1). D-RaNGe then prepares to generate random numbers in Lines 2-6 by ﬁrst selecting DRAM words (i.e., the granularity at which a DRAM module is accessed) containing known RNG cells for generating random data (Line 3). To maximize the throughput of random number generation, D-RaNGe chooses DRAM words with the highest density of RNG cells in each bank (to exploit DRAM parallelism). Since each DRAM access can induce activation failures only in the accessed DRAM word, the density of RNG cells per DRAM word determines the number of random bits D-RaNGe can generate per access. For each available DRAM bank, D-RaNGe selects two DRAM words (in distinct DRAM rows) containing RNG cells. The purpose of selecting two DRAM words in diﬀerent rows is to repeatedly cause bank conﬂicts, or issue requests to closed DRAM rows so that every read request will immediately follow Given the availability of these RNG cells, we use our observations in Section 5.4 to an activation. This is done by alternating accesses to the chosen DRAM words in diﬀerent DRAM rows. After selecting DRAM words for generating random values, D-RaNGe writes a known data pattern that results in high entropy to each chosen DRAM word and its neighboring cells (Line 4) and gains exclusive access to rows containing the two chosen DRAM words as well as their neighboring cells (Line 5). This ensures that the data pattern surrounding the RNG cell and the original value of the RNG cell stay constant prior to each access such that the failure probability of each RNG cell remains reliable (as observed to be necessary in Section 5.4.2). To begin generating random data (i.e., sampling RNG cells), D-RaNGe reduces the value of t values in parallel (Lines 8-15). Lines 8 and 12 indicate the commands to alternate accesses to two DRAM words in distinct rows of a bank to both 1) induce activation failures and 2) precharge the recently-accessed row. After inducing activation failures in a DRAM word, D-RaNGe extracts the value of the RNG cells within the DRAM word (Lines 9 and 13) to use as random data and restores the DRAM word to its original data value (Lines 10 and 14) to maintain the original data pattern. Line 15 ensures that writing the original data value is complete before attempting to sample the DRAM words again. Lines 16 and 17 simply end the loop if enough random bits of data have been harvested. Line 18 sets the t its default value, so other applications can access DRAM without corrupting data. Line 19 releases exclusive access to the rows containing the chosen DRAM words and their neighboring rows. in DRAM, thereby maximizing the rate of generating random data from RNG cells. (Line 6). From every available bank (Line 7), D-RaNGe generates random We ﬁnd that this methodology maximizes the opportunity for activation failures fully from within the memory controller. D-RaNGe generates random numbers using a simple ﬁrmware routine running entirely within the memory controller. The ﬁrmware executes the sampling algorithm (Algorithm 5) whenever an application requests random samples and there is available DRAM bandwidth (i.e., DRAM is not servicing other requests or maintenance commands). In order to minimize latency between requests for samples and their corresponding responses, a small queue of alreadyharvested random data may be maintained in the memory controller for use by the system. Overall performance overhead can be minimized by tuning both 1) the queue size and 2) how the memory controller prioritizes requests for random numbers relative to normal memory requests. needs to decide how to best expose an interface by which an application can leverage D-RaNGe to generate true random numbers on their system. There are many ways to achieve this, including, but not limited to: The operating system may then expose one or more of these interfaces to user applications through standard kernel-user interfaces (e.g., system calls, ﬁle I/O, operating system APIs). The system designer has complete freedom to choose between these (and other) mechanisms that expose an interface for user applications to interact with D-RaNGe. We expect that the best option will be system speciﬁc, depending both on the desired D-RaNGe use cases and the ease with which the design can be In this work, we focus on developing a ﬂexible substrate for sampling RNG cells In order to integrate D-RaNGe with the rest of the system, the system designer •Providing a simpleREQUESTandRECEIVEinterface for applications to request and receive the random numbers using memory-mapped conﬁguration status registers (CSRs) [357] or other existing I/O datapaths (e.g., x86INandOUTopcodes, Local Advanced Programmable Interrupt Controller (LAPIC conﬁguration [133]). •Adding a new ISA instruction (e.g., IntelRDRAND[114]) that retrieves random numbers from the memory controller and stores them into processor registers. implemented. obtained from RNG cells identiﬁed by D-RaNGe passes all of the tests in the NIST test suite for randomness (Section 5.6.1). Second, we analyze the existence of RNG cells across 59 LPDDR4 and 4 DDR3 DRAM chips (due to long testing time) randomly sampled from the overall population of DRAM chips across all three major DRAM manufacturers (Section 5.6.2). Third, we evaluate D-RaNGe in terms of the six key properties of an ideal TRNG as explained in Section 5.2 (Section 5.6.3). Second, we sample each identiﬁed RNG cell one million times to generate large amounts of random data (i.e., 1 Mb bitstreams). Third, we evaluate the entropy of the bitstreams from the identiﬁed RNG cells with the NIST test suite for randomness [ shows the average results of 236 1 Mb bitstreams NIST test suite for randomness. P-values are calculated for each test, hypothesis for each test is that a perfect random number generator would not have produced random data with better characteristics for the given test than the tested sequence [ our chosen level of signiﬁcance, test. We note that all 236 bitstreams pass all 15 tests with similar P-values. Given our acceptable proportions of sequences that pass each test ([0.998,1] calculated by the We evaluate three key aspects of D-RaNGe. First, we show that the random data First, we identify RNG cells using our RNG cell identiﬁcation process (Section 5.5.1). 228]. Since the resulting P-values for each test in the suite are greater than 𝛼= 0.0001, our proportion of passing sequences (1.0) falls within the range of NIST statistical test suite using (1 – sequences). This strongly indicates that D-RaNGe can generate unpredictable, truly random values. Using the proportion of 1s and 0s generated from each RNG cell, we calculate Shannon entropy [ to be 0.9507. the 1) density of RNG cells per DRAM word and 2) bandwidth with which we can access DRAM words when using our methodology for inducing activation failures. Since each DRAM access can induce activation failures only in the accessed DRAM word, the density of RNG cells per DRAM word indicates the number of random bits D-RaNGe can sample per access. We ﬁrst study the density of RNG cells per word across DRAM chips. Figure 5-4 plots the distribution of the number of words containing x RNG cells (indicated by the value on the x-axis) per bank across 472 banks from 59 DRAM devices from all manufacturers. The distribution is presented as a box-and-whiskers plot where the y-axis has a logarithmic scale with a 0 point. The three plots respectively show the distributions for DRAM devices from the three The throughput at which D-RaNGe generates random numbers is a function of manufacturers (indicated at the bottom left corner of each plot). bank across many chips. This means that we can use the available DRAM access parallelism that multiple banks oﬀer and sample RNG cells from each DRAM bank in parallel to improve random number generation throughput. Second, every bank that we analyze has multiple DRAM words containing at least one RNG cell. The DRAM bank with the smallest occurrence of RNG cells has 100 DRAM words containing only 1 RNG cell (manufacturer B). Discounting this point, the distribution of the number of DRAM words containing only 1 RNG cell is tight with a high number of RNG cells (e.g., tens of thousands) in each bank, regardless of the manufacturer. Given our random sample of DRAM chips, we expect that the existence of RNG cells in DRAM banks will hold true for all DRAM chips. Third, we observe that a single DRAM word can contain as many as 4 RNG cells. Because the throughput of accesses to DRAM is ﬁxed, the number of RNG cells in the accessed words essentially acts as a multiplier for the throughput of random numbers generated (e.g., accessing DRAM words containing 4 RNG cells results in 4x the throughput of random numbers compared to accessing DRAM words containing 1 RNG cell). as explained in Section 5.2. reduce the DRAM timing parameters below manufacturer-speciﬁed values. Because We make three key observations. First, RNG cells are widely available in every We now evaluate D-RaNGe in terms of the six key properties of an eﬀective TRNG Low Implementation Cost.To induce activation failures, we must be able to memory controllers issue memory accesses according to the timing parameters speciﬁed in a set of internal registers, D-RaNGe requires simple software support to be able to programmatically modify the memory controller’s registers. Fortunately, there exist some processors [ memory controller register values, i.e., the DRAM timing parameters. These processors can easily generate random numbers with D-RaNGe. controller registers require minimal software changes to expose an interface for changing the memory controller registers [ implementation, the memory controller could be programmed such that it issues DRAM accesses with distinct timing parameters on a per-access granularity to reduce the overhead in 1) changing the DRAM timing parameters and 2) allow concurrent DRAM accesses by other applications. In the rare case where these registers are unmodiﬁable by even the hardware, the hardware changes necessary to enable register modiﬁcation are minimal and are simple to implement [191, 120, 302]. 6ns and 13ns (reduced from the default of 18ns). Given this wide range of failureinducing t parameter registers to a value within this range. extracted from the D-RaNGe-identiﬁed RNG cells pass all 15 NIST tests. We have full reason to believe that we are inducing a metastable state of the sense ampliﬁers (as hypothesized by [ to extract unpredictable random values. dom number generation discussed in Section 5.2, diﬀerent applications have diﬀerent throughput requirements for random number generation, and applications may tolerate a reduction in performance so that D-RaNGe can quickly generate true random numbers. Fortunately, D-RaNGe provides ﬂexibility to tradeoﬀ between the system interference it causes, i.e., the slowdown experienced by concurrently running applica- All other processors that do not currently support direct changes to memory We experimentally ﬁnd that we can induce activation failures with tbetween Fully Non-deterministic.As we have shown in Section 5.6.1, the bitstreams High Throughput of Random Data.Due to the various use cases of rantions, and the random number generation throughput it provides. To demonstrate this ﬂexibility, Figure 5-5 plots the TRNG throughput of D-RaNGe when using varying numbers of banks (x banks on the x-axis) across the three DRAM manufacturers (indicated at the top left corner of each plot). For each number of banks used, we plot the distribution of TRNG throughput that we observe real DRAM devices to provide. The available density of RNG cells in a DRAM device (provided in Figure 5-4) dictates the TRNG throughput that the DRAM device can provide. We plot each distribution as a box-and-whiskers plot. For each number of banks used, we select x banks with the greatest sum of RNG cells across each banks’ two DRAM words with the highest density of RNG cells (that are not in the same DRAM row). We select two DRAM words per bank because we must alternate accesses between two DRAM rows (as shown in Lines 8 and 12 of Algorithm 5). The sum of the RNG cells available across the two selected DRAM words for each bank is considered each bank’s TRNG data rate, and we use this value to obtain D-RaNGe’s throughput. We use Ramulator [ Algorithm 5 with varying numbers of banks. We obtain the random number generation throughput for x banks with the following equation: where TRNG Alg2 Banks. We note that because we observe small variation in the density of RNG cells per word (between 0 and 4), we see that TRNG throughput across diﬀerent chips is generally very similar. For this reason, we see that the box and whiskers are condensed into a single point for distributions of manufacturers B and C. We ﬁnd that when fully using all 8 banks in a single DRAM channel, every device can produce at least 40 Mb/s of random data regardless of manufacturer. The highest throughput we observe from devices of manufacturers A/B/C respectively are 179.4/134.5/179.4 Mb/s. On average, across all manufacturers, we ﬁnd that D-RaNGe can provide a throughput of _Runtimeis the runtime of the core loop of Algorithm 5 when using x 108.9 Mb/s. throughput of random number generation increases linearly as we use more banks. Second, there is variation of TRNG throughput across diﬀerent DRAM devices, but the medians across manufacturers are very similar. number of available channels in a memory hierarchy for a better TRNG throughput estimate for a system with multiple DRAM channels. For an example memory hierarchy comprised of 4 DRAM channels, D-RaNGe results in a maximum (average) throughput of 717.4 Mb/s (435.7 Mb/s). access, the latency of generating random values is directly related to the DRAM access latency. Using the timing parameters speciﬁed in the JEDEC LPDDR4 speciﬁcation [ To calculate the maximum latency for D-RaNGe, we assume that 1) each DRAM access provides only 1 bit of random data (i.e., each DRAM word contains only 1 RNG cell) and 2) we can use only a single bank within a single channel to generate random data. We ﬁnd that D-RaNGe can generate 64 bits of random data with a maximum latency of 960ns. If D-RaNGe takes full advantage of DRAM’s channel- and bank-level parallelism in a system with 4 DRAM channels and 8 banks per channel, D-RaNGe can generate 64 bits of random data by issuing 16 DRAM accesses per channel in parallel. This results in a latency of 220ns. To calculate the empirical minimum latency for D-RaNGe, we fully parallelize D-RaNGe across banks in all 4 We draw two key conclusions. First, due to the parallelism of multiple banks, the We note that any throughput sample point on this ﬁgure can be multiplied by the Low Latency.Since D-RaNGe’s sampling mechanism consists of a single DRAM 141], we calculate D-RaNGe’s latency to generate a 64-bit random value. channels while also assuming that each DRAM access provides 4 bits of random data, since we ﬁnd a maximum density of 4 RNG cells per DRAM word in the LPDDR4 DRAM devices that we characterize (Figure 5-4). We ﬁnd the empirical minimum latency to be only 100ns in our tested devices. across the available channels in a system’s memory hierarchy allows D-RaNGe to cause varying levels of system interference at the expense of TRNG throughput. This enables application developers to generate random values with D-RaNGe at varying tradeoﬀ points depending on the running applications’ memory access requirements. We analyze D-RaNGe’s system interference with respect to DRAM storage overhead and DRAM latency. six DRAM rows per bank, consisting of the two rows containing the RNG cells and each row’s two physically-adjacent DRAM rows containing the chosen data pattern. This results in an insigniﬁcant 0.018% DRAM storage overhead cost. requests, we present one implementation of D-RaNGe. For a single DRAM channel, which is the granularity at which DRAM timing parameters are applied, D-RaNGe can alternate between using a reduced t other hand, when using the default t are serviced to ensure application progress. The length of these time intervals (with default/reduced t number generation requirements. Overall, D-RaNGe provides signiﬁcant ﬂexibility in trading oﬀ its system overhead with its TRNG throughput. However, it is up to the system designer to use and exploit the ﬂexibility for their requirements. To show the potential throughput of D-RaNGe without impacting concurrently-running applications, we run simulations with the SPEC CPU2006 [ Low System Interference.The ﬂexibility of using a diﬀerent number of banks In terms of storage overhead, D-RaNGe simply requires exclusive access rights to To evaluate D-RaNGe’s eﬀect on the DRAM access latency of regular memory , D-RaNGe generates random numbers across every bank in the channel. On the the idle DRAM bandwidth available that we can use to issue D-RaNGe commands. We ﬁnd that, across all workloads, we can obtain an average (maximum, minimum) random-value throughput of 83.1 (98.3, 49.1) Mb/s with no signiﬁcant impact on overall system performance. we use DRAMPower [ DRAM is (1) generating random numbers (Algorithm 5), and (2) idling and not servicing memory requests. We subtract quantity (2) from (1) to obtain the estimated energy consumption of D-RaNGe. We then divide the value by the total number of random bits found during execution and ﬁnd that, on average, D-RaNGe ﬁnds random bits at the cost of 4.4 nJ/bit. DRAM devices that works by exploiting activation failures as a sampling mechanism for observing entropy in DRAM cells. There are a number of proposals to construct TRNGs using commodity DRAM devices, which we summarize in Table 5.2 based on their entropy sources. In this section, we compare each of these works with D-RaNGe. We show how D-RaNGe fulﬁlls the six key properties of an ideal TRNG (Section 5.2) better than any prior DRAM-based TRNG proposal. We group our comparisons by the entropy source of each prior DRAM-based TRNG proposal. Low Energy Consumption.To evaluate the energy consumption of D-RaNGe, To our knowledge, D-RaNGe is the highest-throughput TRNG for commodity Table 5.2: Comparison to previous DRAM-based TRNG proposals. for true random number generation. In particular, since pending access commands contend with regular refresh operations, the latency of a DRAM access is hard to predict and is useful for random number generation. TRNG. First, it harvests random numbers from the instruction and DRAM command scheduling decisions made by the processor and memory controller, which does not constitute a fully non-deterministic entropy source. Since the quality of the harvested random numbers depends directly on the quality of the processor and memory controller implementations, the entropy source is visible to and potentially modiﬁable by an adversary (e.g., by simultaneously running a memory-intensive workload on another processor core [ does not securely generate random numbers. DRAM data retention (Table 5.2), D-RaNGe still outperforms this method in terms of throughput by 211x (maximum) and 128x (average) because a single byte of random data requires a signiﬁcant amount of time to generate. Even if we scale the throughput results provided by [ DRAM channels only 3.40Mb/s as compared with the maximum (average) throughput of 717.4Mb/s (435.7Mb/s) for D-RaNGe. To calculate the latency of generating random values, we assume the same system conﬁguration with [ Prior work [264] proposes using non-determinism in DRAM command scheduling Unfortunately, this method fails to satisfy two important properties of an ideal Second, although this technique has a higher throughput than those based on to generate random bits. To provide 64 bits of random data, [ is signiﬁcantly higher than D-RaNGe’s minimum/maximum latency of 100ns/960ns. Energy consumption for [ on, so we do not compare against this metric. ate random numbers. Unfortunately, this approach is inherently too slow for highthroughput operation due to the long wait times required to induce data retention failures in DRAM. While the failure rate can be increased by increasing the operating temperature, a wait time on the order of seconds is required to induce enough failures [ which is orders of magnitude slower than D-RaNGe. using a hashing algorithm (e.g., SHA-256) on a 4 MiB DRAM block that contains data retention errors resulting from having disabled DRAM refresh for 40 seconds. Optimistically assuming a large DRAM capacity of 32 GiB and ignoring the time required to read out and hash the erroneous data, a waiting time of 40 seconds to induce data retention errors allows for an estimated maximum random number throughput of 0.05 Mb/s. This throughput is already far smaller than D-RaNGe’s measured maximum throughput of 717.4Mb/s, and it would decrease linearly with DRAM capacity. Even if we were able to induce a large number of data retention errors by waiting only 1 second, the maximum random number generation throughput would be 2 Mb/s, i.e., orders of magnitude smaller than that of D-RaNGe. values, its latency for random number generation is extremely high (40s). In contrast, D-RaNGe can produce random values very quickly since it generates random values potentially with each DRAM access (10s of nanoseconds). D-RaNGe therefore has a latency many orders of magnitude lower than Sutar et al.’s mechanism [317]. Prior works [154,317] propose using DRAM data retention failures to gener- 206,155,266,260,164] to achieve high-throughput random number generation, Sutar et al. [317] report that they are able to generate 256-bit random numbers Because [317] requires a wait time of 40 seconds before producing any random We estimate the energy consumption of retention-time based TRNG mechanisms with Ramulator [ 4MiB DRAM region (to constrain the energy consumption estimate to the region of interest), waiting for 40 seconds, and then reading from that region. We then divide the energy consumption of these operations by the number of bits found (256 bits). We ﬁnd that the energy consumption is around 6.8mJ per bit, which is orders of magnitude more costly than that of D-RaNGe, which provides random numbers at 4.4nJ per bit. Unfortunately, this method is unsuitable for continuous high-throughput operation since it requires a DRAM power cycle in order to obtain random data. We are unable to accurately model the latency of this mechanism since it relies on the startup time of DRAM (i.e., bus frequency calibration, temperature calibration, timing register initialization [ the DRAM device in use. Ignoring these components, we estimate the throughput of generating random numbers using startup values by taking into account only the latency of a single DRAM read (after all initialization is complete), which is 60ns. We model energy consumption ignoring the initialization phase as well, by modeling the energy to read a MiB of DRAM and dividing that quantity by [ of random bits found in that region (420Kbit). Based on this calculation, we estimate energy consumption as 245.9pJ per bit. While the energy consumption of [ smaller than the energy cost of D-RaNGe, we note that our energy estimation for [ does not account for the energy consumption required for initializing DRAM to be able to read out the random values. Additionally, [ which is often impractical for applications and for eﬀectively providing a steady stream of random values. [ as [ less entropy. Prior work [330,77] proposes using DRAM startup values as random numbers. 330] to generate random numbers and is strictly worse since [77] results in 31.8x entirely distinct from prior DRAM-based TRNGs that we have discussed in this section. This makes it possible to combine D-RaNGe with prior work to produce random values at an even higher throughput. implement a TRNG, which makes the focus of our work orthogonal to those that design PRNGs. In contrast to prior DRAM-based TRNGs discussed in Section 5.7, we propose using activation failures as an entropy source. Prior works characterize activation failures in order to exploit the resulting error patterns for overall DRAM latency reduction [ (PUFs) [ activation failures or propose using them to generate random numbers. are not based on DRAM. Unfortunately, these proposals either 1) require custom hardware modiﬁcations that preclude their application to commodity devices, or 2) do not sustain continuous (i.e., constant-rate) high-throughput operation. We brieﬂy discuss diﬀerent entropy sources with examples. ﬂash memory devices as an entropy source (up to 1 Mbit/s) [ ﬂash memory is orders of magnitude slower than DRAM, making ﬂash unsuitable for high-throughput and low-latency operation. A more recent work [ that random numbers can be extracted from the variability of write and erase latency in ﬂash memory devices. However this technique suﬀers from low throughput (i.e., up to 0.25 Kb/s) as it depends on long ﬂash memory latencies. We note that D-RaNGe’s method for sampling random values from DRAM is In this work, we focus on the design of a DRAM-based hardware mechanism to 164]. However, none of these works measure the randomness inherent in Many TRNG designs have been proposed that exploit sources of entropy that Flash Memory Read Noise.Prior proposals use random telegraph noise in SRAM-based Designs.SRAM-based TRNG designs exploit randomness in startup values [ continuous, high-throughput operation since they require a power cycle. from GPU-based (up to 447.83 Mbit/s) [ Mbit/s) [ modiﬁcations to commodity GPUs or FPGAs. Yet, GPUs and FPGAs are not as prevalent as DRAM in commodity devices today. non-determinism provided by custom hardware designs (with TRNG throughput up to 2.4 Gbit/s) [ the need for custom hardware limits the widespread use of such proposals in commodity hardware devices (today). D-RaNGe has limitations in implementation that limits its full potential even in these systems. DRAM commands with varying timing parameters on the ﬂy. Without such a memory controller, D-RaNGe is limited to existing systems with tunable timing parameters and may have high overhead in switching latencies for D-RaNGe accesses and regular memory accesses. We believe that a ﬂexible memory controller has many use cases for the system, including Solar-DRAM, the DRAM Latency PUF, and D-RaNGe, and we expect future work to develop such a memory controller. leave D-RaNGe accesses with regular accesses. The scheduler may have to predict and account for many parameters including 1) future memory idle time, 2) priority levels for D-RaNGe accesses and concurrently running applications, 3) anticipated random number throughput and latency requirements, and 4) amount of random numbers GPU- and FPGA-Based Designs.Several works harvest random numbers 225,356,63,122,84] entropy sources. These proposals do not require Custom Hardware.Various works propose TRNGs based in part or fully on While D-RaNGe cam be immediately deployed in certain available systems today, First, D-RaNGe requires a ﬂexible memory controller that has the ability to issue Second, D-RaNGe requires an intelligent memory access scheduler that can intersaved in the buﬀer. Developing such a memory controller would enable systems to satisfy random number requirements of running applications with minimal interference from D-RaNGe. high throughput from unmodiﬁed commodity DRAM devices on any system that allows manipulation of DRAM timing parameters in the memory controller. D-RaNGe harvests fully non-deterministic random numbers from DRAM row activation failures, which are bit errors induced by intentionally accessing DRAM with lower latency than required for correct row activation. Our TRNG is based on two key observations: 1) activation failures can be induced quickly and 2) repeatedly accessing certain DRAM cells with reduced activation latency results in reading true random data. We validate the quality of our TRNG with the commonly-used NIST statistical test suite for randomness. Our evaluations show that D-RaNGe signiﬁcantly outperforms the previous highest-throughput DRAM-based TRNG by up to 211x (128x on average). We conclude that DRAM row activation failures can be eﬀectively exploited to eﬃciently generate true random numbers with high throughput on a wide range of devices that use commodity DRAM chips. We propose D-RaNGe, a mechanism for extracting true random numbers with introduced in 2014, where repeatedly accessing data in a DRAM row can cause bit ﬂips in nearby rows. The RowHammer vulnerability has since garnered signiﬁcant interest in both computer architecture and computer security research communities because it stems from physical circuit-level interference eﬀects that worsen with continued DRAM density scaling. As DRAM manufacturers primarily depend on density scaling to increase DRAM capacity, future DRAM chips will likely be more vulnerable to RowHammer than those of the past. Many RowHammer mitigation mechanisms have been proposed by both industry and academia, but it is unclear whether these mechanisms will remain viable solutions for future devices, as their overheads increase with DRAM’s vulnerability to RowHammer. RowHammer is a circuit-level DRAM vulnerability, ﬁrst rigorously analyzed and In order to shed more light on how RowHammer aﬀects modern and future devices at the circuit-level, we ﬁrst present an experimental characterization of RowHammer on 1580 DRAM chips (408 DRAM modules (60 protection mechanisms disabled, spanning multiple diﬀerent technology nodes from across each of the three major DRAM manufacturers. Our studies deﬁnitively show that newer DRAM chips are more vulnerable to RowHammer: as device feature size reduces, the number of activations needed to induce a RowHammer bit ﬂip also reduces, to as few as 9.6k (4.8k to two rows each) in the most vulnerable chip we tested. accurate simulation in the context of real data taken from our chips to study how the mitigation mechanisms scale with chip vulnerability. We ﬁnd that existing mechanisms either are not scalable or suﬀer from prohibitively large performance overheads in projected future devices given our observed trends of RowHammer vulnerability. Thus, it is critical to research more eﬀective solutions to RowHammer. rate of accesses to a single DRAM row unintentionally ﬂip the values of cells in nearby rows. This phenomenon is known as RowHammer [ from electromagnetic interference between nearby cells. RowHammer is exacerbated by reduction in process technology node size because adjacent DRAM cells become both smaller and closer to each other. Therefore, as DRAM manufacturers continue to increase DRAM storage density, a chip’s vulnerability to RowHammer bit ﬂips increases [170, 239, 241]. by many prior works both from the attack and defense perspectives. Prior works demonstrate that RowHammer can be used to mount system-level attacks for privilege escalation (e.g., [ We evaluate ﬁve state-of-the-art RowHammer mitigation mechanisms using cycle- Modern DRAM devices suﬀer from disturbance errors that occur when a high RowHammer exposes a system-level security vulnerability that has been studied conﬁdential data (e.g., [ eﬀectively demonstrate that a system must provide protection against RowHammer to ensure robust (i.e., reliable and secure) execution. (e.g., [ and software (e.g., [ 351 prevention mechanisms such as Target Row Refresh (TRR) [ performs proprietary operations to reduce the vulnerability of a DRAM chip against potential RowHammer attacks, although these solutions have been recently shown to be vulnerable [ defenses such as increasing the refresh rate [ 333 characteristics, exploits building on it, and mitigation techniques, we refer the reader to [241]. mitigating RowHammer, scientiﬁc literature still lacks rigorous experimental data on how the RowHammer vulnerability is changing with the advancement of DRAM designs and process technologies. In general, important practical concerns are diﬃcult to address with existing data in literature. For example: Prior works propose defenses against RowHammer attacks both at the hardware ,49,350]) levels. DRAM manufacturers themselves employ in-DRAM RowHammer ,345]. For a detailed survey of the RowHammer problem, its underlying causes, Despite the considerable research eﬀort expended towards understanding and • How vulnerable to RowHammer are future DRAM chips expected to be at the circuit level? •How well would RowHammer mitigation mechanisms prevent or mitigate RowHammer in future devices? •What types of RowHammer solutions would cope best with increased circuit-level vulnerability due to continued technology node scaling? While existing experimental characterization studies [ steps towards building an overall understanding of the RowHammer vulnerability, they are too scarce and collectively do not provide a holistic view of RowHammer evolution into the modern day. To help overcome this lack of understanding, we need a unifying study of the RowHammer vulnerability of a broad range of DRAM chips spanning the time since the original RowHammer paper was published in 2014 [170]. RowHammer vulnerability of real DRAM chips at the circuit level changes across diﬀerent chip types, manufacturers, and process technology node generations. Doing so enables us to predict how the RowHammer vulnerability in DRAM chips will scale as the industry continues to increase storage density and reduce technology node size for future chip designs. To achieve this goal, we perform a rigorous experimental characterization study of DRAM chips from three diﬀerent DRAM types (i.e., DDR3, DDR4, and LPDDR4), three major DRAM manufacturers, and at least two diﬀerent process technology nodes from each DRAM type. We show how diﬀerent chips from diﬀerent DRAM types and technology nodes (abbreviated as “type-node” conﬁgurations) have varying levels of vulnerability to RowHammer. We compare the chips’ vulnerabilities against each other and project how they will likely scale when reducing the technology node size even further (Section 6.4). Finally, we study how eﬀective existing RowHammer mitigation mechanisms will be, based on our observed and projected experimental data on the RowHammer vulnerability (Section 6.5). DRAM chips, we experimentally study DDR3, DDR4, and LPDDR4 DRAM chips across a wide range of testing conditions. To achieve this, we use two diﬀerent testing To this end,our goalin this chapter is to evaluate and understand how the We describe our methodology for characterizing DRAM chips for RowHammer. In order to characterize the eﬀects of RowHammer across a broad range of modern infrastructures: (1) the SoftMC framework [ DDR4 DRAM modules in a temperature-controlled chamber and (2) an in-house temperature-controlled testing chamber capable of testing LPDDR4 DRAM chips. setup, we use an FPGA board with a Xilinx Virtex UltraScale 95 FPGA [ DDR4 SODIMM slots, and a PCIe interface. To open up space around the DDR4 chips for temperature control, we use a vertical DDR4 SODIMM riser board to plug a DDR4 module into the FPGA board. We heat the DDR4 chips to a target temperature using silicone rubber heaters pressed to both sides of the DDR4 module. We control the temperature using a thermocouple, which we place between the rubber heaters and the DDR4 chips, and a temperature controller. To enable fast data transfer between the FPGA and a host machine, we connect the FPGA to the host machine using PCIe via a 30 cm PCIe extender. We use the host machine to program the SoftMC hardware and collect the test results. Our SoftMC setup for testing DDR3 chips is similar but uses a Xilinx ML605 FPGA board [ ﬁne-grained control over the types and timings of DRAM commands sent to the chips under test and provide precise temperature control at typical operating conditions. Figure 6-1: Our SoftMC infrastructure [302, 120] for testing DDR4 DRAM chips. industry-developed in-house testing hardware for package-on-package LPDDR4 chips. The LPDDR4 testing infrastructure is further equipped with cooling and heating SoftMC.Figure 6-1 shows our SoftMC setup for testing DDR4 chips. In this LPDDR4 Infrastructure.Our LPDDR4 DRAM testing infrastructure uses capabilities that also provide us with precise temperature control at typical operating conditions. have chips from all of the three major DRAM manufacturers spanning DDR3, DDR4, and two known technology nodes of LPDDR4. We refer to the DRAM type (e.g., LPDDR4) and technology node of a DRAM chip as a DRAM type-node conﬁguration (e.g., LPDDR4-1x). For DRAM chips whose technology node we do not exactly know, we identify their node as old or new. batches of chips based on their manufacturing date, datasheet publication date, purchase date, and RowHammer characteristics. We categorize DDR3 devices with a manufacturing date earlier than 2014 as DDR3-old chips, and devices with a manufacturing date including and after 2014 as DDR3-new chips. Using the same set of properties, we identify two distinct batches of devices among the DDR4 devices. We categorize DDR4 devices with a manufacturing date before 2018 or a datasheet publication date of 2015 as DDR4-old chips and devices with a manufacturing date including and after 2018 or a datasheet publication date of 2016 or 2017 as DDR4-new chips. Based on our observations on RowHammer characteristics from these chips, we expect that DDR3-old/DDR4-old chips are manufactured at an older date with an older process technology compared to DDR3-new/DDR4-new chips, respectively. Table 6.1 summarizes the DRAM chips that we test using both infrastructures. We DDR3 and DDR4.Among our tested DDR3 modules, we identify two distinct This enables us to directly study the eﬀects of shrinking process technology node sizes in DDR3 and DDR4 DRAM chips. manufactured with diﬀerent technology node sizes, 1x-nm and 1y-nm, where 1y-nm is smaller than 1x-nm. Unfortunately, we are missing data from some generations of DRAM from speciﬁc manufacturers (i.e., LPDDR4-1x from manufacturer C and LPDDR4-1y from manufacturer B) since we did not have access to chips of these manufacturer-technology node combinations due to conﬁdentiality issues. Note that while we know the external technology node values for the chips we characterize (e.g., 1x-nm, 1y-nm), these values are not standardized across diﬀerent DRAM manufacturers and the actual values are conﬁdential. This means that a 1x chip from one manufacturer is not necessarily manufactured with the same process technology node as a 1x chip from another manufacturer. However, since we do know relative process node sizes of chips from the same manufacturer, we can directly observe how technology node size aﬀects RowHammer on LPDDR4 DRAM chips. we want to test our chips at the worst-case RowHammer conditions. We identify two conditions that our tests must satisfy to eﬀectively characterize RowHammer at the circuit level: our testing routines must both: 1) run without interference (e.g., without DRAM refresh or RowHammer mitigation mechanisms) and 2) systematically test each DRAM row’s vulnerability to RowHammer by issuing the worst-case sequence of DRAM accesses for that particular row. Disabling Sources of Interference. the circuit level, we want to minimize the external factors that may limit 1) the eﬀectiveness of our tests or 2) our ability to eﬀectively characterize/observe circuitlevel eﬀects of RowHammer on our DRAM chips. First, we want to ensure that we have control over how our RowHammer tests behave without disturbing the desired access pattern in any way. Therefore, during the core loop of each RowHammer LPDDR4.For our LPDDR4 chips, we have two known distinct generations In order to characterize RowHammer eﬀects on our DRAM chips at the circuit-level, test (i.e., when activations are issued at a high rate to induce RowHammer bit ﬂips), we disable all DRAM self-regulation events such as refresh and calibration, using control registers in the memory controller. This guarantees consistent testing without confounding factors due to intermittent events (e.g., to avoid the possibility that a victim row is refreshed during a RowHammer test routine such that we observe fewer RowHammer bit ﬂips). Second, we want to directly observe the circuit-level bit ﬂips such that we can make conclusions about DRAM’s vulnerability to RowHammer at the circuit technology level rather than the system level. To this end, to the best of our knowledge, we disable all DRAM-level (e.g., TRR [ system-level RowHammer mitigation mechanisms (e.g., pTRR [ forms of rank-level error-correction codes (ECC), which could obscure RowHammer bit ﬂips. Unfortunately, all of our LPDDR4-1x and LPDDR4-1y chips use on-die ECC [ failures entirely within the DRAM chip [ ensure that the core loop of our RowHammer test runs for less than 32 ms (i.e., the lowest refresh interval speciﬁed by manufacturers to prevent DRAM data retention failures across our tested chips [ retention failures with RowHammer bit ﬂips. Worst-case RowHammer Access Sequence. from prior work [ pattern. First, a repeatedly accessed row (i.e., aggressor row) has the greatest impact on its immediate physically-adjacent rows (i.e., repeatedly accessing physical row N will cause the highest number of RowHammer bit ﬂips in physical rows N + 1 and N –1). Second, a double-sided hammer targeting physical victim row N (i.e., repeatedly accessing physical rows N – 1 and N + 1) causes the highest number of RowHammer bit ﬂips in row N compared to any other access pattern. Third, increasing the rate of DRAM activations (i.e., issuing the same number of activations within shorter time periods) results in an increasing number of RowHammer bit ﬂips. This rate of activations is limited by the DRAM timing parameter t two successive activations) which depends on the DRAM clock frequency and the 6,180,151,259,183] (i.e., an error correcting mechanism that corrects single-bit DRAM type: DDR3 (52.5ns) [ these observations, we test each row’s worst-case vulnerability to RowHammer by repeatedly accessing the two directly physically-adjacent rows as fast as possible. N , we reverse-engineer the undocumented and conﬁdential logical-to-physical DRAMinternal row address remapping. To do this, we exploit RowHammer’s key observation that repeatedly accessing an arbitrary row causes the two directly physically-adjacent rows to contain the highest number of RowHammer bit ﬂips [ this analysis across rows throughout the DRAM chip, we can deduce the address mappings for each type of chip that we test. We can then use this mapping information to quickly test RowHammer eﬀects at worst-case conditions. We note that for our LPDDR4-1x chips from Manufacturer B, when we repeatedly access a single row within two consecutive rows such that the ﬁrst row is an even row (e.g., rows 2 and 3) in the logical row address space as seen by the memory controller, we observe 1) no RowHammer bit ﬂips in either of the two consecutive rows and 2) a near equivalent number of RowHammer bit ﬂips in each of the four immediately adjacent rows: the two previous consecutive rows (e.g., rows 0 and 1) and the two subsequent consecutive rows (e.g., rows 4 and 5). This indicates a row address remapping that is internal to the DRAM chip such that every pair of consecutive rows share the same internal wordline. To account for this DRAM-internal row address remapping, we test each row N in LPDDR4-1x chips from manufacturer B by repeatedly accessing physical rows N – 2 and N + 2. Additional Testing Parameters. explore two testing parameters at a stable ambient temperature of 50 To enable the quick identiﬁcation of physical rows N – 1 and N + 1 for a given row 1. Hammer count (HC).We test the eﬀects of changing the number of times we access (i.e., activate) a victim row’s physically-adjacent rows (i.e., aggressor rows). We count each pair of activations to the two neighboring rows as one hammer (e.g., one activation each to rows N – 1 and N +1 counts as one hammer). We sweep the hammer count from 2k to 150k (i.e., 4k to 300k activations) across our chips so that the hammer test runs for less than 32ms. RowHammer Testing Routine. ology we use to characterize RowHammer on DRAM chips. For diﬀerent data patterns (DP) (line 2) and hammer counts (HC) (line 8), the test individually targets each row in DRAM (line 4) as a victim row (line 5). For each victim row, we identify the two physically-adjacent rows (aggressor (lines 6 and 7). Before beginning the core loop of our RowHammer test (Lines 11-13), two things happen: 1) the memory controller disables DRAM refresh (line 9) to ensure no interruptions in the core loop of our test due to refresh operations, and 2) we refresh the victim row (line 10) so that we begin inducing RowHammer bit ﬂips on a fully-charged row, which ensures that bit ﬂips we observe are not due to retention time violations. The core loop of our RowHammer test (Lines 11-13) induces RowHammer bit ﬂips in the victim row by ﬁrst activating aggressor HC times. After the core loop of our RowHammer test, we re-enable DRAM refresh 2. Data pattern (DP).We test several commonly-used DRAM data patterns where every byte is written with the same data: Solid0 (SO0: 0x00), Solid1 (SO1: 0xFF), Colstripe0 (CO0: 0x55), Colstripe1 (CO1: 0xAA) [206,260,155]. In addition, we test data patterns where each byte in every other row, including the row being hammered, is written with the same data, Checkered0 (CH0: 0x55) or Rowstripe0 (RS0: 0x00), and all other rows are written with the inverse data, Checkered1 (CH1: 0xAA) or Rowstripe1 (RS1: 0xFF), respectively. (line 14) to prevent retention failures and record the observed bit ﬂips to secondary storage (line 15) for analysis (presented in Section 6.4). Finally, we prepare to test the next HC value in the sweep by restoring the observed bit ﬂips to their original values (Line 16) depending on the data pattern (DP) being tested. RowHammer test routine allows us to compare our test results between the two diﬀerent testing infrastructures. This is because, as we described earlier, we 1) reverse engineer the row address mappings of each DRAM conﬁguration such that we eﬀectively test double-sided RowHammer on every single row, 2) issue activations as fast as possible for each chip, such that the activation rates are similar across infrastructures, and 3) disable all sources of interference in our RowHammer tests. the 1580 DRAM chips we test. Across all of our chips, we sweep the hammer count (HC ) between 2K and 150K (i.e., 4k and 300k activates for our double-sided RowHammer test) and observe whether we can induce any RowHammer bit ﬂips at all in each chip. We ﬁnd that we can induce RowHammer bit ﬂips in all chips except many DDR3 chips. Table 6.2 shows the fraction of DDR3 chips in which we can induce RowHammer bit ﬂips (i.e., RowHammerable chips). based on the increasing fraction of RowHammerable chips from DDR3-old to DDR3-new DRAM chips of manufacturers B and C. Fairly Comparing Data Across Infrastructures.Our carefully-crafted In this section, we present our comprehensive characterization of RowHammer on We ﬁrst examine which of the chips that we test are susceptible to RowHammer. Observation 1.Newer DRAM chips appear to be more vulnerable to RowHammer Table 6.2: Fraction of DDR3 DRAM chips vulnerable to RowHammer when H C < 150k. decreases from DDR3-old to DDR3-new chips, but we also note that the number of RowHammer bit ﬂips that we observe across each of manufacturer A’s chips is very low ( bit ﬂips found in manufacturer B and C’s DDR3-new chips (87k on average across RowHammerable chips) when HC = 150K. Since DDR3-old chips of all manufacturers and DDR3-new chips of manufacturer A have very few to no bit ﬂips, we refrain from analyzing and presenting their characteristics in many plots in Section 6.4. chips using Algorithm 6 with hammer victim_row and 2) data_pattern (as described in Section 6.3.3). with diﬀerent data patterns for a given HC . For each data pattern, we run our RowHammer test routine ten times. We then aggregate all unique RowHammer bit ﬂips per data pattern. We combine all unique RowHammer bit ﬂips found by all data patterns and iterations into a full set of observable bit ﬂips. Using the combined data, we calculate the fraction of the full set of observable bit ﬂips that each data pattern identiﬁes (i.e., the data pattern’s coverage). Figure 6-2 plots the coverage (y-axis) per individual data pattern (shared x-axis) for a single representative DRAM chip from each DRAM type-node conﬁguration that we test. Each row of subplots shows the We ﬁnd that the fraction of manufacturer A’s chips that are RowHammerable <20 on average across RowHammerable chips) compared to the number of To study data pattern eﬀects on observable RowHammer bit ﬂips, we test our We ﬁrst examine the set of all RowHammer bit ﬂips that we observe when testing coverages for chips of the same manufacturer (indicated on the right y-axis), and the columns show the coverages for chips of the same DRAM type-node conﬁguration[memtest@Jeremies-MacBook-Pro:~/Desktop/rowhammer_analysis/ﬁnalﬁgs/dpd]$ (e.g., DDR3-new). Figure 6-2: RowHammer bit ﬂip coverage of diﬀerent data patterns (described in Section 6.3.3) for a single representative DRAM chip of each type-node conﬁguration. sively identifying RowHammer bit ﬂips because no individual data pattern achieves full coverage alone. across chips of the same manufacturer and DRAM type-node conﬁguration. Table 6.3: Worst-case data pattern for each DRAM type-node conﬁguration at split into diﬀerent manufacturers. Observation 2.Testing with diﬀerent data patterns is essential for comprehen- Observation 3.The worst-case data pattern (shown in Table 6.3) is consistent diﬀerent chips because DRAM manufacturers apply a variety of proprietary techniques for DRAM cell layouts to maximize the cell density for diﬀerent DRAM type-node conﬁgurations. For the remainder of this chapter, we characterize each chip using only its worst-case data pattern. RowHammer bit ﬂips across our chips. Figure 6-3 plots the eﬀects of increasing the number of hammers on the RowHammer bit ﬂip rate various DRAM type-node conﬁgurations across the three major DRAM manufacturers. For all chips, we hammer each row, sweeping HC between 10,000 and 150,000. For each HC value, we plot the average rate of observed RowHammer bit ﬂips across all chips of a DRAM type-node conﬁguration. Figure 6-3: Hammer count (HC ) vs. RowHammer bit ﬂip rate across DRAM type-node conﬁgurations. relationship with the log of HC . more accesses to a single row results in more cell-to-cell interference, and therefore We believe that diﬀerent data patterns induce the most RowHammer bit ﬂips in We next study the eﬀects of increasing the hammer count on the number of observed Observation 4.The log of the number of RowHammer bit ﬂips has a linear We observe this relationship between HC and RowHammer bit ﬂip rate because more charge is lost in victim cells of nearby rows. rate in Figure 6-3. We observe that the bit ﬂip rate curve shifts upward and leftward when going from DDR4-old to DDR4-new chips, indicating respectively, 1) a higher rate of bit ﬂips for the same HC value and 2) occurrence of bit ﬂips at lower HC values, as technology node size reduces from DDR4-old to DDR4-new. increasing RowHammer bit ﬂip rates: the same HC value causes an increased average RowHammer bit ﬂip rate from DDR4-old to DDR4-new DRAM chips of all DRAM manufacturers. technology node generations, cell-to-cell interference increases and results in DRAM chips that are more vulnerable to RowHammer bit ﬂips. across our tested chips. In order to normalize the RowHammer eﬀects that we observe across our tested chips, we ﬁrst take each DRAM chip and use a hammer count speciﬁc to that chip to result in a RowHammer bit ﬂip rate of 10 the spatial distribution of bit ﬂips throughout the chip. Figure 6-4 plots the fraction of RowHammer bit ﬂips that occur in a given row oﬀset from the victim all observed RowHammer bit ﬂips. Each column of subplots shows the distributions for chips of diﬀerent manufacturers and each row of subplots shows the distribution for a diﬀerent DRAM type-node conﬁguration. The error bars show the standard deviation of the distribution across our tested chips. Note that the repeatedly-accessed rows (i.e., aggressor rows) are at x = 1 and x = –1 for all plots except in LPDDR4-1x chips from manufacturer B, where they are at x = –2 and x = 2 (due to the internal address remapping that occurs in these chips as we describe in Section 6.3.3). Because We examine the eﬀects of DRAM technology node on the RowHammer bit ﬂip Observation 5.Newer DDR4 DRAM technology nodes show a clear trend of We believe that due to increased density of DRAM chips from older to newer We next experimentally study the spatial distribution of RowHammer bit ﬂips an access to a row essentially refreshes the data in the row, repeatedly accessing aggressor rows during the core loop of the RowHammer test prevents any bit ﬂips from happening in the aggressor rows. Therefore, there are no RowHammer bit ﬂips in the aggressor rows across each DRAM chip in our plots (i.e., y = 0 for x = [–2, –1, 2, 3] for LPDDR4-1x chips from manufacturer B and for x = 1 and x = –1 for all other chips). Figure 6-4: Distribution of RowHammer bit ﬂips across row oﬀsets from the victim row. across DRAM type-node conﬁgurations of a given DRAM manufacturer where newer DRAM technology nodes have an increasing number of rows that are susceptible to RowHammer bit ﬂips that are farther from the victim row. For example, in LPDDR41y chips, we observe RowHammer bit ﬂips in as far as 6 rows from the victim row (i.e., x = –6), whereas in DDR3 and DDR4 chips, RowHammer bit ﬂips only occur in as far as 2 rows from the victim row (i.e., x = –2). We believe that this eﬀect could be due to 1) an increase in DRAM cell density, which leads to cell-to-cell interference extending farther than a single row, with RowHammer bit ﬂips occurring in rows Fraction of RowHammer bit ﬂips!with distance X from the victim row We make three observations from Figure 6-4. First, we observe a general trend increasingly farther away from the aggressor rows (e.g., 5 rows away) for higher-density chips, and 2) more shared structures internal to the DRAM chip, which causes farther (and multiple) rows to be aﬀected by circuit-level interference. nology nodes can exhibit RowHammer bit ﬂips 1) in more rows and 2) farther away from the victim row. from the victim row have fewer RowHammer bit ﬂips than rows closer to the victim row. Non-victim rows adjacent to the aggressor rows (x = 2 and x = –2) contain RowHammer bit ﬂips, and these bit ﬂips demonstrate the eﬀectiveness of a single-sided RowHammer attack as only one of their adjacent rows are repeatedly accessed. As discussed earlier (Section 6.3.3), the single-sided RowHammer attack is not as eﬀective as the double-sided RowHammer attack, and therefore we ﬁnd fewer bit ﬂips in these rows. In rows farther away from the victim row, we attribute the diminishing number of RowHammer bit ﬂips to the diminishing eﬀects of cell-to-cell interference with distance. decreases as the distance from the victim row increases. RowHammer bit ﬂips in all chips except LPDDR4-1x chips from Manufacturer B. However, the rows containing RowHammer bit ﬂips in Manufacturer B’s LPDDR4-1x chips would be even-numbered oﬀsets if we translate all rows to physical rows based on our observation in Section 6.3.3 (i.e., divide each row number by 2 and round down). While we are uncertain why we observe RowHammer bit ﬂips only in physical even-numbered oﬀsets from the victim row, we believe that it may be due to the internal circuitry layout of DRAM rows. in a DRAM array using the same set of RowHammer bit ﬂips. Figure 6-5 shows the distribution of 64-bit words containing x RowHammer bit ﬂips across our tested DRAM chips. We ﬁnd the proportion of 64-bit words containing x RowHammer bit Observation 6.For a given DRAM manufacturer, chips of newer DRAM tech- Second, we observe that rows containing RowHammer bit ﬂips that are farther Observation 7.The number of RowHammer bit ﬂips that occur in a given row Third, we observe that only even-numbered oﬀsets from the victim row contain We next study the spatial distribution of RowHammer-vulnerable DRAM cells ﬂips out of all 64-bit words in each chip containing any RowHammer bit ﬂip and plot the distribution as a bar chart with error bars for each x value. Figure 6-5: Distribution of the number of RowHammer bit ﬂips per 64-bit word for each DRAM type-node conﬁguration. contain up to four RowHammer bit ﬂips. granularity (e.g., a single-error correcting code would only protect a 64-bit word if it contains at most one error), observation 8 indicates that even at a relatively low bit ﬂip rate of 10 with a strong ECC code (e.g., 4-bit error correcting code), which has high hardware overhead. signiﬁcantly in LPDDR4 chips compared to other DRAM types. decay curve for increasing RowHammer bit ﬂip densities with most words containing only one RowHammer bit ﬂip. However, LPDDR4 chips across all manufacturers exhibit a much smaller fraction of words containing a single RowHammer bit ﬂip and Observation 8.At a RowHammer bit ﬂip rate of 10, a single 64-bit value can Because ECC [151,6,248,259] is typically implemented for DRAM at a 64-bit Observation 9.The distribution of RowHammer bit ﬂip density per word changes We ﬁnd DDR3 and DDR4 chips across all manufacturers to exhibit an exponential signiﬁcantly larger fractions of words containing two and three RowHammer bit ﬂips compared to DDR3 and DDR4 chips. We believe this change in the bit ﬂip density distribution is due to the on-die ECC that manufacturers have included in LPDDR4 chips [ and hides most single-bit failures within a 128-bit ECC word using redundant bits (i.e., parity-check bits) that are hidden from the system. This exceeds the ECC’s correction strength and causes the ECC logic to behave in an undeﬁned way. The ECC logic may 1) correct one of the bit ﬂips, 2) do nothing, or 3) introduce an additional bit ﬂip by corrupting an error-free data bit [ ECC makes single-bit errors rare because 1) any true single-bit error is immediately corrected and 2) a multi-bit error can only be reduced to a single-bit error when there are no more than two bit ﬂips within the data bits and the ECC logic’s undeﬁned action happens to change the bit ﬂip count to exactly one. In contrast, there are many more scenarios that yield two or three bit-ﬂips within the data bits, and a detailed experimental analysis of how on-die ECC aﬀects DRAM failure rates in LPDDR4 DRAM chips can be found in [259]. ponent of vulnerability to the double-sided RowHammer attack [ the weakest cell, i.e., the DRAM cell that fails with the fewest number of accesses to physically-adjacent rows. In order to perform this study, we sweep HC at a ﬁne granularity and record the HC that results in the ﬁrst RowHammer bit ﬂip in the chip (HC box-and-whisker plots. 151,6,248,259], which is a 128-bit single-error correcting code that corrects With the failure rates at which we test, many ECC words contain several bit ﬂips. We next study the vulnerability of each chip to RowHammer. One critical comtype-node conﬁguration for the diﬀerent DRAM manufacturers. The x-axis organizes the distributions by DRAM type-node conﬁguration in order of age (older on the left to younger on the right). We further subdivide the subplots for chips of the [memtest@Jeremies-MacBook-Pro:~/Desktop/rowhammer_analysis/ﬁnalﬁgs/ﬁrstfail]$ same DRAM type (e.g., DDR3, DDR4, LPDDR4) with vertical lines. Chips of the same DRAM type are colored with the same color for easier visual comparison across DRAM manufacturers. Figure 6-6: Number of hammers required to cause the ﬁrst RowHammer bit ﬂip (HC more vulnerable to RowHammer bit ﬂips. This is demonstrated by the clear reduction in HC in manufacturer A, or DDR4-old to DDR4-new in manufacturers A and C). both 1) DRAM cell capacitance reduces and 2) DRAM cell density increases as technology node size reduces. Both factors together lead to more interference between cells and likely faster charge leakage from the DRAM cell’s smaller capacitors, leading to a higher vulnerability to RowHammer. We ﬁnd two exceptions to this trend (i.e., ) per chip across DRAM type-node conﬁgurations. Observation 10.Newer chips from a given DRAM manufacturer appear to be values from old to new DRAM generations (e.g., LPDDR4-1x to LPDDR4-1y We believe this observation is due to DRAM technology process scaling wherein a general increase in HC and from DDR4-old to DDR4-new chips of manufacturer B), but we believe these potential anomalies may be due to our inability to identify explicit manufacturing dates and correctly categorize these particular chips. whose weakest cells fail after only 4800 hammers. node sizes will continue to reduce and HC implications further in Section 6.5. Table 6.4 shows the lowest observed HC for any chip within a DRAM type-node conﬁguration (i.e., the minimum values of each distribution in Figure 6-6). Table 6.4: Lowest conﬁguration. of a DRAM chip is common practice, with most system-level [ on-die [ capabilities at the granularity of 64- or 128-bit words. We examine 64-bit ECCs since, for the same correction capability (e.g., single-error correcting), they are stronger than 128-bit ECCs. In order to determine the eﬃcacy with which ECC can mitigate RowHammer eﬀects on real DRAM chips, we carefully study three metrics across each of our chips: 1) the lowest HC required to cause the ﬁrst RowHammer bit ﬂip (i.e., least two RowHammer bit ﬂips (i.e., HC HC required to cause at least three RowHammer bit ﬂips (i.e., HC Observation 11.In LPDDR4-1y chips from manufacturer A, there are chips This observation has serious implications for the future as DRAM technology Eﬀects of ECC.The use of error correcting codes (ECC) to improve the reliability 6,180,151,259,183] ECC mechanisms providing single error correction ) for a given chip (shown in Figure 6-6), 2) the lowest HC required to cause at 64-bit word. These quantities tell us, for ECCs of varying strengths (e.g., single-error correction code, double-error correction code), at which HC values the ECC can 1) mitigate RowHammer bit ﬂips and 2) no longer reliably mitigate RowHammer bit ﬂips for that particular chip. word containing one, two, and three RowHammer bit ﬂips (x-axis) across each DRAM type-node conﬁguration. The error bars represent the standard deviation of HC values across all chips tested. On the same ﬁgure, we also plot with red boxplots, the increase in HC (right y-axis) between the HCs required to ﬁnd the ﬁrst 64-bit word containing one and two RowHammer bit ﬂips, and two and three RowHammer bit ﬂips. These multipliers indicate how HC correcting ECC or moves from a single-error correcting to a double-error correcting ECC. Note that we 1) leave two plots (i.e., Mfr. A DDR3-new and Mfr. C DDR4-old) empty since we are unable to induce enough RowHammer bit ﬂips to ﬁnd 64-bit words containing more than one bit ﬂip in the chips and 2) do not include data from our LPDDR4 chips because they already include on-die ECC [ obfuscates errors potentially exposed to any other ECC mechanisms [259]. Figure 6-7: Hammer Count (left y-axis) required to ﬁnd the ﬁrst 64-bit word containing one, two, and three RowHammer bit ﬂips. Hammer Count Multiplier (right y-axis) quantiﬁes the HC diﬀerence between every two points on the x-axis (as a multiplication factor of the left point to the right point). Figure 6-7 plots as a bar graph the HC (left y-axis) required to ﬁnd the ﬁrst 64-bit by up to 2.78 DRAM chips. correcting code has diminishing returns in DDR4-old and DDR4-new DRAM chips (as indicated by the reduction in the HC multiplier) compared to when moving from a single-error correcting code to a double-error correcting code. However, using a triple-error correcting code in DDR3-new DRAM chips continues to further improve the HC HC increases. We sweep HC between 25k to 150k with a step size of 5k and hammer each DRAM row over 20 iterations. For each HC value, we identify each cell’s bit ﬂip probability (i.e., the number of times we observe a RowHammer bit ﬂip in that cell out of all 20 iterations). We then observe how each cell’s bit ﬂip probability changes as HC increases. We expect that by exacerbating the RowHammer conditions (e.g., increasing the hammer count), the exacerbated circuit-level interference eﬀects should result in an increasing RowHammer bit ﬂip probability for each individual cell. Out of the full set of bits that we observe any RowHammer bit ﬂips in, Table 6.5 lists the percentage of cells that have a strictly monotonically increasing bit ﬂip probability as we increase HC . Table 6.5: Percentage of cells with monotonically increasing RowHammer bit ﬂip probabilities as H C increases. Observation 12. A single-error correcting code can signiﬁcantly improve HC Observation 13.Moving from a double-error correcting code to a triple-error We examine how the failure probability of a single RowHammer bit ﬂip changes as more than 97%) of the cells tested have monotonically increasing RowHammer bit ﬂip probabilities for DDR3 and DDR4 chips. creasing HC increases the probability that a DRAM cell experiences a RowHammer bit ﬂip. However, we ﬁnd that the proportion of cells with monotonically increasing RowHammer bit ﬂip probabilities as HC increases is around only 50% in the LPDDR4 chips that we test. We believe that this decrease is due to the addition of on-die ECC in LPDDR4 chips, which can obscure the probability of observing a RowHammer bit ﬂip from the system’s perspective in two ways. First, a RowHammer bit ﬂip at bit X can no longer be observable from the system’s perspective if another RowHammer bit ﬂip at bit Y occurs within the same ECC word as a result of increasing HC , and the error correction logic corrects the RowHammer bit ﬂip at bit X. Second, the system may temporarily observe a bit ﬂip at bit X at a speciﬁc HC if the set of real RowHammer bit ﬂips within an ECC word results in a miscorrection at bit X. Since this bit ﬂip is a result of the ECC logic misbehaving rather than circuit-level interference, we do not observe the expected trends for these transient miscorrected bits. ogy scaling since DRAM’s increased vulnerability to RowHammer means that systems employing future DRAM devices will likely need to handle signiﬁcantly elevated failure rates. While prior works propose a wide variety of RowHammer failure mitigation techniques (described in Sections 6.5.1 and 6.6), these mechanisms will need to manage increasing failure rates going forward and will likely suﬀer from high overhead (as we show in Section 6.5.2). mitigation mechanisms (e.g., pseudo Target Row Refresh (pTRR) [ Observation 14.For DDR3 and DDR4 chips, an overwhelming majority (i.e., This observation indicates that exacerbating the RowHammer conditions by in- Our characterization results have major implications for continued DRAM technol- While DRAM and system designers currently implement several RowHammer Refresh (TRR) [ choices in these RowHammer mitigation mechanisms that are not discussed in public documentation. Therefore, we cannot fairly evaluate how their performance overheads scale as DRAM chips become more vulnerable to RowHammer. Instead, we evaluate ﬁve state-of-the-art academic proposals for RowHammer mitigation mechanisms [ 194, 303, 372] as well as an ideal refresh-based mitigation mechanism. challenges that they will face going forward as they will need to support DRAM chips more vulnerable to RowHammer: design scalability and system performance overhead. We ﬁrst qualitatively explain and discuss the ﬁve state-of-the-art mitigation mechanisms and how they can potentially scale to support DRAM chips that are more vulnerable to RowHammer. We then quantitatively evaluate their performance overheads in simulation as HC reducing performance overhead in RowHammer mitigation, we also implement and study an ideal refresh-based mechanism that prevents RowHammer by refreshing a DRAM row only immediately before it is about to experience a bit ﬂip. Unfortunately, many of these works have critical weaknesses (e.g., inability to track all DRAM activations) that make them vulnerable to carefully-crafted RowHammer attacks, as demonstrated in some followup works (e.g., [ on evaluating six mechanisms (i.e., ﬁve state-of-the-art hardware proposals and one ideal refresh-based mitigation mechanism), which address a strong threat model that assumes an attacker can cause row activations with precise memory location and timing information. We brieﬂy explain each mitigation mechanism and how its design We evaluate each RowHammer mitigation mechanism in terms of two major There is a large body of work (e.g., [16,341,178,41,358,36,169,351,49,198, 104,135]) that proposes software-based RowHammer mitigation mechanisms. scales for DRAM chips with increased vulnerability to RowHammer (i.e., lower HC values). increasing the overall DRAM refresh rate such that it is impossible to issue enough activations within one refresh window (i.e., the time between two consecutive refresh commands to a single DRAM row) to any single DRAM row to induce a RowHammer bit ﬂip. The study notes that this is an undesirable mitigation mechanism due to its associated performance and energy overheads. In order to reliably mitigate RowHammer bit ﬂips with this mechanism, we scale the refresh rate such that the refresh window (i.e., t to a single row) equals the number of hammers until the ﬁrst RowHammer bit ﬂip (i.e., HC rows that must be refreshed within a refresh window, this mechanism inherently does not scale to HC Adjacent Row Activation) refreshes one or more of the row’s adjacent rows with a low probability p. Due to PARA’s simple approach, it is possible to easily tune p when PARA must protect a DRAM chip with a lower HC PARA, we scale p for diﬀerent values of HC does not exceed 1e-15 per hour of continuous hammering. tables to identify any row that may be activated HC the tables probabilistically to minimize the overhead of tracking frequently-activated DRAM rows. ProHIT [ the victim rows. When a row is activated, ProHIT checks whether each adjacent row is already in either of the tables. If a row is not in either table, it is inserted into the cold table with a probability p entry in the cold table is then evicted with a probability (1 – p Increased Refresh Rate [170].The original RowHammer study [170] describes PARA [170].Every time a row is opened and closed, PARA (Probabilistic ProHIT [303].ProHIT maintains a history of DRAM activations in a set of and the other entries are evicted with a probability p already exists in the cold table, the row is promoted to the highest-priority entry in the hot table with a probability (1 – p with a probability p entry is upgraded to a higher priority position. During each refresh command, ProHIT simultaneously refreshes the row at the top entry of the hot table, since this row has likely experienced the most number of activations, and then removes the entry from the table. the size of the tables and the probabilities for managing the tables (e.g., p must be adjusted. Even though Son et al. show a low-cost mitigation mechanism for a speciﬁc HC these values for arbitrary HC we evaluate ProHIT only when HC ically adjusted based on each row’s access history. This way, according to memory access locality, the rows that have been recorded as a victim more recently have a higher chance of being refreshed. MRLoc uses a queue to store victim row addresses on each activation. Depending on the time between two insertions of a given victim row into the queue, MRLoc adjusts the probability with which it issues a refresh to the victim row that is present in the queue. probability of refresh) are tuned for HC these parameters empirically, and there is no concrete discussion on how to adjust these parameters as HC Loc [ data point, we are unable to demonstrate how their overheads scale as DRAM chips become more vulnerable to RowHammer. For ProHIT [303] to eﬀectively mitigate RowHammer with decreasing HCvalues, MRLoc[372]. MRLoc refreshes a victim row using a probability that is dynam- MRLoc’s parameters (the queue size and the parameters used to calculate the As such, even though we quantitatively evaluate both ProHIT [303] and MR372] for completeness and they may seem to have good overhead results at one TWiCe[194]. TWiCe tracks the number of times a victim row’s aggressor rows are activated using a table of counters and refreshes a victim row when its count is above a threshold such that RowHammer bit ﬂips cannot occur. TWiCe uses two counters per entry: 1) a lifetime counter, which tracks the length of time the entry has been in the table, and 2) an activation counter, which tracks the number of times an aggressor row is activated. The key idea is that TWiCe can use these two counters to determine the rate at which a row is being hammered and can quickly prune entries that have a low rate of being hammered. TWiCe also minimizes its table size based on the observation that the number of rows that can be activated enough times to induce RowHammer failures within a refresh window is bound by the DRAM chip’s vulnerability to RowHammer. the table. If so, the activation count for each row is incremented. Otherwise, new entries are allocated in the table for each row. Whenever a row’s activation count surpasses a threshold t also deﬁnes a pruning stage that 1) increments each lifetime counter, 2) checks each row’s hammer rate based on both counters, and 3) prunes entries that have a lifetime hammer rate lower than a pruning threshold, which is deﬁned as t number of refresh operations per refresh window (i.e., t performs pruning operations during refresh commands so that the latency of a pruning operation is hidden behind the DRAM refresh commands. a couple of complications arise in the design. TWiCe either 1) cannot prune its table, resulting in a very large table size since every row that is accessed at least once will remain in the table until the end of the refresh window or 2) requires ﬂoating point operations in order to calculate thresholds for pruning, which would signiﬁcantly increase the latency of the pruning stage. Either way, the pruning stage latency would increase signiﬁcantly since a larger table also requires more time to check each entry, and the latency may no longer be hidden by the refresh command. refresh intervals in a refresh window ( When a row is activated, TWiCe checks whether its adjacent rows are already in If tis lower than the number of refresh intervals in a refresh window (i.e., 8192), As a consequence, TWiCe does not support tvalues lower than the number of DDR4, LPDDR4). This means that in its current form, we cannot fairly evaluate TWiCe for HC ideal version of TWiCe (i.e., TWiCe-ideal) for HC TWiCe-ideal solves both issues of the large table size and the high-latency pruning stage at lower HC based mitigation mechanism that tracks all activations to every row in DRAM and issues a refresh command to a row only right before it can potentially experience a RowHammer bit ﬂip (i.e., when a physically-adjacent row has been activated HC times). mer mitigation mechanisms (i.e., increased refresh rate [ HIT [ nism. Evaluation Methodology model and a system conﬁguration as listed in Table 6.6, to implement and evaluate the RowHammer mitigation mechanisms. To demonstrate how the performance overhead of each mechanism would scale to future devices, we implement, to the best of our ability, parameterizable methods for scaling the mitigation mechanisms to DRAM chips with varying degrees of vulnerability to RowHammer (as described in Section 6.5.1). SPEC CPU2006 benchmark suite [ mitigation mechanisms on systems during typical use (and not when a RowHammer attack is being mounted). The set of workloads exhibit a wide range of memory intensities. The workloads’ MPKI values (i.e., last-level cache misses per kilo-instruction) Ideal Refresh-based Mitigation Mechanism.We implement an ideal refresh- We ﬁrst describe our methodology for evaluating the ﬁve state-of-the-art RowHam- 303], MRLoc [372], TWiCe [194]) and the ideal refresh-based mitigation mecha- We use Ramulator [3,174], a cycle-accurate DRAM simulator with a simple core Workloads.We evaluate 48 8-core workload mixes drawn randomly from the full range from 10 to 740. This wide range enables us to study the eﬀects of RowHammer mitigation on workloads with widely varying degrees of memory intensity. We note that there could be other workloads with which mitigation mechanisms exhibit higher performance overheads, but we did not try to maximize the overhead experienced by workloads by biasing the workload construction in any way. We simulate each workload until each core executes at least 200 million instructions. For all conﬁgurations, we initially warm up the caches by fast-forwarding 100 million instructions. additional DRAM refresh operations to prevent RowHammer, we use two diﬀerent metrics to evaluate their impact on system performance. First, we measure DRAM bandwidth overhead, which quantiﬁes the fraction of the total system DRAM bandwidth consumption coming from the RowHammer mitigation mechanism. Second, we measure overall workload performance using the weighted speedup metric [ eﬀectively measures job throughput for multi-core workloads [ weighted speedup to its baseline value, which we denote as 100%, and ﬁnd that when using RowHammer mitigation mechanisms, most values appear below the baseline. Therefore, for clarity, we refer to normalized weighted speedup as normalized system performance in our evaluations. Evaluation of Mitigation Mechanisms mechanisms (as described in Section 6.5.1) for chips of varying degrees of RowHammer Metrics.Because state-of-the-art RowHammer mitigation mechanisms rely on Figure 6-8 shows the results of our evaluation of the RowHammer mitigation vulnerability (i.e., 200k overhead in Figure 6-8a and 2) normalized system performance in Figure 6-8b. Each data point shows the average value across 48 workloads with minimum and maximum values drawn as error bars. line to show how each RowHammer mitigation mechanism would impact the overall system when using a DRAM chip of a particular conﬁguration. Above the ﬁgures (sharing the x-axis with Figure 6-8), we draw horizontal lines representing the ranges of HC conﬁguration across manufacturers. We color the ranges according to DRAM typenode conﬁguration colors in the ﬁgure, and indicate the average value with a gray point. Note that these lines directly correspond to the box-and-whisker plot ranges in Figure 6-6. is highly correlated with normalized system performance, as DRAM bandwidth consumption is the main source of system interference caused by RowHammer mitigation mechanisms. We note that several points (i.e., ProHIT, MRLoc, and TWiCe and Ideal evaluated at higher HC an inverted log graph and these points are very close to zero. Second, in the latest DRAM chips (i.e., the LPDDR4-1y chips), only PARA, ProHIT, and MRLoc are viable options for mitigating RowHammer bit ﬂips with reasonable average normalized system performance: 92%, 100%, and 100%, respectively. Increased Refresh Rate and TWiCe do not scale to such degrees of RowHammer vulnerability (i.e., HC as discussed in Section 6.5.1. Third, only PARA’s design scales to low HC ues that we may see in future DRAM chips, but has very low average normalized system performance (e.g., 72% when HC when HC over PARA (e.g., 98% when HC For each DRAM type-node conﬁguration that we characterize, we plot the minimum value found across chips within the conﬁguration (from Table 6.4) as a vertical values that we observe for every tested DRAM chip per DRAM type-node We make ﬁve key observations from this ﬁgure. First, DRAM bandwidth overhead = 128), there are signiﬁcant practical limitations in enabling TWiCe-ideal for Figure 6-8: Eﬀect of RowHammer mitigation mechanisms on a) DRAM bandwidth overhead (note the inverted log-scale y-axis) and b) system performance, as DRAM chips become more vulnerable to RowHammer (from left to right). such low HC exhibit high normalized system performance at their single data point (i.e., 95% and 100%, respectively when HC scaling their mechanisms to lower HC described in Section 6.5.1). Fifth, the ideal refresh-based mitigation mechanism is signiﬁcantly and increasingly better than any existing mechanism as HC below 1024. This indicates that there is still signiﬁcant opportunity for developing a refresh-based RowHammer mitigation mechanism with low performance overhead that scales to low HC at very low HC 93.53% when HC solving RowHammer in future ultra-dense DRAM chips. small performance overheads for mitigating RowHammer bit ﬂips in modern DRAM chips, their overheads do not scale well in future DRAM chips that will likely exhibit higher vulnerability to RowHammer. Thus, we need new mechanisms and approaches to RowHammer mitigation that will scale to DRAM chips that are highly vulnerable to RowHammer bit ﬂips. DRAM storage density and are forecasted to reach 1z and 1a technology nodes within the next couple of years [ chips will likely be increasingly vulnerable to RowHammer. This means that, to maintain market competitiveness without suﬀering factory yield loss, manufacturers will need to develop eﬀective RowHammer mitigations for coping with increasingly vulnerable DRAM chips. We conclude that while existing mitigation mechanisms may exhibit reasonably DRAM manufacturers continue to adopt smaller technology nodes to improve Future Directions in RowHammer Mitigation stack ranging from circuit-level mechanisms built into the DRAM chip itself to systemlevel mechanisms that are agnostic to the particular DRAM chip that the system uses. Of these solutions, our evaluations in Section 6.5.1 show that, while the ideal refresh-based RowHammer mitigation mechanism, which inserts the minimum possible number of additional refreshes to prevent RowHammer bit ﬂips, scales reasonably well to very low HC existing RowHammer mitigation mechanisms either cannot scale or cause severe system performance penalties when they scale. bit ﬂips in DRAM chips with a high degree of RowHammer vulnerability (i.e., with a low HC RowHammer mitigation. Going forward, we identify two promising research directions that can potentially lead to new RowHammer solutions that can reach or exceed the scalability of the ideal refresh-based mitigation mechanism: (1) DRAM-system cooperation and (2) proﬁle-guided mechanisms. The remainder of this section brieﬂy discusses our vision for each of these directions. DRAM-System Cooperation. mechanisms alone ignores the potential beneﬁts of addressing the RowHammer vulnerability from both perspectives together. While the root causes of RowHammer bit ﬂips lie within DRAM, their negative eﬀects are observed at the system-level. Prior work [ stack, and we believe that a holistic solution can achieve a high degree of protection at relatively low cost compared to solutions contained within either domain alone. Proﬁle-Guided Mechanisms. susceptible DRAM cells or memory regions can provide a powerful substrate for building targeted RowHammer solutions that eﬃciently mitigate RowHammer bit ﬂips at low cost. Knowing (or eﬀectively predicting) the locations of bit ﬂips before they RowHammer mitigation mechanisms have been proposed across the computing To develop a scalable and low-overhead mechanism that can prevent RowHammer 244,238] stresses the importance of tackling these challenges at all levels of the occur in practice could lead to a large reduction in RowHammer mitigation overhead, providing new information that no known RowHammer mitigation mechanism exploits today. For example, within the scope of known RowHammer mitigation solutions, increasing the refresh rate can be made far cheaper by only increasing the refresh rate for known-vulnerable DRAM rows. Similarly, ECC or DRAM access counters can be used only for known-vulnerable cells, and even a software-based mechanism can be adapted to target only known-vulnerable rows (e.g., by disabling them or remapping them to reliable memory). ogy today. Our characterization in this work essentially follows the naïve approach of individually testing each row by attempting to induce the worst-case testing conditions (e.g., HC , data pattern, ambient temperature etc.). However, this approach is extremely time consuming due to having to test each row individually (potentially multiple times with various testing conditions). Even for a relatively small DRAM module of 8GB with 8KB rows, hammering each row only once for only one refresh window of 64ms requires over 17 hours of continuous testing, which means that the naïve approach to proﬁling is infeasible for a general mechanism that may be used in a production environment or for online operation. We believe that developing a fast and eﬀective RowHammer proﬁling mechanism is a key research challenge, and we hope that future work will use the observations made in this study and other RowHammer characterization studies to ﬁnd a solution. only three works [ examine how RowHammer failures manifest in real DRAM chips. However, none of these studies show how the number of activations to induce RowHammer bit ﬂips is changing across modern DRAM types and generations, and the original RowHammer study [ Unfortunately, there exists no such eﬀective RowHammer error proﬁling methodol- Although many works propose RowHammer attacks and mitigation mechanisms, 170] is already six years old and limited to DDR3 DRAM chips only. This section highlights the most closely related prior works that study the RowHammer vulnerability of older generation chips or examine other aspects of RowHammer. study [ tal RowHammer failure characterization using older DDR3 devices. However, these studies are restricted to only DDR3 devices and do not provide a scaling study of hammer counts across DRAM types and generations. In contrast, our work provides the ﬁrst rigorous experimental study showing how RowHammer characteristics scale across diﬀerent DRAM generations and how DRAM chips designed with newer technology nodes are increasingly vulnerable to RowHammer. Our work complements and furthers the analyses provided in prior studies. the root cause of the RowHammer vulnerability. While their analysis identiﬁes a likely explanation for the failure mechanism responsible for RowHammer, they do not present experimental data taken from real devices to support their conclusions. 341 169 several patents for RowHammer prevention mechanisms have been ﬁled [ future DRAM generations and do not provide detailed failure characterization data from modern DRAM devices. Similar and other related works on RowHammer can be found in a recent retrospective [241]. aspects of RowHammer that could help to further understand it as a phenomenon. technology node size scales, we are unsure of the exact process technology node sizes Real Chip Studies.Three key studies (i.e., the pioneering RowHammer 170] and two subsequent studies [257,256]) perform extensive experimen- Simulation Studies.Yang et al. [369] use device-level simulations to explore RowHammer Mitigation Mechanisms.Many prior works [372,303,16,178, ,351,83,49,198,350] propose RowHammer mitigation techniques. Additionally, 23,102]. However, these works do not analyze how their solutions will scale to Due to limited resources and available testing time, we were unable to study several First, while we demonstrate a scaling study on RowHammer as DRAM process for several chip generations. Therefore, we could only provide a relative study on how the vulnerability to RowHammer in DRAM chips across technology node generations changes. Given the exact sizes of the technology nodes, we could provide an analytical model such that we could predict the progression of RowHammer vulnerability in future chips. that understanding how aging aﬀects the RowHammer vulnerability in chips is critical to understand whether a system will become more vulnerable over time as its DRAM chip ages in use. Furthermore, if aging can aﬀect the RowHammer vulnerability of a chip to the point that mitigation mechanisms can no longer eﬀectively mitigate RowHammer bit ﬂips, it is important to understand when and how often a DRAM chip should be swapped out to maintain a RowHammer-free system. study that demonstrates how the RowHammer vulnerability of modern DDR3, DDR4, and LPDDR4 DRAM chips scales across DRAM generations and technology nodes. Using experimental data from 1580 real DRAM chips produced by the three major DRAM manufacturers, we show that modern DRAM chips that use smaller process technology node sizes are signiﬁcantly more vulnerable to RowHammer than older chips. Using simulation, we show that existing RowHammer mitigation mechanisms 1) suﬀer from prohibitively large performance overheads at projected future hammer counts and 2) are still far from an ideal selective-refresh-based RowHammer mitigation mechanism. Based on our study, we motivate the need for a scalable and low-overhead solution to RowHammer and provide two promising research directions to this end. We hope that the results of our study will inspire and aid future work to develop eﬃcient solutions for the RowHammer bit ﬂip rates we are likely to see in DRAM chips in the near future. Second, we did not account for chip aging in our RowHammer study. We believe We provide the ﬁrst rigorous experimental RowHammer failure characterization performance, security or reliability. However, these works are all orthogonal in that a subset or all of them can be implemented simultaneously on the same system to provide each individual beneﬁt with little interference. Furthermore, since each mechanism does not require any changes to DRAM, the implementation should be constrained to the memory controller. There are many ways to potentially combine each individual mechanism, but we provide a simple high-level example. proﬁles for their speciﬁc cases on a given DRAM chip. Once the proﬁles are generated via characterization, the storage overhead of the proﬁles is simply the combined size of each proﬁle. Depending on which mechanism is issuing a DRAM access, the access latency is set according to the respective proﬁle. latency of regular DRAM accesses should be set according to the Solar-DRAM proﬁle. In the relatively infrequent event that the system requests a PUF response or a true Each of these four works individually provide mechanisms that improve system Solar-DRAM, The DRAM Latency PUF, and D-RaNGe all rely on latency failure As Solar-DRAM improves system performance with its faster DRAM accesses, the random value, ﬁrmware for The DRAM Latency PUF or D-RaNGe mechanism will override regular DRAM accesses and issue DRAM accesses with latencies adjusted according to their respective proﬁles. DRAM Latency PUF) and generating true random values (via D-RaNGe) come at the cost of additional DRAM accesses interleaved within regular DRAM accesses. These additional DRAM accesses likely will increase the system performance overhead depending on the implementations (as discussed in Sections 5.6.3 and 4.6.2). Fortunately, D-RaNGE and The DRAM Latency PUF both enable ﬂexible implementations and can minimize their overheads depending on the user needs and the importance of and the need for timely PUF evaluations and true random values. terms of both performance and storage, we believe that enabling these mechanisms on a system simultaneously will provide system performance, security, and reliability beneﬁts that outweigh the combined overheads of the mechanisms. formance, security, or reliability, each mechanism has associated costs in deployment. These costs come in the form of 1) performance or energy overheads, 2) proﬁling time prior to deployment (or during online operation), or 3) minor system changes. These costs must be considered when deploying each mechanism on various systems according to the manufacturer and user constraints (e.g., ﬁscal, physical, time, and other resource limitations), and requirements (e.g., performance, reliability, security service level agreements). However, due to the ﬂexibility and orthogonality of these mechanisms, costs can be reduced by implementing any subset of these works on a system according to the constraints and requirements. than present an optimal implementation for each of these works, we cannot provide While Solar-DRAM purely provides performance beneﬁts, evaluating PUFs (via While utilizing these mechanisms in conjunction may have compound overheads in While each individual mechanism provides beneﬁts to the system, whether per- Since our works mainly focus on demonstrating the new fundamental ideas rather a quantitative cost-beneﬁt analysis of each of our mechanisms. However, given the beneﬁts that each mechanism provides with the relatively low overheads in the relatively non-optimized solutions oﬀered in the works, we do foresee future work in supporting near-optimal deployment strategies and implementations for each mechanism that would signiﬁcantly reduce the associated costs and improve the ease of for implementation in real systems. characterization studies of real DRAM chips, that we exploit to develop mechanisms that improve system performance and enhance system security and reliability. real state-of-the-art LPDDR4 DRAM modules, 2) Solar-DRAM, whose key idea is to exploit our observations and issue DRAM accesses with variable latency depending on the target DRAM location’s propensity to fail with reduced access latency, and 3) an evaluation of Solar-DRAM and its three individual components, with comparisons to the state-of-the-art [ improvement over the state-of-the-art DRAM latency reduction mechanism across a wide variety of workloads, without requiring any changes to DRAM chips or software. with high throughput from unmodiﬁed commodity DRAM devices on any system that allows manipulation of DRAM timing parameters in the memory controller. D-RaNGe harvests fully non-deterministic random numbers from DRAM row activation failures, In this dissertation, we present a number of novel observations on DRAM, via First, we introduce 1) a rigorous characterization of activation failures across 282 Second, we propose D-RaNGe, a mechanism for extracting true random numbers which are bit errors induced by intentionally accessing DRAM with lower latency than required for correct row activation. Our TRNG is based on two key observations: 1) activation failures can be induced quickly and 2) repeatedly accessing certain DRAM cells with reduced activation latency results in reading true random data. We validate the quality of our TRNG with the commonly-used NIST statistical test suite for randomness. Our evaluations show that D-RaNGe signiﬁcantly outperforms the previous highest-throughput DRAM-based TRNG by up to 211x (128x on average). We conclude that DRAM row activation failures can be eﬀectively exploited to improve the security of a wide range of systems that use commodity DRAM chips via a highthroughput true random number generator, which can enable a number of security applications such as cryptography. suitable for runtime authentication. The DRAM latency PUF intentionally violates manufacturer-speciﬁed DRAM timing parameters in order to provide many highly repeatable, unique, and unclonable PUF responses with low latency. Through experimental evaluation using 223 state-of-the-art LPDDR4 DRAM devices, we show that the DRAM latency PUF reliably generates PUF responses at runtime-accessible speeds (i.e., 88.2ms on average) at all operating temperatures. We show that the DRAM latency PUF achieves an average speedup of 152x/1426x at 70 compared with a DRAM retention PUF of the same DRAM capacity overhead, and it achieves even greater speedups at lower temperatures. We conclude that the DRAM latency PUF enables a fast and eﬀective substrate for runtime device authentication across all operating temperatures, and we hope that the advent of runtime-accessible PUFs like the DRAM latency PUF and the detailed experimental characterization data we provide on modern DRAM devices will enable security architects to develop even more secure systems for future devices. zation study that demonstrates how the RowHammer vulnerability of modern DDR3, DDR4, and LPDDR4 DRAM chips scales across DRAM generations and technology nodes. Using experimental data from 1580 real DRAM chips produced by the three Third, we introduce and analyze the DRAM latency PUF, a new DRAM PUF Finally, we provide the ﬁrst rigorous experimental RowHammer failure characterimajor DRAM manufacturers, we show that modern DRAM chips that use smaller process technology node sizes are signiﬁcantly more vulnerable to RowHammer than older chips. Using simulation, we show that existing RowHammer mitigation mechanisms 1) suﬀer from prohibitively large performance overheads at projected future hammer counts and 2) are still far from an ideal selective-refresh-based RowHammer mitigation mechanism. Based on our study, we motivate the need for a scalable and low-overhead solution to RowHammer and provide two promising research directions to this end. We hope that the results of our study will inspire and aid future work to develop eﬃcient solutions for the RowHammer bit ﬂip rates we are likely to see in DRAM chips in the near future. and Revisiting RowHammer demonstrate that novel observations via DRAM characterization can be exploited to improve latency, security, and reliability aspects of DRAM by developing mechanisms that exploit the DRAM characteristics when they are accessed with reduced DRAM timing parameters. To explore other methods for improving these aspects of DRAM, our proposal for future work is to 1) characterize various additional timing parameters and circuit-level aspects, and 2) propose mechanisms based on our prior observations. tRCD, to substantially improve DRAM access latency. However, we believe that there are a number of other DRAM timing parameters (e.g., tRP, tWR, tRTP) that could also have substantial impact on overall system performance when reduced below manufacturer-speciﬁed values. Our four preliminary works, Solar-DRAM, the DRAM Latency PUF, D-RaNGe, In Chapter 3, we have shown that we can reduce the DRAM timing parameter, DRAM timing parameters lies in rigorously characterizing DRAM to demonstrate exploitable trends for eﬃciently reducing DRAM timing parameters. While Chapter 3 discusses exploitable spatial distributions for reducing tRCD (i.e., failures constrained to local bitlines), we expect that each timing parameter, when reduced, will result in various exploitable spatial distributions. Using this knowledge gained from characterization, we can then safely and reliably reduce DRAM timing parameters for system performance improvement depending on the region of DRAM being accessed. To further exploit ﬁndings from this type of characterization, we believe the next step would be to determine the behavior of interactions when reducing multiple DRAM timing parameters simultaneously, such that we can further reduce DRAM access latencies reliably. direction. First, it is critical to determine an eﬃcient method for proﬁling DRAM chips (e.g., determining the set of reliable timing parameters). Due to the large search space in parameters (e.g., temperature, data pattern, timing parameter, timing parameter value), a comprehensive characterization for every chip is far too expensive. We believe that searching for correlations between failures of cells with diﬀerent parameters, could help to reduce the characterization phase in such a mechanism. For example, if we ﬁnd that tRCD failures are correlated with tWR failures, then we can simply characterize one of the parameters and set the other value accordingly. Second, it is essential to develop a memory controller and scheme that determines when an access can be issued with reduced timing parameters and dynamically adjusts timing parameters to minimize DRAM access latency while maintaining data correctness during operation. Function (PUF) and a True Random Number Generator (TRNG), for commodity DRAM chips. We believe that there is signiﬁcant room to improve the security and reliability guarantees of each of our proposed mechanisms: The DRAM Latency PUF The key challenge in demonstrating a viable mechanism for reliably reducing We identify two further challenges that are necessary to overcome for enabling this In Chapters 4 and 5, we introduced two security primitives, a Physical Unclonable and D-RaNGe. uniquely identify a larger set of devices and improve the authentication process to minimize the probability for misidentiﬁcation. In the ideal case, the challenge response space would grow exponentially with its input parameters. A PUF with an exponential challenge response space is classiﬁed as a strong PUF which has substantially more use cases compared to a weak PUF. By rigorous characterization of varying DRAM timing parameters, we can determine whether it is feasible to increase the challenge response space simply by using DRAM timing parameters as an additional dimension to the input challenges. Ideally, reducing multiple DRAM timing parameters would result in uncorrelated failure locations such that the input space would grow exponentially with the number of additional DRAM timing parameters used. We believe that following an experimental methodology similar to the one described in Chapter 4 can demonstrate whether this direction is viable. tions of both The DRAM Latency PUF and D-RaNGe for adoption in real devices. We identify various challenges for the DRAM Latency PUF and D-RaNGe. For a practical implementation of The DRAM Latency PUF, it is critical to investigate 1) methods for supporting dynamically changing temperatures at runtime such that a PUF response’s variations due to ambient temperature do not prohibit correct authentication, and 2) a DRAM memory controller that can schedule PUF evaluation accesses with custom DRAM timing parameters and minimize its overhead on running workloads. For a practical implementation of the random number generator, D-RaNGe, we identify three directions that are critical to investigate. First, it is critical to demonstrate a process for quickly identifying RNG cells that work under varying conditions (e.g., temperature). Second, it is critical to investigate a smarter DRAM memory controller that can intelligently schedule D-RaNGe accesses with custom DRAM timing parameters for minimizing the overhead that would come with directly implementing D-RaNGe on any existing memory controller today. Third, it is important to further increase the throughput and decrease the latency. We believe that by combining the First, as discussed in Chapter 4, a PUF with a larger challenge response space can Second, we believe it is important to demonstrate practical end-to-end implementavarying existing DRAM-based TRNGs (discussed in Chapter 4.8), we can invoke the diﬀerent TRNGs (or a combination of them) depending on the running workloads to maximize the utility of idle DRAM time. DRAM storage density and are forecasted to reach 1z and 1a technology nodes within the next couple of years [ chips will likely be increasingly vulnerable to RowHammer. This means that, to maintain market competitiveness without suﬀering factory yield loss, manufacturers will need to develop eﬀective RowHammer mitigation solutions for coping with increasingly vulnerable DRAM chips. DRAM chip itself to system-level mechanisms that are agnostic to the particular DRAM chip that the system uses. Of these solutions, our evaluations in Section 6.5.1 show that, while the ideal refresh-based RowHammer-mitigation mechanism that inserts the minimum possible number of additional refreshes to prevent RowHammer bit ﬂips scales well to relatively low HC when HC ideal mechanism’s beneﬁts (e.g., at least 28% performance degradation when HC is 128). an eﬃcient implementation, we believe it is essential to explore all possible avenues for RowHammer mitigation. Going forward, we identify two promising research directions that can potentially lead to new RowHammer-mitigation solutions that meet or exceed the scalability of the ideal refresh-based mitigation mechanism: (1) DRAM-system cooperation and (2) a proﬁle-guided mechanism. The remainder of this section brieﬂy discusses our vision for each of these directions. DRAM-System Cooperation. mechanisms alone ignores the potential beneﬁts of addressing the RowHammer vul- DRAM manufacturers continue to adopt smaller technology nodes to improve RowHammer-mitigation solutions range from circuit-level mechanisms built into the To achieve the scalability of the ideal refresh-based mechanism while maintaining nerability from both perspectives together. While the root causes of RowHammer bit ﬂips lie within DRAM, their negative eﬀects are observed at the system-level. Prior work [ stack, and we believe that a holistic solution can achieve a high degree of protection at relatively low cost compared to solutions contained within either domain alone. Proﬁle-Guided Mechanisms. susceptible DRAM cells or memory regions can provide a powerful substrate for building targeted RowHammer-mitigation solutions that eﬃciently mitigate RowHammer bit ﬂips at low cost. Knowing (or eﬀectively predicting) the locations of bit ﬂips before they occur in practice could lead to a large reduction in RowHammer-mitigation overhead, providing new information that no known RowHammer-mitigation mechanism exploits today. For example, within the scope of known RowHammer-mitigation solutions, increasing the refresh rate can be made far cheaper by only increasing the refresh rate for known vulnerable DRAM rows. Similarly, ECC and DRAM access counters can be used only for cells that are known to be vulnerable, and even software-based mechanism can be adapted to target only known vulnerable rows (e.g., disabling them, remapping them to reliable memory). ogy today. Our characterization in this work essentially follows the naïve approach of individually testing each row by attempting to induce the worst-case testing conditions (e.g., HC , data pattern, ambient temperature etc.). However, this approach is extremely time consuming due to having to test each row individually (potentially multiple times with various testing conditions). Even for a relatively small DRAM module of 8GB with 8KB rows, hammering each row only once for only one refresh window of 64ms requires over 17 hours of continuous testing, which means that the naïve approach to proﬁling is infeasible for a general mechanism that may be used in a production environment or for online operation. We believe that developing a fast and eﬀective RowHammer proﬁling mechanism is a key research challenge, and we hope that future work will use the observations made in this and other RowHammer characterization studies to ﬁnd a solution. 244] stresses the importance of tackling these challenges at all levels of the Unfortunately, there exists no such eﬀective RowHammer error proﬁling methodoling DRAM device characteristics, we can signiﬁcantly improve system performance and enhance system security and reliability. We have presented four characterization-based works that show by understanding per-chip error characteristics using a proﬁling mechanism, we can develop mechanisms that exploit the chip-dependent error proﬁles to improve system performance or enhance system security and reliability: 1) Solar-DRAM, which exploits our experimental characterization on latency variation within a chip to reduce latency with a proﬁle that identiﬁes regions that can be reliably accessed with lower latencies, 2) The DRAM Latency PUF, which exploits our observation from characterization that latency failures are unique to a DRAM chip due to process manufacturing variation and a proﬁle of these error characteristics can be used generate reliable and unique identiﬁers, 3) D-RaNGe, which demonstrates via characterization that a proﬁle of error characteristics can identify speciﬁc cells in DRAM that can be repeatedly accessed with reduced latency to result in random failures and can be used as an eﬃcient true random number generator, a feature often used in security applications, and 4) Revisiting RowHammer, which demonstrates via characterization that the DRAM-based RowHammer vulnerability is getting worse as technology node size scales and existing RowHammer mitigation mechanisms either do not scale or have prohibitively high overheads to mitigate RowHammer bit ﬂips in future DRAM chips. We conclude and hope that the proposed characterization-based studies of DRAM chips and novel observations will pave the way for new research that can develop new mechanisms to improve system performance, energy eﬃciency, system security, or reliability of future memory systems. In this dissertation, we demonstrated that by rigorously understanding and exploitwith many fellow graduate students from Carnegie Mellon University, ETH Zurich, and other institutions. In this chapter, I would like to acknowledge these works. Minesh Patel, we have developed REAPER [ failures to maintain DRAM reliability while reducing the refresh overhead. We also characterized and studied DRAM chips with on-die ECC [ how on-die ECC aﬀects the DRAM’s properties, and we develop Error-correction Inference (EIN), a statistical inference methodology that infers pre-correction error rates of DRAM with on-die ECC. In a follow-up work called BEER [ a new methodology for determining the full DRAM on-die ECC function (i.e., its parity-check matrix) without hardware support, hardware intrusion, or access to error syndromes and parity information. In collaboration with Hasan Hassan, we developed CROW [ to frequently accessed rows and reduce the overhead of DRAM refresh operations. In collaboration with Yaohua Wang, we developed CAL [ predicts the next access time of a given row, and only partially restores charge to a row that will be accessed soon, and a substrate which can perform cache-block level data relocation within a DRAM bank at a distance-independent latency [ contributed to a retrospective survey [ RowHammer, which was ﬁrst rigorously analyzed in a paper [ before my Ph.D. study. In collaboration with Lucian Cojocar, Stefan Saroiu, and Alec Wolman at Microsoft Research and others, we developed a methodology for testing the RowHammer vulnerability in server nodes and identify an instruction sequence that results in the highest rate of DRAM activations (i.e., hammers). I have also contributed to Ambit [ and a positioning paper describing important domains of work toward the practical construction and widespread adoption of Processing-in-Memory (PIM) architectures. Throughout the course of my Ph.D. study, I have worked on several diﬀerent topics I have worked on a number of other projects on DRAM. In collaboration with 118], a ﬂexible substrate that we use to lower DRAM activation latency I have also contributed to a positioning paper [ remain for the widespread adoption of PIM. nism that provides fairness among requests from diﬀerent applications, SysScale [ a multi-domain power management technique that improves the energy eﬃciency of mobile SoCs, and FlexWatts [ for modern high-end client processors whose goal is to maintain high energy-eﬃciency across the processor’s wide spectrum of power consumption and workloads. I authored GRIM-Filter [ mapping and AirLift [ set of reads from one reference to another reference. I collaborated with Damla Senol on a survey of Nanopore sequencing technologies [ approximate string matching acceleration framework. In addition, I also worked with Can Firtina on Apollo [ algorithm, and with Hongyi Xin on LEAP [ I have also contributed to FLIN [325], a lightweight I/O request scheduling mecha- Another topic that I have developed an interest and worked on was bioinformatics.