The MIPS (maximum inner product search), which nds the item with the highest inner product with a given query user, is an essential problem in the recommendation eld. It is usual that ecommerce companies face situations where they want to promote and sell new or discounted items. In these situations, we have to consider a question: who are interested in the items and how to nd them? This paper answers this question by addressing a new problem called reverse maximum inner product search (reverse MIPS). Given a query vector and two sets of vectors (user vectors and item vectors), the problem of reverse MIPS nds a set of user vectors whose inner product with the query vector is the maximum among the query and item vectors. Although the importance of this problem is clear, its straightforward implementation incurs a computationally expensive cost. We therefore propose Simpfer, a simple, fast, and exact algorithm for reverse MIPS. In an oine phase, Simpfer builds a simple index that maintains a lower-bound of the maximum inner product. By exploiting this index, Simpfer judges whether the query vector can have the maximum inner product or not, for a given user vector, in a constant time. Besides, our index enables ltering user vectors, which cannot have the maximum inner product with the query vector, in a batch. We theoretically demonstrate that Simpfer outperforms baselines employing state-of-the-art MIPS techniques. Furthermore, our extensive experiments on real datasets show that Simpfer is at least two orders magnitude faster than the baselines. The MIPS (maximum inner product search) problem, or𝑘-MIPS problem, is an essential tool in the recommendation eld. Given a query (user) vector, this problem nds the𝑘item vectors with the highest inner product with the query vector among a set of item vectors. The search result, i.e.,𝑘item vectors, can be used as recommendation for the user, and the user and item vectors are obtained via Matrix Factorization, which is well employed in recommender systems [5,8,13,30]. Although some learned similarities via MLP (i.e., neural networks) have also been devised, e.g., in [36,38], [26] has actually demonstrated that inner product-based (i.e., Matrix Factorization-based) recommendations show better performances than learned similarities. We hence focus on inner product between 𝑑-dimensional vectors that are obtained via Matrix Factorization. The𝑘-MIPS problem is eective for the case where a user wants to know items that s/he prefers (i.e., user-driven cases), but ecommerce companies usually face situations where they want to advertise an item, which may be new or discounted one, to users, Table 1: Example of reverse MIPS where q = p. The rows at right illustrate the result of MIPS on P for each u ∈ Q. u⟨3.1, 0.1⟩ p⟨2.8, 0.6⟩ up u⟨2.5, 2.0⟩ p⟨2.5, 1.8⟩ up u⟨1.5, 2.2⟩ p⟨3.2, 1.0⟩ up u⟨1.8, 3.2⟩ p⟨1.4, 2.6⟩ up which corresponds to item-driven cases. Trivially, an eective advertisement is to recommend such an item to users who would be interested in this item. In the context of the𝑘-MIPS problem, if this item is included in the top-k item set for a user, we should make an advertisement of the item to this user. That is, we should nd a set of such users. This paper addresses this new problem, called reverse𝑘-MIPS problem. To ease of presentation, this section assumes that𝑘 =1 (the general case is dened in Section 2). Given a query vectorq(the vector of a target item) and two sets of𝑑-dimensional vectorsQ(set of user vectors) andP(set of item vectors), the reverse MIPS problem nds all user vectors u ∈ Q such that q = arg maxp · u. Example 1. Table 1 illustratesQ,P, and the MIPS result, i.e.,p= arg maxu · p, of each vector inQ. Letq = p, and the result of reverse MIPS is{u, u}becausepis the top-1 item foruandu. Whenq = p, we have no result, becausepis not the top-1 item ∀u ∈ Q. Similarly, when q = p, the result is {u}. From this example, we see that, if an e-commerce service wants to promote the item corresponding top, this service can obtain the users who would prefer this item through the reverse MIPS, and sends them a notication about this item. The reverse𝑘-MIPS problem is an eective tool not only for item-driven recommendations but also market analysis. Assume that we are given a vector of a new item,q. It is necessary to design an eective sales strategy to gain a prot. Understanding the features of users that may prefer the item is important for the strategy. Solving the reverse𝑘-MIPS of the query vectorqsupports this understanding. The above practical situations clarify the importance of reverse MIPS. Because e-commerce services have large number of users and items,|Q|and|P|are large. In addition, a query vector is not pre-known and is specied on-demand fashion. The reverse𝑘-MIPS is therefore conducted online and is computationally-intensive task. Now the question is how to eciently obtain the reverse MIPS result for a given query. A straightforward approach is to run a state-of-the-art exact MIPS algorithm for every vector inQand check whether or notq = arg maxu · p. This approach obtains the exact result, but it incurs unnecessary computation. The poor performance of this approach is derived from the following observations. First, we do not need the MIPS result ofuwhenqdoes not have the maximum inner product withu. Second, this approach certainly accesses all user vectors inQ, although many of them do not contribute to the reverse MIPS result. However, it is not trivial to skip evaluations of some user vectors without losing correctness. Last, its theoretical cost is the same as the brute-force case, i.e.,𝑂 (𝑛𝑚𝑑)time, where 𝑛 = |Q|and𝑚 = |P|, which is not appropriate for online computations. These concerns pose challenges for solving the reverse MIPS problem eciently. To address the above issues, we propose Simpfer, a simple, fast, and exact algorithm for reverse MIPS. The general idea of Simpfer is to eciently solve the decision version of the MIPS problem. Because the reverse MIPS of a queryqrequires a yes/no decision for each vectoru ∈ Q, it is sucient to know whether or not qcan have the maximum inner product foru. Simpfer achieves this in𝑂 (1)time in many cases by exploiting its index built in an oine phase. This index furthermore supports a constant time ltering that prunes vectors in a batch if their answers are no. We theoretically demonstrate that the time complexity of Simpfer is lower than 𝑂 (𝑛𝑚𝑑). The summary of our contributions is as follows: •We address the problem of reverse𝑘-MIPS. To our knowledge, this is the rst work to study this problem. •We propose Simpfer as an exact solution to the reverse MIPS problem. Simpfer solves the decision version of the MIPS problem at both the group-level and the vector-level efciently. Simpfer is surprisingly simple, but our analysis demonstrates that Simpfer theoretically outperforms a solution that employs a state-of-the-art exact MIPS algorithm. •We conduct extensive experiments on four real datasets, MovieLens, Netix, Amazon, and Yahoo!. The results show that Simpfer is at least two orders magnitude faster than baselines. •Simpfer is easy to deploy: if recommender systems have user and item vector sets that are designed in the inner product space, they are ready to use Simpfer via our open source implementation. This is because Simpfer is unsupervised and has only a single parameter (the maximum value of𝑘) that is easy to tune and has no eect on the running time of online processing. This paper is an error-corrected version of [3]. We xed some writing errors and minor bugs in our implementation, but our result is consistent with [3]. Organization.The rest of this paper is organized as follows. We formally dene our problem in Section 2. We review related work in Section 3. Our proposed algorithm is presented in Section 4, and the experimental results are reported in Section 5. Last, we conclude this paper in Section 6. LetPbe a set of𝑑-dimensional real-valued item vectors, and we assume that𝑑is high [22,27]. Given a query vector, the maximum inner product search (MIPS) problem nds The general version of the MIPS problem, i.e., the𝑘-MIPS problem, is dened as follows: Definition 1 (𝑘-MIPS problem). Given a set of vectorsP, a query vectorq, and𝑘, the𝑘-MIPS problem nds𝑘vectors inPthat have the highest inner products with q. For a user (i.e., query), the𝑘-MIPS problem can retrieve𝑘items (e.g., vectors inP) that the user would prefer. Dierent from this, the reverse𝑘-MIPS problem can retrieve a set of users who would prefer a given item. That is, in the reverse𝑘-MIPS problem, a query can be an item, and this problem nds users attracted by the query item. Therefore, the reverse𝑘-MIPS is eective for advertisement and market analysis, as described in Section 1. We formally dene this problem. Definition 2 (Reverse𝑘-MIPS problem). Given a query (item) vectorq,𝑘, and two sets of vectorsQ(set of user vectors) andP(set of item vectors), the reverse𝑘-MIPS problem nds all vectorsu ∈ Q such that q is included in the 𝑘-MIPS result of u among P ∪ {q}. Note thatqcan beq ∈ P, as described in Example 1. We use𝑛and 𝑚 to denote |Q| and |P|, respectively. Our only assumption is that there is a maximum𝑘that can be specied, denoted by𝑘. This is practical, because𝑘should be small, e.g.,𝑘 =5 [16] or𝑘 =10 [4], to make applications eective. (We explain how to deal with the case of𝑘 > 𝑘in Section 4.1.) This paper develops an exact solution to the new problem in Denition 2. Exact 𝑘-MIPS Algorithm.The reverse𝑘-MIPS problem can be solved exactly by conducting an exact𝑘-MIPS algorithm for each user vector inQ. The rst line of solution to the𝑘-MIPS problem is a tree-index approach [9,17,25]. For example, [25] proposed a tree-based algorithm that processes𝑘-MIPS not only for a single user vector but also for some user vectors in a batch. Unfortunately, the performances of the tree-index algorithms degrade for large 𝑑 because of the curse of dimensionality. LEMP [28,29] avoids this issue and signicantly outperforms the tree-based algorithms. LEMP uses several search algorithms according to the norm of each vector. In addition, LEMP devises an early stop scheme of inner product computation. During the computation ofu · q, LEMP computes an upper-bound ofu · q. If this bound is lower than an intermediate𝑘-th maximum inner product,𝑞cannot be in the nal result, thus the inner product computation can be stopped. LEMP is actually designed for the topk inner product join problem: for eachu ∈ Q, it nds the𝑘-MIPS result ofu. Therefore, LEMP can solve the reverse𝑘-MIPS problem, but it is not ecient as demonstrated in Section 5. FEXIPRO [19] further improves the early stop of inner product computation of LEMP. Specically, FEXIPRO exploits singular value decomposition, integer approximation, and a transformation to positive values. These techniques aim at obtaining a tighter upper-bound ofu · qas early as possible. [19] reports that state-ofthe-art tree-index algorithm [25] is completely outperformed by FEXIPRO. Maximus [1] takes hardware optimization into account. It is, however, limited to specic CPUs, so we do not consider Maximus. Note that LEMP and FEXIPRO are heuristic algorithms, and 𝑂 (𝑛𝑚𝑑) time is required for the reverse 𝑘-MIPS problem. Approximation 𝑘-MIPS Algorithm.To solve the𝑘-MIPS problem in sub-linear time by sacricing correctness, many works proposed approximation𝑘-MIPS algorithms. There are several approaches to the approximation𝑘-MIPS problem: sampling-based [7,22,35], LSH-based [15,24,27,33], graph-based [21,23,39], and quantization approaches [10,14]. They have both strong and weak points. For example, LSH-based algorithms enjoy a theoretical accuracy guarantee. However, they are empirically slower than graphbased algorithms that have no theoretical performance guarantee. Literature [4] shows that the MIPS problem can be transformed into the Euclidean nearest neighbor search problem, but it still cannot provide the correct answer. Besides, existing works that address the (reverse) nearest neighbor search problem assume low-dimensional data [34] or consider approximation algorithms [20]. Since this paper focuses on the exact result, these approximation 𝑘-MIPS algorithms cannot be utilized. In addition, approximate answers may lose eectiveness of the reverse𝑘-MIPS problem. If applications cannot contain users, who are the answer of the𝑘MIPS problem, these users may lose chances of knowing the target item, which would reduce prots. On the other hand, if applications contain users, who are not the answer of the𝑘-MIPS problem, as an approximate answer, they advertise the target item to users who are not interested in the item. This also may lose future prots, because such users may stop receiving advertisements if they get those of non-interesting items. To eciently solve the reverse MIPS problem, we propose Simpfer. Its general idea is to eciently solve the decision version of the 𝑘-MIPS problem. Definition 3 (𝑘-MIPS decision problem). Given a queryq,𝑘, a user vectoru, andP, this problem returns yes (no) ifqis (not) included in the 𝑘-MIPS result of u. Notice that this problem does not require the complete𝑘-MIPS result. We can terminate the𝑘-MIPS ofuwhenever it is guaranteed thatq is (not) included in the 𝑘-MIPS result. To achieve this early termination eciently, it is necessary to obtain a lower-bound and an upper-bound of the𝑘-th highest inner product ofu. Let𝜙and𝜇respectively be a lower-bound and an upper-bound of the𝑘-th highest inner product ofuonP. If𝜙 ≥ u· q, it is guaranteed thatqdoes not have the𝑘highest inner product withu. Similarly, if𝜇 ≤ u · q, it is guaranteed thatqhas the𝑘 highest inner product withu. This observation implies that we need to eciently obtain𝜙and𝜇. Simpfer does pre-processing to enable it in an oine phase. Besides, since𝑛 = |Q|is often large, accessing all user vectors is also time-consuming. This requires a ltering technique that enables the pruning of user vectors that are not included in the reverse𝑘-MIPS result in a batch. During the preprocessing, Simpfer arrangesQso that batch ltering is enabled. Simpfer exploits the data structures built in the pre-processing phase to quickly solve the 𝑘-MIPS decision problem. The objective of this pre-processing phase is to build data structures that support ecient computation of a lower-bound and an upper-bound of the𝑘-th highest inner product for eachu∈ Q, for arbitrary queries. We utilize Cauchy–Schwarz inequality for upperbounding. Hence we need the Euclidean norm∥u∥for eachu∈ Q. To obtain a lower-bound of the𝑘-th highest inner product, we need to access at least𝑘item vectors inP. The norm computation and lower-bound computation are independent of queries (as long as 𝑘 ≤ 𝑘), so they can be pre-computed. In this phase, Simpfer builds the following array for each u∈ Q. Definition 4 (Lower-bound array). The lower-bound array𝐿of a user vectoru∈ Qis an array whose𝑗-th element,𝐿, maintains a lower-bound of the 𝑗-th inner product of uon P, and |𝐿| = 𝑘. Furthermore, to enable batch ltering, Simpfer builds a block, which is dened below. Definition 5 (Block). A blockBis a subset ofQ. The set of vectors belonging toBis represented byQ(B). Besides, we use𝐿(B)to represent the lower-bound array of this block, and The block size|Q(B)|can be arbitrarily determined, and we set |Q(B)| = 𝑂 (log 𝑛) to avoid system parameter setting. Pre-processing algorithm.Algorithm 1 describes the pre-processing algorithm of Simpfer. (1) Norm computation: First, for eachu ∈ Qandp ∈ P, its norm is computed. Then, Q and P are sorted in descending order of norm. (2) Lower-bound array building: LetPbe the set of the rst𝑂 (𝑘) vectors inP. For eachu∈ Q,𝐿is built by usingP. That is, 𝐿= u· p, wherep ∈ Pyields the𝑗-th highest inner product for u ∈ P. The behind idea of using the rst𝑂 (𝑘)item vectors inPis that vectors with large norms tend to provide large inner products [21]. This means that we can obtain a tight lower-bound at a lightweight cost. (3) Block building: After that, blocks are built, so that user vectors in a block keep the order and each block is disjoint. Given a new blockB, we insert user vectorsu∈ QintoQ(B)in sequence ← the rst 𝑂 (𝑘) vectors in P while updating𝐿(B), until we have|Q(B)| = 𝑂 (log 𝑛). When |Q(B)| = 𝑂 (log 𝑛), we insertBinto a set of blocksB, and make a new block. Example 2. Figure 1 illustrates an example of block building. For ease of presentation, we use𝑏as a block size and𝑛 =3𝑏. For example, Q(B) = {u, ..., u}, and ∥u∥ ≥ ... ≥ ∥u∥. Generally, this pre-processing is done only once. An exception is the case where a query with𝑘 > 𝑘is specied. In this case, Simpfer re-builds the data structures then processes the query. This is actually much faster than the baselines, as shown in Section 5.7. Analysis.We here prove that the time complexity of this preprocessing is reasonable. Without loss of generality, we assume 𝑛 ≥ 𝑚, because this is a usual case for many real datasets, as the ones we use in Section 5. Theorem 1. Algorithm 1 requires 𝑂 (𝑛(𝑑 + log 𝑛)) time. Proof. The norm computation requires𝑂 ((𝑛 + 𝑚)𝑑) = 𝑂 (𝑛𝑑)time, and sorting requires𝑂 (𝑛 log 𝑛)time. The building of lower-bound arrays needs𝑂 (𝑛 × 𝑘)time, since𝑂 (|P|) = 𝑂 (𝑘). Because 𝑘= 𝑂 (1),𝑂 (𝑛×𝑘) = 𝑂 (𝑛). The block building also requires 𝑂 (𝑛 × 𝑘) = 𝑂 (𝑛)time. In total, this pre-processing requires The space complexity of Simpfer is also reasonable. Theorem 2. The space complexity of the index is 𝑂 (𝑛). Proof. The space of the lower-bound arrays of user vectors isÍ 𝑂 (|𝐿|) = 𝑂 (𝑛), since𝑂 (|𝐿|) = 𝑂 (1). Blocks are disjoint, and the space of the lower-bound array of a block is also𝑂 (1). We hence have𝑂 ()lower-bound arrays of blocks. Now this theorem is Before we present the details of Simpfer, we introduce our techniques that can quickly answer the𝑘-MIPS decision problem for a given queryq. Recall thatQandPare sorted in descending order of norm. Without loss of generality, we assume that∥u∥ ≥ ∥u∥ for each𝑖 ∈ [1, 𝑛 −1]and∥p∥ ≥ ∥p∥for each𝑗 ∈ [1,𝑚 −1], for ease of presentation. Given a queryqand a user vectoru∈ Q, we haveu·q. Although our data structures are simple, they provide eective and “lightweight” lters. Specically, we can quickly answer the𝑘-MIPS decision problem on q through the following observations. Lemma 1. Ifu· q ≤ 𝐿, it is guaranteed thatqis not included in the 𝑘-MIPS result of u. Proof. Letpbe the vector inPsuch thatu· pis the𝑘-th highest inner product inP. The fact that𝐿≤ u· pimmediately derives It is important to see that the above lemma provides “no” as the answer to the𝑘-MIPS decision problem onqin𝑂 (1)time (after computingu· q). The next lemma deals with the “yes” case in𝑂 (1) time. Lemma 2. Ifu· q ≥ ∥u∥∥p∥, it is guaranteed thatqis included in the 𝑘-MIPS result of u. Proof. From Cauchy–Schwarz inequality, we haveu·p≤ ∥u∥∥p∥. Since∥p∥is the𝑘-th highest norm inP,u· p ≤ ∥u∥∥p∥, where pis dened in the proof of Lemma 1. That is,∥u∥∥p∥is an upper-bound ofu· p. Now it is clear thatqhasu· q ≥ u· p if u· q ≥ ∥u∥∥p We next introduce a technique that yields “no” as the answer for all user vectors in a block B in 𝑂 (1) time. Lemma 3. Given a blockB, letube the rst vector inQ(B). If ∥u∥∥q∥ ≤ 𝐿(B), for allu∈ Q(B), it is guaranteed thatqis not included in the 𝑘-MIPS result of u. Proof. From Cauchy–Schwarz inequality,∥u∥∥q∥is an upperbound ofu· qfor allu∈ Q(B), sinceQ(B) = {u, u, ...}. We have𝐿(B) ≤ 𝐿for allu∈ Q(B), from Equation (1). Therefore, if ∥u∥∥q∥ ≤ 𝐿(B), u· q cannot be the 𝑘 highest inner product. □ If a user vectorucannot obtain a yes/no answer from Lemmas 1–3, Simpfer uses a linear scan ofPto obtain the answer. Let𝜏be a threshold, i.e., an intermediate𝑘-th highest inner product foru Algorithm 2: Linear-scan(𝑢) Input: u ∈ Q, P, q, and 𝑘 during the linear scan. By using the following corollaries, Simpfer can obtain the correct answer and early terminate the linear scan. Corollary 1. Assume thatqis included in an intermediate result of the𝑘-MIPS ofuand we now evaluatep∈ P. Ifu· q ≥ ∥u∥∥p∥, it is guaranteed thatqis included in the nal result of the𝑘-MIPS ofu. Proof. Trivially, we have𝑗 ≥ 𝑘. Besides,∥u∥∥p∥ ≥ u· pfor all From this corollary, we also have: Corollary 2. When we have𝜏 > u· q, it is guaranteed thatqis not included in the nal result of the 𝑘-MIPS of u. Algorithm 2 summarizes the linear scan that incorporates Corollaries 1–2. Now we are ready to present Simpfer. Algorithm 3 details it. To start with, Simpfer computes∥q∥. Given a blockB ∈ B, Simpfer tests Lemma 3 (line 4). If the user vectors inQ(B)may have yes as an answer, for eachu∈ Q(B), Simpfer does the following. (Otherwise, all user vectors inQ(B)are ignored.) First, it computesu· q, then tests Lemma 1 (line 7). Ifucannot have the answer from this lemma, Simpfer tests Lemma 2. Simpfer insertsuinto the result setQif u· q ≥ ∥u∥∥p∥. Otherwise, Simpfer conducts Linear-scan(u) (Algorithm 2). If Linear-scan(u)returns 1 (yes),uis inserted intoQ. The above operations are repeated for eachB ∈ B. Finally, Simpfer returns the result set Q. The correctness of Simpfer is obvious, because it conducts Linearscan(·)for all vectors that cannot have yes/no answers from Lemmas 1–3. Besides, Simpfer accesses blocks sequentially, so it is easy to parallelize by using multicore. Simpfer hence further accelerates the processing of reverse 𝑘-MIPS, see Section 5.6. We theoretically demonstrate the eciency of Simpfer. Specically, we have: Input: Q, P, q, 𝑘, and B ← ∅, Compute ∥q∥ Theorem 3. Let𝛼be the pruning ratio (0≤ 𝛼 ≤1) of blo cks inB. Furthermore, let𝑚be the average number of item vectors accessed in Linear-scan(·). The time complexity of Simpfer is𝑂 ((1− 𝛼)𝑛𝑚𝑑). Proof. Simpfer accesses all blocks inB, and|B| = 𝑂 (). Assume that a blockB ∈ Bis not pruned by Lemma 3. Simpfer accesses all user vectors inQ(B), so the total number of such user vectors is(1− 𝛼) × 𝑂 () × 𝑂 (log 𝑛) = 𝑂 ((1− 𝛼)𝑛). For these vectors, Simpfer computes inner products withq. The evaluation cost of Lemmas 1 and 2 for these user vectors is thus𝑂 ((1− 𝛼)𝑛𝑑). The worst cost of Linear-scan(·)for vectors that cannot obtain the answer from these lemmas is𝑂 ((1− 𝛼)𝑛𝑚𝑑). Now the time complexity of Simpfer is 𝑂 (𝑛log 𝑛+ (1 − 𝛼)𝑛𝑑 + (1 − 𝛼)𝑛𝑚𝑑) = 𝑂 (𝑛log 𝑛+ (1 − 𝛼)𝑛𝑚𝑑) Remark.There are two main observations in Theorem 3. First, because we practically have𝑚< 𝑚and𝛼 >0, Simpfer outperforms a 𝑘-MIPS-based solution that incurs𝑂 (𝑛𝑚𝑑)time. (Our experimental results show that𝑚= 𝑂 (𝑘)in practice.) The second observation is obtained from Equation (2), which implies the eectiveness of blocks. If Simpfer does not build blocks, we have to evaluate Lemma 1 for allu ∈ Q. Equation (2) suggests that the blocks theoretically avoids this. This section reports our experimental results. All experiments were conducted on a Ubuntu 18.04 LTS machine with a 12-core 3.0GHz Intel Xeon E5-2687w v4 processor and 512GB RAM. Datasets.We used four popular real datasets: MovieLens, Netix, Amazon, and Yahoo!. The user and item vectors of these datasets were obtained by the Matrix Factorization in [6]. These are 50dimensional vectors (the dimensionality setting is the same as [19, 29]). The other statistics is shown in Table 2. We randomly chose 1,000 vectors as query vectors from P. Evaluated algorithms.We evaluated the following three algorithms. •LEMP [29]: the state-of-the-art all-𝑘-MIPS algorithm. LEMP originally does 𝑘-MIPS for all user vectors in Q. •FEXIPRO [19]: the state-of-the-art𝑘-MIPS algorithm. We simply ran FEXIPRO for each u ∈ Q. •Simpfer: the algorithm proposed in this paper. We set𝑘= 25. These algorithms were implemented in C++ and compiled by g++ 7.5.0 with -O3 ag. We used OpenMP for multicore processing. These algorithms return the exact result, so we measured their running time. Note that [19,29] have demonstrated that the other exact MIPS algorithms are outperformed by LEMP and FEXIPRO, so we did not use them as competitors. (Recall that this paper focuses on the exact answer, thus approximation algorithms are not appropriate for competitors, see Section 3.) In addition, LEMP and FEXIPRO also have a pre-processing (oine) phase. We did not include the oine time as the running time. We rst clarify the eectiveness of blocks employed in Simpfer. To show this, we compare Simpfer with Simpfer without blocks (which does not evaluate line 4 of Algorithm 3). We set 𝑘 = 10. On MovieLens, Netix, Amazon, and Yahoo!, Simpfer (Simpfer without blocks) takes 10.3 (22.0), 58.6 (100.8), 117.6 (446.2), and 1481.2 (1586.2) [msec], respectively. This result demonstrates that, although the speed-up ratio is aected by data distributions, blocks surely yield speed-up. We investigate how𝑘aects the computational performance of each algorithm by using a single core. Figure 2 depicts the experimental results. We rst observe that, as𝑘increases, the running time of each algorithm increases, as shown in Figures 2(a)–2(d). This is reasonable, because the cost of (decision version of)𝑘-MIPS increases. As a proof, Figures 2(e)–2(h) show that the number of inner product (ip) computations increases as𝑘increases. The running time of Simpfer is (sub-)linear to𝑘(the plots are log-scale). This suggests that 𝑚= 𝑂 (𝑘). Second, Simpfer signicantly outperforms LEMP and FEXIPRO. This result is derived from our idea of quickly solving the𝑘-MIPS decision problem. The techniques introduced in Section 4.2 can deal with both yes and no answer cases eciently. Therefore, our approach functions quite well in practice. Last, an interesting observation is the performance dierences between FEXIPRO and Simpfer. Let us compare them with regard to running time. Simpfer is at least two orders of magnitude faster than FEXIPRO. On the other hand, with regard to the number of inner product computations, that of Simpfer is one order of magnitude lower than that of FEXIPRO. This result suggests that the ltering cost of Simpfer is light, whereas that of FEXIPRO is heavy. Recall that Lemmas 1–3 need only𝑂 (1)time, and Corollaries 1–2 need 𝑂 (𝑘)time in practice. On the other hand, for each user vector in𝑄, FEXIPRO incursΩ(𝑘)time, and its ltering cost is𝑂 (𝑑), where 𝑑< 𝑑. For high-dimensional vectors, the dierence between𝑂 (1) and𝑂 (𝑑)is large. From this point of view, we can see the eciency of Simpfer. We next study the scalability to𝑛 = |Q|by using a single core. To this end, we randomly sampled𝑠 × 𝑛user vectors inQ, and this sampling rate𝑠has𝑠 ∈ [0.2,1.0]. We set𝑘 =10. Figure 3 shows the experimental result. In a nutshell, we have a similar result to that in Figure 2. As 𝑛increases, the running time of Simpfer linearly increases. This result is consistent with Theorem 3. Notice that the tendency of the running time of Simpfer follows that of the number of inner product computations. This phenomenon is also supported by Theorem 3, because the main bottleneck of Simpfer is Linear-scan (·). The scalability to𝑚 = |P|by using a single core is also investigated. We randomly sampled𝑠 × 𝑚user vectors inP, as with the previous section. Figure 4 shows the experimental result where𝑘 =10. Interestingly, we see that the result is dierent from that in Figure 3. The running time of Simpfer is almost stable for dierent𝑚. In this experiment,𝑛and𝑘were xed, and recall that𝑚= 𝑂 (𝑘). From this observation, the stable performance is theoretically obtained. This scalability of Simpfer is an advantage over the other algorithms, since their running time increases as 𝑚 increases. We study the gain of multicore processing of Simpfer by setting 𝑘 =10. We depict the speedup ratios compared with the single-core case in Table 3. We see that Simpfer receives the benet of multicore processing, and its running time decreases as the number of available cores increases. We here explain why Simpfer cannot obtain speedup ratio𝑐, where𝑐is the number of available cores. Each core deals with dierent blocks, and the processing cost of a given blockBis dierent from those of the others. This is because it is unknown Figure 2: Impact of 𝑘: Running time (top) and #ip computations (bottom). “×” shows LEMP, “◦” shows FEXIPRO, and “△” shows Simpfer. Figure 3: Impact of |Q |: Running time (top) and #ip computations (bottom). “×” shows LEMP, “◦” shows FEXIPRO, and “△” shows Simpfer. #cores MovieLens Netix Amazon Yahoo! whetherBcan be pruned by Lemma 3. Even if we magically know the cost, it is NP-hard to assign blocks so that each core has the same processing cost [2,18]. Therefore, perfect load-balancing is impossible in practice. The Yahoo! case in particular represents this phenomenon. Because many user vectors in Yahoo! have large norms, blocks often cannot be ltered by Lemma 3. This can be seen from the observation in Figure 3(h): the number of inner product computations on Yahoo! is larger than those on the other datasets. Figure 4: Impact of |P|: Running time (top) and #ip computations (bottom). “×” shows LEMP, “◦” shows FEXIPRO, and “△” shows Simpfer. The costs of Corollaries 1–2 are data-dependent (i.e., they are not pre-known), rendering a fact that Yahoo! is a hard case for obtaining a high speedup ratio. Table 4: Pre-processing time of Simpfer [sec] Last, we report the pre-processing time of Simpfer. Table 4 shows the results. As Theorem 1 demonstrates, the pre-processing time increases as𝑛increases. We see that the pre-processing time is reasonable and much faster than the online (running) time of the baselines. For example, the running time of FEXIPRO on Amazon with𝑘 =25 is 1206 [sec]. When𝑘 =25 (i.e.,𝑘 = 𝑘), the total time of pre-processing and online processing of Simpfer is 15.10+0.16=15.26 [sec]. Therefore, even if𝑘 > 𝑘is specied, re-building blocks then processing the query by Simpfer is much faster. This paper introduced a new problem, reverse maximum inner product search (reverse MIPS). The reverse MIPS problem supports many applications, such as recommendation, advertisement, and market analysis. Because even state-of-the-art algorithms for MIPS cannot solve the reverse MIPS problem eciently, we proposed Simpfer as an exact and ecient solution. Simpfer exploits several techniques to eciently answer the decision version of the MIPS problem. Our theoretical analysis has demonstrated that Simpfer is always better than a solution that employs a state-of-the-art algorithm of MIPS. Besides, our experimental results on four real datasets show that Simpfer is at least two orders of magnitude faster than the MIPS-based solutions. This research is partially supported by JST PRESTO Grant Number JPMJPR1931, JSPS Grant-in-Aid for Scientic Research (A) Grant Number 18H04095, and JST CREST Grant Number JPMJCR21F2.