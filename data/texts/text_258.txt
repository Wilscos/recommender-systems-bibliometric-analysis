School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, 10 Xitucheng Road, Haidian District, With the development of social platforms, people are more and more inclined to combine into groups to participate in some activities, so group recommendation has gradually become a problem worthy of research. For group recommendation, an important issue is how to obtain the characteristic representation of the group and the item through personal interaction history, and obtain the group’s preference for the item. For this problem, we proposed the model GIP4GR (Graph Neural Network with Interaction Pattern For Group Recommendation). Speciﬁcally, our model use the graph neural network framework with powerful representation capabilities to represent the interaction between group-user-items in the topological structure of the graph, and at the same time, analyze the interaction pattern of the graph to adjust the feature output of the graph neural network, the feature representations of groups, and items are obtained to calculate the group’s preference for items. We conducted a lot of experiments on two real-world datasets to illustrate the superior performance of our model. As the speed of information dissemination increases, more and more information begins to emerge in front of people. To solve the problem of the information overload and help people choose information of interest, recommendation systems have been widely deployed in online information systems, such as e-commerce platforms, social media sites, news portals, etc. The recommendation system can not only increase the traﬃc of service providers but also help customers ﬁnd items of interest more easily. At present, most recommendation systems are for individual individuals to recommend, but as people’s communication on social platforms becomes more convenient, people are more inclined to combine into groups to participate in activities. From this perspective, some studies have investigated recommending items to target user groups rather than individual users. This problem is called Group Recommendation [1]. This form of users as a group is very common in online social media, users can organize into groups to participate in some online or oﬄine activities. Figure 1 is a simple example of group interaction and individual interaction. In Figure 1, users Springer Nature 2021 LTEX template u, u, u, u, and uinteracted with items t, t, t, trespectively when they are individuals. When u, u, uform g, they interact with items t. u, uform ginteracted with item t. Our task is to predict the items to be interacted by group gbased on the interaction history of uand u when uand uform group g. Traditional recommendation methods are divided into model fusion and score fusion [2]. The score-based fusion only simulates the group’s scores for items for each group member’s scores (speciﬁc methods can be divided into average scores, maximum scores, and minimum scores). This method only considers the group score arbitrarily. The relationship between the members and the ﬁnal selection of items without further consideration of the inﬂuence of the members of the group on the decision. Model-based fusion is to train individual and item interaction models for each member of the group recently, and aggregate their preferences, so as to obtain a model as a group and then perform score prediction. This method can only be learned the abstract relationship between the group model and the individual model cannot measure the participation of diﬀerent members in the ﬁnal decision based on the interaction history of diﬀerent members, and the model-based method needs to train the model for each group member, and compare the number of some users, for large datasets, the computational cost will be very large, so traditional recommendation methods cannot eﬀectively perform recommendations. Recently, for the problem that previous work cannot eﬀectively measure the participation of individuals in group decision-making, some scholars’ research on group recommendation focuses on how to automatically quantify the relative importance of individuals in group decision-making, that is, through the interaction of each member historical learning then determines the proportion of individuals in the ﬁnal group decision to make trade-oﬀ decisions instead of using empirical strategies. For example, the attention mechanism is used to measure the importance of group members [3, 4], the interaction history of each diﬀerent group member is used to obtain diﬀerent weights, and their weighted characteristics are used as the representation of the ﬁnal group, thus making up for the traditional The shortcomings of recommendation strategies highlight the role played by individuals in group decision-making, and therefore exceed the performance of traditional group recommendation methods. At the same time, some scholars have proposed that the members of the group may have mutual inﬂuence [5], so a model is designed for each member to learn the interaction relationship between him and other members in the group, from his own and the other members of the group. The similarity (or inﬂuence) between others is used to update the individual’s characteristic representation, and ﬁnally, the representations of the members in the group are added to obtain the group’s representation. This method achieves quite good results, but the problem is similar to the traditional one based on the model fusion method is the same, that is, for a group with a large number of group members, it is expensive to train the model for each group member interacting with other group members. And these current methods are recommended based on the existing deep neural network methods. Although deep networks have powerful data ﬁtting capabilities, they ignore the internal topological structure of the data and cannot learn the deep layers of the data more eﬀectively. At the same time, group interaction data can be better realized by graph representation, so if a graph neural network is used, more speciﬁc features can be learned. In reality, many structures can be represented by graphs (such as social networks, protein structures, traﬃc networks, etc.), and the related technologies of graph neural networks have gradually matured [6]. Therefore, graph neural networks have already had a wide range of applications. This also means that the abstract data structure can be expressed more vividly way, and the message passing method is adopted in the feature extraction, that is, a node can aggregate the information of the surrounding nodes, so it has a good eﬀect on information aggregation. Given the advantages of graph neural network, this article uses the graph neural networks method to solve the problem of group recommendations. Some works have begun to use graph neural networks to study group recommendations [7], but only use the connectivity of the nodes of the graph to express the order of interaction, and do not go deep into the graph structure of group interaction. If a graph is used to represent the interaction between group users and items, the structure of the graph may be diﬀerent depending on the dataset. For example, in some scenarios, when users form a group, they will choose to revisit the previous place, that is, the items selected by the group will be a subset of the items the user has interacted with. At the same time, it is also possible that when a user forms a group, they will interact with an item that their group members have not interacted with. Therefore, in these two cases, the use of diﬀerent layers of graph neural networks will lead to a large discrepancy in model performance. In general, the main contributions of this article are as follows: •This article put forward the concept of interactive repetition rate(IRR), which distinguishes the interactive pattern of diﬀerent items (such as movies, tourist attractions, restaurants, etc.), that is, how much users like to conduct group activities when they choose the ones they have interacted with. •We propose a new group recommendation model GIP4GR based on a graph neural network and apply the proposed theory of behavior patterns to this model to form a universal model of group behavior. •We conduct comprehensive experiments on two real-world datasets, and demonstrated the advantages of our model compared with the existing model, and veriﬁed some key designs of the model. The structure of this paper is as follows: Section 2 summarizes the research status of group recommendation related work. Section 3 introduces the problem to be solved and the way the problem is expressed. Section 4 presents our solution to this problem. Section 5 introduces some details of the experiment and analyzes the results. Finally, Section 6 summarizes the paper and outlines the current shortcomings and future research directions. In this section we will introduce the problem and the related algorithms to solve the problem. Speciﬁcally, we will focus on current eﬀort on group recommendation and the development of the graph neural network. Group recommendation requires that the personal preferences of all members in the group be properlyintegrated, so it requires a part of the process than a general personal recommendation. Traditional grouprecommendation methods can be divided into memory-based and model-based methods. The ﬁrst is the memory-based method. O’Connor et al. [1] have proposed some traditional methods to aggregate recommendation results based on scores, such as maximum satisfaction (the highest score of a member in a group is selected as the group score for an item, in order to maximize group satisfaction), minimum pain (for an item, select the lowest score of a member in the group as the group score, thereby pleasing all members in the group), average (for an item, select the average score of the members in the group as the group’s ﬁnal score, thus weighing the maximum satisfaction and minimum pain) and other methods [8], however, these strategys is too empirical and too intuitive, ignoring the relationship within the group (for example, diﬀerent members have diﬀerent decision-making processes contribution). At the same time, each group within each dataset may have diﬀerent aggregation methods, that is, some groups tend to be the most satisﬁed, some tend to be the least painful, and it is impossible to generalize to use a standard. The second is model-based methods [9, 10] mostly using pre-processing methods. Modeling the interaction between individuals and items in the group, and then fusing these user models to obtain a model representing the group, and then Springer Nature 2021 LTEX template calculating the score of the interaction between the group and the item, so only relatively shallow features are learned. However, groups in real life change frequently, and the members of the group may also partially overlap, which are only formed for very few activities. Moreover, it is not possible to reﬁne the participation of each member in the group. At the same time, the probability model is also applied to solve the group recommendation. The PIT model [10] proposes to represent the entire group as the user with the greatest inﬂuence in the group, but ignores the fact that the model will be eﬀective only when the user with the greatest inﬂuence is an expert in this area, otherwise, the group internal attention will also shift in this respect, that is, the inﬂuential ones will listen to the experienced ones. Recently, in order to measure the percentage of participation of members in the group based on the interactions that the group has previously participated in, a deep representation model based on learning has been proposed [3, 5, 11], all of which use the attention mechanism [12], to measure the weight of diﬀerent members of the group. It turns out that they perform better than models based on score aggregation or model aggregation. Algorithms for extracting graph information are also emerging in endlessly. For example, GCN [13] weights and aggregates the information of surrounding nodes for each node in the graph according to the in-degree and out-degree of the node; GraphSAGE [14] is to sample the surrounding nodes of a node and then aggregate and provide a variety of aggregation methods, such as Max-Pooling, LSTM, Mean-Pooling; GAT [15] is to calculate the information around each node, get the weight of each node around, and then obtains the weighted central node information. These methods can be eﬀectively applied to the recommendation system. Most of the recommendation system models introduced before are based on general deep neural networks, etc., and do not take advantage of the powerful representation capabilities of graph neural networks. The relationship between groups and users can be represented by graphs for more signiﬁcant learning, to the characteristics of the group. The application of graph neural network in recommendation system includes: Pin-Sage [16]is a kind of spread representation on the project graph through the graph convolutional layer. NGCF [17] models the bipartite graph of user items, so as to learn to aggregate the interactive information of user items. RippleNet [18] uses a knowledge graph to aggregate interactive items and richer personal information and item information. At the same time, GLS-GRL [7] also uses graph neural networks for the problem of sequential group recommendation, that is, graph neural networks are used to represent longterm and short-term user-item interactions, so as to learn how group interests change over time. SR-GNN [6] uses a graph neural network to learn the sequence relationship of items interacted in a session to generate embedding representation and obtains the score of each item through attention mechanism and linear transformation. It can be seen that the application of graph neural networks in recommendation system have been widely used. However, for some datasets with diﬀerent behavior patterns, the graph neural network with a ﬁxed number of layers is easy to show over-smoothness [19] or under-ﬁtting phenomenon resulting in poor recommendation performance, that is, GNNs that need to adjust the model artiﬁcially for diﬀerent datasets. To solve this problem, we dynamically learn the group interaction mode to obtain the interaction repetition rate(IRR) of the group to give diﬀerent importance to the output of diﬀerent layers as the ﬁnal feature representation. In this section we will formulate the problem we will sovle and the symbol we will use in the following sections. Suppose we have N users U ={u, u,..., u}, S groups G={g, g,..., g}, and M items T ={t, t,. .., t}. The i-th group g∈ G contains members {u, u...}. There are three interaction relationships in the dataset, namely group-user, group-item, and user-item. The entire dataset can be seen as an interact graph, where the items, Springer Nature 2021 LTEX template users, groups, can be seen as the nodes of the graph and their interaction-ship can be seen as the edge of the graph. Problem: For a speciﬁc group, generate a corresponding Top@N recommended item list. Input: user set U, group set G, item set T, users, items, groups’interact graph. Output: A list of items generated for group g, T’ ={t, t,...}. In this part we will introduce component of the model and training details. The overall structure of the model is shown in Figure 2. In general, our proposed model GIP4GR includes three parts. 1) Calculating the interactive repetition rate (section 4.1). The green dashed box (IRR Block) in Figure 2 is mainly used to calculate the interactive repetition rate according to the behavior characteristics of diﬀerent datasets. 2) Learning interactive information (section 4.2) is the blue dashed box (GNN Block) in Figure 2, which is the part of applying graph neural network to learn the in-depth information of the topological structure of the dataset. 3) Get the ﬁnal representation (section 4.3) in Figure 2 is a yellow dashed box (Fusion Block), according to diﬀerent interaction repetition rates and interaction information to get the ﬁnal group, user, and item embedding. Training details are illustrated in section 4.4. This part corresponds to the green dashed frame block in Figure 2, mainly for calculating IRR indicators and applying them to subsequent calculation tasks. In daily life, the scenarios in which we form groups and then choose items to interact can be divided into the following two types: As shown by the red circle in Figure 3, in certain scenes (such as traveling) when u, u, and u are organized into group g, they choose twhich they have not experienced before. That is, when people organize into groups, they may get tired of what they have experienced before or organize into groups to participate in projects suitable for group activities. Figure 1 is also the usual situation. Figure 3 is just the opposite. When u, u, and u are organized into a group g, they choose the t item that uhas experienced before, and then the item tthat uhas experienced before does not work. For example, in a watching movie scenario, users like spend time and energy to form a team to watch the movie they have watched. This leads to diﬀerent interaction patterns for diﬀerent datasets so that the ﬁxed number of GNN layers in the model cannot reach a certain value. In order to measure the extent to which the used dataset belongs to which of these two situations, we propose an indicator of Interactive Repetition Rate (IRR): IRR is used to adjust the weight of the output of diﬀerent layers of the network in the ﬁnal output, which solves the problem of the group interaction type and the number of layers of the graph neural network. This part corresponds to the blue dotted box block in Figure 2, which uses the current mature GraphSAGE and GAT to aggregate and learn the characteristics of user nodes, project nodes, and group nodes. The partial structure of the GNN is shown in Figure 4. We use the sampling and aggregation framework GraphSAGE to aggregate the Springer Nature 2021 LTEX template group-item and user-item (bidirectional edges) interactions. The formula is as follows: e= AGGREGAT E{e, ∀t ∈ N, N∈ T } Where aggtype in this paper that is the MaxPooling method. σ means the sigmoid activation, Wis the learning parameter matrix of the l layer, Nrepresents the one-hop adjoining node of u, the feature of iteration is that the item t nodes around the user node u are aggregated in the formula, and the aggregation of item nodes and group nodes is also the same. In the group-user interaction, we use a oneway edge (from the user to the group), and use the GAT convolution method on this type of edge, aiming to obtain diﬀerent attention weights for users according to diﬀerent user characteristics. The formula is as follows: Among them, actf means the activation function, here we use the LeakyReLU in GAT, α represents the proportion of member u in group g. We only aggregate the user’s characteristics into the group, that is, the user does not aggregate group information. This is to allow the group to use the user’s historical interaction information. Learn the information related to the ﬁnal decision of the group. Here we use only one attention head in GAT (experiments have shown that one Springer Nature 2021 LTEX template head can achieve good results and also reduces computational overhead). It is worth noting that these ﬁve edge convolution methods are all performed at the same time, and the nodes that have been repeatedly convolved are added. (For item tin Figure 3, the information of gand uwill be gathered at the same time, and gthe information of the person is added), that is, all nodes are only equivalent to applying a layer of GNN. This part corresponds to the Fusion Block in the yellow dashed frame in Figure 2 and uses the interactive repetition rate to obtain the weighted sum of the network output of diﬀerent levels as the ﬁnal feature representation. As shown by the red and green circles in Figure 3, the datasets of diﬀerent group interaction repetition rates have diﬀerent graph diameters, so it is very important to choose the number of layers of the graph network. If the number of selected layers is too small, the embedded representation ability will be insufﬁcient, that is, the group representation cannot fully obtain the necessary information of its surrounding nodes, e.g.: Assume that the items selected by the group are not interacted by the group members before. If only one layer of the network is used, the group can only aggregate the information of its members and the information of the items selected by the group, but cannot learn the information of the historical interaction items of the group members, which means that it is impossible to learn from the interaction of a single group member to the aggregation. If the number of layers selected is too large, it may cause an over-smoothing phenomenon [19], that is, the information of the group and the item is similar, e.g.: Assuming that the item selected by the group is previously interacted by a member of the group, then if using two layer GNN will ﬁrst cause the group to interact the item and the group members who have interacted with the item (as shown in g, u, and tin Figure 3), the group gwill be aggregated twice u’s information, twice t’s information, uand tare also similar), the information has been spread many times, resulting in the three expressions being very similar, and secondly, some team members (not enough to contribute to the ﬁnal decision) may have historical interactions information is learned, resulting in information redundancy, and it is diﬃcult to assign diﬀerent weights to group members. Therefore, we propose to use the previously calculated group interaction repetition rate as a trade-oﬀ. We propose to use the two layer GNN model mentioned before for feature learning (from Figure 3, it can be seen that if the group needs to learn the ﬁnal feature, the maximum diameter is item (group selection)-group-member-item (Member selection), that is, it takes two hops to get the group member’s interest information and the item information to be aggregated into the group, so the maximum number of layers is set to two), and we use the interactive repetition rate to neutralize the ﬁrst and second layers The output of is used as the ﬁnal representation. That is, if the interactive repetition rate is relatively large, it means that the information output by the ﬁrst layer of the graph Springer Nature 2021 LTEX template network is more important (only the information of the surrounding nodes of the ﬁrst hop of the node is aggregated), so we use IRR to represent the interactive repetition rate, then there is: e= IRR ∗ e+ (1 − IRR) ∗ e, z ∈ (U, G, T ) In this way, the ﬁnal embedding can dynamically adjust the proportion of the output of the ﬁrst and second layer network in the ﬁnal output according to the group interaction. This idea mainly comes from the classic computer vision algorithm: residual connection [20], which is used to solve the problem of gradient disappearance and gradient explosion on the back layer network. It can also refer to JKnet [21], which is a method speciﬁcally used to solve the problem of excessive smoothing caused by too many layers in the graph network. JKnet stitches the output of all layers at the end, Max-Pooling and LSTM operations to obtain The ﬁnal representation, and we are based on the way the group interacts, and it has to be weighted for speciﬁc purposes.The speciﬁc process of model is Algorithm 1: Algorithm 1 The Method of GIP4GR Input:G(V, E), Nodes: ∀V, V, V∈ V, Nodes’feature: x, ∀v ∈ V Initialization; Step1: Caculate the IRR IRR=P, j ∈ Step2: Get The representation of nodes from all layers of the GNNs e= GNN(G, x); e= GNN(G, e), (z ∈ V); Step3: Get the ﬁnal nodes representation e= IRR ∗ e+ (1 − IRR) ∗ e, z ∈ (U, G, T ) We use the classic problem type in GNN link prediction [22], which is to judge whether there will be an edge between the target group and the target item Among them,ˇyrepresents the probability(or the size of the score) that g and v have edges, e, erepresents the ﬁnal representation of g and v respectively, φ represents the prediction function, it has many forms like dot product, MLP, cosine similarity etc. In our experiment, our prediction function uses the dot product method (the more similar the same subspace, the larger the score), so the predicted result is the score between the items recommended for a group. Regarding group recommendations, we provided display feedback based on negative samples. Based on this, the score of the observed interaction should be higher than the corresponding score of the unobserved for optimization. Our loss function is as follows: arg min{1 − σ(ˇy− ˇy Where Train represents the training set, that is, the group item interaction graph and the (i, k, k’) triplet indicates that the group i has interacted with k items but has not interacted with k’ items (we take the items that have not interacted with the group as negative samples ), where σ represents the sigmoid function,  means the parameters of model. The main purpose of this is to widen the gap between the scores of the positive samples and the negative samples so that the features between the positive samples and the negative samples are prominent. In this section, we conducted a lot of comparative experiments with the current benchmark model on two real-world datasets and answer the following research questions: •RQ 1: Compare our proposed model with the existing models, whether the recommendation performance is better? •RQ 2: Can the experiment prove that including the group interaction repetition rate has an Springer Nature 2021 LTEX template impact on the performance of the model recommendation, that is, can it solve the under-ﬁtting and over-smooth phenomenon when applying graph networks? Due to the previously mentioned dataset [7, 8] may not meet the behavioral characteristics shown in Section 3.2, we reused the two real-world datasets used in [3, 23] datasets CAMRa2011 and MaFengWo. The details of the datasets can be seen in Table 2. It can be seen that the interactive repetition rate of CAMRa2011 and MaFengWo is exactly at the opposite extremes. We adopted the ”leave one” evaluation method, which has been widely used to evaluate the performance of Top@N recommendations. Speciﬁcally, for each group, we randomly delete several items in the interaction for testing, thereby dividing the training set and the test set, and the ratio of the training set to the test set is 10:1. For each group, we randomly select 100 items that have not been interacted with before as the negative sample. In order to evaluate the performance of Top@N recommendations, we have adopted widely used indicators Hit Rate (HR@N) and Normalized Discounted Cumulative Gain (NDCG@N). Where posrepresents the position of the item in the recommended list, and IDCG is an ideal situation for DCG. In the ”Leave One” evaluation, HR measures whether the item used for testing is ranked in the ”top@N” list (1 means yes, 0 means no), and NDCG is given by the ranking position of the item in the recommended list score. In order to facilitate comparison with existing work, we uniformly set the the experiment in HR@10, NDCG@10 and HR@5, NDCG@5. In order to show that our model is superior to the existing models(RQ 1), we compared the current superior models as follow: •NCF+AVG [8, 24]: This is a model that aggregates scores. It uses the NCF framework for each group member to learn and predict the score of the item and regards the average score of the group member for the item as the group’s score for the item. At the same time, there are also the maximum and minimum scores of group members as the group scores, but the eﬀect is not as good as the average strategy, so only this method of aggregation is shown. •AGREE [3]: The model uses the attention mechanism, which determines the weight of each group member’s decision on the item according to the degree of interaction of each member in the group with the target item. •GREE:This method is a variant of AGREE. It removes the attention mechanism of AGREE. It is assumed that each member has the same contribution to the group’s decision-making. It is diﬀerent from NCF+AVG in that it calculates the group feature in advance and then calculates the score of the item for the group feature. In order to illustrate the eﬀect of group interaction repetition rate on recommendation performance(RQ 2), we set up the following experiment to verify our hypothesis in section 4.3: •One Layer GNN: Use only One Layer of the GNN mentioned in section 4.3 to observe the performance diﬀerence between the two datasets. •Two Layers GNN: Use a two layesr GNN to compare the eﬀect of using One Layer on two datasets. •Two Layers GNN with Residual Connection: Compared with the ordinary Two Layers GNN, this residual layer is to directly add the output results of the two layers to verify whether the weighted two-layer network output will be better after the eﬀect of the interactive repetition rate. We implement our method on the Pytorch platform and use the DGL library to implement our GNN model. We use the Adam [25] optimizer to perform all gradient-based calculations. Based on experiments, we found that adjusting the learning rate to 0.05 can achieve the best results. Regarding the size of the data, we found that setting the Embedding size too large will increase the diﬃculty of training, that is, it is diﬃcult to achieve convergence. If it is too small, it will lead to insuﬃcient encoding of the necessary information. Therefore, we set the Embedding size to 32, and in two layers, the ReLU [26] activation function is used between the GNN. In AGREE, GREE and NCF+AVG, the experimental settings we adopted refer to the best settings in [3]. The hyperparameters in One/Two Layer GNN and Two Layers GNN with Residual Connection are consistent with our model parameters. We chose the xavier method for the initialization of the network, and the Embedding of each node uses the initialization based on the normal distribution. To prevent errors, we repeat each experiment 5 times, and the average value of the maximum value of each experiment plus the standard deviation is used as the ﬁnal result of the model. Since the dataset does not have explicit negative feedback, 100 items that are not selected in each group are taken as negative samples. During training, we conducted a lot of experiments on these two datasets and found that the positive and negative sample ratio is adjusted between 1:10 and 1:12. The model works best. The speciﬁc adjustment process is shown in Figure 5 and Figure 6. It can be seen that the changes in HR@10 and NDCG@10 in the CAMRa2011 dataset have not changed much since the beginning of 6, while the two indicators of the MaFengWo dataset have started to level oﬀ from 10. This is due to the problem of data scale. When the amount of data is relatively large and dense, a relatively low negative sampling rate will be required to make the model tend to ﬁt. In summary, we set the positive-negative sample ratio to 1:10. Comparison(RQ 1) Table 3 and Table 4 represent the performance on the datasets CAMRa2011 and MaFengWo, respectively. It can be seen that our proposed model performs much better on HitRate and NDCG than the existing models. Among them, NCF+AVG has the worst eﬀect. (NCF+AVG, AGREE, GREE three models based on ordinary neural networks have not very diﬀerent results in these indicators). Our model uses GNN, each embedding of a node gathers more information from surrounding nodes and can learn more eﬀectively than ordinary neural networks. Therefore, the results of training the MaFengWo model also show a lower variance, indicating that the performance of the model is also relatively stable. At the same time, the method based on the characteristics of group members (AGREE, GREE) is better than relying solely on scores (NCF+AVG). This shows that the degree of preference for an item of each group member cannot reﬂect the preferences of items when gathered in a group. Finally, the model with the Attention mechanism (AGREE) is better than the average of group member characteristics (GREE), which shows that diﬀerent group members have diﬀerent contributions to group decision-making, so it also explains to us the graph’s attention network (GAT) is used when fusing individuals into group features. Performance(RQ 2) It can be seen from Table 5 and Table 6 that due to the relatively large IRR on the CAMRa2011 dataset, the One Layer GNN has better performance than the Two Layers GNN; and the MaFengWo dataset is due to the comparison of the IRR is small, so using a Two Layers GNN is better than using a One Layer GNN. But we pursue a more universal model structure, it is impossible to use diﬀerent models for diﬀerent datasets. Secondly, the eﬀect of Two Layers+Res is more like a compromise between the simple use of One Layer and Two Layers GNN. It simply adds the output of two layers. The price of its universality is its performance. This approach is similar to JKnet, that is, let the model learn the weights of diﬀerent layers of each node. Due to the sparseness of the data, the performance of the model cannot be optimal and the performance is unstable (that is, the standard deviation is relatively large.) so it is diﬃcult to converge during training, and our proposed model has achieved the best results by fusing the features between layers according to the structure of the dataset. Table 5, Table 4 and Table 6 respectively, it can be found that the model performance of GNN based on single layer or double layer is better than the model based on deep neural network, which also shows in the group-user-item interaction graph, there is more structural information that can be obtained through GNN, so the use of GNN in group recommendation is the correct choice. Finally, the representation of the trained nodes embedding is reduced by T-SNE, and conclusions similar to the above results can be obtained intuitively. In Figure 7, the blue dots represent items, the red dots represent users, and the pink dots represent groups. It can be seen from Figure 7 that due to the impact of the IRR of the dataset, the eﬀect of One Layer of GNN and our model GIP4GR are relatively close, and the Two Layers GNN with Residual Connection model is inferior to them, but the three points are separate, Springer Nature 2021 LTEX template you can’t make the diﬀerence signiﬁcant. The Two Layers GNN have the worst eﬀect, which is reﬂected in the fact that the embedding of the group and the embedding of the item are difﬁcult to separate, which is consistent with our experimental data. CAMRa2011 Meanwhile, in the MaFengWo dataset, it can be seen intuitively from Figure 8 that due to the relatively small IRR, the eﬀect of using a One Layer GNN is the worst. Secondly, the eﬀects of the Two layers GNN and our model GIP4GR are relatively close, and the eﬀect of the Two Layers GNN with Residual Connection is inferior to the two, which is consistent with our hypothesis. In this work, we proposed a GNN method to solve the problem of group recommendation and proposed a new model GIP4GR. First, we put forward Springer Nature 2021 LTEX template the concept of group interaction repetition rate for the problem of group recommendation. Secondly, we proposed the use of GNN to learn on the group recommendation dataset, that is, on each layer of the network, GAT is used to learn the contribution of each member in the group decision-making, and GraphSAGE is used to aggregate the group and the item, the user’s representation. Finally, the group IRR is applied to a GNN-based framework, and the characteristics of the dataset itself are used to weighting the ﬁrst layer and second layer GNN for better performance. And a large number of experiments have been conducted on two real-world datasets and compared with the best existing methods, our model has achieved the best results. The current shortcoming of this work is that it does not consider the interaction between members of the group, that is, the members are only regarded as the attribute nodes of the group, and the interactive connections of the group members are not counted as part of the graph. Subsequent work may consider connecting the member nodes in the group, separately considering the message passing between users when the entire interaction graph is used for message passing, and reﬂecting this inﬂuence on the weight of the group are two important issues. Conﬂict of interest The authors declare that they have no conﬂict of interest.