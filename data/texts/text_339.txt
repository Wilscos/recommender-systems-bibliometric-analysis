Defect predictors, static bug detectors and humans inspecting the code can locate the parts of the program that are buggy before they are discovered through testing. Automated test generators such as search-based software testing (SBST) techniques can use this information to direct their search for test cases to likely buggy code, thus speeding up the process of detecting existing bugs. However, often the predictions given by these tools or humans are imprecise, which can misguide the SBST technique and may deteriorate its performance. In this paper, we study the impact of imprecision in defect prediction on the bug detection eectiveness of SBST. Our study nds that the recall of the defect predictor, i.e., the probability of correctly identifying buggy code, has a signicant impact on bug detection eectiveness of SBST with a large eect size. On the other hand, the eect of precision, a measure for false alarms, is not of meaningful practical signicance as indicated by a very small eect size. In particular, the SBST technique nds 7.5 less bugs on average (out of 420 bugs) for every 5% decrements of the recall. In the context of combining defect prediction and SBST, our recommendation for practice is to increase the recall of defect predictors at the expense of precision, while maintaining a precision of at least 75%. To account for the imprecision of defect predictors, in particular low recall values, SBST techniques should be designed to search for test cases that also cover the predicted non-buggy parts of the program, while prioritising the parts that have been predicted as buggy. ACM Reference Format: Anjana Perera, Burak Turhan, Aldeida Aleti, and Marcel Böhme. 2021. How good does a Defect Predictor need to be to guide Search-Based Software Testing?. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Search-based software testing (SBST) techniques search for test cases to optimise a given coverage criterion such as branch coverage, method coverage, or a combination of the two. Coverage related heuristics, such as branch distance [1,2] and approach level [3] are used to guide the search for test cases to cover the uncovered areas in the program. SBST techniques are known to be eective at achieving high code coverage [3,4]. While it is necessary for a test case to cover the buggy code to nd a bug, just covering the buggy code may not be sucient to discover the bug [5, 6]. In fact, SBST techniques guided only by coverage have been shown to struggle in terms of bug detection [5–8]. This is because the SBST techniques have no guidance in terms of where the buggy code is likely to be located, and hence spend most of the search eort in non-buggy code which constitutes a greater portion of the code base. Defect predictors [9] and static bug detectors [10] can estimate the locations of the bugs eectively. Most of the defect predictors use classiers trained on an existing dataset with features related to various metrics like code size, code complexity, change history, etc., and whether the components (e.g., le/class or method) are buggy or not [11]. Static bug detectors statically check the code against pre-dened bug patterns and label the buggy code (e.g., line) with a warning [12]. Both defect predictors and static bug detectors are used in the industry to assist developers in manual code reviews [13– 16]. Defect predictors have also been used to inform automated testing techniques; G-clef [17] is a test case prioritisation strategy that prioritises test cases that cover highly likely to be defective classes, and SBST[6] and BTG [18] are time budget allocation techniques which allocate a higher time budget to highly likely to be defective classes. Often, the predictions produced by defect predictors are not perfectly accurate. The false positives and false negatives can signicantly hinder the potential benets of these tools. For example, false positives (i.e., wrongly labelling a program as buggy) result in SBST techniques looking for bugs in non-buggy areas in code, thus spending valuable search resources in vain. On the other hand, false negatives (i.e., labelling a buggy program as non-buggy) can result in SBST techniques not generating tests for buggy areas in code. Previous work that use defect predictors to guide SBST techniques report on improved bug detection performance of SBST [6,18]. The defect predictors used in these approaches have a relatively high performance, e.g., the defect predictor used by Perera et al. [6] had a recall of 85%, and Hershkovich et al. [18] employed a defect predictor which had an area under curve (AUC) of 0.95. The performance of defect predictors, however can vary, e.g., from as low as 5% and 25% to as high as 95% and 85% for precision and recall, respectively [9]. Given such wavering performance, the question that we address in this paper is “What is the impact of imprecise predictions on the bug dete ction performance of SBST?”. To answer this question, we simulate defect predictors for dierent value combinations of recall and precision in the range 75% and 100% (Section 2.1). Defect predictors having recall and precision above 75% are considered acceptable defect predictors [19]. We employ the state of the art DynaMOSA [3] as the SBST technique which is guided by the defect predictions (DP) (see Section 2.2), which we refer to as SBST guided by DP throughout the paper. We evaluate how the bug detection eectiveness of SBST guided by DP changes with the dierent levels of imprecision when applied to 420 bugs from the Defects4J dataset [20] (Section 3.1). The results from our experimental evaluation reveal that the recall of the defect predictor has a signicant impact on the bug detection eectiveness of SBST with a large eect size. More specifically, SBST guided by DP nds 7.5 less bugs on average (out of 420 bugs) for every 5% decrements of recall. On the other hand, the impact of precision is not of practical signicance as indicated by a very small eect size, hence we conclude that the precision of defect predictors has negligible impact on the bug detection eectiveness of SBST, as long as one uses a defect predictor with acceptable performance, i.e., with precision and recall greater than 75%. Further analysis into the results reveals that the impact of recall is greater for the bugs that are isolated in one method than for the bugs that are spread across multiple methods. In summary, the contribution of this work is a comprehensive experimental analysis of the impact of imprecision of defect predictions on bug detection eectiveness of SBST. The experimental evaluation involving 420 bugs from 6 open source Java projects took roughly 180,750 CPU-hours in total. Based on the results of our study we make the following recommendations; (1)SBST techniques must take potential errors in the predictions into account, in particular the false negatives. One possible solution is to prioritise predicted buggy parts of the program, while guiding the search with a certain probability towards locations that are predicted as not buggy. (2)In the context of combining defect prediction and SBST, it is benecial to increase the recall of the defect predictor by sacricing precision, while maintaining the precision above 75%. One potential solution is to lower the cut-o point of the classier such that more components will be labelled as buggy at the expense of more false positives. The source code of SBST guided by DP, defect predictor simulator, post processing scripts and data are publicly available in the following link: https://gshare.com/s/a8d75f161b8cfa11d297 Our aim is to understand how the defect prediction imprecision impacts the bug nding performance of SBST. To this end, we design a study that addresses the following research question: RQ: What is the impact of the imprecision of defect prediction on bug To address this research question, we measure the eectiveness of SBST in terms of nding bugs when using defect predictors with dierent levels of imprecision. We use DynaMOSA [3], a state-ofthe-art SBST technique, and incorporate predictions about buggy methods in order to guide the search for test cases towards likely buggy methods (see Section 2.2), which we refer as SBST guided by DP throughout the paper. Fine-grained defect predictions such as method level is chosen so that the location of the bug is narrowed down better than coarse-grained defect predictions such as class level. Hence the defect predictors at method level provide additional information to the SBST technique such that it can further narrow down the search for test cases to likely buggy methods. We measure defect predictor imprecision using recall and precision. Recall and precision have been widely used in previous work to report the performance of defect predictors [9,21]. We consider a defect predictor with either recall or precision less than 75% is not an acceptable defect predictor, as recommended by Zimmermann et al. [19]. Hence, we simulate defect predictors for varying levels of recall and precision in the range 75% to 100% (see Section 2.1) and measure the impact on the bug detection performance of SBST by the prediction imprecision. To measure the bug detection performance of SBST against the imprecision of defect predictions, we simulate defect predictor outcomes at various levels of performance in the range 75% and 100% for both precision and recall. Recall is the rate of the defect predictor identifying buggy methods. It is calculated as in Equation.(1), where 𝑡𝑝is the number of true positives, i.e., number of buggy methods that are correctly classied, and𝑓 𝑛is the number of false negatives, i.e., number of buggy methods that are incorrectly classied. Precision is the rate of the correct buggy methods labelled by the defect predictor. It can be calculated as in Equation(2), where𝑓 𝑝is the number of false positives, i.e., number of non-buggy methods that are incorrectly classied as buggy methods. We simulate defect predictions from 75% to 100% recall in 5% steps, with 75% and 100% precision. Thus, there are altogether 12 defect predictor congurations, with the following values of (precision, recall): (75%, 75%), (75%, 80%), (75%, 85%), (75%, 90%), (75%, 95%), (75, 100%), (100%, 75%), (100%, 80%), (100%, 85%), (100%, 90%), (100%, 95%), (100, 100%). Our preliminary experiments suggest that the bug detection performance of SBST guided by DP changes by a small margin when the precision is changed from 100% to 75%, while keeping the recall unchanged. On the other hand, the bug detection performance of SBST guided by DP changes by a large margin when only the recall is changed from 100% to 75%. Hence, we decide to consider only the values of 75% and 100% for precision, while recall is sampled at 5% steps. The output of the simulated defect predictor is binary, i.e., method is buggy or not buggy, similar to most of the existing defect predictors. Some of the existing defect predictors output the likelihood of the components being buggy or the ranking of the components according to their likelihood of being buggy. Since we employ a theoretical defect predictor and not a specic one, we resort to the generic defect predictor, which is the one that gives a binary classication. Algorithm 1 illustrates the steps of simulating the defect predictor outputs for a given recall and precision combination. The procedure SimulateDefectPredictor receives the set of methods in the project with the ground truth labels for their defectiveness, 𝑀 = {𝑚, . . . , 𝑚}, where( and outputs a set of labels for each method in the project,𝐶 = {𝑐, . . . , 𝑐}, where( First, it calculates the number of buggy (𝑑) and non-buggy methods (𝑛𝑑) in the project (lines 2-3 in Algorithm 1). Next, it nds the set of indices of all the buggy (𝑀) and non-buggy methods (𝑀) in the project (lines 4-5). The true positives (𝑡𝑝) and false positives (𝑓 𝑝) are then calculated for the given recall (𝑟) and precision (𝑝) (lines 6-7). The RandomChoice(𝑀, 𝑛) procedure returns𝑛number of randomly selected methods from the set𝑀, where𝑥 ∈ {𝑏, 𝑛}. 𝐶is assigned a set of randomly picked𝑡𝑝number of buggy and𝑓 𝑝 number of non-buggy method indices (line 8).𝐶is the set of buggy method indices as classied by the simulated defect predictor. The output is the set𝐶 = {𝑐, . . . , 𝑐}, where𝑐= 1if the method with index𝑖is labelled as buggy and𝑐= 0if the method with index𝑖is labelled as not buggy (line 9). We incorporate buggy method predictions in DynaMOSA [3], the state-of-the-art SBST technique, to guide the search for test cases towards likely buggy methods. DynaMOSA tackles the test generation problem as a many objective optimisation problem, where each coverage target in the program, e.g., branch and statement, is an objective to optimise. It is more eective at achieving high branch, statement and strong mutation coverage than previously proposed SBST techniques ([22–24]) [3]. In the next sections, we refer to the DynaMOSA approach guided by the defect predictor as SBST guided by DP. SBST guided by DP is presented in Algorithm 2. It shares the same search steps and genetic operators as DynaMOSA, except for the updated steps shown in blue colour in Algorithm 2. SBST guided by DP receives as input a class with methods labelled as buggy or non-buggy, which are labels that can be obtained using existing defect predictors [11,25]. In our study, SBST guided by DP receives these labels from defect predictor simulations (Section 2.1). SBST guided by DP devotes all the search resources to nd tests that cover likely buggy methods, thereby increasing the chances of nding bugs. Initially, SBST guided by DP lters out the coverage targets that are deemed to not contain buggy methods as indicated by the defect prediction information, and keeps only targets that contain likely buggy methods (as shown in line 2 of Algorithm 2 and described in Section 2.2.1). It also generates more than one test case for all the selected buggy targets, hence, further increases the chances of nding bugs (lines 6, 7, 10 and 11 and described in Section 2.2.2) [6]. To generate more than one test case for all the likely buggy targets, SBST guided by DP does not remove a target once it is covered during the search. This is likely to cause SBST guided by DP to miss nontrivial targets in the search and keep on generating tests to cover more trivial targets [23]. To address this, we use a method to dynamically disable targets from the search based on their current test coverage and number of independent paths (line 13 and described in Section 2.2.3). We refer to this as balanced test coverage. This ensures that the nontrivial targets have an equal chance of being covered compared to the targets that are easier to cover. SBST guided by DP randomly generates a set of test cases that forms the initial population (line 5). Then, it evolves this initial population through creating new test cases via crossover and mutation (line 9), and selecting test cases to the next generation (line 14), until a termination criteria, such as maximum time budget, is met. 2.2.1 Filtering Targets with Defect Prediction. A defect predictor classies the methods of the class under test (CUT) as buggy or non-buggy, denoted as 𝑐, where This information is used to lter out the likely non-buggy targets from the set of all targets𝑈using the classications given 𝐶 = {𝑐, . . . , 𝑐} ⊲ the set of defectiveness classications for of independent paths for each edge (line 2). Spending the limited search resources on covering nonbuggy targets is likely to be ineective when it comes to nding bugs. Filtering out targets that are unlikely to be buggy allows the search to focus on test cases that cover the likely buggy targets (i.e., ∀𝑢 ∈ 𝑈), hence, generating more eective test cases faster than other approaches which search for tests in all the targets in the CUT. 2.2.2 D ynamic Selection of Targets and Archiving Tests. There are structural dependencies of targets that should be considered when selecting objectives, i.e., targets, to optimise. For instance, some of the targets can be covered only if their control dependent targets are covered. To better understand this, let us consider the following example in Figure 1. Assume the test generation scenario is to optimise branch coverage and𝑏,𝑏,𝑏,𝑏,𝑏and𝑏are the branches to be covered. Branch𝑏holds a control dependency link to𝑏and𝑏, which means that they can be covered only if𝑏is covered by a test case. If an SBST technique optimises test cases to cover𝑏and𝑏, while𝑏is uncovered, this will unnecessarily increase the computational complexity of the algorithm because of the added objectives, i.e.,𝑏and𝑏, to the search without any added benet. To address this, DynaMOSA dynamically selects targets to the search only when their control dependent targets are covered [3]. In our example,𝑏and𝑏are added to the search only when 𝑏is covered. At the start of the search, SBST guided by DP selects the set of targets𝑈⊆ 𝑈that do not have control dependencies (line 4). These are the targets SBST guided by DP can cover without requiring to cover any other targets in the program. At any given time in the search, it searches for test cases to cover only the targets in𝑈. Once a new population of test cases is generated (lines 5 and 9), the procedure UpdateTargets is executed to update𝑈by adding new targets to the search. The procedure UpdateTargets adds a target𝑢 ∈ 𝑈to𝑈only if the control dependent targets of𝑢are covered as explained with the example above. SBST guided by DP maintains an archive of test cases found during the search which cover the selected targets. Once the search nishes, this archive forms the nal test suite. Unlike in DynaMOSA, we congure the UpdateTargets procedure to not remove a covered target from𝑈and the UpdateArchive procedure (lines 6 and 10) to archive all the test cases that cover the selected targets 𝑢 ∈ 𝑈. This way, SBST guided by DP can generate more than one test case for each target𝑢 ∈ 𝑈, hence increasing the bug detection capability of the generated test suites [6]. Perera et al. [6] showed that DynaMOSA nds up to 79% more bugs when it was congured to not remove covered targets from the search and retain all the generated tests. 2.2.3 Balanced Test Coverage of Targets. As we discussed in Section 2.2.2, SBST guided by DP does not remove covered targets from𝑈to allow the search to nd more than one test case for each target. While this benets SBST guided by DP in terms of bug detection, one downside of this approach is that trivial targets are covered more often than they need to be. Hence, we propose a method to balance the test coverage among the targets in the CUT. We use the control dependency graph (CDG) in Figure 1 to explain how we balance the test coverage. Assume the goal of the test generation scenario is to maximise the branch coverage. There are always more test cases that cover the branches closer to the root node, e.g.,𝑏, than the branches closer to the leaf nodes of the CDG, e.g.,𝑏. This is because the execution of a test case can take many paths in the program once it reaches branches like𝑏. For instance, there are 3 independent paths that leads from𝑏in the example. On the other hand, the execution of a test case can only take one path from𝑏, i.e., the exit path. Therefore, to balance the test coverage of targets in the CUT, we want to ensure all targets have an equal number of tests per an independent path that leads from the respective target. For example, if we assume the number of tests that cover𝑏to be 90, then there should be 60 and 30 test cases that cover𝑏and𝑏, respectively. Those 60 tests that cover 𝑏should be equally distributed among 𝑏and 𝑏. SBST guided by DP calculates the number of independent paths of each edge in the control dependency graph𝐺of the program (line 3). The CDG, i.e.,𝐺 = ⟨𝑁 , 𝐸⟩, consists of nodes𝑛 ∈ 𝑁and edges𝑒 ∈ 𝐸 ⊆ 𝑁 × 𝑁. The nodes represent statements in the program. The edges represent control dependencies between the statements. The procedure IndependentPaths calculates the number of independent paths for each edge𝑒 ∈ 𝐸. When calculating the number of independent paths of an edge𝑒, the IndependentPaths procedure assumes the paths start at𝑒, however, the actual execution of the paths start at the root node, e.g., node𝐴in Figure 1. In our example in Figure 1, the independent paths starting from𝑏are 𝑏− 𝑏and𝑏− 𝑏. Finally, all the targets that are directly control dependent by an edge𝑒have the same number of independent paths as that of 𝑒. Algorithm 3Temporarily Removal of Targets to Balance Test Coverage SBST guided by DP dynamically switches o targets with higher test coverage from𝑈in every iteration to focus more on increasing the test coverage for targets which already have lower coverage (line 13). The procedure SelectPopulation selects test cases to the next generation considering only these targets with low test coverage. Hence, this paves way for the search to nd more test cases in the next generation that cover these targets and eventually make all targets to have an equitable test coverage. This ensures that nontrivial targets also receive a good coverage in the presence of more trivial targets. The procedure SwitchOffTargets starts with nding the set of nodes with predicates𝑁in𝐺(line 2 in Algorithm 3). In our running example,𝑁= {𝐴, 𝐵, 𝐷 }. Next, it fetches the number of independent paths from the outgoing edges of each node𝑛 ∈ 𝑁(lines 5-6). For the node𝐴, the edges𝑏and𝑏have 3 and 1 independent paths, respectively. We consider the test coverage is equal among all the control dependent targets of an edge, including the edge itself as well. Therefore, it randomly selects a control dependent target from each outgoing edge of𝑛(lines 7-8) and nds the test coverage of each edge (lines 9-10). In the case of a test generation scenario for maximising branch coverage, the edges𝑏, 𝑏, etc. also become targets of the CUT. Then, it nds the edge which has the largest test coverage per an independent path, and removes all the control dependent targets of that edge from𝑈 (lines 9-14). If we assume there are 30 and 20 tests in the archive which cover𝑏and𝑏, respectively, then it removes𝑏from𝑈 since𝑏has 20 (=20/1) tests per an independent path, while𝑏has only 10 (=30/3). We design a set of experiments to evaluate the eectiveness of SBST guided by DP in terms of nding bugs when using defect predictors with 12 dierent levels of imprecision as described in Section 2.1 (RQ). We use the bugs from the Defects4J dataset as the experimental subjects [20] (see Section 3.1). To account for the randomness of the defect prediction simulation algorithm (Algorithm 1), we repeat the simulation runs 5 times for each defect predictor conguration (i.e., recall and precision pair). For each of these simulation runs, we repeat the test generation runs 5 times, to account for the randomness in SBST guided by DP. Once tests are generated and evaluated for bug detection, we conduct two-way ANOVA test to statistically analyse the eects of recall and precision of the defect predictor on the bug detection eectiveness of SBST guided by DP. We use the Defects4J dataset (version 1.5.0) [20,26] as our benchmark. It contains 438 real bugs from 6 real-world open source Java projects. In our experiments, we remove 18 bugs altogether from the dataset; 4 deprecated bugs, 12 bugs that do not have buggy methods, and 2 bugs for which SBST guided by DP generated uncompilable tests (e.g., method signature is changed in the bug x). Thus, we evaluate SBST guided by DP on a total of 420 bugs. The bugs are drawn from the following projects; JFreeChart (25 bugs), Closure Compiler (170 bugs), Apache commons-lang (59 bugs), Apache commons-math (104 bugs), Mockito (37 bugs), and Joda-Time (25 bugs). The Defects4J benchmark gives a buggy version and a xed version of the program for each bug in the dataset. The xed version is dierent to the buggy version by the applied patch to x the bug, which indicates the location of the bug. We label all the methods that are either modied or removed in the bug x as buggy methods [27]. Defects4J is widely used for research on automated unit test generation [5,6,28], automated program repair [29], fault localisation [30], test case prioritisation [17], etc. This makes Defects4J a suitable benchmark for evaluating SBST guided by DP, as it allows us to compare our results to existing work. DynaMOSA is implemented in the state-of-the-art SBST tool, EvoSuite [31]. EvoSuite is an automated test generation framework that generates JUnit test suites for java programs [32,33]. EvoSuite is actively maintained and evaluated for its eectiveness in terms of bug nding on both industrial and open source projects [5–7,28]. For the experimental evaluation, we implement the changes described in Section 2.2 for SBST guided by DP. The changes are implemented within EvoSuite version 1.0.7, forked from the GitHub repository [32] on June 18, 2019. We also implement the defect predictor simulator as described in Section 2.1. The prototypes are available to download from here: https://gshare.com/s/a8d75f161b8cfa11d297 We use the default parameter settings of EvoSuite [22] and DynaMOSA [3] except for the parameters mentioned in the next paragraphs. Parameter tuning of SBST techniques is a long and expensive process [34]. According to Arcuri and Fraser [34], EvoSuite with default parameter values performs on par compared to EvoSuite with tuned parameters. Time Budget: We set 2 minutes as time budget per CUT for test generation. In practice, the time budget allocated for SBST tools depends on the size of the project, frequency of test generation runs and availability of computational resources in the organisation. Real world projects are usually very large and can have thousands of classes [35]. If an SBST tool runs test generation for 2 minutes per class, then it will take at least 33 hours to nish the task for the whole project. To address this issue, practitioners can adapt the SBST tools in their continuous integration (CI) systems [36]. However, the introduction of new SBST tools to the CI system should not make the existing processes in the system idle [6]. Thus, given the limited computational resources available in practice [37] and the expectation of faster feedback cycles from testing in agile development prompt the necessity of frequent test generation runs with limited testing budget. Therefore, we decide that 2 minutes per class is a reasonable time budget in a usual resource constrained environment. Coverage criteria: We use branch coverage as coverage criterion in SBST guided by DP. EvoSuite is more eective in terms of nding bugs when it is using branch coverage as the coverage criterion compared to other single criteria [38]. According to Gay [38], some criteria combinations perform better than branch coverage. However, there were other combinations that perform worse than branch coverage. Hence, they did not recommend a strategy to combine criteria. Therefore, we decide to use the most eective single criterion, i.e., branch coverage, in our experimental evaluation. Termination criteria: We use only the maximum time budget as the termination criterion. Stopping the search after it covers all the targets is detrimental to bug detection [6]. The search needs to utilise the full time budget to generate as many tests for each target in the CUT in order to increase the chances of detecting bugs. Therefore, we terminate the search for test cases only when the allocated time budget runs out. Test suite minimisation: We disable test suite minimisation since all the test cases in the archive form the nal test suite (see Section 2.2.2). Assertion strategy: We choose all possible assertions as the assertion strategy because the mutation-based assertion ltering can be computationally expensive and can lead to timeouts [5, 6]. We run experiments with SBST guided by DP using defect predictors with 12 dierent levels of imprecision as described in Section 2.1. For each bug in the Defects4J dataset, we checkout the buggy version of the project and collect the ground truth labels for the buggy and non-buggy methods. If a method is either modied or removed in the bug x, we label that method as a buggy method, and non-buggy otherwise [27]. Then, for each of the six projects in the dataset, we combine the ground truth labels from all the bugs respective to each project. For example, we combine the labels from all the 104 bugs from Apache commons-math project. Then, we simulate defect prediction outcomes for each project using the defect prediction algorithm described in Section 2.1. We assume an application scenario of generating tests to nd bugs not only limited to regressions, but also the bugs introduced to the code in various times in development. Therefore, we run test generation on the buggy version of the projects. We measure the bug nding eectiveness of SBST guided by DP only on the Defects4J bugs. Thus, we only run test generation for buggy classes, i.e., classes that are modied in the bug xes, in the projects. For each level of defect predictor imprecision, we run test generation with SBST guided by DP 25 times for each bug in the dataset. Consequently, we have to run a total of 12 (levels of defect prediction imprecision)∗25 (repetitions)∗482 (buggy classes)=144,600 test generations. Defects4J [20] allows us to evaluate if the 144,600 generated test suites in the experiments nd the bugs. First, we remove the aky test cases in test suites using the ‘x test suite’ interface [20] in Defects4J as described in [5]. We use the ‘run bug detection’ interface [20], which executes a test suite against the buggy and xed versions of a program and determines if the test suite nds the bug by checking if the test execution results are dierent between the two versions. EvoSuite generates assertions assuming the program under test is correct, therefore, the generated tests should always pass when they are run against the buggy version. A test suite is considered broken if it is not compilable or fails when run against the buggy version of the program. The test suite is considered as it has missed detecting the bug if it produces the same execution results when run against the buggy and xed versions of the program, and it is considered as it has detected the bug if the test results are dierent. We present the results for our research question following the method described in Section 3. Our aim is to evaluate the eectiveness of bug nding performance of SBST guided by DP when using imprecise defect predictors. Figure 2 shows the distributions of the number of bugs found by SBST guided by DP as violin plots and the prole plot of the mean number of bugs found by SBST guided by DP for each combination of the factors of six recalls and two precisions. The two lines in our prole plot run almost parallel to each other, i.e., the two lines do not cross each other at any point. This means that there is no observable interaction eect between recall and precision. The two lines descent steeply from recall 100% to 75%. This shows that recall has an eect on number of bugs found by SBST guided by DP. In particular, bug detection eectiveness decreases as recall decreases. The precision=75% line closely follows the precision=100% line while staying slightly above the latter, except at recall=85%, where there is a considerable gap between the two. We can soon see if this dierence is signicant from the two-way ANOVA test results. Figure 2: Distributions of the number of bugs found by SBST guided by DP as violin plots together with the prole plot of mean number of bugs found by SBST guide d by DP for each combination of the groups of recall and precision. To statistically test the eect of each of the metrics, recall and precision, and their interaction on the number of bugs found by SBST guided by DP, we conduct the two-way ANOVA test. Prior to conducting two-way ANOVA test, we have to make sure that our data holds the following assumptions of the test. (1)The dependent variable should approximately follow a normal distribution for all the combinations of groups of the two independent variables. (2)Homogeneity of variances exists for all the combinations of groups of the two independent variables. To check the rst assumption, we conduct the KolmogorovSmirnov test [39] for normality of the distributions (𝛼 = 0.05) of the number of bugs found for each combination of the groups of recall and precision. Based on the results of the tests, we cannot reject our null hypothesis (p-values≥ 0 .131), i.e., H= the number of bugs found is normally distributed, hence we assume all the samples come from a normal distribution (i.e., His true). To check the second assumption, we conduct the Bartlett’s test for homogeneity of variances (𝛼 = 0.05) in each combination of the groups of recall and precision. Based on the results of the test, we cannot reject our null hypothesis (p-value= 0.305), i.e., H= variances of the number of bugs found are equal across all combinations of the groups, hence we assume the variances are equal across all samples (i.e., His true). Table 1: Summary of the two-way ANOVA test results. Df = degrees of freedom, Sum Sq = sum of squares and Mean sq = mean sum of squares. Table 1 shows the summary of the two-way ANOVA test results. According to the two-way ANOVA test, recall and precision explain a signicant amount of variation in number of bugs found by SBST guided by DP (p-values< 0 .001). The test also indicates that we cannot reject the null hypothesis that there is no interaction eect between recall and precision on number of bugs found (p-value = 0.105). That means we can assume the eect of recall on number of bugs found does not depend on the eect of precision, and vice versa. To check if the observed dierences among the groups are of practical signicance, we measure the epsilon squared eect size (b𝜖) [40] of the variations in number of bugs found with respect to recall and precision. We nd that the eect of recall on bug detection eectiveness is large with an eect size of 0.89, while the eect of precision is very small (b𝜖= 0.004) [41], which can be seen from the overlapping distributions in the violin plots in Figure 2 as well. To further analyse which groups are signicantly dierent from each other, we conduct the Tukey’s Honestly-Signicant-Dierence test [42]. The Tukey post-hoc test shows that the number bugs found by SBST guided by DP is signicantly dierent between each of the six levels of recall (p-values< 0.002). The Cohen’s𝑑eect sizes of the dierences between the groups of recall range from medium (𝑑 = 0.77for recall 95% and 100%) to large (𝑑 ≥ 1.33for all other pairs of groups). In summary, the imprecision of the defect predictor has a signicant impact on the bug nding performance of SBST. In particular, when the recall of the defect predictor decreases, the bug detection eectiveness signicantly decreases with a large eect size. On the other hand, we conclude that there is no meaningful practical eect of precision on the bug detection performance of SBST, as indicated by a very small eect size. As shown in Figure 2, SBST guided by DP nds less number of bugs when using defect predictors with a lower recall compared to using one with a higher recall. In particular, SBST guided by DP nds 7.5 less bugs and misses test generation for 15 bugs on average (out of 420) when the recall decreases by 5% in our experiments. SBST guided by DP completely trusts the defect predictor and only generates tests for classes having at least one method predicted as buggy (e.g., true positive). The number of true positives by the defect predictor decreases when the recall decreases. This results in SBST guided by DP generating tests for a fewer number of classes as the recall decreases, hence nding less number of bugs when recall drops from 100% to 75%. We identify this as a weakness of SBST when using defect predictions. To mitigate this, SBST techniques have to take potential errors in the predictions into account. One way to do this is to always generate tests for methods that are predicted buggy, while also generating tests for predicted non-buggy methods at least with a minimum probability. This way the SBST technique gets a chance to search for tests in incorrectly classied buggy methods (when recall <100%), while also giving higher priority to methods that are predicted buggy by the defect predictor. As we discussed previously, when the recall of the defect predictor decreases, SBST guided by DP completely misses test generation for certain bugs, hence leads to poorer bug detection. In our experiments, SBST guided by DP misses test generation for 18.2% of the bugs on average when recall decreases from 100% to 75%. Further analysis of the results indicates that SBST guided by DP only misses test generation for 4.5% of the bugs on average for the bugs that spread across multiple methods, whereas it misses 24.7% of the bugs on average for the bugs that are concentrated into only one method. This suggests that the bugs that are found within only one method are more prone to the impact of recall compared to bugs that are spread across multiple methods. To understand the eects of recall on nding bugs which are found within only one method and spread across multiple methods, we conduct Welch ANOVA test [43] separately for the two subsets of our dataset, i.e., bugs having only one buggy method and bugs having more than one buggy method. The reason for carrying out Welch ANOVA test is because our data fails the assumption of homogeneity of variances for each combination of the groups of recall for bugs having only one buggy method. Table 2: Summar y of the Welch ANOVA test results. Num Df = degrees of freedom of the numerator and Denom Df = degrees of freedom of the denominator. The results of the Welch ANOVA test are shown in Table 2. There are 135 bugs which have more than one buggy method. The results for these bugs show that overall recall has a signicant eect on number of bugs found by SBST guided by DP (p-value <0.001) with a large eect size (b𝜖= 0.53) [44]. However, the Games-Howell post-hoc test reveals that the bug detection eectiveness is not signicantly dierent between recall 80%, 85% and 90%, and 95% and 100%. This can be seen in the violin plots in Figure 3 as well. Figure 3: Distributions of the number of bugs found by SBST guided by DP as violin plots together with the means plot of number of bugs found by SBST guided by DP for the groups of recall. Only for the bugs that have more than one buggy method. Total number of bugs = 135. There are 285 bugs which have only one buggy method. The results of Welch ANOVA test for these bugs show that recall has a signicant eect on number of bugs found by SBST guided by DP (p-value <0.001) with a large eect size (b𝜖= 0.87). The GamesHowell post-hoc test conrms that the number of bugs found by SBST guided by DP is signicantly dierent between each group of recall (p-values <0.001) with large eect sizes (𝑑 ≥ 0.98) as can be seen in Figure 4. Figure 4: Distributions of the number of bugs found by SBST guided by DP as violin plots together with the means plot of number of bugs found by SBST guided by DP for the groups of recall. Only for the bugs that have one buggy method. Total number of bugs = 285. In summary, we nd that recall has a signicant eect on bug detection eectiveness of SBST guided by DP regardless of whether the bugs are found within one method or spread across multiple methods. However, for the bugs that are spread across multiple methods, the eect size of recall eect is smaller when compared to bugs that are found within one method (0.53 < 0.87). In contrast to bugs that are found within one method, the eect of recall is not signicant between the groups of recall 80%, 85% and 90%, and 95% and 100% for the bugs that are spread across multiple methods. According to the two-way ANOVA test, the precision of the defect predictor has a statistically signicant eect, although with a very small eect size, which suggests the eect is not of meaningful practical signicance. Precision is associated with false positives (Equation(2)), i.e., non-buggy methods predicted as buggy by the defect predictor. Change of precision from 100% to 75% means that there are false positives in the defect prediction results. We investigate the buggy method labels produced by the defect predictor and the bug nding results of SBST guided by DP in our experiments to nd out if false positives have actually helped SBST guided by DP to nd more bugs. We nd that the false positives have not contributed to the bug nding performance of SBST guided by DP. We conclude that the impact of precision is not of practical signicance to the bug nding performance of SBST. Defect predictors have mainly been used to provide a list of likely defective parts of a program (e.g., classes and methods) to programmers, who then manually inspect or test the likely defective parts to nd the bugs [13,45]. In this context, the precision of the defect predictor is very important [46]. Poor precision of the defect predictor means there are higher false positives. Higher false positives can waste developers’ time and lead to losing their trust on the prediction results [13]. However, when the defect predictions are consumed by another automated testing technique such as SBST, this may not be the case. In the context of SBST, our study reveals contrasting ndings. We nd that the eect of precision on the bug detection performance of SBST is negligible, while the recall of the predictor has a signicant impact with a large eect size. We recommend that programmers improve the recall of the defect predictor at the cost of precision to achieve good performance in SBST guided by defect prediction. There is a trade-o between recall and precision of a defect predictor [47]. Defect predictors are usually good at detecting bugs (i.e., high recall) at the expense of false positives (i.e., low precision). Our study shows the bug detection eectiveness of SBST guided by DP is highly sensitive to recall, while the eect of precision is negligible. This means that most defect predictors proposed in the literature would be suitable for guiding SBST. As the scope of our study is to analyse the impact of precision in the range of an acceptable defect predictor, i.e., precision≥ 75%, we cannot make any conclusions about defect predictors with precision below 75%. Therefore, we can conclude that it is benecial to increase the recall of the defect predictor by sacricing precision as long as it is above 75%. Construct Validity.To systematically investigate the impact of defect prediction imprecision, we simulate the predictions by assuming a uniform distribution. This means in our simulations, every method has an equal chance of being labelled as buggy or non-buggy. However, real defect predictors may have dierent distributions of their predictions depending on the underlying characteristics and nature of the prediction problem, which may impact the realism of a simulated defect predictor. Nevertheless, in the absence of prior knowledge about defect prediction distributions, it is reasonable to assume a uniform distribution of predictions in the defect prediction simulation. SBST guided by DP generates more than one test case for each target in the CUT. This increases the chances of nding bugs at the cost of larger test suites. Larger test suites are associated with a higher number of assertions in the tests generated by EvoSuite, which need to be manually adapted by developers in practice. We design our study to investigate the impact of imprecision in defect prediction on SBST along one dimension, that is the number of bugs found by the generated test suites. We identify investigating the impact of defect prediction imprecision on SBST in terms of the cost of manual adaptation of the generated assertions as future work, which will complement the ndings of our study. Internal Validity.To account for the randomness in the defect prediction simulation, we repeat the simulations 5 times for each combination of the groups of recall and precision. For each simulation, we repeat the test generation 5 times to account for the non-deterministic behaviour of SBST guided by DP. In total, we conduct 25 test generation runs for each bug and for each level of defect prediction imprecision. Conclusion Validity.To account for any threats to the conclusion validity, we derive conclusions from the experimental results after conducting sound statistical tests; two-way ANOVA test, epsilon squared eect size, Tukey’s Honestly-Signicant-Dierence test, Cohen’s d eect size, Welch ANOVA test and Games-Howell post-hoc test. External Validity.We use 420 real bugs from Defects4J dataset as the experimental subjects. They are drawn from 6 open source projects. At the time of writing this paper, another 401 bugs from 11 projects were added to the Defects4J dataset. However, we understand that these projects do not represent all program characteristics, especially in industrial projects. Nevertheless, Defects4J dataset has been widely used in previous work as a benchmark [5,6,17,29]. Future work needs to be done on investigating the impact of imprecision of defect prediction on SBST with respect to other bug datasets. SBST guided by DP uses defect prediction information at method level. Our ndings may not be generalised to previous work [6,18] which use defect prediction at a dierent level of granularity (class level). Nevertheless, the ndings from our study will help to further explore the opportunities of combining defect predictions and SBST. We investigate the impact of defect prediction imprecision only in the range of 75% to 100% for recall and precision. Therefore, our ndings may not be generalised to the defect predictors which have recall or precision less than 75%. While this choice of performance sampling in our simulation is a threat to external validity, it is also a threat to construct validity for lack of characterising all possible defect predictors. However, we opted to use this range with the justication that this is the range for an acceptable performance for a defect predictor as recommended by Zimmermann et al. [19]. Defect prediction was originally proposed to provide a list of likely defective parts of a program to assist developers in code reviews [13, 14], manual testing [45], etc. More recently, defect predictors have been used to inform automated testing techniques as well. G-clef [17] is a test prioritisation strategy that uses the likelihood of the defectiveness of classes to prioritise test cases and it was shown to be eective at reducing the number of test cases required to nd bugs. FLUCCS [27] is a fault localisation approach that leverages the likelihood of methods being defective and it was shown to signicantly outperform the state-of-the-art spectrum based fault localisation (SBFL) techniques. Perera et al. [6] and Hershkovich et al. [18] used defect predictions at class level to determine the time budget allocated to classes in a project to run test generation with SBST techniques. A highly likely to be defective class according to the defect predictor has more chance of being selected to run test generation [18] or allocated a higher time budget [6]. Despite showing the improved bug detection performance of the proposed SBST techniques, we nd that the defect predictors used in these two works have relatively high performance, e.g., 85% recall in [6] and 0.95 AUC in [18], which can be dicult to achieve for a defect predictor. For example, Zimmermann et al. [19] found that only 21 out of 622 cross-project defect predictor combinations to have recall, precision and accuracy greater than 75%. In their systematic literature review, Hall et al. [9] reported defect predictor performances in the ranges of 5%-95% and 25%-85% for precision and recall, respectively. This leads to the question of how does the variation in defect prediction performance aect the bug detection eectiveness of SBST techniques that incorporate defect prediction information. To address this gap, we study the impact of imprecision in defect predictions on the bug detection performance of SBST. Search-based software testing techniques use search algorithms like genetic algorithms to search for test cases to meet a given criteria like branch coverage [31]. The test generation problem can be formulated in two ways; i) single objective formulation [23,31] and ii) many objective formulation [3,24]. In many objective optimisation, such as MOSA [24] and DynaMOSA [3], SBST techniques aim to nd a set of non-dominated test cases that minimise the tness functions for all the test targets, e.g., branches. In single objective optimisation, SBST techniques optimise whole test suites to minimise a single tness function which is created by aggregating all the individual test target distances. A target distance measures how far away the test suite is from covering that target [31]. Whole test suite generation (WS) [31] and archive-based WS (WSA) [23] are two examples for techniques that use single objective optimisation. Previous work showed that DynaMOSA, a state-of-the-art many objective optimisation technique, is better than single objective optimisation techniques in terms of achieving high code coverage [3]. In this paper, we study the eect of defect prediction imprecision on bug detection performance of an SBST technique that uses many objective optimisation. There is a plethora of defect predictors which have been proposed over the past 40 years [46]. Measures such as recall, precision, fmeasure, AUC, Matthews correlation coecient (MCC) [48], etc. have been used to measure the predictive power of the defect predictors [9]. Out of these measures, recall and precision have been widely used in previous work [9,21] and are often preferred by practitioners [46]. Existing defect predictors have wavering performance. For example, Hall et al. [9] reported defect predictor performances from as low as 5% and 25% to as high as 95% and 85% for precision and recall, respectively. Hosseini et al. [21] also reported similar ndings in their systematic literature review of cross-project defect predictors. It is thus important to study the impact of the wavering defect prediction performance on the bug detection performance of SBST. In our study, we consider the recall and the precision should be greater than 75% to be considered acceptable as recommended by Zimmermann et al. [19], and simulate defect predictions in the range from 75% to 100% for recall and precision. Previous work report the developers’ opinions about the defect predictor performance [13,45,46], showing that false positives cause developers to waste their precious time on inspecting nonbuggy code, which eventually leads to loosing trust on the defect predictor [13,45]. In the eyes of the developers, higher precision is more important compared to higher recall in a defect predictor, because higher precision means low false positives [46]. In the context of using defect prediction to guide SBST, our study reveals contrasting ndings. In particular, precision has a negligible impact on the bug detection performance of SBST, while the eect of recall is signicant. We study the impact of imprecision in defect prediction on the bug detection performance of SBST. We use simulated defect predictors to systematically sample defect predictors in the range of 75% to 100% for recall and precision. We use the state-of-the-art SBST technique, DynaMOSA, and incorporate predictions about buggy methods as given by the simulated defect predictor to guide the search for test cases towards likely buggy methods. Through a comprehensive experimental evaluation on 420 bugs from the Defects4J dataset, we nd that the recall of the defect predictor has a signicant impact on the bug detection eectiveness of SBST with a large eect size. On the other hand, the impact of precision is not of meaningful practical signicance as indicated by a very small eect size. Further analysis of the results shows that the impact of the recall for the bugs that are spread across multiple methods is smaller compared to the bugs that are found within only one method. Based on the results of our study, we make the following recommendations: (1) SBST techniques must take into account potential errors in the predictions, especially the false negatives. One way to do this is to prioritise the likely buggy parts of the program, while guiding the search towards the likely non-buggy parts with at least a minimum probability. (2)In the context of SBST, it is benecial to increase the recall of the defect predictor at the expense of precision as long as it stays above 75%. One straightforward method to do this is to lower the cut-o point of the classier such that more components will be labelled as buggy at the cost of more false positives. We identify the following directions as future work to extend this study; i) investigate the impact of defect prediction imprecision on SBST in terms of the cost of manual adaptation of generated assertions, ii) validate the ndings against other bug datasets [49–51], and iii) explore the options for using the likelihood of defectiveness of methods to guide SBST techniques.