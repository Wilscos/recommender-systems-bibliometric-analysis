While recent work on conjugate gradient methods and Lanczos decompositions have achieved scalable Gaussian process inference with highly accurate point predictions, in several implementations these iterative methods appear to struggle with numerical instabilities in learning kernel hyperparameters, and poor test likelihoods. By investigating CG tolerance, preconditioner rank, and Lanczos decomposition rank, we provide a particularly simple prescription to correct these issues: we recommend that one should use a small CG tolerance ( ≤ 0.01) and a large root decomposition size (r ≥ 5000). Moreover, we show that L-BFGS-B is a compelling optimizer for Iterative GPs, achieving convergence with fewer gradient updates. There are now many methods for scaling Gaussian processes (GPs), including ﬁnite basis expansions (Rahimi et al., 2007; Solin & S¨arkk¨a, 2020), inducing point methods (Snelson & Ghahramani, 2006; Titsias, 2009; Hensman et al., 2013), and iterative numerical methods (Gibbs & MacKay, 1996; Cutajar et al., 2016; Gardner et al., 2018). These methods all work in various ways to scale linear solves and log determinant computations involving covariance matrices. In some type of limit, these methods all generally converge to the “exact” Gaussian process regressor, but at a computational expense. Iterative methods, such as the preconditioned conjugate gradient (CG) and Lanczos algorithms that form the backbone of the popular software library GPyTorch (Gardner et al., 2018), are rapidly growing in popularity, in part due to their accuracy and effective use of GPU parallelization. These methods are different from other scalable GP approximations in that the accuracy of their solves can be New York University. Correspondence to: Wesley J. Maddox <wjm363@nyu.edu>. Proceedings of the OPTML Workshop at International Conference on Machine Learning (ICML), 2021. Copyright 2021 by the author(s). Andrew Gordon Wilson precisely controlled by hyperparameters such as tolerances. Thus these methods are considered “exact” in the same sense as mathematical optimization — accurate to arbitrary precision. In practice, these methods can be more precise than the “exact” Cholesky-based alternatives, due to roundoff errors in ﬂoating point numbers (Gardner et al., 2018). However, these iterative methods have demonstrated mysterious empirical behaviour. For example, in Jankowiak et al. (2020) the “exact” iterative GPs generally have impressive test RMSE, but surprisingly poor test likelihood. Similar results can be found for the CG-based SKI method in Sun et al. (2021). Moreover, we ﬁnd there are sometimes numerical instabilities in training kernel hyperparameters with iterative methods, for instance leading to increasingly large kernel length-scales and increasingly poor test performance, as marginal likelihood optimization proceeds. Recent work has explicitly noted issues with iterative CGbased methods (Artemev et al., 2021; Potapczynski et al., 2021). For example, Potapczynski et al. (2021) show that CG methods can provide biased estimates, and propose random truncations as a remedy. Similarly, Wenger et al. (2021) propose variance reduction techniques to reduce the bias of CG and Lanczos methods. In this work, we study the empirical convergence of iterative Gaussian process models based on conjugate gradients and Lanczos decompositions. Our empirical study evaluates the hyperparameters of these methods, namely the size of the test time Lanczos decomposition r, the tolerance of conjugate gradients τ, and the rank of the preconditioner used for conjugate gradients solutions, w. The primary source of the mysterious empirical issues for iterative methods are overly relaxed defaults in GPyTorch. We provide simple prescriptions for adjusting these defaults in order for practitioners to achieve reliably good performance with iterative methods. We also highlight the practical beneﬁts of L-BFGS-B for marginal likelihood optimization, which is not presently a standard method for learning kernel hyperparameters. In Figure 1, we give a graphical explanation of our results on one output dimension of the sarcos dataset n = 44484, d = 21. In the ﬁrst two panels from left, we consider the optimization trajectories (using Adam) of both high tolerance iterative GPs (with high values of CG tolerance and a small preconditioner) against low tolerance iterative GPs (with our recommendations of a lower CG tolerance value and a larger preconditioner size), ﬁnding that the optimization paths tend to be more stable for low tolerance values. Then, in the center left panel, we compare test set NLL as a function of the root decomposition size, ﬁnding that signiﬁcant improvements in NLL can be made by simply increasing the size of the root decomposition. Finally, in the right panel, we consider test set MSE as a function of CG tolerance, ﬁnding that the CG tolerance also plays an important role in the accuracy of the method. Code for all experiments can be found here. Gaussian processes (GPs) are a non-parametric method that model priors directly over functions, and provide wellcalibrated uncertainties (Rasmussen & Williams, 2008). In this work, for simplicity, we focus on the regression setting with isotropic noise. For a dataset D of size n, the relation between d-dimensional inputs X ∈ R, and corresponding outputs y ∈ R, is modeled using a GP prior f (X) ∼ GP (µ(X), k(X, X)) and a Gaussian obser- vation likelihood y ∼ Nf(X), σI. The prior is fully speciﬁed by a mean function µ(·), and a kernel function k(·, ·), both with parameters collectively denoted as θ. We take the mean function to be zero, as typical in literature. The kernel function induces a covariance matrix between two sets A ∈ Rand B ∈ R, denoted as K∈ R. Therefore, the prior covariance matrix is denoted by K∈ R. Gaussian process inference aims to ﬁnd the posterior over the functions f for nnovel inputs X, which is also a Gaussian and available in closed-form as, p(f(X) | X,D, θ) = N (µ(X), Σ(X wherebK= K+ σI. The parameters θ are chosen by maximizing the marginal log-likelihood (MLL) L, and its gradients are given by, Traditional methods to compute (1) to (3) use Cholesky factorization to solve the linear systems involvingbK, and the determinant computations involvingbK(Rasmussen & Williams, 2008). This incurs an expensive cost of On, making exact GPs feasible only for less than n = 10, 000 data points and severely limiting scalability. Gardner et al. (2018) propose conjugate gradients (CG) to solvebKy, and stochastic trace estimator (Hutchinson, 1989) to compute the derivative of log-determinant TrbKin (3) as, where we use B probe vectors {z}. Notably, we can again use conjugate gradients to solvebKz. Consequently, for kernel matrices K, using r  n iterations of conjugate gradients produces exact inference up to machine precision in Orntime, achieving signiﬁcant computational gains. The MLL can be computed using B + 1 CG solves and is easily parallelized. Exact GPs are recovered when r = n. A ﬂurry of recent work (Wang et al., 2019; Potapczynski et al., 2021; Artemev et al., 2021; Kapoor et al., 2021) has shown the effectiveness of these methods for highly scalable Gaussian processes. Further, Pleiss et al. (2018) propose to store the single solve of m =bKy at test time as the predictive mean cache, while similarly computing a rank k Lanczos decomposition ofbK≈ RR(the predictive covariance cache) and storing that for all new test points. The predictive mean and variance for a test point xare then given by, Computing the Lanczos decomposition costs O(kn), so that computing the predictive mean and covariance after this pre-computation cost only O(knn). In general, Lanczos will tend to ﬁnd the largest and smallest eigenvalues of bK, converging from the outside in so to speak (Demmel, 1997, Ch. 7). Collectively, we call inference involving all the above numerical methods as Iterative Gaussian Processes (Potapczynski et al., 2021). Understandably, the speed and quality of Iterative GPs are crucially reliant on conjugate gradients. (i) First, the number of CG iterations r depend on the condition number of bK, which grow somewhat with n. To alleviate this concern, Gardner et al. (2018) use a pivoted Cholesky preconditioner of rank w, noting that low ranks are sufﬁcient. Figure 1. Far Left: Loss as a function of iteration for both high tolerance (GPyTorch defaults) and low tolerance (CG tolerance of 10 and a larger preconditioner size); low tolerance produces more stable optimization trajectories. Center Left: Outputscale as a function of iteration for both high tolerance and low tolerance CG; again, low tolerance provides more stable trajectories. Center right: Test set negative log likelihood as a function of root decomposition size; a larger root decomposition tends to produce lower NLL (which is better) but this can begin increasing at larger sizes. Far right: Test set MSE as a function of test set CG tolerance; a lower tolerance is more accurate. Alternatively, Artemev et al. (2021) propose an inducing point-based pre-conditioner. (ii) Second, in practice, we often use a pre-determined error tolerance  to truncate CG iterations. A higher tolerance implies that the solve may use fewer iterations than r, while a lower tolerance may require more iterations than r. A high tolerance, however, has been noted to be detrimental to MLL (2) maximization (Artemev et al., 2021), and may lead to noisy training curves affecting ﬁnal performance (Kapoor et al., 2021). (iii) Finally, CG truncations at a value r  n introduces approximation bias, which is also detrimental to the maximization of MLL (Potapczynski et al., 2021). Owing to the considerations above, and their emphasis in prior work (Gardner et al., 2018; Pleiss et al., 2018; 2020), we focus our study on analyzing three key hyperparameters — (i) the CG tolerance , (ii) rank of the pivoted Cholesky pre-conditioner w, and (iii) the rank of the Lanczos decomposition k at test time. In addition, we analyze the interaction of these hyperparameters to both a ﬁrst-order optimizer Adam (Kingma & Ba, 2014), and a second-order optimizer L-BFGS-B (Zhu et al., 1997). Historically, (nearly) second order optimizers such as LBFGS-B have been preferred for optimizing Gaussian process hyper-parameters (Rasmussen & Williams, 2008); however, they tend to perform poorly in the presence of noisy estimates of gradients (Gardner et al., 2018; Balandat et al., 2020). Thus, Gardner et al. (2018); Wang et al. (2019) tend to ﬁt their GP models with ﬁrst order optimizers such as Adam, as L-BFGS-B on all but small datasets tends to be infeasible. One of our goals in our study is to determine the hyper-parameters under which we can use L-BFGS-B to train hyper-parameters of Gaussian process models while still using iterative methods. Proposition 1. For a single test data point x, increasing the rank k of an approximate eigen-decomposition will decrease the posterior variance σ. The proof of Proposition 1 is in Appendix A, and proceeds algebraically. Proposition 1 implies that increasing the rank of a Lanczos decomposition at test reduces the posterior variance of the GP, making the GP more conﬁdent about its predictions. As we show next, this effect can have counterintuitive effects on the test time negative log likelihood (NLL). As a function of the variance σ, the NLL is, where µ is the predictive mean. Differentiating and setting the derivative equal to zero ﬁnds that σ= (µ − y) is a minimum (the global minimum) agrees with standard results where the maximum likelihood estimator of σin regression is the sum of squared errors of the predictor. Figure 2. NLL (5) as a function of the posterior standard deviation σ. The NLL is minimized when σ= (y − µ). We show in Figure 2 that the NLL is minimized when σ is set as the squared error of the prediction. Figure 2 also demonstrates that if we control the posterior variance (the square of σ), then the NLL is not monotonic as we decrease (or increase) the posterior variance. Thus, as we increase the rank of the Lanczos decomposition, we decrease the variance — beginning by decreasing the NLL towards the NLL’s minimizer (which may or may not correspond to the full rank Cholesky solution). However, once we have reached the NLL’s minimizer, continuing to decrease the posterior variance then increases the NLL again. Table 1. Recommended settings for stable training (and testing) with iterative GPs, especially when using L-BFGS-B. These two settings imply two different behaviors — if the NLL decreases as we increase the root decomposition, then the hyper-parameters of the GP model are under-ﬁt as the squared residual error on the test set is less than the estimated posterior variance. If the opposite occurs (NLL increases as we increase the root decomposition), then the GP model itself is over-ﬁt, the squared residual error on the test set is greater than the estimated posterior variance, e.g. we are over-conﬁdent about our predictions. Through our experiments, we establish that (i) increasing the Lanczos decomposition rank r ﬁxes the discrepancy between exact GPs and Iterative GPs; (ii) decreasing the CG tolerance allows us to run L-BFGS-B(Zhu et al., 1997), which has not been investigated in prior literature. Experimental Setup: We consider four benchmark datasets from the UCI repository (Dua & Graff, 2017), elevators, protein, bike and poletele, standardized to zero sample mean and unit sample variance using the training set. We run the Cholesky and Iterative GPs over ﬁve random splits of the data, reporting the mean. As in Gardner et al. (2018), we use the Mat´ern-5/2 kernel with automatic relevance determination (ARD) and constant means. For optimization, we use a learning rate of 0.05 for 2000 steps or until convergence. As established in Section 3, the key hyperparameters of importance are the CG error tolerance , rank of pivoted Cholesky pre-conditioner w, and the rank of the Lanczos decomposition k at test time. In addition, we want to understand the interaction of these parameters with optimizers Adam and L-BFGS-B. As in Wang et al. (2019), which used L-BFGS-B only for pre-training initialization, we use the default setting of 10 memory vectors. Effect of Tolerance and Pre-conditioner Size: We ﬁnd that the the discrepancy between the NLL achieved by Cholesky-based inference and Iterative GPs can be attributed primarily to (i) too large of a CG tolerance, and (ii) too small of a test time root decomposition. Varying the (test-time) root decomposition size on elevators for a large preconditioner, and use a smaller CG toler- We use the full-batch implementation available at https: //github.com/hjmshi/PyTorch-LBFGS. ance (0.01 or less), as in Figure 3, we are able to reach the Cholesky baseline in terms of NLL for a root decomposition size of about 10, 000. By comparison, the RMSE converges much faster to the Cholesky baseline (shown in grey again) for preconditioner sizes of 50 and 100 on the same dataset (Figure 4 left three panels). In addition, we vary the CG tolerance in both double and ﬂoat precision for three different pre-conditioner sizes, ﬁnding that the RMSE converges very quickly, and for CG tolerances less than 0.1, the test time RMSE differences are negligible. While the previous experiments compared a mix of ﬁrst order and second order methods as in Wang et al. (2019), in Figure 5, we show CG tolerance is also important for the success of L-BFGS-B only. In Figure 4 far right, we use a tolerance of 1.0 at train time, varying the test time CG tolerance, ﬁnding that the performance of Adam and L-BFGS-B decay for high tolerances, with the Adam performance decaying more. Finally, in Figure 5, we vary preconditioner size and CG tolerance for ﬁxed root decomposition, ﬁnding again that low test-time CG tolerance is imperative for good results with L-BFGS-B. As shown in Table 1, we recommend a low CG tolerance for training (of around 10), along with a larger preconditioner size (of 50), for more reliable performance. Between these, a lower CG tolerance should be preferred. These recommendations apply to both training with the GP marginal log likelihood as well as computing the predictive mean. For predictive distributions, we also recommend computing a larger Lanczos decomposition rank (of around 5000). As of this writing, default GPyTorch settingsare a tolerance of 1.0 at train time, and 10at test time, and a root decomposition size of 100. We hypothesize that test NLL is not the only quantity that can be signiﬁcantly hindered as a result of using too small a root decomposition, as sampling depends on similar quantities. In the future, we hope to expand our benchmarking to use Iterative GPs for large-scale data in Bayesian optimization (Balandat et al., 2020). https://docs.gpytorch.ai/en/stable/ settings.html Figure 3. Test set NLL as a function of root decomposition size and CG tolerance on elevators. Decreasing the CG tolerance with a preconditioner of size 100 produces more accurate solves and thus more accurate NLL estimation for large root decomposition sizes. Figure 4. RMSEs as a function of preconditioner size on elevators. A small enough CG tolerance (0.1 or less) produces very similar results to the cholesky baseline even for small preconditioner sizes. Figure 5. Left Three Panels: Test set NLL on protein for both Adam (with L-BFGS-B pre-training) and L-BFGS-B as a function of preconditioner size and CG tolerance. CG tolerance strongly matters for performance, even more so for L-BFGS-B. Far Right Panel: Training time on a single GPU; Adam is approximately twice as fast to reach the same training MLL (and ultimately test set NLL). We thank Sam Stanton and Alex Wang for helpful discussions.