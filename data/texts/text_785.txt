Sequential recommendation methods play an important role in real-world recommender systems. These systems are able to catch user preferences by taking advantage of historical records and then performing recommendations. Contrastive learning(CL) is a cuttingedge technology that can assist us in obtaining informative user representations, but these CL-based models need subtle negative sampling strategies, tedious data augmentation methods, and heavy hyper-parameters tuning work. In this paper, we introduce another way to generate better user representations and recommend more attractive items to users. Particularly, we put forward an eective ConsistencyConstraint for sequentialRecommendation(C-Rec) in which only two extra training objectives are used without any structural modications and data augmentation strategies. Substantial experiments have been conducted on three benchmark datasets and one real industrial dataset, which proves that our proposed method outperforms SOTA models substantially. Furthermore, our method needs much less training time than those CL-based models. Online AB-test on real-world recommendation systems also achieves 10.141% improvement on the click-through rate and 10.541% increase on the average click number per capita. The code is available at https://github.com/zhengrongqin/C2-Rec. • Information systems → Recommender systems. Recommender Systems, Sequential Recommendation, Consistency Training ACM Reference Format: Chong Liu, Xiaoyang Liuand Rongqin Zheng, Lixin Zhang, Xiaobo Liang, Juntao Li, Lijun Wu, Min Zhang, Leyu Lin. 2022. C-Rec: An Eective Consistency Constraint for Sequential Recommendation. In Lyon ’22: The ACM Web Conference, April 25–29, 2022, Lyon, France. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/1122445.1122456 Recommendation systems have been extensively applied in online platforms nowadays, e.g., Amazon [30], Google [2,5,20,23], Alibaba [53] and Tencent [11]. Due to the dynamic interactions between users and items, it is essential to capture evolving user interests from users’ historical records. To accurately represent user interests and make an appropriate recommendation, many eorts have been paid to study sequential recommendation methods [16,25,45,50]. Generally, the sequential recommendation task aims to characterize user representation from users’ historical behaviors and predict the expected item accordingly. In viewing the great success of deep learning for sequential dependency modeling and representation learning, many methods based on deep neural networks He et al. [14]have been introduced and proposed to solve this task, covering RNN-Based frameworks [16,32,44], dierent CNNs blocks and structures [45,46,58], Graph Neural Networks (GNNs) [38,50,51], and also model variants [25,28,42] relied on the powerful multi-head self-attention [47]. Though performing well, these methods might suer from the data sparsity problem [25,41,65] for sequential recommendation, especially for models built on the multi-head self-attention mechanism, where only one single item prediction loss is used to optimize the full model parameters for capturing all possible correlations in input interaction sequences. To address the above challenge, various self-supervised learning strategies are introduced [55,64]. Among these, the recently introduced contrastive learning (CL) objective [53] achieves very promising results. Through combing with eective data augmentation strategies and cooperating with the vanilla sequential prediction objective, the CL-based method can learn better sequence-level user representations and enhance the performance of sequential recommendations. However, the eectiveness of CL-based approaches is subject to the correlated data augmentation methods, pretext tasks, ecient negative sampling, and hyper-parameters selection (e.g., the temperature in NCE and InfoNCE losses) [21]. To mitigate the above drawbacks, many eorts have been made to simplify contrastive learning. Gao et al. [9]propose a very simple yet effective contrastive learning scheme that utilizes dropout noise as data augmentation to construct high-quality positive samples for sequence-level representation learning. Although such a scheme is eective for unsupervised representation learning, adapting it into the sequential recommendation task will still encounter the dilemma in which a higher proportion of CL objective in model training will lead to better representations that can easily distinguish positive and negative samples but might result in worse item prediction performance and vice versa. In other words, there is a noticeable gap between the discrimination of positive and negative samples and the task objective for the sequential recommendation. Thus, the key to obtaining better user representations mainly for the item prediction task is designing more training strategies that can address the inherent issues of sequential recommendation models. Inspired by the recent observation on the multi-head attention model that a very simple regularization strategy imposed on the output space of supervised tasks yields striking performance improvement [29] (achieving SOTA on many challenging tasks), we propose to thoroughly explore the eect of consistency training for the sequential recommendation task. We rst introduce the simple bidirectional KL divergence regularization into the output space to constrain the inconsistency between two forward passes with dierent dropouts. Unlike previous machine translation, summarization, and natural language understanding tasks, we argue that the introduced consistency regularization merely in the output space is not enough for the data sparsity setting of sequential recommendation. We then design a novel and simple regularization objective in the representation space. Unlike previous studies that utilize cosine [9] and L2 [36,66] distance to regularize the representation space, we propose to regularize the distributed probability of each user representation over others. Thus, we can extend and leverage the eective bidirectional KL loss to regularize the representation inconsistency. Experiments on three public benchmarks and one newly collected large-scale corpus indicate that our proposed simple consistency training for sequential recommendation (CT4Rec) outperforms state-of-the-art methods based on contrastive learning by a large margin. Extensive experiments further prove that our proposed consistency training can be easily extended to the data side (except for model dropout) and can leverage ubiquitous consistency signals for the sequential recommendation tasks, e.g., the clicked items by the same user should share some attributes. The Online A/B test also conrms the eectiveness of our method. In a nutshell, we mainly have the following contributions: •We propose a simple (with only two bidirectional KL losses) yet very eective consistency training method for sequential recommendation systems. To the best of our knowledge, this is the rst work to thoroughly study the eect of consistency training from dierent perspectives and with a unied training objective for the sequential recommendation task. •Our proposed consistency training method can be easily extended to other inconsistency scenarios, e.g., data augmentation. It can also leverage more task-specic consistency. •Extensive experiments on four oine datasets show the eectiveness of our proposed CT4Rec over SOTA models based on contrastive learning with much better performance and faster convergence time. The Online A/B test also shows signicant improvement over the strong ensemble model. Early sequential recommendation (SR) methods are usually based on Markov Chain (MC), including adopting rst-order MC [40] and high-order MCs [12,13]. As for current deep learning approaches, they can be generally divided into four categories, i.e., RNN-based methods. Concretely, GRU4Rec [16] applies RNN to SR and many variants have been proposed based on GRU4Rec by adding data augmentation GRU4Rec+ [44], hierarchical RNN [39], attention module [3,27,34], ranking losses [15], user-based gated units [7] and contextual information [1]. However, RNN-based methods usually exhibit worse performance than CNN-based and attention-based methods for the data sparsity setting. Caser [45] proposes a convolutional sequence embedding recommendation model, and Tuan and Phuong[46]use 3-dimensional convolutional neural networks to achieve character-level encoding of input data. NextItNet [58] uses dilated convolutions to model long-range item dependence and outperforms GRU4Rec. Meanwhile, attention mechanisms [47] is used to model user behavior sequences and have achieved outstanding performance [8,24,25,28,33,42]. Besides, many researches [38,50,51] combine attention mechanisms and GNNs to solve the SR task. Memory networks [4,19], disentangled representation learning [35], data augmentation Guo et al. [10], Yao et al. [54], transfer learning Zhang et al. [61]and session-based Hu et al. [18] models are also utilized to improve the performance of SR. Recently, the combination of contrastive learning and attention mechanisms [53,64] is widely used in SR and achieves great success. CauseRec [60] performs contrastive learning by contrasting the counterfactual with the observational. StackRec [49] utilizes stacking and ne-tuning to develop a deep but easy-to-train SR model. ICAI-SR [59] focuses on the complex relations between items and categorical attributes in SR. SINE [43] can model multiple interest embeddings for the next-item prediction task. Unlike previous researches, we only introduce two consistency training objectives in the representation and output spaces without any structure modications, extra data, and heuristic patterns from tasks. Though extremely simple, our method outperforms recent SOTA models by a large margin and is more ecient for model training. There are only a few related researches in the elds of natural language process [9] and machine learning [29,36,66]. Specically, Gao et al. [9]introduce dropout as the alternative of data augmentation into contrasting learning for sequence representation learning while we focus on the inconsistency introduced by dropout in the data sparsity SR task. Our proposed method is also extremely simple compared with the paradigm of combining contrastive learning with the traditional item prediction objective. Ma et al. [36], Zolna et al. [66]mainly focus on the gap between training and testing and utilize L2 for regularizing the representation space, which is less eective in the data sparsity setting, represented by the marginal to none performance improvements in Section 5. Dierent from introducing a regularization objective in the output space to constrain the randomness of sub-models brought by dropout [29], we focus on the consistency training of the data sparsity SR task from both the representation and output space. We also propose a simple yet eective regularization strategy in the representation space to compensate and align the output space consistency loss. The overall structure of our model is illustrated in Figure 1. Before elaborating our propose CT4Rec, we rst present some necessary notations to describe the sequential item prediction task. LetU = (𝑢, 𝑢, ..., 𝑢)denotes a set of users, andV = (𝑣, 𝑣, ..., 𝑣) denotes a set of items. The sequence for user𝑢 ∈ Uis denoted as Figure 1: Model structure of CT4Rec. It takes user click sequences as input and outputs user representations for item retrieval in the matching stage of recommendation. The input sequences are transforme d into vector representations via the embedding layer and then encoded by N transformers with dierent hidden dropout masks. In addition, Distributed Regularization Loss and Regularized Dropout Loss are introduced to restrain these representations generated by dierent dropout masks. 𝑠= (𝑣, 𝑣, ..., 𝑣, ..., 𝑣), where𝑣∈ Vis the item that user𝑢interacts at time step𝑡and|𝑠|is the length of sequence 𝑠. Given the historical sequence𝑠, the task of sequential recommendation is to predict the probability of all alternative items to be interacted by user 𝑢 at time step |𝑠| + 1, which is formulated as: Since our proposed consistency training method does not involve structural modication and extra data utilization, we apply it to the widely used SASRec model [25]. Following the original setting in SASRec, the transformer encoder contains three parts, i.e., an embedding layer, the stacked multi-head self-attention blocks, and a prediction layer. Then, we can obtain user representation𝒔= 𝑓 (𝑠), where 𝑓 (·) indicates the transformer encoder. To learn the relation between users and items in sequential recommendation, a similarity function, e.g., inner product, is applied to measure distances between user representation and item representation. Thus, for user representation 𝒔of user 𝑢 at time step 𝑡, we can get a similarity distributionP(𝒔) = P(𝑣|𝒔)to predict the item that user𝑢will interact at time step𝑡 +1. Then, the basic loss function with positive item 𝒗and randomly sampled negative items 𝒗∈ Vis denoted as: P(𝒔; 𝜔) =𝑒𝑥𝑝 (𝒔𝒗) +Í𝑒𝑥𝑝 (𝒔𝒗)(2) where 𝜔 refers to all trainable parameters of the model. Inspired by recent studies on dropout [29], we enhance the user representation from the perspective of reducing the model inconsistency and gap between training and testing. Concretely, we forward twice with dierent dropouts and learn the consistency between these two representations for each user, i.e., each user interaction sequencespassing the forward network twice and obtain two representations 𝒔and 𝒔. Regularized Dropout Loss (RD).Considering two representations𝒔and𝒔for user u, as mentioned in Function (2), we can get two similarity distributionsP(𝒔;𝜔)andP(𝒔;𝜔). Following R-Dropout[29], we introduce a bidirectional KL-divergence loss to regularize the above two distributions, written by: L(𝒔; 𝜔) =12(D(P (𝒔; 𝜔)||P(𝒔; 𝜔)) Distributed Regularization Loss (DR).To better regularize the representation space, we propose a distributed regularization method in which each user is represented by its correlations with other users rather than directly utilizing user representations for consistency regularization. In this paper, we compare users in each mini-batch, e.g.,𝑛users(𝑢, 𝑢, ..., 𝑢)and two representations for each user generated by dropout denoted as(𝒔, 𝒔, ..., 𝒔)and (𝒔, 𝒔, . . . , 𝒔). As shown in Figure 2, for user𝑢, we calculate the similarities between𝒔and all the other user representation 𝒔, dened as𝑠𝑖𝑚(𝒔, 𝒔), and obtain the similarity distribution P(𝒔; 𝜔) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (𝑠𝑖𝑚(𝒔, 𝒔), ..., 𝑠𝑖𝑚(𝒔, 𝒔))(5) Figure 2: Illustration of the DR loss. The click sequences of users rst pass the representation model twice to obtain two dierent representation sets for users 𝒔and 𝒔. We then calculate the similarity between a certain user and others in the same set. We further use a bidirectional KL to constrain the similarity distributions between dierent sets. Then, a bidirectional KL-divergence loss is applied to regularize the two distributions P(𝒔; 𝜔) and P(𝒔; 𝜔), dened as: Final Objective.We combine the above two objectives together with the task-specic loss in the backbone model for model training. The task-specic loss and nal training objective are as: L(𝒔; 𝜔) = −12(𝑙𝑜𝑔P(𝒔; 𝜔) + 𝑙𝑜𝑔P (𝒔; 𝜔))(7) The whole training process of CT4Rec is presented in Algorithm 1. As shown in Line 3-5, we obtain two user representations𝒔and 𝒔by going forward the model twice for each user sequence𝑠. Line 6-7 calculate the Laccording to the loss function 8, and update the model parameters. The training process will continue until convergence. We conduct extensive experiments on three public benchmark datasets that are widely used in recent literature [53] and a new large-scale dataset collected from a real-world recommendation scenario, i.e., PC-WeChat Top Stories. These datasets are very different in domains, platforms, and data scale, where their detailed statistics are presented in Table 1. Table 1: Dataset statistics of four public benchmarks and an oline corpus from real-world application, where avg. refers to the average actions per user. • Amazon:a series of datasets comprise product reviews, which are crawled from one of the largest E-Commerce platforms, i.e., Amazon.com. As introduced in SASREC [25,37], these datasets are separated by top-level product categories. We follow one of the most recent researches [53] to utilize the Beauty and Sports categories for comparison. • Yelp:it is a widely acknowledged dataset for the business recommendation, which is collected from the Yelp platform. Following Zhou et al. [65], we leverage the data after January 1st, 2019, and treat business categories as attributes. • WeChat:this dataset is constructed from WeChat platform for PC Top Stories recommendation (denoted asWeChatto distinguish datasets from other platforms), which consists of interaction records from 7th to 13rd, June 2021. Each interaction provides positive feedback (i.e., click) of an item from a user. Concretely, we collect 9.5 million interactions from 0.74 million users on 0.21 million items and regard data from the rst few days as the train set and the rest for testing. To verify the eectiveness of our proposed method, we introduce four representative baselines and three very recent methods. • GRU4Rec [16].It utilizes GRU modules to model user action sequences for the session-based recommendation. We consider each user’s click sequence as a session. • SASRec [25].It applies the multi-head self-attention mechanism to solve the sequential recommendation task, which is commonly treated as one of the state-of-the-art baselines. In this paper, we utilize SASRec as the backbone of our CT4Rec. • TiSASRec [28].Based on SASRec, it further introduces time interval aware self-attention mechanism to encode the user’s interaction sequence, where the positions and time interval between any two items are considered. • BERT4Rec [42].It uses the deep bidirectional self-attention mechanism to model user interaction history in the sequential recommendation and trains the model like BERT [6]. • CL4SRec [53].It generates dierent views of the same user interaction sequence by using data augmentation methods and adds contrastive learning objective to the original objective of SASRec for sequential recommendation tasks. • CLRec.Zhou et al. [64]design a queue-based contrastive learning method named CLRec to de-bias deep candidate generation in the recommendation system and further propose Multi-CLRec for multi-intention aware bias reduction. In this work, we only compare the CLRec for fairness. • StackRec [49].It rst uses a stacking operation on the pretrained layers/blocks to transfer knowledge from a shallow model to a deep model and then utilizes iterative stacking to obtain a deeper but easier-to-train recommendation model. We use multiple oine metrics with varying granularity and launch an online A/B test to evaluate the performance of all methods. Oline Metrics.Following many previous studies [14,25,53, 65], we employ the leave-one-out strategy to evaluate model performance. Specically, for each user, we take the last interacted item for test. Similar to Kang and McAuley[25], Zhou et al. [65], we randomly sample 500items from the whole dataset for each positive item, and rank them by similarity scores. The model performances are evaluated by top-k Normalizedrand Discounted Cumulative Gain (NDCG@𝑘) and top-k Hit Ratio (HR@𝑘), which are both commonly used in top-k recommendation systems. Specically, we report HR@𝑘 and NDCG@𝑘 with 𝑘 ={5, 10, 20}for all datasets. Online A/B Test.We further perform an online A/B test that resembles Hao et al. [11]to evaluate our CT4Rec in a real-world system. Concretely, we have deployed C4TRec for PC Wechat Top Stories recommendation. To calibrate the eect of CT4Rec for the online system, CT4Rec is implemented as an additional channel in the current matching module with the rest of the system unchanged, where the existing matching module refers to an ensemble model that combines multiple methods, e.g., rule-based, reinforcementbased, sequence-based, DSSM, self-distillation. In the online A/B test, we utilize two metrics, i.e., click-through rate (CTR) and average click number per capita (ACN), to evaluate model performance. The online test lasts for 5 days and involves nearly 2 million users. All models are implemented based on TensorFlow. For baselines with ocial codes, we utilize the implementations provided by authors. As for models without open-accessible codes from the original paper, we prefer the well-tested version from the opensource community. Specically, we use code from https://github. com/Songweiping/GRU4Rec_TensorFlow as the implementation of GRU4Rec [16]. We implement CL4SRec and CLRec based on the model descriptions and experimental settings of the correlated papers since there is no ocial code or popular implementation from the open-source community. For fair comparisons, the embedding dimension size is set to 50, and all models are optimized by Adam. Recall that our proposed CT4Rec method does not modify the model architecture and increases the model scale of the backbone model. Instead, it only involves two eective consistency regularization strategies. Thus, we follow the backbone method, i.e., SASRec [25], to implement our CT4Rec. We use two self-attention layers and set the head number to 2. The maximum sequence length is 50 for all datasets. We optimize the parameters with the learning rate of 0.001 and the batch size as 128. The dropout rate of turning o neurons is set to 0.5. To verify the eect of each component and their combination, we x the structure and other hyper-parameters of the model and only adjust the values of𝛼and𝛽, where𝛼and𝛽 ducing our experiments can be found in our anonymous code. We report the overall performance of all models on four oine datasets to compare their eectiveness in top-K recommendation and further present the results of the online A/B test to learn the applicability of our CT4Rec on real-world recommendation system. Overall Oline Performance.As shown in Table 3, our proposed CT4Rec outperforms all other baseline methods, including multiple representative models and state-of-the-art sequential recommendation solutions, on three benchmark datasets and one large industrial corpus. Compared with other strong methods that utilize data augmentation or/and contrastive learning to obtain better user representations and mitigate the incompatibility between the single item prediction task and the vast amounts of parameters in data sparsity scenarios [25], our CT4Rec is simpler and more eective without requiring any augmented data or delicate training strategies in which only two extra objectives are introduced. The performance advance of CT4Rec over SOTA models based on contrastive learning is conrmed by the signicant improvements of HR@𝑘, and NDCG@𝑘scores over CLRec and CL4SRec. We also observe that the training objective in recent baselines performs much well than the original binary cross-entropy in SASRec, which is demonstrated by the results that SASRec* increase oine scores by a large margin over SASRec. Notice that our CT4Rec is implemented by introducing two training objectives into SASRec*. We compare CT4Re c with SASRec* to calibrate the eect of our consistency training method. The universal enhancement of CT4Rec over SASRec* on four datasets and all HR@𝑘and NDCG@𝑘scores (relative improvements ranging from 5.60% to 16.50%) verify the superiority of our proposed simple method. Table 2: Performance improvement of online A/B test. Performance improvement of online A/B test.The performance improvement of CT4Rec on real-world recommendation Table 3: Model performance of baselines and our proposed CT4Rec on four oline datasets, where ‘*’ refers to modifying the original binary cross-entropy loss in SASRec with the training objective in recent baselines, e.g., CLRec, CL4SRec, StackRec. Our CT4Rec is implemented on SASRec*. Improv. and Improv.* refer to the relative improvement of CT4Rec over SASRec and SASRec*, respectively. The performance improvement over baselines is statistically signicant with 𝑝 < present the experimental results of extra two runs with dierent random seeds in the Appendix. WeChat service is reported in Table 2, which can be seen that only implementing CT4Rec as an additional channel in the matching module with the rest of the system unchanged can yield very impressive performance improvement over the original ensemble model. Our method increases CTR and ACN metrics by 10.141% and 10.541%, respectively. This indicates that our proposed simple method is not only eective for oine benchmarks and evaluation metrics but also generalizes well to the real-world online system. We have demonstrated the impressive performance of our proposed simple CT4Rec on both oine benchmarks and the online A/B test. In this section, we launch extensive experiments to understand and analyze CT4Rec from dierent perspectives. For convenience, these studies are performed on the Beauty dataset. More specically, we mainly focus on:1)the eect of each introduced objective (Ablation Study),2)the inuence of several important hyper-parameters (Hyper-Parameter Analysis),3)the exploration of more consistency regularization for sequential recommendation (More Consistency Training),4)the extension of CT4Rec to data augmentation (Extension to Data Augmentation),5)the change of training process and cost resulted by CT4Rec (Training and Cost Analysis). We perform an ablation study to explore the eect of two objectives in Section 3, i.e., regularized dropout (RD) and distributed regularization (DR), where the results are illustrated in Figure 3. RD Objective.As shown in the left sub-gure (a), the introduced RD objective achieves signicant performance improvement over the backbone SASRec* model. Compared with DR objective and its variants, RD loss contributes most to the performance increase, which concludes that launching consistency regularization in the model output space is the most benecial to SASRec*, which is similar to the observations in other tasks [29]. The possible reason might be that RD consistency regularization aects directly on the same space of the model output probability distribution. DR Objective.We can see that the unsupervised DR loss can also yield substantial performance gains, which points out that unsupervised consistency regularization in user representation space for data sparsity sequential recommendation task is also essential. We also adapt two typical unsupervised strategies in previous studies [9,36,66] to regularize user representations, including cosine similarity and L2 distance. As shown in the right two sub-gures in Figure 3, these two methods have not brought meaningful improvement upon the backbone method, which further proves that our designed DR objective is more preferable for consistency regularization in the representation space. Figure 3: Evaluation results for ablation study and analysis of 𝛼 and 𝛽. (a) CT4Rec with L and L. (c) and (d) replace Lwith cosine and L2 loss, respectively, where the orange lines are the performance of SASRec*. Figure 4: The impact of dierent dropout rates on the performance of CT4Rec and SASRec* on the Beauty dataset. We mainly consider several critical hyper-parameters in this part, including 𝛼 and 𝛽 in Equation 8 and the dropout rate. The Eect of 𝛼.Figure 3 also gives the results of dierent𝛼 values. Considering that there are many feasible combinations of the (𝛼,𝛽) grid, we temporarily remove the DR objective and only examine the inuence of𝛼for RD. It can be observed that a small value of 𝛼 can bring meaningful performance improvement. With the increase of𝛼, the performance improvement further increases in which the best result is achieved when𝛼 =2.0. These results further conrm that the consistency regularization directly performed on the output space is very eective even when it only takes a small proportion for the nal training objective. With a further increase of𝛼, the model will pay more attention to the consistency of model outputs, which will dilute the original item prediction objective, resulting in worse performance (e.g., 𝛼 = 2.0 v.s.,𝛼 = 2.0). The Inuence of 𝛽.Similar to the analysis of𝛼, we only study the impact of𝛽for each single unsupervised regularization loss (i.e., DR, cosine, and L2). Dierent from𝛼, the DR loss has no inuence on the overall performance when the value of𝛽<0.3. This is probably because the consistency regularization on the user representations is overwhelmed and adapted when passing to the output space. With the increase of𝛽, DR loss gradually produces a more important role and consistently improves model performance until𝛽>2.0. And also, a large𝛽value will force the model to focus on the consistency of user representation rather than perform the item prediction task. For the cosine objective, dierent𝛽values have a limited inuence on evaluation results. As for the L2 loss, a large 𝛽 value will cause inferior performance. Analysis on Dropout Rates.Besides the above analysis, we also study how the dropout rate aects the eectiveness of our CT4Rec since dierent dropout rates will lead to varying degrees of inconsistency. As demonstrated in Figure 4, dropout rates have a signicant inuence on the performance of the backbone model, and our proposed consistency training is applicable and eective when the dropout rate<0.7. However, our proposed CT4Rec will lead to a negative eect when the dropout rate>0.8. We speculate that the reason behind this might be the data sparsity issue and irreconcilable inconsistency. The two consistency regularization objectives introduced above are designed to address the inconsistency in both representation and output space for data sparsity sequential recommendation tasks. In this part, we explore to utilize more helpful consistency signals based on task-specic prior in training instead of solving the inconsistency problem of the model. Concretely, we investigate the eect of a very simple and straightforward consistency constrain, i.e., the clicked items by the same user might share some traits. To utilize such inherent signals, we apply consistency regularization on two positive item embeddings of each user, which can also regularize the item representation space to some extent. Specially, we randomly sample two positive items from historical sequences of each user and calculate the similarity (i.e., cosine similarity) of these two positive items as an additional consistency objective to CT4Rec denoted as CT4Rec. As shown in Table 5, CT4Reccontinuously improves the model performance based on CT4Rec, which indicates that consistency training on ubiquitous task-related information can further enhance the sequential recommendation performance. Table 4: Performance comparison of dierent consistency regularization strategies in the data augmentation setting. Table 5: Exp erimental results of more consistency training. The above studies prove the eectiveness of each component of our CT4Rec and its capability of leveraging more consistency regularization signals. We then study its generalization ability to other inconsistency scenarios. More concretely, we utilize our consistency training method to regularize the inconsistency brought by data other than the above-mentioned model-side inconsistency, i.e., we replace dropout with data augmentation methods to create two different user representations, and the corresponded task outputs for each user so as to conduct consistency regularization. To align with recent sequential recommendation methods based on constrastive learning, we leverage two easily implemented augmentation strategies from CL4SRec [25], i.e., Reorder and Mask. In doing so, we can directly compare the eects of dierent training objectives (besides the item prediction one for the sequential recommendation) on the augmented data, including unsupervised methods (i.e., CL, cosine, L2, DR), the supervised regularization in the output space (RD), and the combination of DR and RD (CT4Rec). Table 4 presents the experimental results for the data augmentation setting. We can observe that: 1) Our introduced DR objective is the most eective compared with other single methods, i.e., the consistency regularization on the representation space is the most preferable for than data augmentation scenario, which is in contrast with the observation for the dropout setting. We speculate that the labelinvariant data augmentation methods can lead to permuted and perturbed representation variants, which need more consistency regularization, while the label-invariant strategy does not deteriorate the inconsistency in the output space. 2) The combination of the consistency regularization in the representation space and the output space (CT4Rec) still performs the best with consistent and signicant performance over other training objectives. Figure 5: NDCG@10 and HR@10 curves on the valid set along with training epoch and time on the Beauty dataset. Since our CT4Rec does not modify the model structure of the backbone model or introduce extra augmented data, we mainly analyze the changes in the training process. We plot the curves of HR@10and NDCG@10 scores on the valid set along the training epoch number and time (seconds) for SASRec*, CL4SRec and our CT4Rec models, shown in Figure 5. At the early training stage, the backbone SASRec* model converges quickly with the same training epochs (around 15 epochs) and model performance as CT4Rec, but our CT4Rec can continuously improve the performance on the welltrained SASRec* model. It concludes that our consistency training objectives do not lead to more training epochs on the backbone SASRec*. But for CL4SRec that combines contrastive learning objective with SASRec*, it converges with much more training epochs and only moderate performance improvement. As for convergence time (seconds), our model indeed is slower than the backbone model since our consistency training objectives need an extra forward process (dropout) in each training step but achieves a much superior performance. Compared with CL4SRec, our CT4Rec is much more ecient and eective with a much better nal optimum and less convergence time even when we haven’t calculated the time cost of data augmentation and negative sampling for CL4SRec. Through the analysis, we can conclude that: 1) CT4Rec is much ecient and eective than the method based on contrastive learning even without counting its time cost of data augmentation and negative sampling; 2) CT4Rec indeed introduces extra training time for the backbone model, which can be mitigated by early stop insomuch as our CT4Rec can quickly surpass the backbone model in the early stage and with a much better nal convergence performance. In this paper, we proposed a simple yet very eective consistency training method for the sequential recommendation task, namely CT4Rec, which only involves two bidirectional KL losses. We rst introduce a top-performed regularization in the output space by minimizing the bidirectional KL loss of two dierent outputs. We then design a novel consistency training term in the representation space by minimizing the distributed probability of two user representations. Extensive experiments and analysis demonstrate its eectiveness, eciency, generalization ability, and compatibility. Juntao Li is the corresponding author.