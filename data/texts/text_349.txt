Learning node representations is at the heart of Graph Representation Learning (GRL), and has been proven successful in establishing the state-of-the-art in multiple domains including recommendation systems. In this paper, we benchmark node embeddings generated through Singular Value Decomposition (SVD) of adjacency matrix for embedding generation of users and items and use a two-layer neural network on top of these embeddings to learn relevance between useritem pairs. Inspired by the success of higher-order learning in GRL, we further propose an extension of this method to include two-hop neighbors for SVD through the second order of the adjacency matrix and demonstrate improved performance compared with the simple SVD method which only uses onehop neighbors. Empirical validation on three publicly available datasets of recommendation system demonstrates that the proposed methods, despite being simple, beat many stateof-the-art methods and for two of three datasets beats all of them up to a margin of 10%. Through our research, we want to shed light on the effectiveness of matrix factorization approaches, speciﬁcally SVD, in the deep learning era and show that these methods still contribute as important baselines in recommendation systems. Index Terms— Graph Neural Networks, Singular Value Decomposition, Node Embeddings, Representation Learning Graph Representation Learning (GRL) presents a very promising direction in terms of machine learning on graphs [1, 2, 3, 4]. A central idea in GRL is to represent each node in the graph as a vector of ﬂoating-point numbers to capture some desired properties of a node with respect to the graph. For node2vec[1], this property is the neighborhood of the node; for GraphSage[5], it is about capturing the feature neighborhood of a node based on a selected neighbor set; for Graph Convolutional Network (GCN) [2], the purpose of a node embedding is to capture both feature and neighborhood similarity. These methods have been profoundly useful in several domains such as bio-inspired machine learning and genomics [6, 7, 8], spam detection [9, 10], natural language processing [11, 12, 13, 14] and recommendation systems [15, 16, 17, 18, 19]. In recommendation systems, GRL has been applied to further advance collaborative ﬁltering algorithms by considering multi-hop relationships between users and items [15]. The authors in [15] further proposed the notions of message dropout and node dropout to reduce overﬁtting in GCN like methods. In a follow-up study [17], it was demonstrated that simplifying GCN network by reducing non-linearity from the network can give a boost to the performance of these higherorder methods. Their work also corresponded with a similar study done in [20] where the authors argued that for GCN, even after removing non-linearity and collapsing weight matrices into a single one, the performance does not degrade in downstream tasks. The research carried out in the above papers compares several state-of-the-art methods to the proposed methods and shows that the simplicity of models is leading to higher performance, credited to better generalization of the models. Motivated by these studies, we set out to benchmark a simple SVD based approach in this paper on the recommendation systems problem to understand if further simplicity of the modelling approach can improve the performance metrics. In the proposed method, we ﬁrst generate user and item embeddings using SVD of the adjacency matrix of the user-item interaction graph and then employ a two-layer neural network with these embeddings as inputs to estimate the relevance of an item to a user. Given the success of multi-hop graph neural network models in previous studies, we augment the simple SVD method to consider a two-hop adjacency matrix for generating the embeddings and found that this method outperforms the simpler one-hop SVD method as well. Empirical results on three public datasets demonstrate that the performance of the proposed methods is indeed comparable to state-of-art approaches, and these methods beat many of them despite their simplicity. For two out of three datasets, the methods even outperform all compared approaches and effectively establish new state-of-the-art performance with the margin of improvement as much as 10%. The rest of the paper is divided into three sections: Section 2 describes the proposed methods, Section 3 contains the empirical experiments, and Section 4 provides the conclusion and future work. In this section, we elaborate on the proposed methods. We ﬁrst discuss the SVD based baseline followed by an extension of the same by using two-hop matrices. In the last part, we describe the loss function and model training. Before discussing the proposed methods, we list our notations in Table Matrix factorization is a well-studied problem in linear algebra and has been extensively applied to recommendation systems [21, 22, 23, 24], typically in the form of collaborative ﬁltering. In this paper, we propose a simple approach to generate user and item embeddings using Single Value Decomposition (SVD) [25] of the adjacency matrix between users and items. Using a two-layer perceptron model, we transform these embeddings in a supervised fashion to learn the relevance between the user and item pairs. We call this method Simple SVD Baseline (SSB). To compute the SVD embeddings, we consider the adjacency matrix of the user-item interaction graph, A. We ﬁrst convert the asymmetric matrix to a symmetric matrix as follows: We then compute a Laplacian Normalization of Aas discussed in [2]:eA = DAD, whereeA is the Laplacian Normalized of adjacency matrix, A, and D is the degree matrix derived from A. We perform matrix factorization oneA using Truncated SVD on this normalized matrix to generate user embeddings (e) and item embeddings (e), where the number of components in Truncated SVD correspond to the embedding dimension. We use Truncated SVD [26] since it has shown to be scalable on large matrices. After generating these embeddings, we transform them through a two-layer perceptron model (f (x) = x, as the activation function) and concatenate the output of both the layers of the perceptron model along with original SVD embedding to generate a user embedding (e) or an item embedding (e). The intuition behind using the perceptron model is to allow supervised transformation of eand eto learn the relevance between user and item. Fig. 1 shows the model architecture. The dot product between eand eacts as the relevance score for the user-item pair and is optimized by the model through tuning of the weights of the two-layer perceptron model via back-propagation. Motivated by the success of multi-hop graph neural networks and the performance of the SSB approach on recommendation Fig. 1: Model architecture for SSB. eand eare the user embedding and item embedding respectively generated from the Truncated SVD [26] ofeA. eand eare the embedding outputs from the ﬁrst and second layers of MLP respectively. Finally, e, eand eare concatenated together to form the user embedding e. The item embedding e is constructed in the same way as e. Dot product between e and eis used the score for the user-item pair and the same is optimized through backpropagation using pairwise BPR loss similar to the previous studies[15, 17]. tasks, we attempt at joining both of these into a single method to capture higher-order relationships between users and items, similar to graph neural networks like GCN [2]. The overall model architecture remains the same as SSB, except for the change in how the eand eembeddings are computed. To compute an embedding that can capture the two-hop signals, we compute the second power of the Laplacian Normalized adjacency matrix,eA, and then compute its Truncated SVD. We ﬁnally concatenate the embeddings from SVD ofeA (corresponding to one-hop neighborhood) and SVD ofeA(corresponding to two-hop neighborhood) to generate eand eembeddings for this approach. The embedding size of TSA is the size of the vector after this concatenation. Since this approach contains twohop signals from the graph, we denote this method as TwoHop SVD Approach (TSA). The learnable parameters in the proposed methods are only the weights of the multi-layer perceptron model. To optimize the user-item relevance, we employ the Bayesian Personalized Ranking (BPR) loss [27] similar to [15]. It is a pair-wise loss that encourages correct predictions on observed instances than on unobserved instances. We use the Adam optimizer [28] in a mini-batch setting, where the batch size is a hyperparameter. We use the same three datasets (Gowalla, Yelp2018 and Amazon-Book) as [15, 17] with the same train and test split in order to make a fair comparison with the already reported results. Table 2 summarizes the dataset statistics. We refer the reader to [15] for more details of the datasets. We evaluate the performance on mean NDCG@K and mean Recall@K per user for K=20. We keep K=20 to enable fair comparison with previous studies which use the same metrics [15, 17]. For the rest of the paper, we denote Recall@20 as Recall and NDCG@20 as NDCG. It should be noted that for both Recall and NDCG, the items retrieved for top-20 are solely from the test partition of the dataset. For the proposed methods, there are four key hyperparameters - SVD embedding size (|e| and |e|), batch size, learning rate and size of the multi-layer perceptron. For this study, we ﬁx the size of the multi-layer perceptron to be 512 neurons each and keep the learning rate as 10motivated by the experiments in [17]. We tune the SVD embedding size over the following set: {2, 2, 2, 2, 2}. We keep the batch size as 1024 for Gowalla and Yelp2018, and 2048 for Amazon-Book as used in the study of NGCF [15] and LightGCN [17]. (j = {1, 2}) ( j = {1, 2}) In this section, we report the performance metrics for the proposed methods - Simple SVD Baseline (SSB) and Two-Hop SVD Approach (TSA). We benchmark the approach against NGCF [15], Mult-VAE[29], GRMF [30], LightGCN [17], MF [27], and NeuMF [31]. Although MF [27], and NeuMF [31] methods are relatively older methods to compare. However, we report their performance here as these are closely related to matrix factorization in the context of recommendation systems. LightGCN [17] is the state-of-the-art method showing the best performance compared to all related approaches as shown in their paper. Table 3 shows the performance metrics for the proposed methods and the compared methods. We replicate the results of these approaches from the original papers of NGCF [15] and LightGCN [17]. We follow the same experimental methodology as stated in the papers and followed in the code and datasets made available by the authors of these studies to make a fair comparison. We can observe that TSA performs considerably better for Amazon-Book dataset than all the compared state-of-art methods, including LightGCN and NGCF. The relative gain of TSA over LightGCN, which performs the best among the compared methods, is approximately 9.86% in Recall@20 and 13.4% in NDCG@20. The performance of SSB is also Table 3: Comparison of the proposed methods - SBB and TSA with related methods. It can be observed that the proposed methods, despite being very simple, beat all the compared methods for the Yelp2018 and Amazon-Book datasets. For the Gowalla dataset, the proposed methods prove to be a strong baseline and was able to beat all but one state-of-the-art methods on this dataset. |e| is the size of Truncated SVD embedding which is same for both users and items. ψ Results reused from [17]; φ Results reused from [15]. higher than all considered approaches in terms of NDCG and only 0.7% short of LightGCN and still better than all compared approaches. We can see that for Yelp2018 dataset, TSA performed marginally better than LightGCN [17] in terms of both Recall ( 1% relatively) and NDCG ( 2% relatively). In contrast, SBB performs marginally lower than LightGCN [17] in terms of Recall but has a slightly higher NDCG. However, both SBB and TSA performs signiﬁcantly better than all other compared approaches, including NGCF [15] which uses multi-hop relations among users to exploit higher-order signals for predicting relevance. For the Gowalla dataset, the proposed methods perform poorly compared to LightGCN. There is 1.3% absolute difference in Recall and 1.5% absolute difference in NDCG. However, despite being fairly plain, the proposed approaches outperform all other approaches, including some which are inherently more complex such as Mult-VAE and NGCF. We believe that the generalization power of the proposed methods has led to the improvement in performance in Yelp2018 and Amazon-Book datasets. However, in the case of Gowalla, the method seems to be underﬁtting because the signals of SBB and TSA are only limited to one-hop and two-hop neighbors respectively. As shown in [17], as the number of layers of GCN is increased to four (equivalent to four-hops of neighborhood), the performance increases. We leave this aspect of experimentation to be addressed in future work. In this section, we point out the comparison between SBB and TSA approaches. In Table 3, we can observe that TSA always performs better than SBB, given the additional signal from Table 4: Comparison of the proposed methods with each with respect to training metrics on Yelp-2018 dataset. It should be noticed that SVD is only performed once at the start of training the model. two-hop neighbors. Comparing the proposed methods, SBB and TSA, we only see a small relative increase in performance in Gowalla and Yelp-2018 and a more signiﬁcant uplift in the case of TSA for Amazon-Book dataset. This is in line with the studies of LightGCN [17], and NGCF [15] where it was shown that as the number of layers is increased for the GCN model (which is equivalent to covering more hops in neighborhood), the performance increases in equivalent amount. We believe the improvement of metrics in TSA is the addition of signal from two-hops away which is not present in SBB. Interestingly, for Yelp2018 and Amazon-Book datasets, the two-hop proposed approach, TSA, is able to outperform the four-hop approaches of LightGCN and NGCF. Regarding training loss, we observe that SSB, on convergence, has a higher training loss than TSA. While TSA performs better than SSB, it does so at the expense of additional time to compute the two-hop adjacency matrix; and the additional Truncated SVD on the two-hop matrix. In Table 4, we summarize the comparison between SSB and TSA on the training metrics, while test performance is shown in Table 3. We run our experiments on Intel(R) Xeon(R) CPU E5-2690 Fig. 2: Training Loss and Test Recall@20 per epoch for the Yelp2018 for TSA method. For higher embedding dimension, Training Loss is lower and Test Recall is higher. v4 @ 2.60GHz with 6 cores and 110 Giga Bytes of RAM. Out of the four hyperparameters discussed, we only optimize the SVD based embedding size for users and items, which become the input to the multi-layer perceptron. In this section, we will describe the observations on how the performance and training loss changes as we change the embedding dimensions for TSA, which performs better than SSB across datasets. We vary the embedding dimension as follows: {2, 2, 2, 2, 2}. Fig. 2(a) shows the training loss for different embedding sizes for the TSA approach. As expected, it can be seen that as the embedding size increases, the loss decreases faster and also to a lower value. We observe a similar trend in test performance metrics, and Fig. 2(b) shows Recall@20 for the test set for different embedding sizes, and it can be seen that higher embedding sizes lead to better performance. In this paper, we started out to benchmark SVD based methods against the state-of-the-art GRL methods. We propose two approaches for the same, and experiments on three realworld open datasets demonstrate that these methods are powerful enough to beat many GRL methods and even come out as state-of-the-art themselves in two out three datasets. We observed the most signiﬁcant relative gain of over 10% against the state-of-the-art methods. This particular work raises many research questions, and we envision the following future work. We plan on investigating how to generalize the approach from two-hop to n-hop since we saw in earlier research articles that with a higher order of neighborhood, we could expect better performance. There is also a need to propose an inductive version of these methods since transductive versions (as proposed in this paper), do not work with new nodes or new edges in the graph and would require frequent retraining in the current form. We also plan to investigate the aspects around the better implementation of SVD in big data frameworks such as Spark [32, 33]. This would help us further understand the time taken for the proposed methods for the training of models. We also want to explore how matrix factorization or SVD can be integrated with GRL to improve empirical performance and is it possible to extract the goodness from both methods and merge them. We also plan to work on data proﬁling for the proposed methods and existing literature to understand why one approach performs well on some datasets but does not perform equally well on other datasets. On the empirical investigation front, we also intend to benchmark these approaches on the Open Graph Benchmark [34]. We also plan to test out these approaches in tasks beyond recommendation systems such as graph-based formulations in NLP, social network modelling and graph applications in biology. Through our work, we want to highlight that matrix factorization based methods still contribute as important baselines and should not be ignored in empirical benchmarking while making more advances in GRL or recommendation systems. [1] Aditya Grover and Jure Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 855– 864. [2] Thomas N Kipf and Max Welling, “Semi-supervised classiﬁcation with graph convolutional networks,” arXiv preprint arXiv:1609.02907, 2016. [3] Rianne van den Berg, Thomas N Kipf, and Max arXiv preprint arXiv:1706.02263, 2017. attention network,” in The World Wide Web Conference, 2019, pp. 2022–2032. [5] William L Hamilton, Rex Ying, and Jure Leskovec, “Inductive representation learning on large graphs,” in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 1025–1035. [6] Guadalupe Gonzalez, Shunwang Gong, Ivan Laponogov, Michael Bronstein, and Kirill Veselkov, “Predicting anticancer hyperfoods with graph convolutional networks,” Human Genomics, vol. 15, no. 1, pp. 1–12, 2021. [7] Ivan Laponogov, Guadalupe Gonzalez, Madelen Shepherd, Ahad Qureshi, Dennis Veselkov, Georgia Charkoftaki, Vasilis Vasiliou, Jozef Youssef, Reza Mirnezami, Michael Bronstein, et al., “Network machine learning maps phytochemically rich “hyperfoods” to ﬁght covid19,” Human Genomics, vol. 15, no. 1, pp. 1–11, 2021. [8] Yifan Wu, Min Gao, Min Zeng, Feiyang Chen, Min Li, and Jie Zhang, “Bridgedpi: A novel graph neural network for predicting drug-protein interactions,” arXiv preprint arXiv:2101.12547, 2021. [9] Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong Li, and Le Song, “Heterogeneous graph neural networks for malicious account detection,” in Proceedings of the 27th ACM International Conference on Information and Knowledge Management, 2018, pp. 2077– 2085. [10] Xiaoru Qu, Zhao Li, Jialin Wang, Zhipeng Zhang, Pengcheng Zou, Junxiao Jiang, Jiaming Huang, Rong Xiao, Ji Zhang, and Jun Gao, “Category-aware graph neural networks for improving e-commerce review helpfulness prediction,” in Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 2020, pp. 2693–2700. [11] Rahul Ragesh, Sundararajan Sellamanickam, Arun Iyer, Ramakrishna Bairi, and Vijay Lingam, “Hetegcn: heterogeneous graph convolutional networks for text classiﬁcation,” in Proceedings of the 14th ACM International Conference on Web Search and Data Mining, 2021, pp. 860–868. [12] Liang Yao, Chengsheng Mao, and Yuan Luo, “Graph convolutional networks for text classiﬁcation,” in Proceedings of the AAAI conference on artiﬁcial intelligence, 2019, vol. 33, pp. 7370–7377. Wen, and Liang Wang, “Every document owns its structure: Inductive text classiﬁcation via graph neural networks,” arXiv preprint arXiv:2004.13826, 2020. Xiaoli Li, “Heterogeneous graph attention networks for semi-supervised short text classiﬁcation,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 4823–4832. Tat-Seng Chua, “Neural graph collaborative ﬁltering,” in SIGIR, 2019. [16] Huan Zhao, Quanming Yao, Jianda Li, Yangqiu Song, and Dik Lun Lee, “Meta-graph based recommendation fusion over heterogeneous information networks,” in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2017, pp. 635–644. [17] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang, “Lightgcn: Simplifying and powering graph convolution network for recommendation,” in Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, 2020, pp. 639–648. [18] Jianing Sun and Yingxue Zhang, “Multi-graph convolutional neural networks for representation learning in recommendation,” . [19] Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, and Haifeng Li, “T-gcn: A temporal graph convolutional network for trafﬁc prediction,” IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 9, pp. 3848–3858, 2019. Fifty, Tao Yu, and Kilian Weinberger, “Simplifying graph convolutional networks,” in International conference on machine learning. PMLR, 2019, pp. 6861– 6871. [21] Lyle H Ungar and Dean P Foster, “Clustering methods for collaborative ﬁltering,” in AAAI workshop on recommendation systems. Menlo Park, CA, 1998, vol. 1, pp. 114–129. [22] Xiaofeng Li and Dong Li, “An improved collaborative ﬁltering recommendation algorithm and recommendation strategy,” Mobile Information Systems, vol. 2019, 2019. [23] Souvik Debnath, Niloy Ganguly, and Pabitra Mitra, “Feature weighting in content based recommendation system using social network analysis,” in Proceedings of the 17th international conference on World Wide Web, 2008, pp. 1041–1042. [24] Keunho Choi, Donghee Yoo, Gunwoo Kim, and Yongmoo Suh, “A hybrid online-product recommendation system: Combining implicit rating-based collaborative ﬁltering and sequential pattern analysis,” electronic commerce research and applications, vol. 11, no. 4, pp. 309–317, 2012. [25] Virginia Klema and Alan Laub, “The singular value decomposition: Its computation and some applications,” IEEE Transactions on automatic control, vol. 25, no. 2, pp. 164–176, 1980. [26] Per Christian Hansen, “Truncated singular value decomposition solutions to discrete ill-posed problems with illdetermined numerical rank,” SIAM Journal on Scientiﬁc and Statistical Computing, vol. 11, no. 3, pp. 503–518, 1990. [27] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme, “Bpr: Bayesian personalized ranking from implicit feedback,” arXiv preprint arXiv:1205.2618, 2012. method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014. [29] Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara, “Variational autoencoders for collaborative ﬁltering,” in Proceedings of the 2018 world wide web conference, 2018, pp. 689–698. Inderjit S Dhillon, “Collaborative ﬁltering with graph information: Consistency and scalable methods,” in Advances in Neural Information Processing Systems, 2015, pp. 2107–2115. [31] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua, “Neural collaborative ﬁltering,” in Proceedings of the 26th international conference on world wide web, 2017, pp. 173–182. [32] Athanasios Alexopoulos, Georgios Drakopoulos, Andreas Kanavos, Phivos Mylonas, and Gerasimos Vonitsanos, “Two-step classiﬁcation with svd preprocessing of distributed massive datasets in apache spark,” Algorithms, vol. 13, no. 3, pp. 71, 2020. [33] Zhongyi Sun, Fengke Chen, Mingmin Chi, and Yangyong Zhu, “A spark-based big data platform for massive remote sensing data processing,” in International Conference on Data Science. Springer, 2015, pp. 120–126. Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and arXiv:2005.00687, 2020.