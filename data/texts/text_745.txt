Virtual support agents have grown in popularity as a way for businesses to provide better and more accessible customer service. Some challenges in this domain include ambiguous user queries as well as changing support topics and user behavior (non-stationarity). We do, however, have access to partial feedback provided by the user (clicks, surveys, and other events) which can be leveraged to improve the user experience. Adaptable learning techniques, like contextual bandits, are a natural t for this problem setting. In this paper, we discuss real-world implementations of contextual bandits (CB) for the Microsoft virtual agent. It includes intent disambiguation based on neural-linear bandits (NLB) and contextual recommendations based on a collection of multi-armed bandits (MAB). Our solutions have been deployed to production and have improved key business metrics of the Microsoft virtual agent, as conrmed by A/B experiments. Results include a relative increase of over 12% in problem resolution rate and relative decrease of over 4% in escalations to a human operator. While our current use cases focus on intent disambiguation and contextual recommendation for support bots, we believe our methods can be extended to other domains. • Computing methodologies → Reinforcement learning; Neural networks; Learning linear models; Natural language processing; Batch learning; • Applied computing → Enterprise computing. multi-armed bandits, contextual bandits, chat bots ACM Reference Format: Sandra Sajeev, Jade Huang, Nikos Karampatziakis, Matthew Hall, Sebastian Kochman, and Weizhu Chen. 2021. Contextual Bandit Applications in a Customer Support Bot. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’21), August 14–18, 2021, Virtual Event, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/ 10.1145/3447548.3467165 Virtual support agents have become ubiquitous in customer service for businesses. The Microsoft virtual agent is a customer support agent with which millions of users engage everyday. A typical session between the virtual agent and the user is shown in gure 1. Challenges that come with operating this system include adapting to changing user behavior and support topics. For instance, bugs associated with the release of new operating system𝑌can result in users asking more questions about𝑌rather than previous operating system𝑋. In addition, editors will create new support articles about operating system𝑌to help address common problems; such articles should be suggested by the system when relevant as opposed to outdated content. Another example is that the rise in employees working from home due to a pandemic may result in users asking more questions than before about dual monitor detection or how to connect audio wirelessly. Machine learning, specically reinforcement learning, has many opportunities to help optimize virtual support agents and meet these challenges head-on. We drew inspiration from advances in recommendation systems in news [12] and video [6,17]. The industry standard for recommendation systems has recently been switching to CBs [11], a simplied single-step reinforcement learning (RL) paradigm which requires exploration but does not require dealing with credit assignment. In addition, CBs are data-ecient, which is helpful in our scenario where exploration of riskier actions can hurt the end-user experience and have negative monetary implications. In the context of the virtual agent, we deal with a partial information setting, meaning we only receive feedback (click, survey response, etc.) for the actions the user took. This can easily be modeled with CBs. Our work presents applications of CBs for two scenarios of the Microsoft virtual agent: intent disambiguation and contextual recommendations. In intent disambiguation, the goal is to provide recommendations that tailor to the user query about a topic. For contextual recommendations, the objective is to provide a set of recommendations to the user as soon as they engage the virtual agent prior to any user query, based on a set of context features. These contextual features can include information such as the website from where the user engages the virtual agent. We have deployed a MAB solution for contextual recommendations and a NLB solution for intent disambiguation, both of which showed positive impact on key performance indicators (KPIs) in online A/B experiments. The rest of the paper is organized as follows. First, we present the problem setting and notation. Next, we describe the methods used, going into detail about the learning paradigms. Then, in the implementation section, we outline how we incorporated MABs for Figure 1: The Microsoft virtual agent. In response to the user query, an intent clarication dialog presents a slate of topics generated by a policy trained using NLBs. contextual recommendations and NLBs for intent disambiguation. Finally, we present the ecacy of our solutions through results from online experiments in the evaluation section. The application motivating our work is the Microsoft virtual agent – an interactive dialogue system providing rst-line customer support for many Microsoft products. The virtual agent can be accessed via multiple channels – most commonly through the Microsoft support websiteor the Get Help app which comes with Windows 10. In the Microsoft virtual agent’s original architecture, manually congured rules dictated the behavior of the system. Most of these rules were not important from a business perspective, but rather assumptions made by application developers due to a lack of data. Moreover the original system could not adapt automatically to the diversity of user intents and a changing environment, such as software updates causing new issues or updated support content. RL addresses some of these challenges and is a good t for this problem setting for the following reasons: (1)Microsoft products (including Xbox, Oce, Windows, Skype) have a large customer base, hence the virtual agent’s incoming trac is also signicant, making RL methods feasible. (2)We have access to multiple feedback signals including click behavior, escalation to a human agent, and responses to survey questions such as “Did this solve your problem?”. Some of these signals are tracked by the product team as their KPIs. At the same time, the application poses interesting challenges for the use of RL. Goal-oriented dialogue systems need to not only understand the user’s original intent, but also be able to carry the state of the dialogue and guide the user towards their goal. While our long-term plan is to be able to have a single RL agent in charge of the whole conversation, the requirements of such an endeavor, including the of number of samples needed to learn non-trivial policies, are substantial. Therefore, we started by applying RL to the virtual agent in isolated components rst, ignoring issues of credit assignment and thus working in the setting of CBs. In this work, we focus on two specic scenarios within the Microsoft virtual agent. In the following subsections, we describe them in more detail, and then formalize the notation that is shared between both scenarios. The domain of customer support, especially for a large company such as Microsoft, is complex and has a signicant number of intents. Our intent disambiguation policy is tasked with deciding when the user’s query is clear enough to directly trigger a solution, such as a troubleshooting dialogue, or to ask a clarication question. The inputs to this policy are the statement of the issue by the user (the query), user context features, and a list of candidate actions. The query can range from a few keywords to long and complex sentences or even paragraphs. The user context includes information like the user’s operating system and its version (e.g., Oce products work on Windows, Mac, iOS, and Android). The candidate actions are a collection of pre-authored dialogue intents or solutions related to the user’s query pulled from the Web. Retrieval of these candidates is currently performed by several strategies. One of them includes a deep learning model similar to the one described in [8]. Another uses Bing Custom Search for customized document retrieval. These retrieval components are currently out-of-scope for RL-based optimization so we will focus on the policy that operates with a small list of retrieved candidates. Given the input, the policy can do one of the following: •Directly trigger a single intent or a solution: this can start a troubleshooting dialogue or display a rich text solution. •Ask a yes/no question: “Here’s what I think you are asking about: . ..Is that correct?” •Ask a multiple-choice question: “Which one did you mean?”, followed by titles of two to four intents as well as option “None of the above.” •Give up: “I’m sorry, I didn’t understand. It helps me when you name the product and briey describe the issue.” Figure 1 presents an example of a multiple-choice question action taken by the disambiguation policy. The “Settings” app is a desktop application in Windows 10, where a user can click “Get help” from any Settings page (e.g., Bluetooth, Display etc.) and interact with the Microsoft virtual agent. We will refer to the page the user is coming from as the source page - it is a crucial part of the user context that is available to us. The goal is to provide contextual recommendations, i.e., to recommend a slate (sequence) of solution topics before the user types anything, simply based on the context sent by the app (see gure 2). The baseline experience is a xed mapping from source pages to slates of up to six topics manually picked by human editors. We were motivated by several reasons to leverage feedback data to learn better contextual recommendations and improve upon the Figure 2: Contextual recommendations in Settings in Windows 10, for the source page “Printers”. baseline. First, maintaining manually-specied dialogues does not scale to the approximately 200 source pages present in the app (most of them served the same xed slate of six topics). Second, a large amount of trac ows through the app, which suggests that data-driven techniques could do well here. Third, the available user context includes more signals than just source page, which can help with suggesting relevant solutions in certain situations, such as the device network type (wired, wi) and battery status (charging, discharging). Given the context, the agent can take one of the following actions: (1) Suggest up to six solutions out of about 3000 candidates. (2)Choose to fall back to default behavior of recommending six xed options chosen by editors for the current source page. The feedback signals available to learn from are the same in both scenarios described above: • Click.When a slate of topics is presented, the option selected by the user is recorded. This signal is censored in scenario 2.1 when the policy decides to directly trigger. While useful, it is not a metric important to the business, so we do not optimize for it alone. • Survey.After providing a nal answer to the user, the virtual agent asks whether the solution has resolved their problem. The user may respond “yes”, “no”, or decline to answer. The product team tracks this signal, aggregated per user session and averaged, as the most important KPI called Problem Resolution Rate (more details in section 5.1). Hence, this feedback signal is also our main reward signal. • Escalation.The user can decide at any time to talk to a human agent. This negative reward signal is directly related to the actual cost of running a call center. In general, this metric is monitored but is not being optimized. The contextual recommendation and intent disambiguation scenarios that we focus on in this paper can be modeled as combinatorial CBs. We adapt and expand notation proposed in [19] to the scenarios found in the Microsoft virtual agent. In this setting, an agent interacts with the environment repeatedly as follows: For time step 𝑡 = 1, 2, . . .: (1)The world produces a context𝑥∈ X(e.g., the user sends a query 𝑥to the system). (2)The agent chooses a slates= (𝑎, ..., 𝑎) ∈ S(𝑥), of variable length𝑙(0 ≤ 𝑙 ≤ 𝐿), consisting of actions𝑎∈ A(𝑥). We callA(𝑥)an item action space andS(𝑥)a slate space. Position 𝑗 in a slate is called a slot. (3)Given the context and slate, the world produces feedback signals, including: • 𝑐𝑙𝑖𝑐𝑘∈ {0, 1}informing of the slot selected by the user. In this work, we assume it is either a one-hot vector or a zero vector (i.e., no more than a single item can be selected). • 𝑠𝑢𝑟𝑣𝑒𝑦∈ {yes, no, skipped}indicating the user’s assessment of the solution presented by the virtual agent, provided via a survey. Values “yes” and “no” mean positive and negative feedback, respectively, while “skipped” means the user has not responded to the survey. • 𝑒𝑠𝑐𝑎𝑙𝑎𝑡𝑖𝑜𝑛∈ {0, 1}indicates whether the user has decided to escalate the case to a human agent (1) or not (0). (4)The reward for each slot𝑗in the slatesis denoted by𝑟. It is computed as a function of one or more of the feedback signals. Note that the item action spaceA(𝑥)is not a xed set but can vary based on the context. The actions are parametric, with features such as title, type of content, etc. In our case, the majority of actions represent support documents and troubleshooting dialogues, but A(𝑥)may also include the special action null item, denoted as ⊥. It gives the user a choice of indicating that none of the other actions is relevant. If selected via𝑐𝑙𝑖𝑐𝑘, the system moves on to other suggestions. To gain intuition about this notation, let’s consider some examples: • The intent disambiguation scenario presented in the gure 1. The query “i lost my email” is the main property of the context𝑥. Topics “Microsoft account recovery form” and “Recover email that is still in the deleted items folder” are some actions𝑎and𝑎. The “None of the above” option rendered in the dialog translates to the null item⊥- it gives the user an opportunity to explore other topics or ask another question. Hence, the slate chosen by the policy at that time step was s = (𝑎, 𝑎, ⊥). The intent disambiguation policy is also allowed, for some contexts, to directly trigger a topic. Such a slate would look likes = (𝑎), without the⊥action. What kind of slates are allowed in what contexts depends on the function S(𝑥). • The contextual recommendations scenario presented in the gure 2. The source page “Printers” is the main property of the context𝑥. Ignoring the presented topics and typing a query manually is equivalent to selecting the null item⊥- hence, the slate can be represented ass = (𝑎, 𝑎, . . . , 𝑎, ⊥). In the contextual recommendations scenario, every slate in S(𝑥), in every context𝑥, must always include⊥(i.e.: topics are never directly triggered). The reward used by our CB algorithms is always a function of feedback signals. Decoupling them from the reward denition, while not a standard approach in the RL literature (as in [5,11]), is useful in the learning algorithms we are presenting in later sections. Some feedback signals may be delayed. For example, a long conversation may occur between the action𝑎and the point at which we ask the user if the proposed solution addressed their problem (to generate the signal𝑠𝑢𝑟𝑣𝑒𝑦). We currently treat all variations that can happen there as part of the environment that generates a noisy version of the reward. This simplication is a practical choice and can be removed with better modeling of the interaction between the user and the bot. We divide discussion about the solutions into two main categories: learning from a discrete context (corresponding to the contextual recommendations scenario described in section 2.2) and learning from a rich context (relevant in the intent disambiguation scenario from section 2.1). For each discrete context𝑥∈ X, let there be two MABs corresponding to two feedback signals: clicks and survey responses. At time step 𝑡 , each MAB can be described as follows: •We have collected a history of reward successes and failures over𝑤total past time steps in each context𝑥separately for each action 𝑎. This 𝑤is specic to each context. •We have a function𝑆𝑎𝑚𝑝𝑙𝑒 (𝑠𝑢𝑐𝑐𝑒𝑠𝑠𝑒𝑠, 𝑓 𝑎𝑖𝑙𝑢𝑟𝑒𝑠)that samples scores for all actions. • Actions are ordered by these sampled scores. • The slate ends at the position at which ⊥ is ranked. •successes: the total instances action𝑎has been clicked for the past 𝑤time steps •failures: the total instances an action𝑎has been observed (ranked above ⊥) but not clicked For the survey response bandit and context𝑥, we ignore the cases that the survey is not answered, considering • successes: total “yes” answers • failures: total “no” answers 3.1.1 Sampling with a Discrete Context. To balance exploitation with exploration, we use a sampling algorithm to produce plausible estimates of the probability of a click and the probability of a “yes” survey. We compared both Thompson sampling [16,20] and EwS [13], and qualitatively we found that results in our application looked better with Thompson sampling so we focus on it here. However, EwS is reasonable to use as well. Let𝛼represent the successes for action𝑎in context𝑥, given the data until time step𝑡, within window𝑤. Likewise, let𝛽 represent the failures for action𝑎in context𝑥, given the data until 𝑡, within window𝑤. We start with a uniform prior distribution 𝐵𝑒𝑡𝑎(𝛼 = 1, 𝛽 = 1)and assume each event is iid. Then the posterior probability is𝐵𝑒𝑡𝑎(𝛼+ 1, 𝛽+ 1). In other words, we add one to all success and failure counts. In practice, we track successes𝛼 and trials 𝑁 , and failures are then computed as 𝛽 = 𝑁 − 𝛼. Each bandit samples an estimated action value𝑄 (𝑥, 𝑎)from the posterior distribution𝐵𝑒𝑡𝑎(𝛼+ 1, 𝛽+ 1)for each action 𝑎. To represent the nal score for an action𝑎based on both bandits, we initially combined the scores from the two bandits using a naive product to model the joint probability 𝑃 (𝑐𝑙𝑖𝑐𝑘) · 𝑃 (𝑠𝑢𝑟𝑣𝑒𝑦 = 𝑦𝑒𝑠|𝑐𝑙𝑖𝑐𝑘). Later, having amassed more logged data via online experimentation, we switched to a more exible approach of using a context-specic interpolation score𝜆which weights the click score lower if there are sucient survey response trials. log(𝑄(𝑥, 𝑎)) ≜ 𝜆log(𝑄(𝑥, 𝑎))+(1−𝜆) log(𝑄(𝑥, 𝑎)) This combined score is used to rank actions in descending order. The size of the slate visible to the user is determined by the minimum of the position of⊥in the slate and action space rules dictating the maximum slate length. In other words, the slate that the user actually sees is a subset of the scored actions. All actions with a higher sampled score than⊥is considered observed by the user. Having assembled the slate, the user then provides feedback signals. As mentioned in 2.4, the𝑐𝑙𝑖𝑐𝑘signal is a one-hot vector: 1 for the clicked slot and 0 for the rest. The𝑠𝑢𝑟𝑣𝑒𝑦signal is considered observed only for the slot that was clicked. Note that the downside of Thompson sampling in comparison to EwS is that it does not produce closed-form probabilities of choosing an action in a given state. Collecting such probabilities is often useful for o-policy learning and evaluation methods. This can be mitigated by logging the𝛼and𝛽values at the time of making a decision and estimating these probabilities oine in a Monte-Carlo fashion. A rich context is dened by large cardinality of the setX, which makes it dicult to learn the value function for each state. Therefore value function approximation is required for rich contexts, like natural language embeddings and image data. Assume there is a representation function𝜙that takes in a rich context𝑥 ∈ X and action𝑎 ∈ Aand outputs a set of low-dimensional features, 𝜙 (𝑥, 𝑎) ∈ R. We assume that the expected reward is linear with respect to these features. In other words, there exists an unknown vector𝑤, which models the linear relationship of the expected rewardE[𝑟 |𝑥, 𝑎], for action 𝑎 and a context 𝑥. 3.2.1 Neural-Linear Bandit. Correct quantication of uncertainty is critical in bandit algorithms because the algorithm is in charge of how data is collected. If the algorithm uses uncertainty estimates that are too wide, it over-explores and unnecessarily tries suboptimal actions. If the uncertainty estimates are too narrow, the algorithm under-explores and runs the risk of not discovering the best action. In this work we use the Neural-Linear Bandit strategy from [15], which compares various methods for quantifying uncertainty with deep learning models. There it was demonstrated that the neurallinear approach is often the best single-model method (i.e., not relying on an ensemble) for uncertainty quantication. We have slightly adapted this method for our purposes. The NLB consists of two main parts. First is the representation function𝜙:X×A ↦→ R which produces a d-dimensional vector of neural features. The function𝜙is generated by tting a neural network,𝑁:X × A ↦→ R, which predicts the observed reward from query-answer pairs. The last layer of this network is removed to represent 𝜙 (𝑥, 𝑎). The second part is the bandit function, where we try to estimate the𝑤that satises(2)and the uncertainty around it. This is the simplifying assumption in NLBs: all uncertainty in the model can be approximated just by the uncertainty in the last layer of the network. Due to the assumption that the reward is linear with respect to the neural features, the focus is on creating a posterior distribution for the solution. Note that validating this assumption becomes dicult in practice due to the large dimensions of the neural features. This is a simple least squares problem that we need to solve each time we update the bandit. To do this eciently and support rolling windows of training data we maintain the sucient statistics: which help us rewrite eq.(3)as𝐵^𝑤= 𝑓. We solve the latter with the help of the Cholesky factorization𝐿𝐿= 𝐵which we also use for obtaining the uncertainty for new predictions^𝑟 (𝑥, 𝑎) = 𝜙 (𝑥, 𝑎)^𝑤. To do this we need a notion of a count of how many times action𝑎has been tried in context𝑥. For linear bandits this is given by The bandit gives more benet of doubt to an action𝑎with small 𝑏(𝑎). Similarly to the number of times an action has been tried in the case of a MAB,𝑏(𝑎)is unit-less even if𝜙 (𝑥, 𝑎)has units: the matrix𝐵cancels them. The inverse of the quadratic form is also necessary for other reasons. If𝜙were one-hot vectors, then𝐵 would be diagonal with the count for action𝑎in context𝑥on the diagonal. Therefore, the inverse quadratic form is the count. 3.2.2 Sampling with a Rich Context. The NLB policy samples actions𝑎to generate the slate𝑠according to a sampling algorithm. The main algorithms we explored for sampling in the rich context are the linear versions of Thompson sampling [3] and EwS [1]. Both sampling algorithms result in stochastic policies. Linear Thompson sampling [3] requires specication of a prior covariance matrix for𝑤, typically chosen to be a multiple of the identity matrix. However, there is no easy way to set this hyperparameter as it can have a hard-to-predict eect on the learning dynamics. In addition, this algorithm doesn’t provide the probability of sampling action𝑎in an explicit form which is useful for o-policy evaluation. Linear EwS [1] was chosen since it has no requirement of setting a prior. In Linear EwS, the probability of selecting an action𝑎has an explicit form. Given context𝑥we rst compute the action the model currently prefers and the gaps between the best action and all actions Finally, as noted in [1], Linear EwS samples action𝑎proportional to exp(−2𝑏(𝑎)𝑔(𝑎)) where 𝑏(𝑎) is given by (6). 3.3.1 Making Exploration Safer. If a strong baseline already exists, to make exploration safer, we can compare the total value of the sampled slate to the value of the baseline slate. Whichever slate has a higher value is then presented to the user. In our case, we consider the slate value to be the sum of the sampled scores of the actions in the slate. This simple trick can reduce variance in the KPIs, normally expected from exploration, after initial deployment of an automatic learning system. Note that the baseline slate can be still used to collect feedback signals and improve future predictions. 3.3.2 Sampling to Obtain a Slate. One can simply sample for expected rewards from possible actions once to obtain a full slate of actions. If there are per-slot rules dictated by business requirements, they can be applied after the sampling. This section outlines key implementation details about our MAB solution for contextual recommendations and NLBs for intent disambiguation. Both training pipelines were written in Python. All training and evaluation was done asynchronously using Azure Batch. The inference code was written in C# to easily integrate with our partner team’s product, the Microsoft virtual agent. The intent disambiguation models were trained using PyTorch [14] and converted to ONNX [4] for inference in the production system. ONNX Runtime is used to load the models in .NET. 4.1.1 Problem Formulation. The discrete context in our case is categorical: the source page from where the user is clicking “Get Figure 3: Feedback loop for contextual recommendations. help”. In oine analysis, we found that while there are other interesting attributes in the context such as battery value and network type, the source page is the most indicative of what the user may free-type when they choose not to select from the given list of “top solutions”. For example, users clicking “Get help” from the Printers page tend to ask printer-related questions. The candidate content is represented simply by a unique id and no other features. We associate the null item ⊥ with the user free-typing. User clicks and surveys are logged and aggregated every four hours using asynchronous jobs over a moving window. This moving window ensures that content that is not performing well and not interacted with will disappear from recommendations eventually. Note that this window is context-specic, as content performing poorly in one context does not have an eect on another context. At runtime, a user clicks on “Get help” from source page𝑥. First, we gather available candidates that have click and survey counts. We have limitations on the size of our logged data, so we perform an initial sampling of the candidates from over 1000 candidates down to around 25 using Thompson sampling as described in 3.1.1. With this pared-down set of candidates, we sample once more using Thompson sampling to obtain a nal slate of actions. The length of the slate is determined by the minimum of the following: the null item’s position and the congured max length of the slate 𝐿(in our case𝐿 = 7including⊥). We apply the trick of making exploration safer from 3.3.1 as there is a strong human-authored baseline. 4.1.2 Automatic Action Space Expansion. So far as we’ve described, this experience has a concrete set of candidates per source page from which it draws to recommend to the user. Occasionally, an editor may author new content relevant to a source page which performs well within the context of a source page but outside of the initial recommendation, e.g., the user free-types a query that triggers this content, nds it helpful, and answers ‘yes’ to the endof-chat survey. Thanks to our work with intent disambiguation, these interactions are also logged. To address this, we have another asynchronous job that runs once a day. This job merges statistics from our intent disambiguation ow that occur within the context of a source page with statistics from the initial Settings recommendations ow. For each source page, we use the null item’s score as a success threshold for candidates in consideration from intent disambiguation. We rst lter out candidates with insucient trials, e.g., 10 trials. For each potential candidate𝑎we then compute a bound on the probability that the expected reward of 𝑎 is larger than the expected reward of ⊥. We then lter out candidates whose bound on the probability is less than our congured allowable false positive probability𝑓 𝑝. Intuitively, this allowable false positive probability is the maximum probability we allow that an individual new candidate has lower survey response success than⊥, i.e., the probability of a candidate not being a false positive is≥ 1 − 𝑓 𝑝. The qualifying candidates’ counts are copied over from the intent disambiguation space to the Settings space–however, we zero out the click counts as the trials between the two spaces are not comparable. 4.2.1 Problem Formulation. For the intent disambiguation scenario, the context is rich. The input data to the policy contains the text embedding of the user query and candidate title generated in a manner similar to [8]. It also includes a collection of categorical user context features, such as the website from where the user initiated the virtual agent. When the user engages the virtual agent by typing a question, our policy samples from a set of candidates described in 3.3.2 to generate a slate. The slate length is determined by the null item. 4.2.2 Training. To train the NLB policy, we rst featurize the logged data, limiting to sessions that have explicit user survey responses (“yes” or “no”). We initially dened the reward to be a function of the survey feedback of only the clicked action (8). In subsequent iterations, we experimented with combining multiple feedback signals for the reward as described in section 5.5. After featurization, we follow the learning procedure for NLBs dened in 3.2.1. The training objective of the network𝑁was set to minimize the squared error between the reward of the clicked action and the prediction. This network is used to generate𝜙with𝑑 = 2048neural features. Note that𝑑cannot be excessively large as the bandit needs to store 𝐵or 𝐿which is of size 𝑑 × 𝑑 to compute eq. (6). We train the bandit in xed periods (e.g., one day). For each xed period, we solve the least squares problem dened in(3). When training this model in practice, the matrix𝐵can be very close to low rank. To be able to have a usable inverse, we used principal component regression and retained enough principal components to capture 99% of the variation in the neural features. The initial NLB policies were xed: trained on a set number of days of data and deployed to production. We later explored autoupdating policies with hourly retraining. The virtual agent product team tracks several KPIs where some were the metrics for which we optimized. Our metrics are reported over user sessions and each user session is treated independently. We do not track an individual user’s behavior over time, e.g. if a user re-engages with the Microsoft virtual agent, that is treated as a separate user session. • Problem Resolution Rate (PRR):a ratio of user sessions with a positive survey response to all sessions with any survey response. This is the main metric we track while running oine or online experiments. • Engaged Assisted Support (EAS):fraction of user sessions that end up with escalation to human-assisted support. • Self Help Success (SHS):fraction of user sessions that ended with assumed success (i.e., an answer is delivered and the user does not engage assisted support or indicate the answer was not useful). • User Engagement (UE):fraction of user interactions where the user sends at least one message after the virtual agent’s greeting (where the greeting may include a slate with contextual recommendations, in case of help for the Settings app). We can approximate some of these metrics using feedback signals described in sections 2.3 and 2.4. E.g., for a time period𝑡 = 1, ...,𝑇 we can dene the following approximations: Where is an indicator function. The dierence between the product team’s KPIs and the approximated values is that the real metrics are aggregated across a user session rather than across an individual event within a user session. A single session lasts from the moment the user connects with the virtual agent until the last event that occurs within the conversation. Specic aggregation depends on the metric - e.g., the user may respond negatively to one survey, but then continue conversation with the virtual agent, and eventually respond to the second survey positively. The product team’s KPI would indicate this as a single event with success contributing towards PRR, while our simplied denition would consider this as two events - one failure and one success. Our CB algorithms, as currently dened, are capable of optimizing only the approximated metrics like^𝑃𝑅𝑅or^𝐸𝐴𝑆. While aware of this problem, we observe that a CB policy optimizing a simplied metric will usually cause movement of the real KPI in the same direction, when deployed to production, as the following sections show. Before attempting online experimentation with the NLB policy for intent disambiguation, we rst used oine evaluation to check whether a newly trained policy seemed promising. This helped limit the negative impact to actual users in production trac. Since we made use of EwS sampling, our behavior policy was stochastic and we logged the probabilities. This allows us to run o-policy evaluations to estimate the impact of our policy. We use the SNIPS [18] estimator to measure how much more reward our current policy would have provided compared to the logging policy. In our counterfactual evaluation, we were only able to use data where the logged policy and the new policy agreed on the clicked action, rather than the entire slate. The primary metrics we observed were how the logged policy and new policy compared in terms of rewards based on the survey and deection feedback signals. A policy had to achieve higher reward estimates than the logging policy with low variance to be promoted to an online experiment. The requirement of low variance was enacted to reduce mismatch between oine evaluation and online experiments. In the case of contextual recommendations, we do not perform oine evaluation beyond qualitative analysis as described in 3.1.1 and careful monitoring of the auto-updated results. The updated model is deployed automatically without any experiments or human in the loop, although with an automated monitoring system alerting about unusual behavior. We conducted several online A/B experiments to verify the ecacy of both our new contextual recommendation and intent disambiguation policies in comparison to a control policy (i.e., the current deployed production policy) on user trac from the Microsoft virtual agent. Each A/B experiment should be viewed independently, as the control policy and the amount of user trac varies from experiment to experiment. The A/B experiments gave key insights into how well the polices improved KPIs relative to control. Some of these KPIs were only approximated by oine evaluation and others were not captured in oine evaluation, like UE or SHS (see section 5.1). Furthermore, A/B experiments reected the actual environment in which the policy was deployed, which diered in some cases from the training data. For instance, the NLB policies were trained only with data that included the survey signals, "yes" or "no", leaving out data where the user did not answer the survey. Finally, these A/B experiments allowed us to quantify impact, which was especially key for contextual recommendations which did not have formal oine evaluation. The results from these A/B experiments were the deciding factor in whether to promote these new policies to the default experience, thus serving general production trac in the future. When looking at results from online A/B experiments, there were several KPIs that we observed as described in section 5.1. Optimizing for several KPIs was a balancing act. Some of these metrics like PRR were almost directly optimized for as reward signals during training, others like EAS were used as limitations where we sought to prevent harmful movement, and others such as SHS and UE were assumed to be correlated to reward signals. We rst ran an A/B experiment for an automatically updating implementation of our MAB approach based on Thompson sampling that used the naive combined score𝑃 (𝑐𝑙𝑖𝑐𝑘) · 𝑃 (𝑠𝑢𝑟𝑣𝑒𝑦 = 𝑦𝑒𝑠|𝑐𝑙𝑖𝑐𝑘) described in section 3.1.1. The control was a human-authored xed mapping from source pages to slate. The experiment showed gains in SHS and UE, which led us to deploy this new policy to all production trac. Afterwards, we set up a small experiment to monitor the autoupdating policy’s performance over time. The MAB policy was set as the champion policy and the old control behavior as the challenger. The challenger only ran with a small amount of trac as to provide minimal impact on performance. Running this monitoring experiment was a crucial decision, as over time the champion policy began to show negative movement in PRR as well as smaller gains in SHS and UE. As PRR is our primary KPI, gains in SHS and UE was not worth a drop in PRR. Motivated in part by the negative results of the monitoring experiment, we worked on implementing a more dynamic interpolation scoring approach described by equation 1. We ran an A/B experiment for a second automatically updating MAB based on this approach. This experiment also showed positive movement in SHS and UE for the treatment compared to the control without any drop in PRR, as shown in table 1. This second policy was then deployed to production, replacing the previously deployed MAB policy. Afterwards, we set up another small experiment to monitor the new auto-updating policy’s performance over time. Over several weeks, the policy began to show improvements in PRR and EAS as well as greater gains in SHS and UE as seen in table 2. It is important to note that a reduction in EAS is a positive improvement, as it is reducing the number of escalations to a human agent and practically, reducing cost. Table 2: Dynamic Thompson Sampling Policy Monitoring A/B Results In the rst A/B experiment we ran with our NLB policy, our initial NLB policy showed improvement over the then-current policy, a deep network trained via policy gradient, where both policies had the same input. The greatest improvement was in PRR, which showed a large increase over our previous policy, as shown in table 3. There was also a small improvement in SHS. We also ran an A/B experiment that compared our NLB policy with a greedy rules-based policy. This rules-based policy was the original solution for intent disambiguation in the Microsoft virtual agent. The experiment results showed that while the NLB policy improved PRR, the greedy rules-based policy had a better EAS score. This is likely because the NLB policy can determine if there is no candidate that matches the user’s intent, and thus shows no result more frequently, leading to escalations. After promoting the new NLB policy to be the default policy for production trac, we experimented with reducing the intent disambiguation slate size from four to three. The objective of reducing slate size was to decrease the likelihood of a user getting distracted by suboptimal candidates in the slate. This change was tested in an A/B experiment against a control of the NLB policy with a slate size of four. The experiment results showed very little movement amongst all KPIs. This gave us the condence to reduce our slate size to three for all production trac, also helping reduce the action space of our intent disambiguation policy. To study the eect of optimizing dierent feedback signals, we ran an experiment with a model optimized for EAS in addition to PRR with the hope of lowering EAS. This A/B experiment shows promising results with improvements in both EAS and PRR as seen in table 5. Interestingly, there was also a drop in SHS. We plan to pursue this method of training in the future. Table 4: Neural-Linear Bandit Policy Optimized for Escalation A/B Results To achieve the objective of adapting to changing user trac, we began experimenting with auto-updating policies. For this experiment, the control was the NLB policy with a slate size of three. The treatment auto-updating NLB policy was initialized with the same parameters as the control aside from a higher exploration rate. The higher exploration rate was chosen to counteract the overtting eects of training with less data. Then, we retrained NLB’s banditlayer hourly on new data from the treatment trac. There were promising improvements in PRR over the two-week experiment. With this experiment, we successfully closed the loop for the intent disambiguation scenario using the auto-updating NLB policy. In recent years, there have been many advances in using RL for recommendation systems. Policy gradient-based methods are one such approach to model recommendation systems, such as REINFORCE [6]. This method is more stable in regards to policy convergence compared to Q-learning techniques. Recent work on slate recommendation like SlateQ [9] allows the decomposition of a slate’s total value as a function of the items. In the SlateQ approach, a Q-Network is trained with a user choice model that approximates the click-through rate. One caveat is that SlateQ makes signicant assumptions about user behavior through the use of a user choice model. CBs are a lightweight single-step RL method that works well with interactive systems. It is the augmented form of a K-armed bandit problem with context𝑥as formalized in [11]. In addition, CBs oer key properties that simplify the problem space. For instance, the reward is only associated with the selected action. CBs enable exploration via sampling algorithms like Thompson sampling, epsilon-greedy, or EwS [11,13,20]. Furthermore, CBs have been successfully utilized in recommendation systems. In [12], MABs modeled personalized news article recommendation with LinUCB sampling. CBs have also been used in search engines for ranking recommendations [19]. Inspired by the application of CBs in traditional recommendation systems, we extend these ideas to be applied in intent disambiguation and contextual recommendations for a virtual support agent. For the implementation of our CB solutions, we extended Decision Service [2], a simple interface for any developer to use RL. To learn more details about the implementation of our RL system and practical lessons, refer to [10]. In this work, we describe two applications of CBs in the Microsoft virtual agent in the areas of intent disambiguation and contextual recommendations. We have demonstrated the ecacy of these solutions through online A/B experiments in the Microsoft virtual agent. The NLB solution for intent disambiguation provided 12.45% improvement in the primary KPI of the Microsoft virtual agent, PRR. Our MAB models for contextual recommendations also increased PRR by 2.36% in addition to increasing SHS and UE, while lowering EAS (escalations to a human agent). We have since deployed these solutions to production and impact millions of users each day. For future work, we are exploring how we can apply the insights from MABs and NLBs to multi-domain support bots as well as other products. We are also exploring real-world applications of episodic RL where it is important to do proper credit assignment. For this we plan to rely on a reduction approach [7] which can operate well with the rest of our existing system. We thank Paul Mineiro and John Langford for providing research advice for the CB scenario, Eslam Kamal and the Microsoft Power Virtual Agents Team for their support with the Intent Disambiguation use case, Mary Buck, Sean Quigley, and the Microsoft Digital Customer Support team for their help with both the Contextual Recommendation and Intent Disambiguation scenarios.