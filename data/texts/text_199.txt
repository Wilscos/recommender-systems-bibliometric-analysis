Conversations aimed at determining good recommendations are iterative in nature. People often express their preferences in terms of a critique of the current recommendation (e.g., “It doesn’t look good for a date”), requiring some degree of common sense for a preference to be inferred. In this work, we present a method for transforming a user critique into a positive preference (e.g., “I prefer more romantic”) in order to retrieve reviews pertaining to potentially better recommendations (e.g., “Perfect for a romantic dinner”). We leverage a large neural language model (LM) in a fewshot setting to perform critique-to-preference transformation, and we test two methods for retrieving recommendations: one that matches embeddings, and another that ﬁne-tunes an LM for the task. We instantiate this approach in the restaurant domain and evaluate it using a new dataset of restaurant critiques. In an ablation study, we show that utilizing critiqueto-preference transformation improves recommendations, and that there are at least three general cases that explain this improved performance. Conversational recommendation systems (CRSs) are dialog-based systems that aim to reﬁne a set of options over multiple turns of a conversation, envisioning more natural interactions and better user modeling than in non-conversational approaches. However, the resulting dialogs still do not necessarily reﬂect how real conversations unfold. Most CRSs fall into two categories: they either frame the problem as a slot-ﬁlling task within a predeﬁned feature space, such as Sun and Zhang (2018); Zhang et al. (2018); Budzianowski et al. (2018), which is closer to how people make decisions but not as ﬂexible as real conversations; or they elicit preferences by asking users to rate speciﬁc items, such as Christakopoulou et al. (2016), which is independent of a feature space but not as natural to users. Figure 1: An example of our system transforming a critique into a positive preference and then using a customer testimonial to sell the user on a new option. When we examine situations involving real human agents (Lyu et al., 2021), decisions typically require multiple rounds of recommendations by the agent and critiques by the user, with the agent continuously improving the recommendations based upon user preferences that can be inferred from such critiques. These inferences can be compared to the types of common sense inferences that have been studied recently with LMs (Davison et al., 2019; Majumder et al., 2020; Jiang et al., 2021). However, use of LMs for critique interpretation remains underexplored, despite the important role of critiques in communicating preferences—a very natural realworld task. Working in the restaurant domain, we prompt GPT3 (Brown et al., 2020) to transform a free-form critique (e.g., “It doesn’t look good for a date”) into a positive preference (e.g., “I prefer more romantic”) that better captures the user’s needs. Compared with most previous work on common sense inference, which relies on manuallyconstructed question sets, our task presents an opportunity to study common sense inference within a naturally arising, real-world application. We test the effect of our novel critique interpretation method on the quality of recommendations using two different methods: one that matches the embedding of an input statement (e.g., “I prefer more romantic”) to persuasive arguments found in customer reviews (e.g., “Perfect for a romantic dinner”); and another one that ﬁne-tunes BERT (Devlin et al., 2018) in using an input statement to rank a given set of arguments. Our work differs from previous critiquing-based systems that strongly limit the types of critiques that can be used (Chen and Pu, 2012) and aligns with a recent trend in the CRS literature towards more open-ended interactions (Radlinski et al., 2019; Byrne et al., 2019). To the best of our knowledge, Penha and Hauff (2020) are the closest prior work investigating whether BERT can be used for recommendations by trying to infer related items and genres. Here, we focus speciﬁcally on critiqueto-preference inferences, aiming at more natural dialogs and better recommendations. Our contributions are the following: 1. We propose a critique interpretation method that does not limit the feature space a priori; 2. We demonstrate that transforming critiques into preferences improves recommendations over two fold when matching embeddings and by 19-59% when ﬁnetuning an LM to rank recommendations, and present three possible explanations for this; and 3. We release a new dataset of user critiques in the restaurant domain, contributing a new applied task where common sense has great practical value. In this section, we describe three methods: A critique interpretation method (2.1), an embeddingsbased recommender (2.2.1), and an LM-based recommender (2.2.2). Critique interpretation is the task of transforming a free-form critique into a positive preference. Our critique interpretation method uses GPT3 in a fewshot setting similarly to Brown et al. (2020), which can be represented in a 3-shot version as follows: To prime GPT3 for our task, we include ten examples in its prompt, ﬁve related to food and ﬁve to the atmosphere.We then append the critique that we would like to transform followed by the string “I prefer”, which conditions GPT3 to generate a positive preference. In our experiments, positive preferences are sampled using OpenAI’s Completion API (the DaVinci model, temperature = 0.7, top p = 1.0, response length = 20, and no penalties). Besides not requiring a hand-crafted feature set, this method is also capable of more ﬂexible interpretation of language, such as transforming “How come they only serve that much?”—with no clearly negative words—into “I prefer larger portions.” 2.2 Content-based Recommendations 2.2.1 Recommendation Search Our embeddings-based recommender,f, takes a preference statement and searches for persuasive arguments in customer reviews. As seen in Figure 1, we can deﬁne a persuasive argument as a review sentence that conveys clearly positive sentiment while being as speciﬁc as possible w.r.t the user’s preferences. To incorporate this deﬁnition inf, ﬁrst we parse sentences in customer reviews using spaCy (Honnibal and Montani, 2017) and use EmoNet (Abdul-Mageed and Ungar, 2017) to keep the sentences with at least a minimum amount of “joy” (≥ 0.7) as our set of argument candidates A. Then we use the Universal Sentence Encoder (Cer et al., 2018) to calculate the similarity of all these argument candidates w.r.t a given user preference. We calculate the cosine similarity between their representations in this embedding space, select the argument with maximum alignment, and recommend the associated restaurant: As with critique interpretation,fcan take any natural language statement as input to search for potential recommendations. We denotefwhen it uses an inferred positive preference as input (“I prefer more romantic”) andfwhen it directly uses a critique (“It doesn’t look good for a date”). In our ﬁrst ablation study, we usefas a baseline to test the efﬁcacy offin retrieving better recommendations. 2.2.2 Recommendation Ranking Besides using pretrained embeddings to search for recommendations from customer reviews, we design a more computationally intensive method, f, that ﬁne-tunes BERT to rank a set of arguments A considering a given input statement. We use the currently top performing open-source solution (Han et al., 2020; Pasumarthi et al., 2019) on the MSMARCO passage ranking leaderboard to ﬁne-tune three versions of BERT:fuses a positive preference as input (“I prefer more romantic”),fuses a critique (“It doesn’t look good for a date”), andfuses a concatenation of both a critique and a preference (“It doesn’t look good for a date. I prefer more romantic”). Hypothetically, the more powerful LM method could learn to satisfy the user’s preferences without the need of critique interpretation if the performances In our experiments, BERT-Base is ﬁned-tuned for 10,000 steps, with learning rate =10, maximum sequence length = 512, and softmax loss, using a Nvidia Quadro RTX 8000 for 3-6h per run (when ranking 15 and 30 arguments, respectively) and two runs per model (2-fold cross validation). We run two ablation studies to evaluate the hypothesis that critique interpretation would be beneﬁcial to the overall recommendation approach. First, we analyze our embeddings-based recommender,f, to check whether the performance off> f. Secondly, we analyze our LM ﬁne tuning-based recommender,f, to check iff> for f> f. Finally, we discuss qualitative differences between the tested arms. Our methods were instantiated in a system comprising 15 restaurants selected from two of the largest metropolitan areas in the United States, covering a variety of price ranges and cuisines. For each restaurant, up to 100 four- or ﬁve-star customer reviews were collected from Google Places. This resulted in a total of 1455 reviews comprising 5744 sentences, 2865 of which pass the threshold for being identiﬁed as positive review sentences. We compiled a set of user critiques from two sources: a set of 46 unique critiques from user studies that were conducted to test an earlier system prototype (Bursztyn et al., 2021), and 294 additional critiques adapted from the Circa dataset (Louis et al., 2020). Circa was designed to study indirect answers to yes-no questions, such as “Are you a big meat eater?” answered with ”I prefer leafy greens”, from which the critique “I’m not a big meat eater” can be generated. We end with a total of 340 individual critiques after examining 1205 similar examples. We generated a positive preference for each individual critique using our critique interpretation method in 2.1, without discarding any critiques. Our method yielded accurate preferences for 298 critiques (87.6%). For the remaining 42, we found GPT3 mostly undecided and vague (e.g., “Jalapeños are my limit” generates “I prefer food without jalapeños”). In our experiments, for these edge cases, we kept the best of three trials, but we believe that results using just the ﬁrst generation would have been qualitatively similar. The 340 critiques were randomly combined into pairs and triples in order to simulate longer conversations, i.e., two- and three-round critiques. We sampled 340 pairs and 340 triples, substituting only exceptional combinations that contained contradictory statements (e.g., “I’m not a big meat eater.” paired with “I’m not in the mood for vegetables.”), for a total of 1020 critiques. Compound critiques were concatenated into single statements as well as their corresponding preferences. This curated dataset of 1020 restaurant critiques and inferred preferences is made available to the research community. For evaluating our embedding-based methodsf, we use critiques as input tofand their positive preferences as input tof. For each query we retrieve the top 3 arguments, which are labeled as accurate or inaccurate by a human judge (illustrated in Table 1). To measure labeling consistency, a second human annotator redundantly labeled a sample of 100 arguments resulting in a Cohen’s Kappa of 0.71, which indicates strong agreement. We then measure Precision@1, Precision@2, and Precision@3 in Table 2 for the embeddingsbased method with (f) and without critique interpretation (f). Table 2: Precision@1, 2, and 3 for fand f. Table 3: nDCG1, 3, 5, and 10 for fon each task. To train and evaluate the BERT-based method f, we retrieve the top 15 arguments fromf and the top 15 arguments fromffor 100 queries. Each argument receives a score from 3 (very relevant) to 1 (irrelevant). Again, a second human annotator relabeled 100 arguments for a Cohen’s Kappa of 0.73, also indicating strong agreement. We design three ranking tasks:taskconsists of ranking the 15 arguments originally retrieved with f, hence closer to critiques in the embedding space;taskconsists of ranking the 15 arguments originally retrieved withf, hence closer to preferences; andtaskconsists of ranking both sets, i.e., 30 arguments. For each task we trainf, f, andf. We then measure nDCG@1, nDCG@3, nDCG@5, and nDCG@10 in Table 3 averaged after 2-fold cross validation. We found that using the positive preferences yields substantial improvements in information retrieval. Forf, in Table 2,fincreases Precision@1 by124%, Precision@2 by118%, and Precision@3 by110%. This gap is also present, with marginal variations, when separately analyzing single-, two-, and three-round critiques. Forf, in Table 3,foutperformsfby19%on nDCG@1 even attask, wherefcould have an edge. This gap persists fortask(foutperforms by19%), increases fortask(foutperforms by59%), and tends to narrow towards nDCG@10. Overall, we found strong evidence in support of our hypothesis. Table 1 shows three examples in which the use of positive preferences was clearly beneﬁcial. These examples represent three critique patterns that cause systematic errors if critique interpretation is turned off: 1. When the user implies a preference for a feature using the polar opposite (e.g., “It looks too casual” implying “I prefer a fancier place”); 2. When the user draws on common sense to express a preference (“It has a freaking band!” implying “I prefer a more quiet place”); and 3. When the user implies a ﬁlter within a set of related features (e.g., “I don’t really like seafood” implying preference for alternatives in the meat category). Analyzing the results offandffor the 340 single-round critiques, we found 170 cases wherefoutperformedf. Within these, 40 belong to the ﬁrst pattern (24%), 78 to the second (46%), and 38 to the third (22%).A common trait behind the three patterns is that critiques can be lexically very distinct from their corresponding preference statements, and critique interpretation helps to bridge this gap. In this paper, we presented an open-ended approach to content-based recommendations for CRS. We developed a novel critique interpretation method that uses GPT3 to infer positive preferences from freeform critiques. We also developed two methods for retrieving recommendations: one that matches embeddings and another that ﬁne-tunes BERT for the task. We ran two ablation studies to test if transforming critiques into positive preferences would yield better recommendations, conﬁrming that it improves performance across both methods. Finally, we described three critique patterns that cause systematic errors in recommendation search if critique interpretation is turned off. For future work, we will strive to use critiques to identify and remove unsuitable restaurants; we speculate that the sparsity of customer reviews generally makes it harder to “rule out” than to “rule in.” We will also study other issues such as when to ask clariﬁcation questions to resolve ambiguity in the scope of a critique. We would like to thank reviewers for their helpful feedback. This work was supported in part by gift funding from Adobe Research and by NSF grant IIS-2006851.