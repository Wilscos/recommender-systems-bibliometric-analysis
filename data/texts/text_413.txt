{shuyuan.xu,juntao.tan}@rutgers.edu,{shelby.heinecke,jia.li}@salesforce.com,yongfeng.zhang@rutgers.edu Recommender systems may be confounded by various types of confounding factors (also called confounders) that may lead to inaccurate recommendations and sacriced recommendation performance. Current approaches to solving the problem usually design each specic model for each specic confounder. However, realworld systems may include a huge number of confounders and thus designing each specic model for each specic confounder is unrealistic. More importantly, except for those “explicit confounders” that researchers can manually identify and process such as item’s position in the ranking list, there are also many “latent confounders” that are beyond the imagination of researchers. For example, users’ rating on a song may depend on their current mood or the current weather, and users’ preference on ice creams may depend on the air temperature. Such latent confounders may be unobservable in the recorded training data. To solve the problem, we propose a deconfounded causal collaborative ltering model. We rst frame user behaviors with unobserved confounders into a causal graph, and then we design a front-door adjustment model carefully fused with machine learning to deconfound the inuence of unobserved confounders. The proposed model is able to handle both global confounders and personalized confounders. Experiments on real-world datasets show that our method is able to deconfound unobserved confounders to achieve better recommendation performance. Recommender Systems; Deconfounded Recommendation; Causal Analysis Recommender systems occupy an expanding role in daily decision making, such as e-commerce, video streaming and social media. Most of the existing recommendation models learn from the collective historical interactions to achieve good recommendation performance. However, the collected data may be confounded by various types of confounders that aect both treatment assignment (which item is exposed) and outcome (whether the user likes the item). As a result, the collected data may not represent the accurate preference of users. Recommendation algorithms that are trained with confounded data may result in inaccurate recommendations and sacriced performance. The existence of confounders naturally leads to a question: can we develop models to eliminate the inuence of confounders so as to estimate more accurate user-item preferences? Current approaches to answering this question usually identify a specic confounder based on researchers’ expertise and design a specic model to tackle this confounder. For instance, many models are proposed to handle the popularity confounders [13,35,40–43], click Figure 1: Examples of global and personal confounders. confounders [28,30,31], exposure confounders [1,9,27,38], etc. Existing eorts have performed well on combating these manually identied confounders. However, real-world systems may involve a huge number of confounders, which makes it unrealistic to design each specic model for each specic confounder. Moreover, the confounders are not limited to the “explicit confounders” that can be manually identied by experts based on their knowledge, but also include many “latent confounders” that can be hardly identied or measured. For example, a user’s interaction and rating with music recommendations can be aected by his/her current mood or spirit, however, the mood or spirit cannot be quantitatively measured or logged in the training data. The large number of confounders and the existence of latent confounders make it important to develop “universal” deconfounded recommendation models that are agnostic to certain types or certain numbers of confounders. Broadly speaking, the confounders can be classied into global confounders and personal confounders, as shown in Figure 1. Global confounders are those that inuence user preferences in a consistent way, while personal confounders are those that aect dierent users dierently. For example, the air temperature can be a global confounder for users’ preferences on ice creams—during normal times, some users tend to like ice creams while others tend to dislike ice creams, however, an extremely high temperature may increase both groups’ likeness on ice creams. On the other hand, the weather can be a personal confounder for users’ preferences on household PlayStations such as Xbox. For example, some users like sunny days and they choose to take outdoor activities, as a result, their preferences on playstations decrease when sunny while increase when rainy since they have to stay at home. However, some other users may dislike sunny days due to skin protection reasons and thus they choose to stay at home when sunny with an increased preference on playstations, while their preference on playstation decreases when rainy since they choose to go out when the UV exposure is low. This complex nature of confounders makes it challenging to design deconfounded recommendation models. Fortunately, causal inference techniques—especially front-door adjusted models—make it possible to design a unied deconfounding model for both global and personal confounders. In particular, the user, item, and preference can be formulated into a causal graph, while unobserved global or personal confounders present possible connections to the user, item and preference nodes (Figure 1). Technically, we identify a mediator𝑀between item and preference that is independent from the inuence of confounders (Figure 3, more details later). Based on mediator analysis, we are able to estimate the deconfounded user-item preference𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣))based on front-door adjustments rooted in causal theory, where𝑢, 𝑣is a pair of user and item, and𝑦is the preference score between them. In the experiments, we compare our model with both state-ofthe-art deconfounded recommendation algorithms and traditional association-based recommendation models. The results show that our deconfounded algorithm achieves better recommendation performance than all of the baselines. In summary, we list our key contributions as following: •We design a causal graph to represent the eects of unobserved confounders in the recommendation scenario. •To mitigate the eects of unobserved confounders which are not directly measurable, we adapt the front-door adjustment into the proposed deconfounded causal collaborative ltering. •We design a sample-based approach integrated with exposure models to make the front-door adjustment calculable despite the large item space. •Experiments on two real-world datasets show that our deconfounded model outperforms existing deconfounded recommendation models and traditional association-based models. The remainder of this paper is organized as follows. We discuss the related works in Section 2. In Section 3, we introduce some notations, basic concepts and theorems to help readers gain a better understanding of the fundamentals. We introduce our proposed model in Section 4. In Section 5, we experiment on real-world datasets and make discussions. Finally, we conclude the work and discuss future directions in Section 6. Deconfounded recommender systems aim to remove or reduce the unwanted eect of confounders, which is important to the accurate estimation of user preferences. Among the large number of methods mitigating the confounding eects, causal technique plays an important role. Inverse Propensity Score (IPS) based method is an important approach for deconfounding. The basic idea is to re-weight the observations with inverse propensity scores so as to mitigate the inuence of selection bias [28,31], exposure bias [27,38], position bias [3,8,11,29], etc. IPS-based methods are well used approaches for deconfounded learning with observational data, but they still suer from some known issues. For example, the performance of IPS-based methods highly depends on the accurate estimation of propensity scores [26] and usually suers from the high variance of the propensity scores [5]. Besides IPS, leveraging causal graph is a powerful approach to deconfounded recommendation. Many existing methods construct a specic causal graph based on certain assumptions to incorporate specic confounders into the data generation process, and then apply causal inference techniques to mitigate the confounding bias. Just to name a few as examples, Zhang et al. [40] assume that item popularity aects the item exposure and user interaction, and constructed a causal graph incorporated with item popularity to estimate the user-item preferences; Qiu et al. [22] observed visual bias in visual-aware recommendation and that users’ attention to visual features does not always reect the real preference, and thus developed a causal graph to remove the eect of visual confounders. Li et al. [14] noticed that users’ sensitive features may lead to unfair recommendations and thus developed causal graphs to deconfound the inuence of sensitive features in recommendation; Wang et al. [30] identied that user clicks may be inuenced by item popularity and proposed a deconfounding method to alleviate the amplication of popularity bias. Many other research works are conducted to address dierent types of confounders, including but not limited to item popularity [2,7,13,35,41–43], item exposure [1,9,27,33, 34,38], user selection [16,19,28,30,31], and ranking positions [3, 4, 10, 18, 21, 36]. However, due to the large number of confounders and the existence of unobserved latent confounders, it is unrealistic to design each specic model to tackle each specic confounder. Some recent works took advantage of a subset of unbiased data as supervision to remove confounding eects [6,39]. However, unbiased data may not always exist and collecting unbiased data can be dicult since randomized recommendations may hurt the user experience. As a result, a broad-spectrum deconfounded recommendation model to mitigate the confounding eects based on observed data is urgently needed. As we will show later, we incorporate the unobserved confounders into the causal graph and leverage front-door adjustments to mitigate the eect of confounders, which is able to deconfound recommender systems based on observed data without the need to enumerate confounders or collect unbiased datasets. In this section, we introduce the notations used in this paper, as well as some concepts and theorems in causal inference so as to prepare readers with the basics. In this paper, we use uppercase letters to represent random variables. In particular, we use𝑈 ,𝑉 , 𝑀, 𝑌,𝐶to represent user, item, mediator, preference, and confounders. We use lowercase letters to indicate the specic value of a random variable. In particular, we use𝑢, 𝑣, 𝑚,𝑦 to indicate a specic user, item, mediator, and preference value. We use uppercase calligraphy to represent set. For exampleU = {𝑢, 𝑢, · · · , 𝑢}is user set andV = {𝑣, 𝑣, · · · , 𝑣}is item set. Moreover, we use bold font lowercase to represent the latent vector embedding of users, items and mediators, such asu, v ∈ R, m ∈ R, where𝐷and𝐷are the dimensions of the embedding vectors and they are not necessarily equal. Figure 2: 𝑋 represents treatment variable, 𝑌 represents outcome variable, 𝑀 represents mediator variable, 𝐶 represents confounder variable. Figure (a) shows variable 𝐶 as observed and measurable, while in Figure (b), variable𝐶 is unobserved or unmeasurable. In this section, we will introduce back-door adjustment and frontdoor adjustment in causal inference. We will mathematically illustrate why back-door adjustment is not applicable in our setting and why we use front-door adjustment instead. Definition 1. (Back-door Criterion) [20, p.61] Given an ordered pair of variables(𝑋, 𝑌 )in a causal graphG, a set of variables𝑍 satises the back-door criterion with respect to(𝑋, 𝑌 )if𝑍satises the following conditions: – No node in 𝑍 is a descendant of 𝑋 ; – 𝑍 blocks every path between 𝑋 and 𝑌 that contains an arrow into 𝑋 . With the help of a set of variables that satisfy the back-door criterion, we can adjust for the eect of measured confounders. We take the causal graph in Figure 2(a) as an example. Considering the treatment variable𝑋and the outcome variable𝑌, we want to estimate the eect of𝑋on𝑌, which is denoted as𝑃 (𝑌 = 𝑦|𝑑𝑜 (𝑋 = 𝑥)). Due to the existence of confounder𝐶, we cannot conclude that 𝑃 (𝑌 = 𝑦 |𝑑𝑜(𝑋 = 𝑥)) = 𝑃 (𝑌 = 𝑦|𝑋 = 𝑥). However, since variable𝐶 satises the back-door criterion, we use it to adjust the eect, in other words, we are accounting for and measuring all confounders [24]. Therefore, we compute 𝑃 (𝑌 = 𝑦|𝑑𝑜(𝑋 = 𝑥)) as follows: 𝑃 (𝑌 = 𝑦 |𝑑𝑜(𝑋 = 𝑥)) =𝑃 (𝑌 = 𝑦 |𝑋 = 𝑥, 𝐶 = 𝑐)𝑃 (𝐶 = 𝑐) (1) The above equation is based on the assumption that the confounder variable, which also satises the backdoor criterion, is measurable. However, in recommender systems, there may exist various unobserved or unmeasurable confounding variables. To address this setting, we introduce the front-door criterion. Definition 2. (Front-door Criterion) [20, p.69] Given an ordered pair of variables(𝑋, 𝑌 )in a causal graphG, a set of variables𝑍 satises the front-door criterion with respect to(𝑋, 𝑌 )if𝑍satises the following conditions: – 𝑍 intercepts all directed paths from 𝑋 to 𝑌 . – There is no unblocked path from 𝑋 to 𝑍. – 𝑋 blocks all back-door paths from 𝑍 to 𝑌 . Given a set of variables that satises the front-door criterion, we can identify the causal eect with unobserved confounders. Definition 3. (Front-door Adjustment) [20, p.69] If a set of variables𝑍satisfy the front-door criterion related to an ordered pair of variables(𝑋, 𝑌 ), and if𝑃 (𝑥, 𝑧) >0, then the causal eect of𝑋on𝑌 is identiable and is given by We take Figure 2(b) as an example. Although variable𝐶satises the back-door criterion, it is not measurable, so the back-door adjustment (Eq.(1)) cannot be applied in this example. Although the back-door adjustment is not applicable here, given that variable𝑀satises the front-door criterion, we can use the front-door adjustment to handle unmeasurable or unobserved confounders. Intuitively, the desired eect can be expressed as follows Since the only parent node of variable𝑀is𝑋, and variable𝑀 satises the back-door criterion with respect to(𝑀, 𝑌 )so that we can apply back-door adjustment for𝑃 (𝑦|𝑑𝑜(𝑚)), therefore, Eq.(3) can be further derived as We can see Eq.(4)is exactly the front-door adjustment as Eq.(2). In this section, we rst introduce our designed causal graph depicting user behaviors with unobserved confounders. We then elaborate on our deconfounded causal collaborative ltering model based on the front-door adjustment. We rst formulate the process of user-system interaction as a causal graph. First, the user will determine which recommended item(s) to interact with. Then, according to the corresponding item features (not aected by other variables), user will reveal his or her preference feedback on the item. The whole process can be aected by confounders. Combining global confounders and personal confounders as introduced in Figure 1, we design the causal graph as shown in Figure 3. In this causal graph, we are actually taking item features as mediators between item node𝑉and preference node𝑌, i.e.,𝑀is not directly inuenced by user node𝑈and confounders𝐶. This is intuitive and reasonable because item features such as the color or brand of products and the genre or director of movies are inherent and xed properties of an item, which are not inuenced by users or external confounders. It is worth clarifying that the causal graph is used for describing how data is generated. We ultimately build a recommendation model that can leverage this causal graph to produce better estimations of user preferences. In this section, we estimate user preference from a causal view. In particular, we estimate a user’s preference if a certain item is recommended to the user, which can be represented as𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣)). The estimation of the preference score𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣))is based on the designed causal graph in Figure 3. Although there exist a set of variables{𝐶,𝑈 }that satisfy the back-door criterion in Figure Figure 3: The meaning of variables are: 𝑈 represents user, 𝑉 represents item, 𝑀 represents mediator (e.g. item feature), 𝑌 represents user’s preference, and 𝐶 represents unobserved confounders. Dashed nodes represent unobserved variables and dashed arrows represent causal relations pointing from unobserved variables. 3, the back-door adjustment (as Eq.(1)) is not applicable because variable𝐶is unobserved or unmeasurable. Therefore, we apply front-door adjustment to estimate user’s preference towards items with unobserved confounders. We estimate preference score of a (𝑢, 𝑣) pair as follow: 𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣)) =𝑃 (𝑚|𝑣)𝑃 (𝑣|𝑢)𝑃 (𝑦|𝑢, 𝑣 where𝑦represents preference score and𝑚represents item features. For simplicity, we treat item features as values retrieved from the dataset. We dene 𝑃 (𝑚|𝑣) as follows where𝐹:V → Mis a database retrieval function which returns item feature from the dataset. It is worth mentioning that the item features can be extracted from many sources, such as image or text information. Combining Eq.(5)and Eq.(6), we can rewrite the estimation of 𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣)) as follow: where𝑚= 𝐹 (𝑣)is the feature representation of item𝑣.𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣)) is calculated by a weighted average over all items as denoted in Eq.(7). However, real-world systems may include a large scale of items, therefore, it is unrealistic to involve all items to estimate the preference of a user-item pair. To tackle this issue, we modify the original front-door adjustment to a sample-based one that is more suitable and realistic for the recommendation scenario. Specically, we randomly sample a set of items while calculating𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣)). The item set for a user-item pair(𝑢, 𝑣)is represented as𝑅(𝑢, 𝑣) (including sampled items and item𝑣), and the number of sampled items is𝑛, which is a hyper-parameter to be selected. Therefore, we can rewrite Eq.(7) into a sample-based format. For each item𝑣∈ 𝑅(𝑢, 𝑣)in Eq.(8), its exposure probability (i.e. 𝑃 (𝑣|𝑢)) and conditional preference (i.e.𝑃 (𝑦|𝑢, 𝑣,𝑚)) are two required values. We will introduce how to calculate them respectively. 4.2.1Calculate the Exposure Probability. The exposure probability𝑃 (𝑣 |𝑢)represents the probability of an item𝑣being exposed to a certain user𝑢, which plays an important role in our model. The implicit feedback in the dataset contains binary interactions, which indicates which user interacted with which item. If we take the interacted items as exposed (positive) samples and non-interacted items as unexposed (negative) samples (we will relax this assumption in the following), then we can train an exposure model based on pair-wise learning to rank as follows where𝜎 (·)is the sigmoid function:𝜎 (𝑥) =,𝑣and𝑣are positive and negative items for user𝑢respectively,𝑃is the predicted exposure probability for user-item pair(𝑢, 𝑣), which represents 𝑃 (𝑣 |𝑢)in Eq.(8). More specically, we can apply Matrix Factorization (MF) [12] to calculate 𝑃: wherepandqare embedding representations for user𝑢and item 𝑣,𝑏is the user preference oset term,𝑏is item preference oset term, and 𝑏is global oset term. Although we can obtain exposure probabilities based on Eq.(10), it ignores the bias in the observational data, that the non-interacted items may not always represent negative samples. To address this issue and get more accurate and unbiased exposure probabilities, some works are available, such as doubly robust model [32] and predictive model [15]. For simplicity, we apply the simple IPS-based debiasing technique [27]. Concretely, we use propensity scores to correct the predicted probability. where𝑝is the propensity score for user-item pair(𝑢, 𝑣). We still apply the pair-wise ranking framework as Eq.(9)to train the exposure probability Eq.(11). Theoretically, Eq.(11)will return more accurate and unbiased exposure probabilities [38]. We will discuss the empirical inuence of dierent exposure models in the experiments section. 4.2.2Calculate the Conditional Preference. To estimate the desired value of𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣)), the conditional preference𝑃 (𝑦|𝑢, 𝑣,𝑚) is an essential component. Recall that𝑢represents a certain user, 𝑣represents a specic item, and𝑚represents the item feature of the treatment item𝑣. We design a function𝑓:𝑈 ,𝑉 , 𝑀 → 𝑌which takes users, items and item features as input and returns preference scores such that𝑃 (𝑦|𝑢, 𝑣,𝑚)is proportional to the corresponding conditional preference score. For a triple(𝑢, 𝑣,𝑚), we rst use a Multilayer Perceptron (MLP) to encode the item and item feature into a vector, and then use the dot product between the obtained vector and the user representation to get the preference score. 𝑓 (𝑢, 𝑣,𝑚) = u · 𝜙 (W𝜙 (W𝜙 (. . . 𝜙 (Wvm) . . . )))(13) whereu, v∈ Rare the user and item latent embedding vectors in𝐷-dimensional space (and it is not same aspandqin Eq.(11)); (a) Retrieve feature representation(c) Conditional preference for each item Figure 4: The process of estimating preferenceˆ𝑦. mis the feature representation for item 𝑣; 𝑣 𝑓is the conditional preference 𝑓 (𝑢, 𝑣,𝑚) of user 𝑢 on item 𝑣(Eq.(13)); 𝑃 the 𝑃value according to Eq.(11));ˆ𝑦is the nal preference score between user 𝑢 and item 𝑣 (Eq.(14)). Algorithm 1Deconfounded Causal Collaborative Filtering (DCCF) Input: Training Dataset, including observed interactions as user-item pair (𝑢, 𝑣) and the item feature representation m for all items in 𝐷-dimensional space; L2-norm regularization weight: 𝜆; number of sampled items for each user-item pair: 𝑛 Output: user representations U, item representations V, MLP network W m∈ Ris the item feature representation in𝐷-dimensional space (𝐷and𝐷are not necessarily equal), which is retrieved from data;ℓis the number of layers in MLP;W, 𝑖 =1, . . . , ℓare weight matrices to be learned; and𝜙 (·)is the rectied linear unit (ReLU) activation function: 𝜙 (𝑥) = max(0, 𝑥). 4.2.3Calculate the Expectation. According to Eq.(8), the estimation𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣))is the conditional expectation of𝑃 (𝑦|𝑢, 𝑣,𝑚). Combined with Eq.(12), we have our estimation as follows: 𝑃 (𝑦|𝑢, 𝑑𝑜(𝑣)) ∝ˆ𝑦𝑃 (𝑣|𝑢)𝑓 (𝑢, 𝑣,𝑚 where𝑃 (𝑣|𝑢)is obtained by Eq.(11)and𝑓 (𝑢, 𝑣,𝑚)is calculated by Eq.(13). The calculation ofˆ𝑦is shown in Figure 4. is the exposure probability of item 𝑣on user 𝑢 (i.e., In this section, we will introduce details about model learning, i.e., how to learn the estimation as Eq.(14). In this work, we use the pairwise learning to ranking algorithm [23] for training. Suppose we observe user𝑢interacted with item𝑣in the dataset, The estimated preferenceˆ𝑦is obtained from Eq.(14). We sample another item 𝑣∈ Vthat user𝑢did not interact with as a negative sample. The estimated preference for negative sample𝑣is represented asˆ𝑦. The dierence between two estimated preferences is noted asˆ𝑦. Given the observed interactions, we randomly sample a negative item for each user-item pair in implementation. Specically, we use Vto represent the observed items set for user𝑢(𝑣∈ V) and Vto represent the sampled negative item set for user𝑢(𝑣∈ V). The recommendation loss function can be written as: whereΘrepresents all parameters of the model;𝜆is the L2-norm regularization weight;𝜎 (·)is the sigmoid function:𝜎 (𝑥) =. It is worth mentioning that the representation of item featuremand exposure probability𝑃 (𝑣|𝑢)are xed during training. The overall algorithm is provided in Algorithm 1. In this section, we conduct experiments to show how our deconfounded recommender works on real datasets. We examine its recommendation performance and compare it to the traditional association-based recommendation models and existing deconfounding methods. We will rst describe datasets, baselines and implementation details and then provide our results and analysis. Our experiments are conducted on the Amazon Review Datasets [17] which include user, item, rating and product metadata spanning from May 1996 to October 2018. More specically, we experiment with two datasets, (1) Electronics and (2) CDs and Vinyl. For each dataset, we sample 70% of user purchase as training data, 10% as validation data and the remaining 20% as testing data. The statistics about the datasets are shown in Table 1. For each dataset, we apply two splitting strategies. The rst one is the traditional random splitting strategy (denoted as RAND), which considers each data sample with equal probability to be chosen into the testing set. Another is skewed splitting strategy (denoted as SKEW) following [5], which exposes as uniformly as possible each user to each item in testing set. We rst calculate the popularity of each item and nd the least popular item. For all other items, We use its popularity ratio compared with the least popular item to calculate the probability of being divided into the test set. For example, a 100 times more popular item than the least popular item will have a probability of being sampled in the test set as 1%. To avoid from the least popular item being not available in training, similar to [5], we cap the maximum probability for a user-item interaction record to be put into the test set as 0.9. The SKEW dataset is used to test if our model can perform well on an unbiased testing set. All data and source code will be released when paper is published. The baselines include both classical association-based recommendation models and state-of-the-art deconfounded recommendation models, as well as a variant of our own model. We introduce some details about the baselines used in our experiments. • BPR-MF[23]: The Bayesian Personalized Ranking model for recommendation. Specically, We use Matrix Factorization (MF) as the prediction function under the BPR framework, which considers user, item and global oset terms for MF. • DMF[37]: Deep Matrix Factorization is a deep learning model for recommendation, which lters the user-item interaction matrix through a neural network architecture for prediction. • IPW-MF[28]: Inverse Propensity Weighting Matrix Factorization for recommendation, which adapted causal inference (inverse propensity weighting, specically) on the matrix factorization model to handle the selection bias in observational data. • DCF[34]: The deconfounded recommendation model, which uses an exposure model to construct a substitute confounder and then conditions on the substitute confounder for modeling. • DCCF_ND: This is a no-deconfounding (ND) variant of our model. Specically, for a user-item pair(𝑢, 𝑣), this variant has no sampled item set𝑅(𝑢, 𝑣)and only uses𝑓 (𝑢, 𝑣,𝑚)in Eq.(13)as the estimated preference. It can be considered as our model without the front-door adjustment, which loses the deconfounding ability. Finally, we useDCCFto denote the complete version of our model. We use the same train, validation and test set for all models, including our models and baseline models. We evaluate models on top-𝐾 recommendation task. More specically, the models are evaluated based on nDCG@5, Recall@5, and Precision@5 Metrics. For evaluation, we apply real-plus-N [25] to calculate the values of each metric. Concretely, for each user in the validation and testing set, we randomly sample 1000 negative items for ranking evaluation. All recommendation models adopt the Bayesian Personalized Ranking (BPR) [23] framework for pair-wise learning. During the training, we sample one negative item, that the user did not interact with, for each interacted user-item pair. We optimize the models using mini-batch Adam with a batch size of 128. To get the feature representation𝑀of the items, we apply a pre-trained sentence embedding modelto represent the textual information of an item into a dense vector embedding with dimension 768. The textual information comes from the title, feature, and literal description of each item, which are included in the meta-data of the amazon review dataset. We use the above pre-trained sentence transformer to convert them into separate embeddings and further take the average as the feature representation of the item. As for the parameters, we consider the embedding dimension from {64,128}, the structure of neural network is two-layer MLP with dimension 64 for deep models. For all recommendation models, we consider the learning rate from {0.0005,0.001,0.003,0.005}, the ℓ-regularization weight is chosen from {1e-3, 1e-4, 1e-5}. For fair comparison, we tune the parameters to the best performance for each model on the validation data based on nDCG@5. For our model, we sample 20 items for each user-item pair for calculating the expectation (i.e., the number of sampled items𝑛in Section 4.2). To get the exposure probability calculated based on Eq.(11), we estimate the user independent propensity scores as [27]: Here𝑦is an indicator:𝑦=1 if the user-item pair(𝑢, 𝑣)is observed in the dataset,𝑦=0 otherwise. We set𝜂 =0.5 in the experiment. The running time of our model is about 40 seconds/epoch for Electronics and 15 seconds/epoch for CDs and Vinyl. The recommendation performance on baselines and our model are shown in Table 2. We evaluate both Electronics and CDs and Vinyl datasets with RAND and SKEW splitting, respectively. For RAND and SKEW splitting, we strictly follow the 7:1:2 ratio to split train, validation and test data for each user, which makes the testing data the same size for each user on both splitting strategies. The SKEW strategy simulates a test set with unbiased user feedback, and the test distribution is dierent from the training distribution. Comparing the RAND and SKEW strategies, we can see that the recommendation performance with RAND strategy is consistently better than the performance with SKEW strategy on both of Electronics and CDs and Vinyl datasets. This shows that learning recommendation models when the training and testing Table 2: Recommendation performance for Electronics and CDs and Vinyl on RAND and SKEW splitting datasets. We evaluate on ranking task with metrics nDCG@5, Recall@5 and Precision@5. The best results are highlighted in bold. distributions are dierent (SKEW) is a more challenging task than under the same train-test distribution (RAND). The following observations are consistent no matter what the splitting strategy is. First, the recommendation models shown in Table 2 can be categorized into classic association-based models and deconfounded models, where the dierence between the two is whether the eects of confounders are mitigated or not. From the results, we can see that the deconfounded recommendation models (IPW-MF, DCF and DCCF) achieve better recommendation performance than classic recommendation models (BPR-MF and DMF) on both datasets. When averaging across all deconfounded recommendation models on all datasets and splitting strategies, the deconfounded models get 36.3% relative improvement on nDCG@5 than classic recommendation models, 38.0% on Recall@5, and 33.6% on Precision@5. According to this comparison, we can know that the existence of confounders will lead to inaccuate recommendations and deconfounded recommendation models will improve the recommendation performance by eliminating the eect of confounders. Among three deconfounded recommendation models, we can see that our DCCF model achieves the highest recommendation performance. Compared with the strongest deconfounded model in baseline, when averaging across all datasets and splitting strategies, our model gets 10.1% improvement on nDCG@5, 11.8% improvement on Recall@5, and 8.4% improvement on Precision@5. These observations imply that by applying the front-door adjustment into the estimation of user’s preference, our model is capable of reducing the eect of unobserved confounders and further improving the recommendation performance. Moreover, based on the results, applying the front-door adjustment is more eective than applying inverse propensity weighting or substituting the unobserved confounders. Although the front-door adjustment is eective on reducing the eect of unobserved confounders, it requires a mediator in the causal graph, i.e., the variable𝑀in Figure 3, which represents item feature. We want to conrm that the improved performance is indeed brought by the deconfounding eect instead of the use of item features. As a result, we consider a variant of our model DCCF_ND, which also involves item feature representation into estimation but without the front-door adjustment as deconfounding component. More specically, unlike DCCF that applies the frontdoor adjustment with weighted sum over sampled items to get the estimation of a user-item pair, DCCF_ND only considers the user-item pair itself when calculating the estimation. From Table 2, we can see that DCCF_ND is even worse than classic models in most cases. Therefore, the improvement of our model is brought by deconfounding with the front-door adjustment instead of involving item feature representation into calculation. In this section, we will discuss the inuence of the exposure models. As we mentioned before, the exposure probability, which is required by the front-door adjustment but not available in the feedback data, is an essential component of our model. To empirically show the inuence of it, we design three versions of our model with dierent exposure probabilities. The only dierence among them is how to get the exposure probability. • DCCF_Random: In this model, the exposure probabilities are not learned from the data, instead, we randomly generate a matrix to represent the exposure probability. • DCCF_Bias: In this model, we estimate the exposure probability as Eq.(10), and use the pair-wise learning method to train the model based on implicit feedback. • DCCF_Unbias: This model is the same asDCCFin Table 2. Concretely, we consider the bias in the implicit feedback data, and apply Eq.(11)to get unbiased estimation of exposure probabilities. We report the recommendation performance of above three version in Table 3. Following the main experiment, we evaluate two datasets on two splitting strategies. The performance is also based on ranking metrics. For each of the version, we use the same item feature representation and sample 10 items for each pair to calculate the estimated preference. From the results, we can see that DCCF_Unbias gets the best performance and DCCF_Random gets the worse performance. This observation is consistent on both RAND and SKEW splitting strategies. Comparing the three versions with dierent exposure models shows that accurate exposure probability will lead to accurate estimation under the front-door adjustment. The exposure probability is not observed in the feedback data, therefore, we need to learn a model to estimate it from observational feedback data. DCCF_Unbias uses the unbiased estimation and gets the most accurate exposure probability, while DCCF_Random randomly generate exposure probabilities, which is irrelevant with the feedback data, and thus will get the least accurate probability. We can see that the ranking of the exposure probability accuracy matches the ranking of recommendation performance. Table 3: The recommendation performance of DCCF under dierent exposure models. We report the evaluatation on ranking task with metrics nDCG@5, Recall@5 and Precision@5. The best results are highlighted in bold. The exposure model aects our model in many ways, not only the best recommendation performance, but also the trend of the recommendation performance as the number of sampled item increases. As we mentioned in Section 4 and Eq.(14), we apply the front-door adjustment over a set of sampled items to make it suitable for the recommendation scenario. Ideally, if there exist the ground truth values of exposure probability and we apply them into the calculation, then increasing the number of sampled items may reduce the eciency but will at least not hurt the performance signicantly. In order to investigate how our model changes with increasing the number of sampled items under dierent exposure models, we plot nDCG@5 for three versions of the model with dierent numbers of sampled items, as shown in Figure 5. As we can see from the gure, the performance of DCCF_Unbias increases with the increasing of sample number, the performance of DCCF_Bias increases rst and then drops after 5 samples, and the performance of DCCF_Random is monotonically decreasing. Meanwhile, for a given number of sampled items, the performance among the three models follows the conclusion as Table 3 in most cases, i.e., DCCF_Unbias>DCCF_Bias>DCCF_Random. For DCCF_Unbias, since it has the most accurate exposure probability, it will get better performance with more sampled items. For DCCF_Bias, the probability is learned from data but not accurate enough. Therefore, when the number of sampled items is small, the model can get slightly better performance with the help of sampled items, but when more sampled items are involved into the estimation, inaccurate probabilities may mislead front-door adjustment and hurt the performance, thus resulting in performance drops. For DCCF_Random, the sampled items do not provide useful information but instead introduce noises that completely mislead the front-door adjustment, and thus hurt the performance. In summary, accurate exposure probability will improve the performance while inaccurate probability hurts the performance. Therefore, it is important to adopt an accurate exposure model. In this paper, we notice that the unobserved confounders may result in inaccurate recommendations. To solve this problem, we propose a deconfounded causal collaborative ltering model. Specically, we rst design a causal graph to depict user behaviors with unobserved confounders. We then apply the front-door adjustment to design the model for eliminating the inuence of unobserved confounders. Considering the large scale of items in real-world system, we use the sample-based front-door adjustment to calculate the deconfounded estimation of users’ preference. Experiments on real-world datasets with both random splitting and skewed splitting show that our deconfounded model can outperform both classical association-based models and deconfounded recommendation models. Furthermore, the experiments on the inuence of exposure models show that the performance of our deconfounded causal collaborative ltering model highly depends on the accuracy of the learned exposure probabilities. Therefore, a proper exposure model helps improve the recommendation performance of our deconfounded recommendation model. There are still spaces for future improvements. First, our current model depends on the quality of exposure models, in the future, we can design a deconfounded recommendation model that is less aected by the exposure model. Second, our model treats all the confounders as bias terms and removes all of their inuence from the recommender system. However, the confounders may not always be harmful from the provider side. In practice, certain confounders can be useful and their aects should be remained in the recommendation. This requires a mixed recommendation strategy that considers partially associative rules and partially causal rules. Finally, the causal model used in DCCF could also be adapted to increase the interpretability of the recommender systems, which is another important aspect to be considered for recommendation. This work was supported in part by NSF IIS-1910154, IIS-2007907 and IIS-2046457. Any opinions, ndings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reect those of the sponsors.