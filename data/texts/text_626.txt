Abstract—A recommender system predicts users’ potential interests in items, where the core is to learn user/item embeddings. Nevertheless, it suffers from the data-sparsity issue, which the cross-domain recommendation can alleviate. However, most prior works either jointly learn the source domain and target domain models, or require side-features. However, jointly training and side features would affect the prediction on the target domain as the learned embedding is dominated by the source domain containing bias information. Inspired by the contemporary arts in pre-training from graph representation learning, we propose a pre-training and ﬁne-tuning diagram for cross-domain recommendation. We devise a novel Pre-training Graph Neural Network for Cross-Domain Recommendation (PCRec), which adopts the contrastive self-supervised pre-training of a graph encoder. Then, we transfer the pre-trained graph encoder to initialize the node embeddings on the target domain, which beneﬁts the ﬁne-tuning of the single domain recommender system on the target domain. The experimental results demonstrate the superiority of PCRec. Detailed analyses verify the superiority of PCRec in transferring information while avoiding biases from source domains. Index Terms—Recommender system; Cross-domain; Pretraining; Contrastive learning A recommender system predicts the potential interests of users to items, where the core is to learn user/item embeddings. Matrix factorization is an early method for collaborative ﬁltering (CF), where it learns user/item embeddings and reconstructs their interactions with inner product [1], [2]. The general idea of CF approach is based on observation of the preferences of other users that are similar to the historical preferences of the target user. Later on, deep recommender systems [3]–[6] have shown that capturing deep features in a supervised or unsupervised manner is more appealing than shallow models such as CF to capture similarity and implicit relationship between items. However, due to the coldstart and data-sparsity issues [7], [8], models learned from a single domain are unable to achieve satisfactory performance. Therefore, Cross-Domain Recommendation (CDR) [9]–[13] has been proposed. It transfers the information from source domains to a target domain, such that the recommendation performance on the target domain can be improved. The CDR has been veriﬁed as an effective way to alleviate the aforementioned issues [10]–[13]. The essential idea in CDR is to use the common users on both domains to transfer relevant information, which can be achieved from two , Tao Zhang, and Philip S. Yu perspectives: 1) modeling user information in source domain as auxiliary information in target domain [10], [12]; or 2) jointly training shareable parameters on both domains [11], [14]. Meanwhile, some recent works [13], [15] try to combine both. However, existing works assume that the information from a source domain is relevant to the prediction objectives in the target domain, which is not necessarily true. If a source domain contains dominant bias against the target domain, the prediction on the target domain would be misled, which thus spoils the recommendation performance. To this end, we should devise a new CDR paradigm, which can not only transfer the information but also protect the prediction on the target domain from being dominated by the bias from source domains. Inspired by recent developments of pre-training frameworks in computer vision [16], [17], natural language processing [18], graph representation learning [19], and sequential recommendation [20], we intent to design a novel pre-training framework for the CDR problem. The advantages are twofold: Firstly, the pre-trained model on source domains transfer auxiliary information to the target domain. Secondly, the ﬁne-tuning step on target domain ensures the prediction is dominated by the information in the target domain, thus overcoming the bias from source domains. Though existing works have discussed how to pre-train a model on source domains and ﬁne-tune it on the target domain [21], it is still challenging to propose a suitable framework for the CDR problem. Firstly, previous works [18], [19], [22] pre-train a model on a large corpus with side information available, which is not the case for CDR. The only available data in CDR is the user-item interactions, which is very sparse. Additionally, it is unclear how to exert the pretrained model while avoiding its bias impeding the prediction on the target domain. Therefore, we propose to Pre-train a graph neural network for Cross-domain Recommendation (PCRec) by transferring the graph structural information from source domain graphs to the target domain. During the pre-training stage, we adopt the Self-Supervised Learning (SSL) [23]–[25] scheme to train a graph encoder. SSL reduces the prediction bias from source domains as it pre-trains the model without prediction objectives. To be more speciﬁc, we sample and embed two subgraphs of a node as positive pairs and employ contrastive learning to maximize the agreements between them. Then, we transfer the pre-trained graph encoder to the target domain. However, since the encoder is also dominated by the structural bias in the source domain, we thus exert the pre-trained encoder to initialize node embeddings for another single domain model. According to our empirical study, we recognize that adopting a simple Matrix Factorization (MF) model on the target domain during the ﬁne-tuning stage [26] signiﬁcantly outperforms other complex models, such as LightGCN (LGCN) [27]. We hypothesis that a simple MF retains a balance between source domain and target domain information, while other complex models over-emphasize the data on the target domain and thus diminish the source domain information. The contributions are as follows: vestigating pre-training GNN to tackle the CDR problem. lem, which pre-trains a graph encoder on source domain to initialize the user/item embeddings in the target domain. and analyze our proposed framework. The cross-domain recommendation system mainly alleviates two major CF-based model bottlenecks: user/item data sparsity and the cold-start problem. The trained model may be overﬁtting when user/item data is too sparsity to cover the universal data distribution. The cold start problem often exists when new users have no historical shopping records and the lack of features of new products. Both issues lead to a decline in the effectiveness of the recommendation system. Conventional CDR has two approaches [28] to address the above problems: (1) content-based transfer, and (2) embedding-based transfer. Content-based transfer mainly links different domains by identifying similar content information while no common users/items in this situation [29], [30]. In contrast, Embedding-based transfer focuses on user/item relevance, such as multiple domains have common users or common items. This approach ﬁrst trains the CF-based model (bayesian personalized ranking [26], neural collaborative ﬁltering [5], etc.)to extract user/item embeddings and then transfer these embeddings through common or similar users/items across domains. In this work, we focus on embedding-based transfer. Unlike learning common users/items attributes, we adopt self-supervised contrastive learning and graph neural networks to learn users’ structural embedding from the source domain and then transfer it to the targer domain. GNNs models couple with contrastive learning to learn graph or node level representations without relying on supervisory data [31]. Then the trained model can transfer the learned representations to a priori unknown downstream tasks. In general, the contrastive learning method needs to create multiple views for each instance in the dataset through various data augmentations [32]. Two views are positive pairs, one is the original instance, and another is the augmented instance. We also need two negative views generated from the different instances. The ultimate goal of contrastive learning is to shorten the distance of the positive pair while pushing the negative sample away. Mutual information (MI) is often the measurement in contrastive learning. Following existing works [19], [33], we model the useritem interactions as a bipartite graph. We denote the graph as G = (U, I, E), where U, I and E denote the set of users, items, and edges, respectively. Regarding the CDR problem, we denote a source domain graph as Gand a target domain graph as G. For each node, we extract its context information from the r-ego network which is deﬁned as: Deﬁnition 1 (An r-ego network.): The r-hop neighbors for a node u are deﬁned as S= {i : d(u, i) ≤ r} where d(u, i) is the shortest path distance between u and i in the graph G. The r-ego network of vertex u, denoted as G, is a sub-graph composed by Sand the corresponding edges between S. Next, we present how to pre-train a graph encoder f : G → Rupon the source domain graph Gby adopting the selfsupervised learning scheme [19], [24]. We adopt the SSL scheme during the pre-training phase, which employs contrastive learning to optimize the graph encoder. Speciﬁcally, the SSL has three components: 1) the data augmentation, which constructs positive and negative subgraph pairs of a node, 2) the graph encoder to embed the subgraphs, and 3) the contrastive loss to optimize the encoder. 1) Data Augmentation: Contrastive learning requires the construction of positive pairs and negative samples of a node. As the only available data in the source domain graph is the interactions, we construct positive pairs as two sub-graphs of one node. The sub-graphs should share similar structure information to warrant two sub-graphs to be positive pairs. Therefore, we sample them from the r-ego network of a node. We ﬁrst conduct two random walks on node u’s r-ego network G(the superscript is ignored for simplicity), to generate two sub-graphs gand g, which are regarded as a positive pair. After constructing positive sub-graph pairs for nodes, we treat those sub-graphs generated from different rego networks as negative samples. We demonstrate the subgraph construction process in Figure 1(a), where gand gare a positive pair since they are sampled from the same node. We use gand gto denote the negative samples, which are sub-graphs sampled from the r-ego network of another node. 2) GNN Encoder: After retrieving those sub-graphs, we feed them into two graph encoders fand f, which is illustrated in Figure 1(a). We encode the sub-graph gwith graph encoder f, while encoding other sub-graphs with f. Correspondingly, we generate low-dimensional representative vectors eand efor the positive pair gand g, respectively. In this work, we choose the Graph Isomorphism Network (GIN) [34] to be the graph encoder because GIN exhibits powerful ability in distinguishing a broad class of graphs [35]. In general, other GNN models can also be used as the encoder. We leave this study as future work. 3) Contrastive Loss Function: We adopt the contrastive loss InfoNCE [36] to self-supervisedly optimize the graph encoder, which maximizes the agreements between positive pairs. The InfoNCE loss is formulated as follows: where τ is the temperature hyper-parameter. Minimizing this objective is equivalent to maximizing the similarity between positive pairs, i.e. eand e, while minimizing the similarity between negative pairs, i.e. eand ewhere i 6= k. In practice, we view those instances as a query embedding eand a set of key embeddings {e}|. The contrastive loss looks up a single key (denoted by e) that ematches in the key set. In contrastive learning, maintaining the K-size look-up key set is essential. Intuitively, as the denominator in Eq. (1) expresses, larger key set size leads to better sampling of the underlying data space. Due to the computational constraint, we adopt the MoCo [37] training scheme, which maintains a dynamic set of keys with a queue and a moving-averaged encoder. MoCo is able to increase the key set size without additional backpropagation costs. Formally, if denoting the parameters of fas θand those of fas θ, MoCo updates θ as θ← mθ+ (1 − m)θ, where m ∈ [0, 1) is a momentum hyper-parameter. After obtaining the pre-trained GIN model from the source domain, we should transfer the model to the target domain. However, due to the pre-trained encoder is dominated by the structural bias of the source domain graph, directly ﬁne-tuning the encoder in the target domain cannot avoid the interference of the bias from the source domain. Instead, we employ the pre-trained GIN model to initialize the node embeddings in the target domain. Then, we ﬁne-tune a simple Matrix Factorization (MF) [26] model with initialized embeddings to infer the ﬁnal embeddings of nodes. We illustrate this process as in Figure 1(b). The ﬁne-tuning on the target domain is optimized with the Bayesian Personalized Ranking (BPR) loss function, which is formulated as follows [26]: L= −log σ(ˆy− ˆy) + λkΘk(2) where Ndenotes the set of items which are the neighbors of node u, ˆy, and ˆydenote the rating of user u on item i, and the rating of user u on item j individually. Alternatively, the MF-based recommeder system can be substituted by any other single domain recommendation models, e.g., LightGCN. However, the empirical results indicate that a simple MF-based model outperforms a complex signal domain recommender system. We will use the embedding optimized during the ﬁne-tuning stage to make a further recommendation. We compute the score between a user and an item as: ˆr= ee, where e∈ Rand e∈ Rare the user and item embeddings, respectively. The score ˆris used to rank those items for users. We conduct experiments to respond the following research questions (RQs): cross-domain recommendation methods? information aggregation is helpful to represent the node embedding? the information from source domain to target domain? We conduct experiments on two datasets from the Amazon Review Data (2018): Grocery and Gourmet Food (AmazonGGF) and Prime Pantry (Amazon-PP). Their statistics are shown in Table I. For cross-domain models, we set AmazonGGF as the source domain and Amazon-PP as the target domain, while for single domain model, we solely use the target domain data. Due to distinct sparsity on both datasets, we adopt the 5-coresetting for Amazon-PP, and 10-core setting for Amazon-GGF. The number of common users between Amazon-PP and Amazon-GGF is 4,275. B. Experimental Settings 1) Baselines: We compare our proposed PCRec with three cross-domain and one single-domain methods. CMF [38] is a matrix factorization-based cross-domain rating prediction model. CoNet [13] and JSCN [33] both are joint learning model with differernt ways to transfer one domian knowledge to the other. One single domain method is LightGCN, which devises a light graph convolution for training efﬁciency and generation ability. 2) Evaluation Protocol: We randomly split 80% and 20% of the interactions in the target domain as training and testing set, respectively. We randomly choose 10% of the training data for validation during training. We use Recall@K and MAP@K to evaluate the top-K recommendation performance where K = [20, 40]. 3) Hyper-parameter Settings: In the pre-training, we apply Adam optimizer with a learning rate of 0.005, τ is 0.07, and MoCo (K=512) momentum m is 0.999. We change the learning rate to 0.001 in adam optimizer, and the early stopping strategy is the same as LightGCN. Furthermore, PCRec method inherits the optimal values of other shared hyper-parameters. C. Overall Comparison (RQ1) We compare PCRec with various baselines regarding the performance on the target domain Amazon-PP. The overall comparison is reported in Table II and we have the following ﬁndings: recall, e.g., achieving 42.43% on Recall@20. against the second-best one. This is because PCRec can effectively transfer the information from source domains to target domain while protecting the recommendation on the target domain from being dominated by the source domain. The performance variant between MAP and Recall is because the number of positive samples in the data is small. spect to Recall@{20,40}, even better than CDR methods, which indicates the effectiveness of using GNN to learn node embeddings. Nevertheless, it is still much worse than PCRec since it is cannot transfer the information from the source domain. shows the beneﬁts of using spectral graph convolution to encode user-item interaction. However, it performs worse than PCRec, because it jointly learns the source and target domain embeddings, which leads to the interference of the noise in the source domain. D. Variants Analysis (RQ2) In this section, we aim to analyse the speciﬁc designs of our proposed PCRec framework regarding the neighbors aggregation. At the pre-training stage, how to generate the subgraphs is critical. Regarding this point, two designs can be modiﬁed: neighbors. We change the r from 2 to 3, and results are generated by applying the pre-trained GIN without ﬁnetuning to reﬂect the intrinsic effect. Intuitively, PCRec should perform better at 3-hop rather than 2-hop due to more neighbor information being included. However, as presented in Table III, PCRec-2hop outperforms PCRec3hop. We argue that this is because, with the distance increasing, the similarity between positive samples will decrease, thus hindering node representation learning. augmentation method. Usually, there are four ways to augment graph data, node dropping, edge dropping, random walk, and attribute masking [39]. The ﬁrst three mechanisms do not require side information. Thus they can be adopted for the CDR problem. In PCRec, we generate the subgraphs through random walk, which can be supported by the graph structure assumption [40]. The other two methods can be explored in the future. For the ﬁne-tuning stage, we study how the complexity of ﬁne-tuning model will impact the performance. We change PCRec’s MF to 1-layer LGCN (PCRec-L1) and 3layer LGCN (PCRec-L3). There is no signiﬁcant difference between PCRec-L1 and PCRec-L3, and they are both much worse than PCRec. We hypothesis that a complex model will over-emphasize the target domain, thus hindering knowledge transfer. Therefore, a simple model with elaborate initialization may be able to retain the balance. E. Transfer in CDR (RQ3) To explore the process of implementing knowledge transfer in CDR, we present how do we ﬁnally get to the PCRec framework by altering three key components step by step: We transfer the source knowledge by the pre-trained GIN model without ﬁne-tuning, which is named Pre-Only. As shown in Table IV, the performance of Pre-Only is the worst, which implies that naively adopting the pre-trained model would bring severe bias to the target domain. Therefore, we add a ﬁne-tuning model to PCRec. edge to common users: 1) straightly using the pre-trained embeddings as the initialization of ﬁne-tuning (named CU-PE), 2) applying the pre-trained model to generate the initialization of ﬁne-tuning (named CU-PM). As presented in Table IV, CU-PM not only performs better than CU-PE but also surpasses the LGCN, while CU-PE falls behind it. In the CDR problem, the result indicates that transferring the inherited structure information from the model has better performance than copying the common users’ embedding from the source domain to the target domain directly. users, which leads to our ﬁnal PCRec. In CDR problem, other nodes should be carefully processed. But as they can also receive the information transferred by the pretrained encoder, we suggest that they should be included. The experiment result demonstrates that the target performance would not be dominated by the source domain in this way of transferring. In this work, we investigate the possibility of pre-training a GNN to transfer structural representations in the source domain to the target domain to address the cold-start problem in the CDR task. We propose a novel framework, PCRec, which pre-trains a graph encoder to learn node embeddings from the source domain and apply it to the target domain to initiate embedding. A simple MF method during the ﬁnetuning stage can signiﬁcant outperform other complicated methods on all metrics. This work is supported in part by NSF under grants III1763325, III-1909323, III-2106758, and SaTC-1930941.