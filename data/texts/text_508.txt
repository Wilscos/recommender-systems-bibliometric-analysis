Nan Wang Department of Computer Science University of Virginia Charlottesville, VA 22903, USA Lu Lin Department of Computer Science University of Virginia Charlottesville, VA 22903, USA Department of Electrical and Computer Engineering Department of Computer Science, and School of Data Science University of Virginia Charlottesville, VA 22903, USA Department of Computer Science University of Virginia Charlottesville, VA 22903, USA Graph embedding is an indispensable building block in modern machine learning approaches that operate on graph-structured data (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016; Kipf and Welling, 2017; Goyal and Ferrara, 2018). Graph embedding methods map each node to a low-dimensional embedding space that reﬂects the nodes’ structural information from the observed connections in the given graph. These node embedding vectors are then employed to solve downstream tasks, such as friend recommendation in social networks (i.e., link prediction) or user interest prediction in e-commerce platforms (i.e., node classiﬁcation) (Wang et al., 2016; Ou et al., 2016). Graph embedding techniques have been increasingly employed in real-world machine learning tasks on graph-structured data, such as social recommendations and protein structure modeling. Since the generation of a graph is inevitably aﬀected by some sensitive node attributes (such as gender and age of users in a social network), the learned graph representations can inherit such sensitive information and introduce undesirable biases in downstream tasks. Most existing works on debiasing graph representations add ad-hoc constraints on the learned embeddings to restrict their distributions, which however compromise the utility of resulting graph representations in downstream tasks. In this paper, we propose a principled new way for obtaining unbiased representations by learning from an underlying bias-free graph that is not inﬂuenced by sensitive attributes. Based on this new perspective, we propose two complementary methods for uncovering such an underlying graph with the goal of introducing minimum impact on the utility of learned representations in downstream tasks. Both our theoretical justiﬁcation and extensive experiment comparisons against state-of-the-art solutions demonstrate the eﬀectiveness of our proposed methods. Keywords: unbiased graph embedding, sensitive attributes, bias-free graph tributes (such as gender and age of a user in a social network), which should be withheld from the downstream tasks (Pfeiﬀer et al., 2014). Without proper intervention, the learnt node embeddings can inherit undesired sensitive information that can lead to bias or fairness concerns when used in downstream tasks (Rahman et al., 2019; Bose and Hamilton, 2019). For example, in a social network, if the users with the same gender tend to connect more often, the learnt embeddings can inherit such gender information and lead to gender bias by only recommending friends to a user with the same gender identity. And from the data privacy perspective, this also opens up the possibility for extraction attacks from the learnt node embeddings (Sun et al., 2018). cially in classical classiﬁcation problems (Kamishima et al., 2012; Zemel et al., 2013; Chouldechova, 2016). Unbiased graph embedding has just started to attract research attentions in recent years. To date, the most popular recipe for unbiased graph embedding is to add adversarial regularizations to the loss function, such that the sensitive attributes cannot be predicted by the learnt embeddings (Madras et al., 2018; Bose and Hamilton, 2019; Agarwal et al., 2021; Dai and Wang, 2021). For example, making a discriminator built on the node embeddings fail to predict the sensitive attributes of the nodes. However, such a regularization is only a necessary condition for unbiased node embeddings, and it usually hurts the utility of the embeddings in downstream tasks (a trivial satisfying solution is to randomize the embeddings). Besides these regularization-based solutions, Fairwalk (Rahman et al., 2019) modiﬁes the random walk strategy in the node2vec algorithm (Grover and Leskovec, 2016) into two levels: when choosing the next node on a path, it ﬁrst randomly selects a group deﬁned by sensitive attributes, and then randomly samples a reachable node from that group. DeBayes (Buyl and De Bie, 2020) proposes to capture the sensitive information by a prior function in Conditional Network Embedding (Kang et al., 2018), such that the learned embeddings will not carry the sensitive information. Nevertheless, both Fairwalk and DeBayes are based on speciﬁc graph embedding methods; and how to generalize them to other types of graph embedding methods such as GAT (Veliˇckovi´c et al., 2018) or SGC (Wu et al., 2019) is not obvious. principled new framework for the purpose with theoretical justiﬁcations. Our solution is to learn node embeddings from an underlying bias-free graph whose edges are generated without any inﬂuence from sensitive attributes. Speciﬁcally, as suggested by Pfeiﬀer et al. (2014), the generation of a graph can be treated as a two-phase procedure. In the ﬁrst phase, the nodes are connected with each other solely based on global graph structural properties, such as degree distributions, diameter, edge connectivity, clustering coeﬃcients and etc., resulting in an underlying structural graph, free of inﬂuences from node attributes. In the second phase, the connections are re-routed by the node attributes (including both sensitive and non-sensitive attributes). For example, in a social network, users in the same age group tend to be more connected than those in diﬀerent age groups, leading to the ﬁnal observed graph biased by the age attribute. Hence, our debiasing solution is to ﬁlter out the inﬂuence from sensitive attributes on the underlying structural graph to create a bias-free graph (that only has non-sensitive attributes or no attributes) from the observed graph, and then perform embedding learning on the bias-free graph. node embeddings. The ﬁrst is a weighting-based method, which reweighs the graph reconstruction based loss function with importance sampling on each edge, such that in expectation the derived loss is as calculated on the bias-free graph. This forms a suﬃcient condition for learning unbiased node embeddings: when the reconstruction loss is indeed deﬁned on the corresponding bias-free graph, the resulting node embeddings are unbiased, since the bias-free graph is independent from the sensitive attributes. The second way is via regularization, in which we require the probabilities of generating an edge between two nodes with and without the sensitive attributes to be the same. This forms a necessary condition for learning unbiased node embeddings: when the learning happens on the biasfree graph, the learnt embeddings should not diﬀerentiate if any sensitive attributes participated in However, the observed connections in a graph are inevitably aﬀected by certain sensitive at- There is rich literature in enforcing unbiasedness/fairness in algorithmic decision making, espe- Moving beyond the existing unbiased graph embedding paradigm, in this paper, we propose a We propose two alternative ways to uncover the bias-free graph from the given graph for learning the generation of observed graph, i.e., the predicted edge generation should be independent from the sensitive attributes. These two methods are complementary and can be combined to control the trade-oﬀ between utility and unbiasedness. prove the eﬀectiveness of the proposed framework. It achieved encouraging trade-oﬀ between unbiasedness and utility of the learnt embeddings in the downstream applications. And the results also suggest the embeddings from our debiasing methods lead to fair predictions in the downstream tasks. The rest of paper is organized as follows: In Section 2, we discuss the related work. We introduce the notation and preliminary knowledge on unbiased graph embedding in Section 3. We formally deﬁne the underlying bias-free graph in Section 4, and propose the unbiased graph embedding methods in Section 5. We evaluate the proposed methods in Section 6 and conclude the work in Section 7. Graph embedding. Graph embedding aims to map graph nodes to low-dimensional vector representations such that the original graph can be reconstructed from these embeddings. Traditional approaches include matrix factorization and spectral clustering techniques (Ng et al., 2002; Belkin and Niyogi, 2001). Recent years have witnessed numerous successful advances in deep neural architectures for learning graph embeddings. Deepwalk (Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016) utilize a skip-gram (Mikolov et al., 2013a) based objective to recover the node context in random walks on a graph. Graph Convolutional Networks (GCNs) learn a node’s embedding by aggregating the features from its neighbors supervised by node/edge labels in an end-to-end manner. These techniques are widely applied in various real-world applications, such as friend recommendation in social network (Liu et al., 2019), content recommendation (Xie et al., 2016), protein structure prediction (Jumper et al., 2021) and many more. Unbiased/fair node embeddings. In real-life scenarios, sensitive attributes such as age, gender, skin color, religion, and region are inevitably involved in the generation and evolution of graph data, which raises severe bias and fairness concern when applying graph embedding techniques for real-world tasks. For example, people with similar age tend to connect closer; thus, graph embedding models built on such input for downstream applications, such as loan application and job recommendation, may unintentionally favor or disregard one group, causing biased and unfair treatments. This realistic and ethical concern sets a higher bar for the graph embedding models to learn eﬀective and unbiased embeddings. and post-processing steps in the learning pipeline. The pre-processing solutions modify the training data to reduce the leakage of sensitive attributes (Calmon et al., 2017). Fairwalk (Rahman et al., 2019) is a typical pre-processing method which modiﬁes the sampling process of random walk on graphs by giving each group of neighboring nodes an equal chance to be chosen. However, such pre-processing may well shift the data distribution and thus leads the trained model to inferior accuracy and fairness measured on test set. The post-processing methods employ discriminators to correct the learnt embeddings to satisfy speciﬁc fairness constraints (Hardt et al., 2016). However, such ad-hoc post-correction is detached from model training which may heavily degrade model’s prediction quality. prevent bias from the node embeddings. The most popular algorithmic solution is adding (adversarial) regularizations as constraints to ﬁlter out sensitive information (Bose and Hamilton, 2019; Dai and Wang, 2020; Agarwal et al., 2021). Compositional fairness constraints (Bose and Hamilton, 2019) are realized by a composition of discriminators for a set of sensitive attributes jointly trained with the graph embedding model. Similarly, FairGNN (Dai and Wang, 2020) adopts a fair discriminator but focuses on debiasing with missing sensitive attribute values. Diﬀerent from regularization based methods. DeBayes (Buyl and De Bie, 2020) reformulates the maximum likelihood estimation Comprehensive experiments on three datasets and several backbone graph embedding models Recent eﬀorts on unbiased and fair graph embedding mainly focus on pre-processing, algorithmic Our work falls into the category of algorithmic methods, which modify the learning objective to with a biased prior which absorbs the information about sensitive attributes; but this solution is heavily coupled with the speciﬁc embedding method thus is hard to generalize. Our method diﬀers from these previous works by learning embeddings from an underlying bias-free graph. We investigate the generation of the given graph and remove the inﬂuence from sensitive attributes in the generative process to uncover a bias-free graph for graph embedding. Generative graph models. Generative graph models (Airoldi et al., 2008; Pfeiﬀer et al., 2014) focus on the statistical process of graph generation by modeling the joint distributions of edges conditioned on node attributes and graph structure. For example, Attributed Graph Model (AGM) (Pfeiﬀer et al., 2014) jointly models graph structure and node attributes in a two step graph generation process. It ﬁrst exploits a generative graph model to compute underlying structural edge probabilities based on the structural properties of an observed graph. AGM then learns attribute correlations among edges from the observed graph and combines them with the structural probabilities to sample graphs conditioned on attribute values, while keeping the expected edge probabilities and degrees of the input structural graph model. This process motivates us to uncover an underlying bias-free graph by separating out sensitive attributes and only conditioning on non-sensitive attributes for calculating edge probabilities. In this section, we ﬁrst introduce our notations and general graph embedding concepts. Since the bias/fairness issues emerge most notably in prediction tasks involving humans, such as loan application (Kulik et al., 2000) or criminal justice (Berk et al., 2021), we will use user-related graphs as running examples to discuss our criterion for unbiased graph embedding. But we have to emphasize this setting is only to illustrate the concept of unbiased graph embedding; and our proposed solution can be applied to any graph data to remove biases caused by the selected sensitive attributes in the learnt embeddings. Let G = (V, E, A) be an undirected, attributed graph with a set of N nodes V, a set of edges E ⊆ V × V, and a set of N attribute vectors A (one attribute vector for each node). We use (u, v) to denote an edge between node u ∈ V and node v ∈ V. The number of attributes on each node is K, and A = {a loss of generality, we assume only the ﬁrst m attributes are sensitive, and a stands for the ﬁrst m sensitive attributes and the rest of the attributes that are non-sensitive, respectively. We assume all attributes are categorical and S attribute i. For example, if node u is a user node, and the i-th attribute is gender with possible values S maps each node u to a d-dimensional embedding vector z embedding setting which does not require node labels and the embeddings are learned via the link prediction task. In this task, a scoring function s the probability of an edge between node u and node v in the given graph, i.e., (u, v) ∈ E. The loss for learning node embeddings and parameters of the encoder and scoring function are deﬁned by: where L the likelihood of observed edges in the given graph, comparing to the negative samples of node pairs where edges are not observed (Mikolov et al., 2013b; Grover and Leskovec, 2016). = {Female, Male, Unknown}, then a[i] = Female indicates u is a female. In the problem of graph embedding learning, we aim to learn an encoder ENC : V → Rthat Given a user node u, we consider its embedding z is independent from the attribute, z learnt embeddings by their ability to predict the value of the sensitive attributes (Bose and Hamilton, 2019; Palowitch and Perozzi, 2020; Buyl and De Bie, 2020). For example, such work evaluates the unbiasedness of learnt embeddings by training a classiﬁer on a subset of nodes’ embeddings and their sensitive attributes. If the classiﬁer cannot predict the sensitive attribute values of nodes in the test set, one claims that the embeddings have low bias. If the prediction accuracy equals to that from random embeddings, the learnt embeddings are considered bias-free. In fact, such classiﬁers are often used as discriminators in adversarial methods where the classiﬁer and the embeddings are learnt jointly: the embeddings are pushed in directions where the classiﬁer has low prediction accuracy (Madras et al., 2018; Bose and Hamilton, 2019). tunity to deﬁne the unbiasedness of learnt embeddings (Hardt et al., 2016; Buyl and De Bie, 2020). But we should point out that such fairness measures can only evaluate the fairness of the ﬁnal prediction results for the intended downstream tasks, but cannot assess whether the embeddings are biased by, or contain any information about, sensitive attributes. In particular, fairness in a downstream task is only a necessary condition for unbiased embedding learning, not suﬃcient. The logic is obvious: unbiased embeddings can lead to fair prediction results as no sensitive attribute information is involved; but obtaining fairness in one task does not suggest the embeddings themselves are unbiased, e.g., those embeddings can still lead to unfair results in other tasks or even the fair results are obtained by other means, such as post-processing of the prediction results (Woodworth et al., 2017). In Section 6, we will use both the prediction accuracy on sensitive attributes and fairness measures on ﬁnal tasks to evaluate the eﬀectiveness of our unbiased graph embedding methods. In this section, we discuss the generation of an observed graph by explicitly modeling the eﬀects of node attributes in the process. In particular, we assume that there is an underlying structural graph behind an observed graph, whose edge distribution is governed by the global graph properties such as degree distributions, diameter, and clustering coeﬃcients. Then the attributes in A will modify the structural edge distribution based on eﬀects like homophily in social networks, where links are rewired based on the attribute similarities of the individuals (McPherson et al., 2001; La Fond and Neville, 2010). The new distribution is then used to generate the observed graph. properties of the underlying structural graph. In particular, this set of parameters Θ from node attributes in A. We consider the class of generative graph models that represent the set of possible edges in the graph as binary random variables E indicates (u, v) ∈ E. The model M assigns a probability to E Therefore, the edge set E can be considered as samples from Bernoulli(P many such structural models M such as the Chung Lu model (Chung and Lu, 2002) and Kronecker Product Graph Model (Leskovec et al., 2010). Note that M does not consider attributes in A in the generation of the graph. V, j ∈ V} be a random variable indicating the attribute value combination of a pair of nodes u and v, which is independent from Θ can be the same, as many diﬀerent node pairs can share the same attribute value combination. attributes on the incident nodes and structural parameters Θ There are also studies that use fairness measures such as demographic parity or equalized oppor- Formally, let M be a generative graph model and Θbe the set of parameters that describe Now we involve the attributes in the generation of a graph. Speciﬁcally, let C∈ {(a, a)|i ∈ = 1|C= a, Θ) is the conditional probability of an edge given the corresponding Figure 1: Illustration of UGE. The color of nodes represents the value of their sensitive attributes, and diﬀerent line styles suggest how the observed edges are inﬂuenced by sensitive attributes in the generative process. observed attribute value combination on nodes u and v. Then based on Bayes’ Theorem, we have where the prior distribution on E value combinations. Therefore, the edge distribution used to generate the observed graph with node attributes is a modiﬁcation of an structural graph distribution deﬁned by M and Θ to note that in the generative process, the node attributes are given ahead of graph generation. They are the input to the generation model, not the output. Hence P same for all edges whose incident nodes have the attribute combination C are, since C to the probability ratio that modiﬁes the structural graph into the observed graph by In this way, we explicitly model the eﬀect of node attributes by R(a graph distribution P In this section, we describe our proposed methods for learning unbiased node embeddings that are independent from sensitive attributes, based on the generative modeling of their eﬀects in Section 4. (E= 1|Θ), and the posterior distribution accounts for the inﬂuences from the attribute To simplify the notation, let us deﬁne a function that maps the attribute value combination a In a nutshell, we aim to get rid of sensitive attributes and modify the structural edge distribution by only conditioning on non-sensitive attributes. This gives us the edge distribution of a bias-free graph, from which we can learn unbiased node embedding. Consider a world without the sensitive attributes, and the attribute vector of node u becomes sensitive attributes in a ˜a, ∀u ∈ V, and attributes. If we can learn node embeddings from to be unbiased with respect to sensitive attributes. Speciﬁcally, the edge probabilities used for generating where sensitive attributes, and P methods that learn embeddings from illustrate the general principle of UGE in Figure 1. Next we discuss two instances of UGE. The ﬁrst is called UGE-W, which reweighs the per-edge loss such that the total loss is from The second method is called UGE-R, which adds a regularization term to shape the embeddings to satisfy the properties as those directly learnt from We modify the loss function in Eq (1) by reweighing the loss term on each edge as embeddings from Proof We take expectation over the edge observations in G as ∈ {(˜a,˜a)|i ∈ V, j ∈ V} is the random variable indicating attribute value combinations without And the following theorem shows that in expectation this new loss is equivalent as learning graph The step marked by ∗ uses Eq (3) and Eq (4). UGE-W is related to the idea of importance sampling (Kloek and van Dijk, 1978), which analyzes the edge distribution of the bias-free graph thing needed for deploying UGE-W in existing graph embedding methods is to calculate the weights Note that the estimation of P unobservable. But we can approximate P after grouping node pairs by non-sensitive attribute value combinations only re-route the edges but do not change the number of edges in each group, and thus we have calculated once instead of for each pair. This can be done by ﬁrst grouping the pairs by their attribute value combinations and then perform estimation in each group. However, when there are many attributes or attributes can take many unique values, the estimates may become inaccurate since there will be many groups and each group might only have a few nodes. In this case, we can make independence assumptions between attributes. For example, by assuming they are independent, the estimate for attribute combinations becomes the product of estimates for each attribute. The nonsensitive attributes can be safely removed under this assumption with needs to be estimated as R(a to the loss, the optimization based on it will not increase the complexity of any graph embedding method. We propose an alternative way for UGE which adds a regularization term to the loss function that pushes the embeddings to satisfy properties required by the bias-free graph )/R(a). To estimate R(a), we need estimates for two probabilities P(C= a|E= ) and P(C= a|Θ). With maximum likelihood estimates on the observed graph, we For node pairs with the same attribute value combination, Eq (8)-Eq (11) only need to be node embeddings are learnt from without the sensitive attributes. To enforce this condition, we need to regularize the discrepancy between P embeddings. We can use the scores in s and v, i.e., high s to marginalize out the eﬀect of Θ where we use Q is the number of node pairs that has the attribute value combination a attribute value combination, Q with non-sensitive attribute value combination where λ controls the trade-oﬀ between the per-edge loss and the regularization. Hamilton, 2019; Agarwal et al., 2021; Dai and Wang, 2021), UGE-R takes a diﬀerent perspective in regularizing the discrepancy between graphs with and without sensitive attributes induced from the embeddings. All previous regularization-based methods impose the constraint on individual edges. We should note the regularization term is summed over all node pairs, which has a complexity of O(N sampling batches of node pairs in each iteration during model update, and use λ to compensate the strength of the regularization. As hinted in section 1, UGE-W is a suﬃcient condition for unbiased node embeddings and UGE-R is a necessary condition. We can combine them to trade-oﬀ the debiasing performance and utility, where we use L advantages of both UGE-W and UGE-R to achieve better trade-oﬀs between the unbiasedness and the utility of node embeddings. In this section, we study the empirical performance of UGE on three benchmark datasets in comparison to several baselines. In particular, we apply UGE to ﬁve popularly adopted backbone graph embedding models to show its wide applicability. To evaluate the debiasing performance, the node embeddings are ﬁrstly evaluated by their ability to predict the value of sensitive attributes, where = 1|C= a, Θ) by aggregating all node pairs with the same attribute value combination , Θ) can be represented by Q, which can be obtained by aggregating the scores over pairs as the regularization In contrast to adversarial regularizations employed in prior work (Madras et al., 2018; Bose and ) and can be costly to optimize. But in practice, we can add the regulariztaion by only Table 2: Unbiasedness evaluated by Micro-F1 on Pokec-z and Pokec-n. Bold numbers highlight the best in each row. Pokec-n +node2vec lower prediction performance means better debiasing eﬀect. Then a task-speciﬁc metric is used to evaluate the utility of the embeddings. Besides, we also apply fairness metrics in the link prediction results to demonstrate the potential of using embeddings from UGE to achieve fairness in downstream tasks. • Dataset. We used three public user-related graph datasets, Pokec-z, Pokec-n and MovieLens1M, where the users are associated with sensitive attributes to be debiased. The statistics of these three datasets are summarized in Table 1. Pokec contains anonymized data of millions of users (Takac and Zabovsky, 2012). Based on the provinces where users belong to, we used two sampled datasets named as Pokec-z and Pokec-n adopted from (Dai and Wang, 2020), which consist of users belonging to two major regions of the corresponding provinces, respectively. In both datasets, each user has a rich set of features, such as education, working ﬁeld, interest, etc.; and we include gender, region and age as (sensitive) attributes whose eﬀect will be studied in our evaluation. MovieLens-1M benchmark, which contains around one million user ratings on movies (Harper and Konstan, 2015). In our experiment, we construct a bipartite graph which consists of user and movie nodes and rating relations as edges. The dataset includes gender, occupation and age information about users, which we treat as sensitive attributes to be studied. We do not consider movie attributes, and thus when applying UGE, only user attributes are counted for our debiasing purpose. • Graph embedding models. UGE is a general recipe for learning unbiased node embeddings, and can be applied to diﬀerent graph embedding models. We evaluate its eﬀectiveness on ﬁve representative embedding models in the supervised setting with the link prediction task: GCN (Kipf and Welling, 2016), GAT (Veliˇckovi´c et al., 2017), SGC (Wu et al., 2019) and node2vec (Grover and Leskovec, 2016) are deep learning models, and we use dot product between two node embeddings to predict edge probability and apply cross-entropy loss for training. MF (Mnih and Salakhutdinov, 2008) applies matrix factorization to the adjacency matrix. Each node is represented by an embedding vector learnt with pairwise logistic loss (Rendle et al., 2012). • Baselines. We consider three baselines for generating unbiased node embeddings. (1) Fairwalk (Rahman et al., 2019) is based on node2vec, which modiﬁes the pre-processing of random-walk generation by grouping neighboring nodes with their values of the sensitive attributes. Instead of randomly jumping to a neighbor node, Fairwalk ﬁrstly jumps to a group and then sample a node from that group for generating random walks. We extend it to GCN, GAT and SGC by sampling random walks of size 1 to construct the corresponding per-edge losses for these embedding models. (2) Compositional Fairness Constraints (CFC) (Bose and Hamilton, 2019) is an algorithmic method, which adds an adversarial regularizer to the loss by jointly training a composition of sensitive attribute discriminators. We apply CFC to all graph embedding models and tune the weight on the regularizer in {1, 5, 10, 25, 35, 45, 55, 65}, where larger weights are expected to result in embeddings with less bias but also lower utility. (3) Random embeddings are used as a baseline which is considered bias-free. We generate random embeddings by sampling the value of each embedding dimension from [0, 1] uniformly at random. based on the conditional network embedding (CNE) (Kang et al., 2018). It includes the sensitive information in a biased prior for learning unbiased node embeddings. We did not include it since it is limited to CNE and cannot be easily generalized to other graph embedding models. Besides, we found the bias prior calculation in DeBayes does not scale to large graphs and the utility of resulting node embeddings is close to random. The original paper (Buyl and De Bie, 2020) only experimented with two small graph datasets with less than 4K nodes and 100K edges. By default, UGE follows Fairwalk to debias each of the three sensitive attributes separately without independence assumption between attributes. CFC debiases all sensitive attributes jointly as suggested in the original paper • Conﬁgurations. For the Pokec-z and Pokec-n datasets, we apply GCN, GAT, SGC and node2vec as embedding models and apply debiasing methods on top of them. For each dataset, we construct positive examples for each node by collecting N degree, and randomly sample N node, we use 90% positive and negative examples for training and reserve the rest 10% for testing. For Movielens-1M, we follow common practices and use MF as the embedding model (Rahman et al., 2019; Bose and Hamilton, 2019) to evaluate CFC. We do not evalaute Fairwalk on this dataset since there is no user-user connections and fair random walk cannot be applied. The rating matrix is binarized to create a bipartite user-movie graph for MF. We use 80% ratings for training and 20% for testing. For all datasets and embedding models, we set the node embedding size to d = 16. We include more details about model implementations and hyper-parameter tuning in Appendix A. ing methods. We evaluate fairness resulted from the embeddings on the link prediction task in Section 6.3. We study the unbiasedness-utility trade-oﬀ in UGE compared to that in CFC in Section 6.4. Since there is a large number of experimental settings composed of diﬀerent datasets, embedding models, and baselines, we report results from diﬀerent combinations in each section to maximize the coverage in each component, and include the other results in Appendix B due to space limit. We ﬁrst compare the unbiasedness of node embeddings from diﬀerent debiasing methods. For each sensitive attribute, we train a logistic regression classiﬁer with randomly sampled 80% nodes with their embeddings as features and attribute values as class labels. Then we use the classiﬁer to predict the attribute values of the rest of 20% nodes and evaluate the performance with MicroF1. The Micro-F1 score can be used to measure the bias in the embeddings, i.e., a lower score means lower bias in the embedding with respect to the sensitive attribute. In expectation, random embeddings should have the lowest Micro-F1 and embeddings without debiasing should have the It is important to mention a recent work called DeBayes (Buyl and De Bie, 2020), which is In Section 6.2, we compare the unbiasedness and utility of embeddings from diﬀerent debias- Figure 2: Trade-oﬀ between utility by NDCG@10 (y-axis) and unbiasedness by micro-f1 (x-axis) of embeddings from diﬀerent methods. Random embeddings without any bias give the lowest MicroF1 (green line), and no debiasing gives the best NDCG@10 (blue line). An ideal debiasing method should locate itself at the upper left corner. highest Micro-F1. We show the results on Pokec-z with GAT as base embedding model and Pokec-n with node2vec as base embedding model in Table 2. against all baselines with respect to all sensitive attributes and datasets. This conﬁrms the validity of learning unbiased embeddings from a bias-free graph regarding the sensitive attributes. Besides, by combining weighting and regularization, UGE-C usually produces the best debiasing eﬀect, which demonstrates the complementary eﬀect of our two methods. downstream tasks. In particular, we use NDCG@10 evaluated on the link prediction task to measure the utility of the embeddings. Speciﬁcally, for each target node, we create a candidate list of size 100 that includes all its observed neighbor nodes in the test set and randomly sampled negative nodes. Then NDCG@10 is evaluated on this list with predicted edge probabilities from node embeddings. Figures 2a and 2b show the unbiasedness as well as the utility of embeddings from diﬀerent methods From the results, we can ﬁnd that embeddings from UGE methods always have the least bias Besides the unbiasedness, the learnt embeddings are required to be eﬀective when applied to Figure 3: Visualization of embeddings learnt on Pokec-n using GCN. Node color represents the region attribute label. in correspondence to the two datasets and embedding models in Table 2. Figure 2c shows the results on MovieLens-1M with MF as embedding model. shows Micro-F1 measuring the bias in embeddings. Diﬀerent embedding methods are represented by diﬀerent shapes in the ﬁgures, and we use diﬀerent colors to diﬀerentiate UGE-W, UGE-R and UGE-C. Again, random embeddings do not have any bias and provide the lowest Micro-F1 (denoted by the green line), while embeddings without any debiasing gives the best NDCG@10 (denoted by the blue line). An ideal debiasing method should locate at the upper left corner to achieve the best utility-unbiasedness trade-oﬀ. As shown in the ﬁgures, UGE methods achieve the most encouraging trade-oﬀ on these two contradicting objectives in most cases. UGE-C can usually achieve better debiasing eﬀect, without sacriﬁcing too much utility in most cases. UGE-W and UGE-R maintain high utility but are less eﬀective than the combined version. CFC can achieve descent unbiasedness in embeddings, but the utility is seriously compromised (such as in Pokec-z and MovieLens-1M). Fairwalk unfortunately does not present an obvious debiasing eﬀect on sensitive attributes. learned node embeddings on Pokec-n to a 2-D space and visualize them in Figure 3. The left plot shows the embedding learned via GCN without debiasing, and the right plot exhibits the debiased embeddings by applying UGE-C on GCN to debias the region attribute. Node colors represent the region value. Without debiasing, the embeddings are clearly clustered with respect to region. After applying UGE-C, embeddings with diﬀerent region values are blended together, showing the eﬀect of removing the region information from embeddings. We study whether the embeddings can lead to fairness in downstream tasks. We adopt two popular fairness metrics called demographic parity (DP) and equalized opportunity (EO) to evaluate the fairness in the link prediction task from the embeddings. In particular, DP requires that the predictions are independent from sensitive attributes, measured by the maximum diﬀerence of prediction rates between diﬀerent combinations of the sensitive attribute values. EO measures the independence between true positive rate (TPR) of predicting edges and the sensitive attributes, and it is deﬁned as the maximum diﬀerence of TPRs between diﬀerent sensitive attribute value combinations. For both DP and EO, lower values suggest better fairness. We use the exact formulation of DP and EO in (Buyl and De Bie, 2020) and use sigmoid function to convert each edge score for a pair of nodes to a probability. attributes in Pokec-n with node2vec as embedding model. In each plot, the x-axis is the fairness In these plots, the y-axis denotes NDCG@10 evaluating the utility of embeddings and x-axis To further illustrate the debiasing eﬀect from UGE, we use the t-SNE algorithm to project the We show the results on fairness vs., utility in Figure 4, which are evaluated on three sensitive Figure 4: Fairness metrics evaluated on link prediction task on Pokec-n with node2vec as the embedding model. metric and y-axis is the NDCG@10 on link prediction. Similar to Figure 2, the ideal debiasing methods should locate at the upper left corner. Except for EO on the age attribute where all methods performed similarly, UGE methods can achieve signiﬁcantly better fairness than the baselines on both DP and EO, while maintaining competitive performance on the link prediction task. UGE-C that combines UGE-W and UGE-R can achieve the most fair predictions. This study shows UGE’s ability to achieve fairness in downstream tasks by eﬀectively eliminating bias in the learnt node embeddings. Last but not least, we study the unbiasedness-utility trade-oﬀ in UGE-C by tuning the weight on the regularization. Although UGE-W itself can already achieve promising debiasing eﬀect, we expect that the added regularization UGE-R can complement it for a better trade oﬀ. In particular, we tuned the regularization weights λ in both CFC and UGE-C and plot Micro-F1 (x-axis) vs., NDCG@10 (y-axis) from the resulting embeddings in Figure 5. Weight values are marked on each point and also listed in Appendix A. The results are obtained on Pokec-z with GAT and the two ﬁgures correspond to debiasing gender and region, respectively. With the same extent of bias evaluated by Micro-F1, embeddings from UGE-C have a much higher utility as indicated by the Figure 5: Trade-oﬀ comparison between CFC and UGE-C on Pokec-z with GAT as the embedding model. vertical grids. On the other hand, embeddings from UGE-C have much less bias when the utility is the same as indicated by horizontal grids. This experiment proves a better trade-oﬀ is achieved in UGE-C, which is consistent with our design on UGE-W and UGE-R. UGE-W learns from a bias-free graph without any constraints, and it is suﬃcient for it to achieve unbiasedness without hurting the utility of embeddings. On the other hand, UGE-R requires the embeddings to satisfy the properties of those learnt from the bias-free graph, which is necessary for the embeddings to be unbiased. We propose a principled new way for learning unbiased node embeddings from graphs biased by sensitive attributes. The idea is to infer an unbiased graph where the inﬂuence from sensitive attributes is removed, and then learn the node embeddings from this graph. This new perspective motivates our design of UGE-W, UGE-R and their combined methods UGE-C. Extensive experiment results demonstrated strong debiasing eﬀect from UGE as well as better unbiasedness-utility tradeoﬀs in downstream applications. embeddings from bias-free graphs. For example, instead of modeling the generation process and perform debiasing statistically, we can directly generate one or multiple bias-free graphs from the underlying graph model, and perform graph embedding on them. The regularization UGE-R can be reﬁned with better moment matching mechanism than minimizing the l UGE-W can be modeled and learned for better debiasing eﬀects. Besides, it is possible and promising to directly design unbiased GNN models that directly aggregate edges based on the inferred unbiased graph. This paper is based upon the work supported by the National Science Foundation under grant IIS-1553568, IIS-1718216, IIS-2007492, and IIS-2006844. We expect the principle of UGE can inspire better future designs for learning unbiased node