Yonsei UniversityHanyang University qbxlvnf11@yonsei.ac.krtaerik@hanyang.ac.kr There were erce debates on whether the non-linear embedding propagation of GCNs is appropriate to GCN-based recommender systems. It was recently found that the linear embedding propagation shows better accuracy than the non-linear embedding propagation. Since this phenomenon was discovered especially in recommender systems, it is required that we carefully analyze the linearity and non-linearity issue. In this work, therefore, we revisit the issues of i) which of the linear or non-linear propagation is better and ii) which factors of users/items decide the linearity/nonlinearity of the embedding propagation. We propose a novelHybrid Method ofLinear and non-linEar collaborative lTering method (HMLET, pronounced as Hamlet). In our design, there exist both linear and non-linear propagation steps, when processing each user or item node, and our gating module chooses one of them, which results in a hybrid model of the linear and non-linear GCN-based collaborative ltering (CF). The proposed model yields the best accuracy in three public benchmark datasets. Moreover, we classify users/items into the following three classes depending on our gating modules’ selections: Full-Non-Linearity (FNL), Partial-NonLinearity (PNL), and Full-Linearity (FL). We found that there exist strong correlations between nodes’ centrality and their class membership, i.e., important user/item nodes exhibit more preferences towards the non-linearity during the propagation steps. To our knowledge, we are the rst who design a hybrid method and report the correlation between the graph centrality and the linearity/nonlinearity of nodes. All HMLET codes and datasets are available at: https://github.com/qbxlvnf11/HMLET. • Information systems → Recommender systems. Seoul, KoreaSeoul, Korea jjsjjs0902@yonsei.ac.krjeongwhan.choi@yonsei.ac.kr Recommender Systems, Collaborative Filtering, Embedding Propagation, Graph Neural Network ACM Reference Format: Taeyong Kong, Taeri Kim, Jinsung Jeon, Jeongwhan Choi, Yeon-Chang Lee, Noseong Park, and Sang-Wook Kim. 2022. Linear, or Non-Linear, That is the Question!. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM ’22), February 21–25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3488560. 3498501 Recommender systems, personalized information ltering (IF) technologies, can be applied to many services, ranging from E-commerce, advertising, and social media to many other online and oine service platforms [38]. One of the most popular recommender systems, collaborative ltering (CF), provides personalized preferred items to users by learning user and item embeddings from their historical user-item interactions [3, 4, 7, 8, 16, 18, 23, 24, 30, 34]. One of the mainstream research directions in recommender systems is how to learn high-order connectivity of user-item interactions while ltering out noises. Recently, GCN-based CF methods became popular in recommender systems because they show strong points to capture such latent high-order connectivity. Since existing GCNs are originally designed for graph or node classication tasks on attributed graphs, however, two limitations had been raised out when it comes to GCN-based CF methods: training diculty [6,15,37] and over-smoothing [5,6,15]. Over-smoothing degrades the recommendation accuracy by considering the connectivity information too much [6]. To overcome these problems, a couple of linear GCNs (linear embedding propagation-based GCNs) were proposed [6,15]. These methods eectively alleviate the aforementioned two limitations and show superior performance over non-linear GCNs (non-linear embedding propagation-based GCNs). Even though linear GCNs show the state-of-the-art performance in many benchmark CF datasets, it is questionable in our opinion whether they can properly handle users and items with various characteristics and whether linear GCNs are consistently superior to non-linear GCNs in all cases. In addition, we are curious about, if one outperforms the other, which factors of graphs decide it. To this end, we propose aHybridMethod ofLinear and nonlinEar collaborative lTering (HMLET, pronounced as Hamlet), a GCN-based CF method. HMLET has the following key design points: i) We adopt a gating concept to decide between the linear Figure 1: Four variants of HMLET in terms of the location of the non-linear propagation. HMLET(End) shows the best accuracy in our experiments. It was known that the problem of over-smoothing happens with more than 2 non-linear propagation layers, and we use up to 2 non-linear layers. and non-linear propagation for each node in a layer. ii) We perform residual prediction, where the embeddings from all layers are aggregated and used collectively for nal predictions. Therefore, we let our gating modules decide which of the linear or non-linear propagation is used for a certain node at a certain layer instead of relying on manually designed architectures. This gating mechanism’s key point is how to generate appropriate one-hot vectors. For this purpose, we adopt the Gumbel-softmax [11, 27]. To our knowledge, we are the rst who combines the linear and non-linear embedding propagation in a systematic way, i.e., via the gating in our paper. Our gating mechanism can be considered as a sort of neural architecture search (NAS) for the GCN-based CF method. However, our proposed mechanism is more sophisticated because it provides the switching function for each user/item and the overall GCN architecture can be varied from a node’s perspective to another. We conduct experiments with three benchmark CF datasets and compare our HMLET with various state-of-the-art CF methods in terms of the normalized discounted cumulative gain (NDCG), recall, and precision. We also dene several variations of HMLET in terms of the locations of the non-linear propagation layers (see Fig. 1). Among all of them, HMLET(End) shows the best performance in all datasets. Furthermore, we dene three classes of nodes, i.e., users and items, depending on their preferences on the linear or nonlinear propagation: Full-Non-Linearity (FNL), Partial-Non-Linearity (PNL), and Full-Linearity (FL). An FNL (resp. FL) node means that our gating module chooses the non-linear (resp. linear) propagation every time for the node and a PNL node has a mixed characteristic. At the end, we analyze the class-specic characteristics in terms of various graph centrality metrics and reveal that there exist strong correlations between the graph centrality, i.e., the role of a node in a graph, and the linear/non-linear gating outcomes (see Table 1). Our discovery shows that recommendation datasets are complicated because the linearity and non-linearity are mixed. Contributions of our paper can be summarized as follows: Table 1: The characteristics of node classes in terms of various metrics. FNL (resp. FL) means a class of nodes for which our gating modules select only the non-linear (resp. linear) propagation in all layers. For PNL, our gating modules choose dierent propagation methods in dierent layers. •We propose HMLET, which dynamically selects the best propagation method for each node in a layer. •We reveal that the role of a node in a graph is closely related to its linearity/non-linearity, e.g., our gating module prefers the non-linear embedding propagation for the nodes with strong connections to other nodes. •Our experiments on three benchmark datasets show that HMLET outperforms baselines in yielding better performance. In this section, we review recommender systems and the Gumbelsoftmax used in our proposed gating module. Traditional recommender systems have focused on matrix factorization (MF) techniques [19,23]. Typical MF-based methods include BPR [30] and WRMF [18]. These MF-based methods simply learn relationships between users and items via dot products. Therefore, they have limitations in considering potentially complex relationships between users and items inherent in user-item interactions [16]. To overcome these limitations, deep learning-based recommender systems, e.g., Autoencoders [21,33] and GCNs [2,12, 22,37], have been proposed to eectively learn more complicated relationships between users and items [32, 35, 38, 39]. Recently, recommender systems using GCNs [32,35,38] are gathering much attention. GCN-based methods can eectively learn the behavioral patterns between users and items by directly capturing the collaborative signals inherent in the user-item interactions [35]. Typical GCN-based methods include GC-MC [32], PinSage [38], and NGCF [35]. In general, GCN-based methods model a set of useritem interactions as a user-item bipartite graph and then perform the following three steps: (Step 1) Initialization Step:They randomly set the initial𝐷dimensional embedding𝒆of all user𝑢and item𝑣, i.e.,𝒆, 𝒆∈ R. (Step 2) Propagation Step:First of all, this propagation step is iterated𝐾times, i.e.,𝐾layers of embedding propagation. Given the𝐾layers, the embedding of a user node𝑢(resp. an item node𝑣) in𝑖-th layer is updated based on the embeddings of𝑢’s (resp.𝑣’s) neighbors 𝑁(resp. 𝑁) in (𝑖 − 1)-th layer as follows: where𝜎denotes a non-linear activation function, e.g., ReLU, and 𝑾∈ Ris a trainable transformation matrix. There exist some other variations: i) including the self-embeddings, i.e.,𝑁= 𝑁∪ {𝑢}and𝑁= 𝑁∪ {𝑣 }, ii) removing the transformation matrix, Table 2: GCN-based recommender systems. In each layer, the gating module in HMLET cho oses either of the linear or the non-linear propagation for each node. and iii) removing the non-linear activation function, which is in particular called as linear propagation [6, 15]. (Step 3) Prediction Step:The preference of user𝑢to item𝑣is typically predicted using the dot product between the user𝑢’s and item 𝑣’s embeddings in the last layer 𝐾, i.e.,ˆ𝑟= 𝒆⊙ 𝒆. However, these GCN-based methods have two limitations: i) training diculty [6,15,37] and ii) over-smoothing, i.e., too similar embeddings of nodes [5,6,15]. First, the training diculty is caused by their use of a non-linear activation function in the propagation step [6,15,37]. Specically, the non-linear activation function complicates the propagation step, and even worse, this operation is repeatedly performed whenever a new layer is created. Thus, they suer from the training diculty of the non-linear activation functions for large-scale user-item bipartite graphs [6, 37]. Next, the over-smoothing is caused as they use only the embeddings updated through the last layer in the prediction layer [6]. Specically, as the number of layers increases, the embedding of a node will be inuenced more from its neighbors’ embeddings. As a result, the embedding of a node in the last layer becomes similar to the embeddings of many directly/indirectly connected nodes [5,6]. This phenomenon prevents most of the existing GCN-based methods from eectively utilizing the information of high-order neighborhood. Empirically, this is also shown by the fact that most of non-linear GCN-based methods show better performance when using only a few layers instead of deep networks. Recently, LR-GCCF [6] and LightGCN [15], which are GCNbased recommender systems to alleviate the problems, have been proposed. First, to alleviate the former problem, they perform a linear embedding propagation without using a non-linear activation function in the propagation step. In order to mitigate the latter problem, they utilize the embeddings from all layers for prediction. After that, they perform residual prediction [6,15], which predict each user’s preference to each item with the multiple embeddings from the multiple layers. In [6,15], the authors demonstrated that a GCN architecture with the linear embedding propagation and the residual prediction can signicantly improve the recommendation accuracy by successfully addressing the two problems. In summary, GCN-based recommender systems can be characterized by, as shown in Table 2, the propagation and prediction types. We note that all existing methods consider only one of the linear or non-linear propagation, i.e., they assume only one type of user-item interactions. However, we conjecture that user-item interactions are neither only linear nor only non-linear, for which we will conduct in-depth analyses in Section 4.3. In this paper, therefore, we propose aHybridMethod ofLinear and non-linEar collaborative lTering method (HMLET), which considers both the two disparate propagation steps and selects an appropriate embedding propagation for each node in a layer. The Gumbel-max trick [11,27] provides a way to sample a one-hot vector from a categorical distribution with class probabilities 𝝅: where𝑔...𝑔are drawn from the unit Gumbel distribution. The arg maxoperator does not allow the gradient ow via the Gumbelmax, because it gives zero gradients irrespective of how𝝅was created. To this end, the Gumbel-softmax [20] generates𝒚that approximates𝒛via the reparameterization trick dened as follows: where𝑦is𝑖-th component of the vector𝒚, and𝜏is a temperature that determines how closely the function approximates𝝅. However, the Gumbel-softmax is challenging to use if it needs to sample discrete values because, when the temperature is high, its output is not categorical. To solve this problem, the straight-through Gumbelsoftmax (STGS) [20] can be used. STGS always generates discrete values for its forward pass, i.e., 𝒚 is a one-hot vector, while letting the gradients ow through𝒚for its backward pass, even when the temperature is high. This makes neural networks with the Gumbelsoftmax trainable. This Gumbel-softmax has been widely used to learn optimal categorical distributions. One such example is network architecture search (NAS) [13,17,25,36]. In NAS, we let an algorithm nd optimal operators (among many pre-determined candidates prepared by users) and their connections. All these processes can be modeled by generating optimal one-hot (or multi-hot) vectors via the Gumbel-softmax [20]. Another example is multi-generator-based generative adversarial networks (GANs) [10]. Park et al. showed that data is typically multi-modal, and it is necessary to separate modes and assign a generator to each mode of data, e.g., one generator for long-hair females, another generator for short-hair males, and so on for a GAN generating facial images [29]. In our case, we try to separate the two modes, i.e., the linear and non-linear characteristics of nodes. We rst formulate our problem of top-𝑁recommendation as follows: Let𝑢 ∈ 𝑈and𝑣 ∈ 𝐼denote a user and an item, respectively, where𝑈and𝐼denote the sets of all users and all items, respectively; 𝑁denotes a set of items rated by user𝑢. For each user𝑢, the goal is to recommend the top-𝑁items that are most likely to be preferred by 𝑢 among her unrated items, i.e., 𝐼 \ 𝑁. In this section, among several variations of HMLET, we mainly describe HMLET(End) for ease of writing because it shows the best accuracy — other variations can be easily modied from HMLET(End) and we omit their descriptions. Its key concept is to adopt the gating between the linear and non-linear propagation in a layer. In other words, we prepare both the linear and non-linear propagation steps in a layer and let our gating module with STGS decide which one to use for each node. Table 3 summarizes a list of notations used in this paper. Figure 2 illustrates the overall workow of HMLET(End). After constructing the user-item interaction as a user-item bipartite graph, Figure 2: The detailed workow of HMLET(End). One can consider our gating module as a relay switch between the linear and non-linear propagation. While calculating an embedding for a user or an item in the third and fourth layer, therefore, our gating module learns the optimal selection between them for each node. For instance, it can select a sequence of linear → linear → linear → non-linear for some nodes while it can select a totally dierent sequence for other nodes. HMLET initializes the user and item embeddings in the initialization step. After that, each embedding is propagated to its neighbors through𝐾propagation layers. The gating module in HMLET selects either of the linear or the non-linear propagation in a layer for each node (Section 3.1). To this end, we use the gating module with STGS. In order to predict each user’s preference on each item, the dot product of the user embedding and the item embedding in each layer is aggregated and we use their sum for prediction (Section 3.2). We omit the description of the initialization step due to its obviousness. HMLET propagates the embedding𝒆(resp.𝒆) of each user𝑢(resp. each item𝑣) through the propagation layers. In this subsection, we describe the propagation process. We rst formally dene the linear and the non-linear propagation steps used in this paper. Then, we present our gating module. 3.1.1Propagation.Recently, the authors of LightGCN [15] found that the feature transformation and the non-linear activation do not have a positive eect on the eectiveness of CF. So, LightGCN removed the feature transformation and the non-linear activation from Eq.(1), and it shows better performance than existing nonlinear GCNs for recommendation. In HMLET, we adopt the linear layer denition of LightGCN. Therefore, our linear embedding propagation is performed as follows: where𝒆and𝒆are the linear embeddings for user𝑢and item 𝑣. Since our gating module, which will be described shortly, selects between the linear and the non-linear embeddings,𝒆and𝒆 means the embeddings selected by our gating module in the previous𝑖-th layer. If𝑖 =0,𝒆= 𝒆and𝒆= 𝒆, i.e., initial embeddings. √is a symmetric normalization term to restrict the scale of embeddings into a reasonable boundary. For the non-linear embedding propagation, we design a variant of the linear embedding propagation by adding non-linear activation functions. Its propagation is preformed as follows: where𝜙is a non-linear activation function, e.g., ELU, Leaky ReLU. For instance, as shown in Figure 2, HMLET(End) bypasses the nonlinearity propagation on the rst and second layers to address the over-smoothing problem and then propagates the non-linear embedding in the third and fourth layers. 3.1.2Gating Module.Now, we have the two types of the embeddings for each node, created by the linear and non-linear propagation in Eqs.(4)and(5), respectively, in the previous𝑖-thlayer. Therefore, we should select one of the linear and non-linear embeddings for the propagation in the next (𝑖 + 1)-th layer. Toward this end, we add a gating module, which dynamically selects either of the linear or non-linear embedding after understanding the inherent characteristics of nodes. A separate gating module should be added whenever the linear and the non-linear Input: Linear embedding 𝒆, Non-linear embedding 𝒆, Temperature 𝜏, Gating type 𝜉 propagation co-exist in a layer. The intuition behind this technique is that i) the embeddings of nodes may exhibit both the linearity and the non-linearity in their characteristics and ii) the linearity and the non-linearity of nodes may vary from one layer to another. The process of the gating module with STGS is shown in Algorithm 1. For simplicity but without loss of generality, we use the symbol𝒆and𝒆to denote the linear and non-linear embeddings, respectively, after omitting other subscripts and superscripts. We support three gating types: i) choose the linear embedding (bypassing the non-linear propagation), ii) choose the non-linear embedding (bypassing the linear propagation), and iii) let the gating module choose one of them. The variable𝜉notates the gating type. If𝜉is the rst or second type, a designated embedding type is selected. If𝜉is the third type, the input embeddings, i.e., the linear and non-linear embedding, are concatenated and then passed to an MLP (multi-layer perceptron) (Lines 7 and 8 in Algorithm 1). The result of the MLP is a logit vector𝒍, an input for STGS (Line 9). The logit vector𝒍corresponds tolog 𝝅explained in Section 2.2. 𝒈represents a linear or non-linear selection by the gating module, i.e.,𝒈is a two-dimensional one-hot vector. Therefore,𝒆is the same as either of 𝒆or 𝒆(Line 10). 3.1.3Variants of HMLET.As shown in Figure 1 and Table 4, there can be four variants of HMLET, denoted as HMLET(All), HMLET(Front), HMLET(Middle), and HMLET(End), depending on the locations of the non-linear layers. Each method except HMLET(All) uses up to 2 non-linear layers since it is known that more than 2 non-linear layers cause the problem of over-smoothing [6]. Moreover, we test with various options of where to put them. First, HMLET(Front) focuses on the fact that GCNs are highly inuenced by close neighborhood, i.e., in the rst and second layers [31]. Therefore, HMLET(Front) adopts the gating module in the front and uses only the linear propagation layers afterwards. Second, HMLET(Middle) only uses the linear propagation in the front and last and then adopts the gating module in the second and third layers. Last, as the gating module is located in the third and fourth layers, HMLET(End) focuses on gating in the third and fourth layers — our experiments and analyses show that HMLET(End) is the best among the four variations of the proposed method. We select𝒆or𝒆 at the third layer and𝒆or𝒆at the fourth layer via the gating modules, respectively. If the linear embeddings are selected for a node in all layers, it is the same as using a linear GCN with𝐾 =4 Table 4: Variants of HMLET in terms of their setting for the non-linear propagation in Eq. (5) and the gating type 𝝃 Train the parameters of the gating modules with the BPR Loss for processing the node. If the non-linear embedding is selected for other node in all layers, it reduces to a non-linear GCN with𝐾 =2. Likewise, HMLET(End) can be considered as a node-wise dynamic GCN (between the linear and non-linear propagation) with varying 𝐾 ∈ {2, 4}. After propagating through all𝐾layers, we predict a user𝑢’s preference for an item𝑣. To this end, we create a dot product value of 𝒆and𝒆in each layer and use the following residual prediction: In some layers, a gating module can be missing. In such a case, there is only one type of embeddings, but we also use𝒆/𝒆to denote these embeddings for ease of writing. In most previous GCN-based recommender system research, only the embedding of the last layer was used to predict, but in HMLET, the above residual predictionˆ𝑟with𝛽is used. Similar to LightGCN,𝛽is set to 1/(𝐾 +1). This residual prediction can produce good performance by using not only the embedding in the last layer but also the embeddings in previous layers. For training HMLET, we employ the Bayesian Personalized Ranking (BPR) loss [30], denoted𝑳, which is frequently used in many CF methods. The BPR loss is written as follows: Table 5: Statistics of public benchmark datasets where𝜎is the sigmoid function.Θis the initial embeddings and the parameters of the gating modules, and𝜆controls the𝐿regularization strength. We use each observed user-item interaction as a positive instance and employ the strategy used in [15] for sampling a negative instance. We employ STGS for a smooth optimization of the gating module. We can train the network with annealing the temperature𝜏, and we use the temperature decay for each epoch (Line 5 in Algorithm 2). In order to calculateˆ𝑟as in Eq.(6), we accumulate the dot product results (Lines 6 and 12). Then, we train the initial embeddings (Line 13) and the parameters of the gating modules (Line 14). In this section, we evaluate our proposed approach via comprehensive experiments. We design our experiments, aiming at answering the following key research questions (RQs): • RQ1:Which variation of HMLET is the most eective in terms of recommendation accuracy? • RQ2:Does gating between the linear and non-linear propagation provide more accurate recommendations than baseline methods? • RQ3:What are the characteristics of the nodes that use i) only the linear propagation, ii) only the non-linear propagation, or iii) dierent propagation steps in dierent layers? 4.1.1Datasets.For evaluation, we used the following three realword datasets: Gowalla, Yelp2018, and Amazon-Book from various domains. They are all publicly available. Table 5 shows the detailed statistics of the three datasets. • Gowallais a location-based social networking website where users share their locations by checking-in [26]. This dataset contains user-website interactions. • Yelp2018is a subset of small business and user data used in Yelp Dataset Challenge 2018. This dataset contains user-business interactions. • Amazon-Bookcontains purchase records of Amazon users [14]. This dataset contains user-item interactions. Amazon-Book has the highest sparsity among these three public datasets. Following [35], we ltered out those users and items with less than ten interactions in all datasets, i.e., a 10-core setting. For testing, we then split a dataset into training (80%), validation (10%), and test (10%) sets in the same way as in [35]. 4.1.2Baseline Methods.We compare HMLET with the following ve state-of-the-art methods to verify its eectiveness: • BPR[30] is a matrix factorization (MF) trained by the Bayesian Personalized Ranking (BPR) loss. Figure 3: The comparison of NDCG@20 with all types of HMLET in three public benchmarks. • WRMF[18] is an MF solved by the weight alternating least square (WALS) technique. • NGCF[35] is a non-linear GCN-based recommender system performing residual prediction. • LR-GCCF[6] is a linear GCN-based recommender system which removes the non-linear activation function but still use the transformation matrix in Eq.(1). This method performs the residual prediction. • LightGCN[15] is yet another linear GCN-based recommender system performing the residual prediction. This method diers from LR-GCCF in that it does not use the transformation matrix. For MF-based methods, we use the implementations in the popular open-source library, called NeuRec.For GCN-based methods, we use the source codes provided by the authors [6,15,35]. To evaluate accuracy, we use the top-20 recommendations and measure the accuracy in terms of the normalized discounted cumulative gain (NDCG), recall, and precision, which are all frequently used in recommendation research [6, 15, 35]. 4.1.3Hyper-parameter Seings.We choose the best hyperparameter set via the grid search with the validation set. The best setting found in HMLET is as follows: the number of linear layers is set to 4; the number of non-linear layers is set to 4 in HMLET(All) and 2 for HMLET(Front), HMLET(Middle), and HMLET(End); the optimizer is Adam; the learning rate is 0.001; the𝐿regularization coecient𝜆is 1E-4; the mini-batch size is 2,048; the dropout rate is 0.4. And, we use the temperature𝜏with an initialization to 0.7, a minimum temperature of 0.01, and a decay factor of 0.995. Also, for fair comparison, we set the embedding sizes for all methods to 512. In non-linear layers, we test two non-linear activation functions: Leaky-ReLU (negative slope = 0.01) and ELU (𝛼= 1.0). For baseline models, we tuned their hyper-parameters via the grid search in the ranges suggested in their respective papers. 4.2.1Comparison among Model Variations (RQ1).For answering RQ1, we rst compare the accuracies of HMLET(All), HMLET(Front), HMLET(Middle), and HMLET(End). Figures 3 illustrates the results where X-axis represents the types of HMLET, and Y-axis represents NDCG@20. HMLET(End) is the best among all variations of HMLET. The accuracies of all variations except HMLET(End) are similar. Specifically, the dierence between HMLET(End) and other variations is around 7%, 0.9%, 1% in Amazon-Book, Yelp2018, and Gowalla, Table 7: The selection ratio by gating modules in AmazonBook respectively. However, the dierences in accuracy among HMLET(All), HMLET(Front), and HMLET(Middle) are as small as 0.2%. These results indicate that i) the eectiveness of the gating module greatly depends on the location where the gating module exists, and ii) the non-linear propagation is useful to capture distant neighborhood information — note that we added the gating modules at the last two layers in HMLET(End). As shown in Table 7, each variation has a quite dierent linear/non-linear embedding selection ratio. HMLET(End), the best model, uses the non-linear propagation in the layers 3 and 4, and their selection ratios are signicant, e.g., 5.19% of linear vs. 94.81% of non-linear in the third layer. This observation also applies to the second best model, HMLET(All). Similar selection ratio patterns are observed in the other two datasets. 4.2.2Comparison with Baselines (RQ2).Table 6 illustrates our main experimental results. In them, the values in boldface indicate the best accuracy in each column, and the values in italic mean the best baseline accuracy. Also, ‘%Improve’ indicates the degree of accuracy improvements over the best baseline by HMLET(End). Lastly, we conduct𝑡-tests with a 95% condence level to verify the statistical signicance of the accuracy dierences between HMLET(End) and the baselines. We summarize the results shown in Table 6 as follows. First, among the ve baseline methods, we observe that LightGCN consistently shows the best accuracy in all datasets. Second, HMLET(End) consistently provides the highest accuracy in all datasets and with all metrics. Specically, HMLET(End) outperforms LightGCN by 6.00%, 3.33%, and 1.56% for the datasets in terms of NDCG@20, respectively. The𝑝-values are below 0.05, indicating that the dierences are statistically signicant. We highlight that HMLET(End) shows remarkable improvements in Amazon-Book which is the largest dataset in this paper. In this subsection, we dene three dierent classes of nodes, depending on their preferences on the linear or non-linear propagation, and perform in-depth analyses on them. In order to analyze accurately, we use the embeddings learned by HMLET(End), the best performing variation of HMLET, for Amazon-Book. Due to space limitations, we omit the results for the other datasets, which show similar patterns to those in Amazon-Book. 4.3.1Node Class and Graph Centrality.We rst classify all nodes into one of the following three classes according to the embedding types selected by the gating modules: • Full-Non-Linearity (FNL)is a class of nodes in which all embeddings selected by the gating modules are non-linear embeddings. • Partial-Non-Linearity (PNL)is a class of nodes in which the embeddings selected by the gating modules are mixed with linear embeddings and non-linear embeddings. • Full-Linearity (FL)is a class of nodes in which all embeddings selected by the gating modules are linear embeddings. We next introduce three graph centrality metrics to study the characteristics of the classes: • PageRank[28] measures the relative importance of nodes in a graph. A node is considered as important, even though its connectivity with other nodes is not that strong, if connected to other important nodes. • Betweenness Centrality[1] measures the centrality of a node as an intermediary in a graph. The more a node appears in multiple shortest paths, the higher the betweenness centrality of the node. • Closeness Centrality [9] measures the centrality of a node by considering general connections to other nodes in a graph. The less hops it takes for a node to reach all other nodes, the higher the closeness centrality. 4.3.2Characteristics of Node Classes (RQ3).In this subsection, we analyze the characteristics of the nodes in each class in terms of the various centrality metrics. Table 8 shows the relative class size in our three datasets. In Amazon-Book and Yelp2018, most nodes were classied as FNL and PNL (about 47-48% and 50-52%, respectively), and a few nodes were classied as FL (about 1-2%). However, in Gowalla, the ratio of FL is about 12%, which is relatively higher compared to the other two datasets. Figure 4 shows the relative sizes of the three classes by degree, and Figure 5 shows the statistics of the centrality scores in each class. From them, it can be seen that the degree and centrality scores increase in order of FL, PNL, and FNL. Now, we deliver the meaning of the above results for each class. Figure 4: The class ratio of nodes sorted by degree. 𝑖-th bin in X-axis means a range of [(10 ∗ (𝒊 −1))-th percentile, (10 ∗ 𝒊)-th percentile) in terms of degree. Nodes with a high degree are the most likely to be in FNL (10th), and nodes with a small degree are likely to be in FL (1st). We also nd that nodes classied as PNL are more evenly distributed than other classes. • FNL Attributes:A node in FNL is either an active user or a popular item with more direct/indirect interaction information, i.e., a high degree and closeness centrality, and higher inuence, i.e., a high PageRank and betweenness centrality, than nodes in other classes. So, they will receive a lot of information during the propagation step. Therefore, the sophisticated non-linear propagation is required to correctly extract useful information from much potentially noisy information. • FL Attributes:A node in FL is either a user or an item that does not have much direct/indirect interaction information, i.e., a low degree and closeness centrality, and little inuence, i.e., a low PageRank and betweenness centrality, compared to nodes in other classes. The information they receive during the propagation step may mostly consist of useful information related to themselves with little noise. Therefore, the simple linear propagation is required to take useful information as it is, rather than rening it. • PNL Attributes:A node in PNL, compared to nodes in other classes, is a user or an item with neither too large nor too small direct/indirect interaction information, i.e., a moderate degree and closeness centrality, and inuence, i.e., a moderate PageRank and betweenness centrality. In other words, although they have many direct neighbors, there are few indirect neighbors connected to the direct neighbors, or even if there are few direct neighbors, their indirect neighbors can be many. Therefore, they need to perform one of the non-linear or linear operations depending on the information they receive from neighbors. In order to double-check our interpretations, we show the statistics of the similarity of embeddings for each class. So, we calculate the cosine similarity between a node and its direct neighbors by using the embeddings learned by HMLET(End). The results are shown in Table 9. From these results, we can conrm that the neighbors of a node in FNL consist of diversied nodes, i.e., a low mean and high variance. Also, FL nodes’ neighbors mainly consist of similar nodes, i.e., a high mean and low variance. Lastly, PNL nodes’ neighbors ·10 1.5·100.3 FNL PNL FL0FNL PNL FLFNL PNL FL0.15 Figure 5: The statistics of PageRank, betweenness centrality, and closeness centrality. Table 9: The statistics of the cosine similarity between a node’s embedding and its direct neighbors’ embeddings in each node class are in between the previous two cases, i.e., a moderate mean and variance between nodes in other classes. In this paper, we presented a novel GCN-based CF method, named as HMLET, that can select the linear or non-linear propagation step in a layer for each node. We further analyzed how the linear/nonlinear selection mechanism works using various graph analytics techniques. To this end, we rst designed our linear and non-linear propagation steps, being inspired by various state-of-the-art linear and non-linear GCNs for CF. Then, we used STGS to learn the optimal selection between the linear and non-linear propagation steps. The intuition behind such design choice is that it is not optimal to put both the linear and the non-linear propagation in every layer. In this sense, we have dened several variations of HMLET in terms of combining the linear and non-linear propagation steps. Through extensive experiments using three standard benchmark datasets, we demonstrated that HMLET shows the best accuracy in all datasets. Furthermore, we presented in-depth analyses of how the linearity and non-linearity of nodes are decided in a graph. Toward this end, we classied nodes into three classes, i.e., FullNon-Linearity, Partial-Non-Linearity, and Full-Linearity, depending on our gating module’s selections and studied correlations between nodes’ centrality scores and their class membership. We conjecture that GCNs for CF should somehow consider both linear and non-linear operations. We do not say that our specic mechanism to combine the linear and the non-linear propagation steps is optimal. We hope that our discovery encourages much follow-up research work. The work of Sang-Wook Kim was supported by Samsung Research Funding & Incubation Center of Samsung Electronics under Project Number SRFC-IT1901-03. The work of Noseong Park was supported by the Yonsei University Research Fund of 2021 and the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No. 20200-01361, Articial Intelligence Graduate School Program (Yonsei University)).