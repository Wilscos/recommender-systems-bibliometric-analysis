<title>Long-Tail Session-based Recommendation from Calibration</title> <title>Jiayi Chen , Wen Wu , Liye Shi , Wei Zheng and Liang He</title> School of Computer Science and Technology, East China Normal University, 200062, Shanghai, China. Party Working Committee, East China Normal University, 200062, Shanghai, China. *Corresponding author(s). E-mail(s): wwu@cc.ecnu.edu.cn; Contributing authors: jychen@ica.stc.sh.cn; lyshi@ica.stc.sh.cn; wzheng@admin.ecnu.edu.cn; lhe@cs.ecnu.edu.cn; <title>arXiv:2112.02581v5  [cs.IR]  2 Jan 2022</title> <title>1 Introduction</title> recommendation list. The other one is distribution alignment, which aims to align the distribution of the recommendation list towards that of the ongoing session. Then we propose a curriculum training strategy to organize the training data and ensure the eﬀectiveness of our calibration module. Sessions are classiﬁed into two categories by the threshold of the ratio of tail items of the session, and they are processed separately. During the evaluation stage, the recommendation list is also generated according to such a strategy. In addition, we propose an extension strategy based on the threshold-based data splitting to further organize the samples. The contribution of this paper is listed as follows: We propose a novel CSBR model to handle the long-tail session-based recommendation from the training objective and paradigm. We design a calibration module to recommend long-tail items according to the ratio of long-tail items in the ongoing session. Meanwhile, we apply a curriculum training strategy to organize the data and mitigate the eﬀect of popularity bias. Experiments on benchmark datasets show that our model can both achieve competitive accuracy and recommend more tail items. In addition, the recommendation list is more calibrated compared to baseline models. The rest of this paper is organized as follows. We review the existing literature in Section 2. Then we introduce our model in Section 3, and experimental details in Section 4. We show experimental results and analysis Sections 5. Finally, we make a discussion of our work in Section 6 and conclude our paper and indicate some future work in Section 7. <title>2 Related Work</title> <title>2.1 Session-based Recommendation</title> with the ranking-based loss functions and parallel training strategy [7]. Furthermore, they improved the GRU4REC by considering rich features of items [35] and advanced loss functions [36]. Li et al. used a hybrid global-local encoder with attention mechanisms to capture the main purpose in the session [12]. The dwell time on an item is also considered to adjust the attention weights of previous clicks in the session [11]. Also, combination-based models were proposed to improve the performance [10, 37]. Though RNN-based models achieved progress compared to traditional models, the inherent limitation was that it can only model sequences by consecutive items, which cannot learn complex item transitions in the session. Therefore, with the development of graph-based models, GNNs-based models have become the trend in session-based recommendation [14, 15, 17, 38, 39]. For example, Wu et al. proposed SR-GNN which treats sessions as graphs and learns item transitions and embeddings by Graph Neural Networks [14]. Abugabah et al. designed a dynamic attention mechanism to model the relation between the user’s historical interests and the current session [38]. Wang et al. built session-level graph and global-level graph to incorporate information from other sessions to enhance the recommendation performance [15]. Besides RNNs and GNNs, other architectures were also applied for session-based recommendation, such as Convolutional Neural Networks [40] and Multi-Layer Perceptrons [41]. <title>2.2 Long-Tail Recommendation</title> the training objective and paradigm were not investigated. On the one hand, users’ preferences of tail items were not taken into account, leading to an uncontrollable ratio of long-tail items in recommendation lists. On the other hand, these studies did not investigate the inherent drawback of the training paradigm of SBR, where the imbalance problem caused by popularity bias was retained. Therefore, we are interested in handling long-tail SBR from the user perspective and designing a training paradigm to handle popularity bias. <title>3 The CSBR Model</title> In this section, we introduce our CSBR model in detail. We ﬁrst provide a preliminary about the deﬁnitions of SBR and concepts used in our CSBR model. Then we introduce our proposed calibration module and curriculum training strategy of our CSBR model. <title>3.1 Preliminary</title> vector to present an item from the popularity perspective: For a given session s = {x , ..., x }, the distribution q of the session is calculated by: pop(x ) (2) where q (0) and q (1) are the proportions of head items in the session, respectively. Similarly, for the recommended list RL of the session s generated by a session, the distribution can be calculated as follows: where N is the size of the recommendation list. <title>3.2 Backbone Model</title> The paradigm of deep learning based models for SBR can be concluded as follows. It can be seen as a black box which takes the session as input, and outputs the representation of the session and items (red box in Fig. 2). Brieﬂy, the procedure of the session-based recommendation is summarized as: where h is the session representation of session s generated by the SBR model, and f denotes the session encoder. By multiplying with item embeddings or multi-layer perceptrons, scores over all items are computed and RL is the recommendation list containing N items with the highest scores. In our work, how to model complex item transitions to acquire higher prediction accuracy is not the main purpose. Therefore, we do not design the session encoder f . Instead, we apply the NARM [12] and SR-GNN model [14] as backbone models which are representative models of RNNs and GNNs in SBR. <title>3.3 Calibration Module</title> The calibration module is shown in Fig. 2. It contains three objectives: distribution prediction, distribution alignment and accurate next-item prediction. Distribution Prediction Given a session, we take it as the input of the backbone model f , and obtain the session representation h and the recommendation list RL . The distribution p of RL is computed at the same time. where ˆp is the predicted distribution which has the same dimension to p . We choose Kullback–Leibler (KL) Divergence as the objective function to make the prediction more accurate: where i ∈ {0, 1} corresponds to head and tail probability, respectively. In this stage, our goal is to train the FFN to predict the distribution by the session representation. Therefore, only parameters of FFN is updated while parameters of the backbone model is ﬁxed. Distribution Alignment From calibration perspective, the ideal distribution of recommendation list p is close to q to reﬂect the user’s preference towards tail items. Therefore, we align the predicted distribution ˆp generated by the same FFN towards q with the similar loss function: where ˆp is the predicted distribution by FFN mentioned before. Note that in this stage, the parameters of FFN is ﬁxed in order to update the parameters of the backbone model under the constraint of calibration. To further handle the imbalance problem in distribution, we add a weight based on the Calibration loss function: In this weighted loss function, we aim to reward the sessions that contain more tail items, because they are usually rare. Accurate Prediction Besides calibration loss, the goal of accurate prediction is still important. Therefore, we preserve the Cross Entropy loss function in the calibration module. The ﬁnal loss function can be written as: where y = 1 if item i is the next item x of session s, and ˆy is the predicted probability of item i. <title>3.4 Curriculum Training</title> In this section, we introduce the training procedure of our model and the generation of calibrated recommendation list. Concretely, we propose a curriculum training strategy with a threshold-based data organization and pre-train ﬁne-tune training procedure. Moreover, we introduce the extension of threshold-based data organization. Threshold-based Data Organization In our model, the ratio of tail items in the ongoing session performs as the goal of calibration module. Due to the popularity bias, the ratio does not subject to a balanced distribution, where sessions with low q (1) occupy the majority. Therefore, it is necessary to organize the training sessions to avoid the imbalance. As shown in Fig. 3, we split the training set into two categories according to sessions’ q (1) value: which means if the ratio of tail items in the session s is greater than θ, it belongs to the Tail subset S , and the Head subset S otherwise. When θ = 0, sessions which do not have tail items are moved to S . Only sessions in S will be further processed by our calibration module. To train the model, we follow a pre-train and ﬁne-tune paradigm. We ﬁrst train the parameters of the backbone model on the entire training set S to learn the information of item transition following the traditional sequence-toitem training paradigm of SBR [12, 14, 16]. After pre-training, we make a copy of the trained backbone model for S . Then we optimize the model by our calibration algorithm on S . Note that for S , there is no further execution so that outputs of sessions in S are exactly the same as the backbone model. For recommendation and evaluation, given a session s, the recommendation list is also based on its q . If q (1) ≤ θ, the output is from the original backbone model. Otherwise, the output is from our optimized model. Finally, the Top-N recommendation list is generated. K-fold Extension The data organization can be further extended on the . When θ = 0, S contains sessions that have at least one tail item. For these sessions, we split them into K diﬀerent subsets according to the ratio of tail items (i.e., the value of q (1)): where index(s) ∈ {1, 2, ..., K} is the subset index which the session s belongs to, and dxe represents the ceil function which provides the lowest integer t that satisﬁes t ≥ x. We assign a separate backbone model which is initialized by the pre-trained backbone model for each subset, and there are K + 1 models including the backbone model of S . During the training stage, each model is trained separately on the subset. Similarly, the recommendation list is also generated by the corresponding model according to index(S). <title>4 Experiments</title> To demonstrate the eﬀectiveness of our model, we conducted experiments on real-world datasets and compared our model with existing models. <title>4.1 Dataset</title> We adopted two commonly-used benchmark datasets to evaluate the performance of our model. The ﬁrst one is Yoochoose which was released in Recsys 15 and collected from e-commerce platform. The other one is Lastfm which is used for music artist recommendation. We further pre-processed the original datasets following [12, 15, 18]. The statistics are listed in Table 2. <title>4.2 Comparison Models</title> We select the following methods as baselines: GRU4REC [7] which applies RNNs to model sequential behaviors and employs ranking-based loss functions. NARM [12] utilizes attention mechanism with RNNs to learn user’s main purpose and sequential behavior. It is selected as the backbone model. SR-GNN [14] is one of the state-of-the-art SBR model which applies graph neural networks to learn item transition. It is also the backbone of our model. Tail-Net [22] designs preference mechanism for long-tail session-based recommendation. NISER [16] applies normalization methods to mitigate the long-tail eﬀect. And our CSBR model include two types according to the selection of backbone models: CSBR(N) applies NARM as its backbone model. CSBR(S) applies SR-GNN as its backbone model. <title>4.3 Evaluation Metrics</title> We evaluate models from accuracy and long-tail perspectives. Accuracy-based metrics measure the model performance by the ranking position of the user’s next behavior in the recommendation list. Following previous work[12, 14], we use Recall and MRR as evaluation metrics. in the Top-N items of list. where 1() is an indication function whose value equals to 1 when the condition in brackets is satisﬁed, and 0 otherwise. MRR@N is another important metric which takes the rank of correct items into account. The score is computed by the reciprocal rank when the rank is within K, otherwise the score is 0. The long-tail based metrics measure the performance on how many tail items can be recommended to users. Following [22], we use Coverage@N, TailCoverage@N, and Tail@N as the evaluation metrics: Coverage@N (Cov@N) is a widely used metric. It represents the proportion of recommended items towards entire item set. TailCoverage@N (TCov@N) is similar to Coverage but only computes the proportion of tail items that recommended on the test set. <title>4.4 Experiment Setup</title> We use the grid search to ﬁnd the optimal hyper-parameters on the validation dataset which is 10% of the training set. The dimension of all hidden states is set to 100, following [14]. We tune the learning rate from {0.1, 0.01, 0.001}. We use the Adam optimizer with the batch size of 128. We report the performance under the model parameters with the optimal prediction accuracy in the validation set. For the hyper-parameter λ, K and θ, we report the performance when λ = 1, K = 1 and θ = 0, and analyze the performance change in the next section. For the Top-N recommendation, we set N = 20 following [12, 14, 22]. <title>5 Results and Analysis</title> We aim to answer the following research questions by conducting the experiments: RQ1: Does our CSBR model outperform the state-of-the-art session-based recommendation models? RQ2: Does our CSBR model generate a more calibrated recommendation list compared to the baselines? RQ3: How well does CSBR model perform when hyper-parameters change? RQ4: How do the calibration module and curriculum training strategy of CSBR model inﬂuence the performance respectively? <title>5.1 RQ1: Overall Performance</title> NARM. For example, NARM achieves 51.77%, 40.34% and 3.84% in terms of Cov@20, TCov@20 and Tail@20 on the Lastfm dataset. While the performances of Cov@20, TCov@20 and Tail@20 are 71.90%, 65.29%, 5.57% for our CSBR(N), with the improvement of 38.88%, 61.84%, 45.05%, respectively. The performance comparison shows that our model can mitigate the popularity bias and expose more tail items compared to the existing SBR models which do not consider popularity bias. We also focus on the diﬀerence between the performances on the two datasets. On the Yoochoose dataset, the results of Rec@20 and MRR@20 are approximately 70% and 30%, respectively. However, on the Lastfm dataset, the accuracy performances are around 21% and 8%. For long-tail-based metrics, the performance on the Lastfm dataset is better than on the Yoochoose dataset. Take Cov@20 as an example, SR-GNN achieves 67.44% on the Lastfm dataset which is higher than 42.35% on the Yoochoose dataset. A possible reason is that the sequential patterns of behaviors on the Yoochoose dataset are more ﬁxed than on the Lastfm dataset. Fixed sequential patterns make recommendations more determinate, and easier to get higher accuracy. In contrast, relatively loose sequential patterns make it hard to achieve high accuracy, but more items are covered in recommendations. Finally, we show the performance of our K-fold extension strategy. We change K from {1, 2, 4, 8} to observe the performance, where K = 1 represents the original CSBR(S) model. The results are listed in Table 5. In general, the accuracy of recommendation declines with the increment of K. For example, it turns from 71.44% to 70.52% when K is changed from 1 to 8 on the Yoochoose dataset. In contrast, the ability to recommend tail items is enhanced by increasing K. When K = 2 and 4, the performance of Cov@20, TCov@20 and Tail@20 are improved compared to the original CSBR(S) model on the two datasets. Compared to K = 1, each subset generated by higher value of K contains fewer samples and distribution labels (e.g., q and p ). The calibration module predicts the distribution more precisely by narrowing the scope of the distribution label, especially for sessions with a high ratio of tail items. When K = 1, the calibration module tends to predict and align to the low value of q (1) because of the data imbalance. The K-fold extension strategy avoids this problem, leading to the improvement of long-tail recommendation. In exchange, the accuracy of recommendation is sacriﬁced. Note that the value of K is not as large as it should be. When K = 8, the model achieves the worst performance in terms of both accuracy and long-tail recommendation. This is because distribution labels in each subset become homogeneous, which turns out an imbalance problem. In this situation, the calibration module cannot learn information but brings only noise, leading to poor performance. <title>5.2 RQ2: Calibrated Recommendation</title> Sec. 3.4), because our CSBR model optimizes parameters on this subset by a curriculum training strategy. To evaluate the calibration, we apply the C metric to compute the KL-Divergence between the recommendation list distribution and session distribution: performance (37.65% for CSBR(S) compared to 26.83% of SR-GNN and 30.37% of NISER). For each recommendation list, our model can provide more tail items compared to NISER and SR-GNN (28.92%vs.23.29%vs.22.28%). On the Lastfm dataset, our CSBR(S) also achieves the best performance, as shown in Fig. 4. Note that our CSBR model also performs best in terms of C The results imply that methods such as SR-GNN and NISER do not provide enough tail items in the recommendation list compared to the ongoing session because these methods do not consider the proportion of tail items in sessions. In contrast, our CSBR framework matches the ratio of tail items in the recommendation list with the session, improving the performance on long-tail-based metrics. <title>5.3 RQ3: Parameter Inﬂuence</title> In the CSBR framework, the parameter λ (see Eq. 12) controls the importance of the calibration loss function, and θ is the threshold for training and test dataset splitting (see Sec 3.4). Therefore, we would like to analyze the performance change on diﬀerent value settings of λ and θ. In this section, we analyze the performance change under diﬀerent settings of the hyper-parameter λ. We tune λ in {1, 2, 5, 10, 50, 100, 200, 500}, and the results are shown in Table 7 and 8. With the increase of λ, the accuracy of recommendation remains stable at the beginning. On the Yoochoose dataset, Rec@20 changes from 71.34% to 71.17% and MRR@20 decreases from 31.91% to 31.85%, when λ is tuned from 1 to 10. The tendency is similar on the Lastfm dataset. For the long-tailbased metrics, the performance increases with the change of λ, and decreases when λ is set to a large value (e.g., λ = 500). For example, on Yoochoose dataset, our CSBR(S) model achieves 6.74%, 7.4% and 5.41% of Tail@20 when λ = 1, 50, 500, respectively. The larger λ can emphasize more on the calibration loss function (see Eq. 12) in the calibration model. Therefore, the Cov@20, TCov@20 and Tail@20 all increase at the beginning. However, extremely large λ causes a negative impact on both accuracy-based and long-tail-based performance. As shown in Table 7 and 8, the performances of Cov@20, TCov@20 and Tail@20 all decrease when λ = 500. It is normal that over-emphasis on calibration can lead to a decrease in prediction accuracy, but the performance of long-tail metrics also declines. One of the possible reasons is that the long-tail performance is aﬀected by the accuracy of prediction. If a model cannot accurately predict the next behavior, it still suﬀers from popularity bias. Head items are easy to predict because of adequate sequential information, while prediction on tail items distinguishes the abilities of recommendation models. The comparison among SR-GNN, NARM and GRU4REC in Table 3 and 4 also demonstrates this opinion, where SR-GNN outperforms NARM and GRU4REC in terms of both accuracy-based metrics and long-tail-based metrics. We also would like to analyze how the performance changes when θ becomes larger. We make θ change from 0 to 1, and the performance on ﬁve metrics is displayed in Fig. 5. As shown in Fig. 5 (a) and Fig. 5 (b), the accuracy of recommendation does not change too much, especially on the Yoochoose dataset. This is because of the ﬁxed sequential patterns on the Yoochoose dataset. For long-tail-based metrics, it becomes diﬀerent. As shown in Fig. 5 (c) and Fig. 5 (d), the performance in terms of long-tail-based metrics increases at the beginning of the tuning process on two datasets in general. If we choose a relatively higher value for θ, the long-tail performance decreases dramatically. A possible reason that can explain this phenomenon is the session distribution on the training set. We make a statistic according q (1) of sessions in the training set, and calculate the proportion of diﬀerent levels of q (1) value. The result is shown in Fig. 6. When θ becomes much higher, there are fewer sessions for our CSBR model to train. Though the sessions on S can be calibrated to a certain degree, other sessions are sacriﬁced and the performance is decreased. On the other hand, high value of θ makes the subset S close to the entire training set, leading to a similar performance of the backbone model. In addition, we also conduct an analysis on the performance change of C when θ changes. The performance on two datasets is shown in Fig. 7. The solid and dashed lines stand for the performance in terms of C and Tail@20, respectively. In general, a larger θ results in a larger C . When θ comes to a high value (e.g., 0.8 or 0.9), the Tail@20 is not improved equivalently while C becomes larger. This is because either our model or the SR-GNN model aims to achieve accurate predictions, and popularity bias is included. Compared to the backbone SR-GNN model, our CSBR(S) framework achieves higher Tail@20 and lower C under all settings of θ, showing the eﬀectiveness of our calibration module. This can also explain the decrease of overall performance in terms of longtail-based metrics with the increase of θ as shown in Fig. 5 (c) and Fig. 5 (d). The higher θ does not earn the equivalent improvement on Tail@20, but sacriﬁces the sessions whose q (1) are lower than θ. Though our CSBR(S) model can obtain improvement on the subset S , the improvement cannot compensate for the sacriﬁce because the size of S is relatively small compared to the whole training set. On the Lastfm dataset, the Tail@20 increases at the beginning. A possible reason that can explain this phenomenon is that sequential patterns on the Lastfm dataset are less ﬁxed than the Yoochoose dataset, which makes it easier to recommend tail items (see Sec 5.1). Therefore, the gain of increasing θ is greater than its side eﬀect, leading to the improvement of the overall performance. <title>5.4 RQ4: Ablation Study</title> In this section, we answer the question RQ4 about the contribution of the calibration module and the separately training mechanism of our CSBR model. Therefore, we conduct an ablation study of CSBR(S) model by comparing with backbone model and its variants: w/o Calib: removing the alignment of our model (λ = 0 in Eq. 12). w/o CE: removing the Cross Entroy loss function (L in Eq. 12) in our model. w/o CT: removing the curriculum training strategy (see Sec 3.4) in our model, where the calibration module is computed on all sessions. suﬀers from the imbalance problem. As shown in Fig. 6, most sessions do not have tail items. When removing the curriculum training module, these sessions interfere with the alignment of the CSBR model, and “no tail items” becomes the goal of alignment. It is optimal for the calibration module because the loss function is minimized, but it is harmful to the entire model performance of calibration and long-tail recommendation. Removing curriculum training achieves even worse performance compared to the backbone SR-GNN model in terms of the long-tail-based metrics, which can demonstrate the importance of curriculum training. Under the curriculum training setting, the cross entropy and calibration loss function both contribute to the improvement of long-tail recommendation. On the one hand, the cross entropy loss function ensures the ability of accurate prediction on tail items, which lacks enough knowledge in the traditional training process. The accurate prediction leads to higher coverage on tail items. For example, on the Lastfm dataset, the Cov@20 decreases from 78.60% to 65.42% by removing the cross entropy function, and TCov@20 decreases from 73.25% to 56.78%. The result indicates that it is not exactly contradictory between accuracy and coverage. On the other hand, removing calibration is a special set of hyper-parameter λ = 0. When λ = 0, there is almost no diﬀerence on the Yoochoose dataset. On the Lastfm dataset, the performance of long-tail-based metrics decreases (e.g., Tail@20 decreases from 5.74% to 5.08%). A possible reason is the ﬁxed sequential patterns on the Yoochoose dataset (see Sec 5.1), which leads to the high accuracy and low coverage. A lower value of λ does not exert suﬃcient inﬂuence on the training process, while enlarging the λ can improve the performance of long-tail recommendation (e.g., when λ = 10 in Table 7). On the Lastfm dataset, sequential patterns are not as ﬁxed as on the Yoochoose dataset, leading to the decline of performance by removing the calibration module. In conclusion, all of these modules contribute to the improvement of our CSBR(S) model. Concretely, the curriculum training strategy helps our model avoid the imbalance problem to a certain degree by removing the sessions that do not have tail items. Meanwhile, the calibration module with cross entropy loss function ensures the coverage on tail items, and the calibration loss functions improve the ratio of tail items recommended to users. <title>6 Discussion</title> In summary, experiments on benchmark datasets show that our CSBR model achieves competitive prediction accuracy with the state-of-the-art SBR models, and outperforms these models in terms of long-tail-based metrics. In detail, the calibration module provides recommendation lists that contain more tail items compared to baselines, and the curriculum training strategy helps the calibration module avoid the imbalance problem. Though our work achieves improvement on long-tail session-based recommendation, there are some limitations in our work. Our work optimizes the recommendation list according to the number of tail items in historical clicks in the session. For sessions that only contain head items, it is contradictory when considering long-tail recommendation and calibration. The long-tail recommendation requires models to provide more items for users, because users may feel bored after receiving too many head items. However, a calibrated recommendation list should not contain tail items because no tail items have occurred in the session. Because of the contradictory situation, we do not apply our CSBR model to these sessions. Meanwhile, for sessions containing a large number of tail items (e.g., (1)>0.7), our model may not oﬀer a corresponding amount of tail items (see Sec 5.3.2) because of the constraint of accurate prediction. Though our CSBR model outperforms baselines in terms of long-tail-based metrics and calibration, the performance is still not satisfying compared to the ratio of tail items in the ongoing session. Finally, we proposed a curriculum training strategy in our CSBR model. Concretely, it includes a threshold-based data organization method and a pre-train and ﬁne-tune paradigm. Despite of the eﬀectiveness of our model, the threshold-based data organization is relatively simple. The data splitting strategy also increases the size of model (e.g., there are K + 1 separate backbone models when applying K-fold extensions). Moreover, knowledge transfer among curriculum is not deeply investigated in our model. <title>7 Conclusion and Future Work</title> between long-tail and calibrated recommendations for these sessions. In this scenario, we aim at providing recommendation lists with long-tail items supported by theory and data. In addition, for sessions containing a large number of tail items, our model may not oﬀer a corresponding amount of tail items. Disentangling users’ intentions into preference and conformity is a possible way to improve the calibration and long-tail recommendation for these sessions. Finally, it is worth investigating the application of the advanced curriculum learning framework. We intend to utilize a dynamic scoring mechanism on training samples rather than setting a threshold to enhance the performance and reduce the size of the model. In addition, applying transfer learning to transfer information among curriculum is also one of our future directions. Statements and Declarations. This work is funded by National Natural Science Foundation of China (under project No. 61907016) and Science and Technology Commission of Shanghai Municipality, China (under project No. 19511120200 and No. 21511100302) for sponsoring the research work. All authors certify that they have no aﬃliations with or involvement in any organization or entity with any ﬁnancial interest or non-ﬁnancial interest in the subject matter or materials discussed in this manuscript. <title>References</title> [36] Hidasi, B., Karatzoglou, A.: Recurrent neural networks with top-k gains for session-based recommendations. In: Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, pp. 843–852 (2018). https://doi.org/10.1145/3269206.