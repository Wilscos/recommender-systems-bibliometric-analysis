Developers often search for reusable code snippets on generalpurpose web search engines like Google, Yahoo! or Microsoft Bing. But some of these code snippets may have poor quality in terms of readability or understandability. In this paper, we propose an empirical analysis to analyze the readability and understandability score from snippets extracted from the web using three independent variables: ranking, general-purpose web search engine and recommended site. We collected the top-5 recommended sites and their respective code snippet recommendations using Google, Yahoo!, and Bing for 9,480 queries, and evaluate their readability and understandability scores. We found that some recommended sites have signicantly better readability and understandability scores than others. The better-ranked code snippet is not necessarily more readable or understandable than a lower-ranked code snippet for all general-purpose web search engines. Moreover, considering the readability score, Google has better-ranked code snippets compared to Yahoo! or Microsoft Bing. readability, understandability, code snippets, web search engines Code snippets (or code examples) are some lines of reusable source code to show how to solve a specic programming problem [12]. Developers often search for reusable code snippets on the web [22], especially on programming sites as StackOverow [11], or using general-purpose web search engines like Google, Yahoo! or Microsoft Bing to nd examples for their respective programming tasks [16]. Those code snippets may be straightforwardly reused in software under development, to decrease the time to perform the programming tasks and accelerate the development process [7]. Although search engines like Google have over 200 dierent factors to rank the results [6], the top-ranked pages could have poor quality code examples in terms of readability and reusability features [8]. Although Google is the most popular general-purpose web search engine having more than 90% of market share, the other generalpurpose web search engines as Yahoo! and Microsoft Bing are available to search code snippets, and the developers could be interested to know how dierent web search engines ranks top-level readable and understandable code snippets. Another issue consists in evaluating how readability inter-relates to understandability. While readability is associated to reading and comprehending the syntax, understandability is associated to the semantic aspect of code snippets, e.g., the statements, beacons or motifs [17]. In this context, we propose an investigation to assess how general-purpose web search engines as Google, Microsoft Bing, and Yahoo! rank code examples using features, such as readability and understandability. The study is driven by the following research questions: • RQ #1)How are code snippets recommended by generalpurpose web search engines ranked in terms of readability and understandability? This research question analyses if code snippets with the higher score in readability and understandability features are generally ranked in a specic position in the rank interval [1,5]. • RQ #2)How do general-purpose web search engines compare to each other in terms of readability and understandability features? This research question compares recommended code snippets by Google, Microsoft Bing, and Yahoo!, to verify if any of these general-purpose web search engines recommend code snippets with the higher score in readability and understandability. • RQ #3)How do recommended sites containing code snippets compare to each other in terms of readability and understandability features? This research question compares the code snippets from the ve most popular sites recommended by the general-purpose web search engines to verify if any site has code snippets with the higher score in readability and understandability. The paper is organized as follows. Section 2 shows a motivating example. Section 3 discusses the related researches. Section 4 presents the study design proposed to collect the web pages, code snippets, and metrics. The results are reported and discussed in Section 5. Section 6 presents the qualitative discussion about the results. Section 7 has the threats that could aect the validity of this study. And nally, Section 8 summarizes our observations in lessons learned and outlines directions for future work. The motivational example was extracted from Hora’s study [9]. In a search for File.mkdirs examples in Google, the Figures 1 and 2 in [9] shows two recommended sites (Tutorialspoint and JavaTutorialHQ) and their respective code snippets. The suggestion from Tutorialspoint has worst readability and reusability metric values, but it is better ranked by Google compared to the JavaTutorialHQ solution. A possible explanation is because Tutorialspoint has natural language explanations similar to the input query. To verify how Google rank the code snippets without natural language, Hora[9]has build a site with web pages having only code snippets for some tasks. After Google has indexed the new site, the same Google search was performed on that new web site, and then the JavaTutorialHQ code snippet was better ranked compared to the Tutorialspoint snippet. The motivational example raises two hypotheses: • H1)A better-ranked code snippet could not necessarily have higher readability or understandability score. • H2)Some recommended sites could have overall better readability or understandability score than other sites. Some works have been proposed to analyze the readability of source code snippets. In a related paper with ours, Hora[9]investigated how Google is ranking the code snippets in terms of the readability and reusability features. This research constructed a new site with 1,000 web pages with code snippets from 100 Java APIs. After Google indexed its web pages, some input queries were performed using Google narrowing that site. The objective was to identify how Google ranked the web pages containing only code snippets. Google does not necessarily prioritize code snippets with high readability or reusability metric values, but web pages with multiple code snippets would probably be ranked rst. Our research has dierences because we consider other general-purpose web search engines, such as Microsoft Bing and Yahoo!, and also consider the understandability feature. Other works uses readability feature to improve the code snippet overall score. Hora[10]constructed the API Sonar toolranking code snippets with readability feature proposed by Scalabrino et al. [18]. Moreno et al. [13]developed the Muse approach to rank code examples using readability feature proposed by Buse and Weimer[2]. These related works shows how readability is a well used feature to rank code snippets. Our work diers because it is not ranking code snippets, but compares code snippets readability and understandability features in three independent variables: ranking, recommended websites, and general-purpose web search engine. Some related works reinforce the relevance of our study: Treude and Robillard[20]shows that only 49% of StackOverow code snippets are fully self-explanatory, which explains why developers are interested in explanations accompanying StackOverow code snippets [14]. Our work helps to compare the readability and understandability scores between StackOverow accepted answers and other sites, to identify potential sites with more comprehensive code snippets. This section presents the overall approach to answer the research questions. The major steps are: (1) Select Input Queries, (2) Collect Top-n Web Pages, (3) Extract Code Snippets, (4) Calculate Metrics, and (5) Analysis Methods. The details of each step are in the following subsections. A replication package, including the tools, scripts, evaluations and the instructions for reproduction is available [5]. In this step, we selected 10,000 input queries performed by users on CROKAGE tool. CROKAGE is a code search engine that extract code snippets written in Java language and their explanations from StackOverow [4]. These input queries were performed by users from more than 80 countries, searching for programming tasks . We removed duplicated queries and queries manually labeled as not applicable (e.g., non-Java programming languages) by the CROKAGE research [4]. This step consists in collecting the top-5 web pages recommended by Google, Yahoo!, and Microsoft Bing for each input query. These queries receive the additional tokens "example in java". The "example" token aims to nd code snippets instead of only explanations [19]. The token in java aims to nd code snippets written in Java language. General queries as how to add element in a list could return code snippets written in other popular programming languages like Python or Javascript [9]. To collect the recommended web pages, we used regular expressions in each general-purpose web search engine, extracting the links to recommend web pages from their html tags . We discarded queries with less than 5 web pages recommendations, resulting in 9,480 queries and a total of 47,400 links to web pages from 5,355 distinct sites. Figure 1 shows the ve most popular recommended sites from each web search. This step consists in selecting the ve most popular sites for the input queries and creating regular expressions to extract the code snippets from their html tags. The ve popular web sites are: • stackoverow.com • www.geeksforgeeks.org • www.javatpoint.com • www.tutorialspoint.com • www.codegrepper.com For stackoverow.com, we extracted the code snippets from each accepted answer written in Java, because the questioner should have the best judgement of whether the answer solves the problem [23]. In the other sites, the regular expressions search for source code and natural language description with the tokens "example" and "Java" inside the html tags. The www.javatpoint.com site has specic CSS class for each programming language, making it easier to nd Java source code. This step consists in calculating the readability and understandability scores on the extracted code snippets. To measure readability, this research uses the prediction model proposed by Scalabrino et al. [18]. This model has been used in other works to extract readability scores from code snippets [9] [10], and the classes inside GitHub repositories [15]. This modelincludes a set of metrics including comments, identiers consistency, textual coherence, number of meanings, and concepts. The output score is a real number in the interval [0,1] where 0 meanslow readability and 1 means high readability. Figure 1: Most popular sites with recommended web pages ranked on top-5 for 9,480 queries performed on Google, Microsoft Bing and Yahoo! To measure understandability, the selected code-based metric is cognitive complexity proposed by Campbell[3]with the source code is available in SonarSource tool. The cognitive complexity could be used to measure some understandability aspects [1] [21]. The output score is a natural number, where 0 meanshigh understandability, and if the score is equal or higher than 15, is considered low understandability. To measure understandability in the same interval [0,1] as readability, we propose the metric as follows:( 𝑢𝑛𝑑𝑒𝑟𝑠𝑡𝑎𝑛𝑑𝑎𝑏𝑖𝑙𝑖𝑡𝑦(𝑐𝑠) =1 −if #𝑐𝑐 < 15 #cc is the complexity cognitive value extracted from SonarSource tool for the code snippet𝑐𝑠. The #mcc is the maximum recommend complexity cognitive value, #mcc = 15. If a code snippet reaches #cc >= 15, the metric output will be 0. To compare the distribution between the dependent variables (readability score, understandability score) and independent variables (general-purpose web search engine, ranking and recommended sites), we apply the analysis of variance (ANOVA) using 5% condent level (i.e., p-value<0.05). To nd which groups are signicantly dierent from each other, we use Tukey test. This analysis have three groups on general-purpose web search engines (Google, Microsoft Bing! and Yahoo), ve groups on ranking (top-1 to top-5), and ve groups on the selected sites (stackoverow, geeksforgeeks, javatpoint, tutorialspoint and codegrepper). In this section, the results are shown according to each research question. To interpret the Tukey test in Figures 2,3 and 4, there is signicant dierence in mean loss for each group which the 0.00 value is outside of the condence interval. RQ #1) How code snippets recommended by general-purpose web search engines are ranked in terms of readability and understandability? Extracting the ANOVA on ranking independent variable, we obtained p-value = 0.0034 for readability and p-value = 0.0003 for understandability. This result means that there is signicant dierences between ranking code snippets in readability and understandability. The Figure 2 shows the Tukey Test with a small eect size, i.e., -0.02 to 0.01 in readability and -0.01 to 0.02 in understandability. The Top-2 code snippet shows overall better readability and understandability score than Top-1, Top-4 and Top-5 code snippets, as Top-3 shows better understandability score than Top-1 and Top-2 code snippets. RQ #1 Answer:The ranking analysis conrms theH #1hypothesis, i.e., a better-ranked code snippet is not necessarily more readable or understandable than a lower-ranked code snippet. The eect seems small, but we found Top-2 code snippets with overall better readability and Top-3 with overall better understandability score. RQ #2) How do general-purpose web search engines compare to each other in terms of readability and understandability features? Extracting the ANOVA on the general-purpose web search engine independent variable, we obtained p-value = 1.207e-12 for readability and p-value = 0.0364 for understandability. This result means that there are signicant dierences between general-purpose web search engine code snippets in readability and understandability. The Figure 3 shows the Tukey Test with small eect, i.e., -0.02 to 0.02 in readability and -0.01 to 0.005 in understandability. In readability, the Google code snippets have overall better readability than Microsoft Bing and Yahoo!. In understandability, Google shows a small dierence to Microsoft Bing. RQ #2 Answer:Google has a better overall readability score compared to Microsoft Bing and Yahoo, but with a small eect. Google has a better overall understandability score compared to Microsoft Bing. RQ #3) How do recommended sites containing code snippets compare to each other in terms of readability and understandability features? Extracting the ANOVA on recommended site independent variable, we obtained p-value = < 2.2e-16 for readability and p-value = < 2.2e-16 for understandability. This result means that there is signicant dierences between recommended sites code snippets in readability and understandability. The Figure 4 shows the Tukey Test with a medium eect for readability, i.e., -0.15 to 0.10 and small eect on understandability, i.e., -0.04 to 0.08. In readability, geeksforgeeks shows the higher overall score, and tutorialspoint has overall better understandability. Figure 3: Tukey Test condence intervals (x-axis) with the dierences between web search engines groups (y-axis) Figure 4: Tukey Test condence intervals (x-axis) with the dierences between recommende d sites groups (y-axis) RQ #3 Answer:The recommended sites analysis conrms the H #3hypothesis, i.e., there is signicant dierences between the readability and understandability score between the recommended sites. geeksforgeeks has the overall best readability score, and tutorialspoint has the overall best understandability score. The geeksforgeeks is a tutorial programming site containing code snippets with one comment per line of code, and high cohesion with one concept. These features contributes to produce a higher score on readability metric. For example, on the input query "How to append to a string?", the geeksforgeeks code snippethad readability score = 0.94, and the tutorialspoint code snippethas readability score = 0.44. The geeksforgeeks code have more comments, and tutorialspoint code snippet have more concepts in the same line, e.g. new BueredWriter(new FileWriter("lename")), instead of geeksforgeeks code snippet with one concept per line. The understandability metric used in this research had a low eect in all analyses between independent variables. Figure 5 shows 58.3% of the code snippets have maximum understandability scores. Many code snippets has few lines of API calls, without if/else conditions or for/while loops. This result suggests that the understandability feature is more feasible to be used in complete classes from git repositories. Number of sites to extract code snippets: we selected ve popular websites, which represent between 34% and 38.1% of the recommended sites for the input queries employed in this research. But the results could have variations if we increase the recommended sites. Multiple code snippets in same web page: in this approach, we extracted the rst code snippet of each recommended site (for StackOverow, we extracted the code snippet from the accepted answer). The developer would test the rst recommended code snippet on the web page. But some sites have more than one code snippet on the same web page, which a heuristic could be employed to extract readability and understandability scores from these multiple code snippets. Readability and Understandability score precision: we carefully selected the readability and understandability state-of-art tools, but their score could have some false positives/negatives, or even the readability metric score used in this research would not be reliable in the range [0,416, 0,600] [15]. Queries modications: the addition of "example in java" tokens into each query could inuence the web-search engines to produce their rankings, i.e., the query without modications could produce a dierent ranking between the selected code snippets. An investigation about tokens additions is necessary for future works. Extract code snippets heuristics: compared to the other sites, StackOverow has a dierent heuristic on extracting code snippets, using the accepted answer. These heuristics require a sensitivity analysis for future works. In this empirical study, the recommended site independent variable has the highest eect on the readability score. Programming tutorial sites as geeksforgeeks generally have code snippets in a specic format, but Q&A sites as StackOverow has many users sharing code snippets, which could lead to dierent code snippet formats. The readability standard deviation score on geeksforgeeks is 0.11, and in StackOverow is 0.20, which conrms more variance on Q&A sites. The understandability feature have low eect in most of the scenarios, because most of them has few statements, beacons or motifs. These results provide insights for future improvements. A qualitative study could be conducted to better understand the reasons for the variability in the readability and understandability score. A complementary study could include more sites, or even create a general regular expression to automatically extract code snippets from a large variety of sites. The general-purpose web search engines could be compared to specic code search engines, e.g., StackOverow. Moreover, a heuristic could be proposed to evaluate the websites with multiple code snippets.