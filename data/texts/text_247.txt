provide interesting information that matches users’ preferences, and thereby can help alleviate the information overload problem. In the past two decades, recommender systems have been extensively studied, and lots of recommendation methods have been developed [1]. These methods usually make personalized recommendations based on user’s preferences, item features, and user-item interactions. Some recommendation methods also leverage other additional information such as social relationships among users (e.g., social recommendation), temporal data (e.g., sequential recommendation), and location-aware information, e.g., POI (short for ’Point-of-Interests’) recommendation. Recommendation technologies usually make use of various information to provide potential items for users. In realworld scenarios, the recommender system suggests items according to the user-item interaction history, and then receives user feedback to make further recommendations. In other words, the recommender system aims to obtain users’ preferences from the interactions and recommend items that users may be interested in. Toward this end, the early recommendation research mainly focuses on developing content-based and collaborative ﬁltering-based methods [2], [3]. Matrix factorization is one of the most representative traditional recommendation methods. Recently, motivated by the quick developments of deep learning, various neural recommendation methods have been developed in recent years [4]. Nevertheless, existing recommendation methods usually ignore the interactions between a user and the recommendation model. They cannot effectively capture the user’s timely feedback to update the recommendation model, thus usually lead to unsatisfactory recommendation results. In general, the recommendation task can be modeled as such an interactive process - the user is recommended an item and then provides feedback (e.g., skipping, click or purchase) for the recommendation model. In the next interaction, the recommendation model learns from the user’s explicit/implicit feedback and recommends a new item to the user. From the user’s point of view, an efﬁcient interaction means helping users ﬁnd the accurate items as soon as possible. From the model perspective, it is necessary to balance novelty, relevance, and diversity in the multiturn of recommendations. The interactive recommendation approach has been successfully applied in real-world recommendation tasks. However, it often suffers from some problems, e.g., cold-start [5] and data sparsity [6], as well as the challenges, e.g., interpretability [7] and safety [8]. As a machine learning area focusing on how intelligent agents interact with the environment, Reinforcement Learning (RL) provides potential solutions to model the interactions between users and the agent. The recent success of RL has boosted research on artiﬁcial intelligence [9], [10]. In particular, Deep Reinforcement Learning (DRL) [11] has powerful representation learning and function approximation properties to address the challenges in artiﬁcial intelligence. And it has been applied across various ﬁelds, e.g., games [12], robotics [13], and networking [14]. Recently, the application of RL to solve recommendation problems has become a new research trend in recommendation research [15], [16], [17]. Speciﬁcally, RL enables the recommender agent to constantly interact with the environment (e.g., users and/or logged data) for learning optimal recommendation policy. In practice, RL-based recommender systems have been applied to many speciﬁc scenarios, such as e-commerce [18], e-learning [19], movie recommendation [20], music recommendation [21], news recommendation [22], job skill recommendation [23], health care [24], and energy optimization [25]. To facilitate the research about RL-based recommender systems, we summarize existing relevant solutions to the recommendation issues, systematically analyze the challenges of applying RL in recommendation approaches, and discuss future potential research directions. From the RL perspective, we review existing research work covering environment construction, prior knowledge, reward function deﬁnition, learning bias, and task structuring. The environment construction may alleviate the explorationexploitation trade-off. Prior knowledge and reward deﬁnition are essential to make the recommendation decision. Moreover, task structuring can address the curse of dimensionality well. From the application perspective, we also provide a comprehensive survey of RL-based recommender systems following value-function, policy search, and ActorCritic, respectively. Note that [26] also provides a review of the RL- and DRL-based algorithms developed for recommendations, and present several research directions in recommendation list, architecture, interpretability, and evaluation. Besides, [27] provides an overview of DRL-based recommender systems mainly according to model-based methods and model-free algorithms, and highlights some open issues and emerging topics of DRL-based recommendation. Differing from [26] and [27], we overview existing (D)RL-based recommendation methods according to other classiﬁed algorithms (i.e., value-function, policy search, and Actor-Critic), and then analyze the challenges of applying (D)RL in recommender systems. The main contributions made in this work are as follows: developed for ﬁve typical recommendation scenarios. For each recommendation scenario, we provide detailed descriptions of representative models, summarize speciﬁc RL algorithms used in the literatures, and make the necessary comparison. in recommender systems, including environment construction, prior knowledge, reward function deﬁnition, learning bias, and task structuring. lenges of this ﬁeld, and suggest some possible future directions for the research and application of recommender systems. The remaining parts of this paper are structured as follows. Section 2 introduces the background of RL, deﬁnes related concepts, and lists commonly used approaches. Section 3 presents a standard deﬁnition of the RL-based recommendation method. Section 4 provides a comprehensive review of the RL algorithms developed for recommender systems. Then, Section 5 discusses the challenges and relevant solutions of applying RL in recommender systems. Next, Section 6 discusses various limitations and potential research directions of RL-based recommender systems. Finally, Section 7 concludes this study. Different from supervised and unsupervised learning, RL focuses on goal-directed learning that maximizes the total reward achieved by an agent when interacting with its environment. Trial-and-error and delayed rewards are the two most important characteristics to distinguish RL from the other types of machine learning methods. In general, RL algorithms can be classiﬁed into two main groups, i.e., model-free algorithms and model-based algorithms. The model-free algorithms directly learn the policy without any model of the transition function, whereas the model-based algorithms employ a learned or pre-determined model to learn the policy. Based on the actions conducted by the agent, RL algorithms fall into the following major groups, i.e., value-function methods, policy search methods, and Actor-Critic methods. In this section, we present a short review of existing RL methods based on the second classiﬁcation taxonomy. 2.1 Markov Decision Process In RL, the agent learns from its interactions with the environment to maximize the total reward for sequential decision making. As shown in Figure 1(a), the learning process of RL can be formulated as a 5-tuple Markov Decision Process (MDP) < S, A, P, R, γ >, where S denotes a ﬁnite set of states, A denotes a ﬁnite set of actions, P is a state transition probability matrix that maps S × A × S into probabilities in the rang [0, 1], R denotes the reward function, and γ ∈ [0, 1] is a discount factor of the reward. At each time step t, the agent receives the environment states S, · · · , S∈ S and selects a corresponding action A∈ A. Then, the agent receives a numerical reward R∈ R and makes itself into a new state S. Thus, the agent and MDP together form a sequence as follows [28]. where T is the maximum time step in a ﬁnite MDP. Formally, for any s, s, S, S∈ S, r, R∈ R, and a, A∈ A, the state transition probability matrix P is deﬁned as follows, p(s, r|s, a).= P r{S= s, R= r | S= s, A= a}, where Sand Rdenote the state and reward at time t respectively. To maximize the return in the long run, the agent tries to select actions so that the cumulative reward it receives over the future is maximized. In this case, we need to introduce the concept of discounting. In general, the agent selects an action Ato maximize the expected discounted return G[28]: The discount factor γ affects the return. If γ = 0, the agent only maximizes immediate rewards then it reduces the return in the long run. As γ approaches 1, the agent takes future rewards into account more strongly. As a result, the return is increased. 2.2 Value-function Approaches Many traditional RL methods usually achieve a global optimum return by obtaining the maximal value in terms of the best action. These methods are called value-function approaches, which utilize the maximal value to construct the optimal policy indirectly. Intuitively, the maximal value is generated by the best action afollowing the optimal policy π, that is, the state-value under an optimal policy equals the expected return for the best action from the state. This is the Bellman equation for the optimal state-value function v(s), or called the Bellman optimality equation deﬁned as: v(s) = maxE[R+ γv(S)|S= s, A= a] where E[·] denotes the expected value of a random variable given by following the optimal policy π. Similarly, the Bellman optimality equation for the actionvalue function q(s, a) is deﬁned by q(s, a) = E[R+ γ maxq(S, a)|S= s, A= a] Different from the optimal state-value function v(s), the optimal action-value function q(s, a) explicitly reﬂects the effects of the best action, which is more prone to be adopted by many algorithms. In general, the optimal value function is estimated by dynamic programming, Monte Carlo methods or temporaldifference (TD) learning, such as Dyna [29], Sarsa [28], Q-Learning [30], and Deep Q-Networks (DQN) [31]. The value-function approaches have a high utilization rate of sampling. Nevertheless, they are not suitable for complex application scenarios due to the slow convergence. 2.3 Policy Search Methods In contrast to value-function approaches, policy-search methods directly optimize the optimal policies, which are parameterized by a set of policy parameters θ. The expected return will be increased by iterative updates of the policy parameters as follows. [32]: where α refers to the step size that inﬂuences the learning rate. The policy-search methods can be split into two classes: gradient-free and gradient-based algorithms. The gradientfree algorithms deal with low-dimensional problems well. As one of the most popular RL algorithms, the gradientbased algorithms are more competent to solve complex issues. To search for the optimal policies, gradient-based algorithms learn the policy parameter with the gradient of some performance measure J(θ). Formally, their updates approximate gradient ascent in J (θ) by [28] where ∇J (θ) denotes a stochastic estimate that approximates the gradient of J(θ) in terms of θ. The policy gradient methods usually follow this equation. For policy-search methods, there is a challenge of tuning the policy parameter to guarantee performance improvement. The policy gradient methods address the challenge well, since they give an analytic expression for the gradient of J(θ). Besides, they provide an appropriate equation proportional to the policy gradient, which may need MonteCarlo based method of sampling their expectation that approximates the equation. Thus, the REINFORCE algorithm [33] that adopts the Monte-Carlo policy gradient method can be established by where the symbol ∝ refers to ”proportional to”, the distribution µ(s) is the on-policy distribution under the policy π, and the gradients are column vectors of partial derivatives with respect to the policy parameter θ. Some advanced algorithms have been proposed to address the shortcoming of policy-search methods. For example, policy gradient methods with function approximation [43] ensure the stability of the algorithms. By adjusting super-parameters artiﬁcially or adaptively, Proximal Policy Optimization (PPO) Algorithms [34] and Trust Region Policy Optimization (TRPO) [35] speed up the convergence of the algorithms. Moreover, Guided Policy Search (GPS) [36] utilizes the path optimization algorithm to guide the training process of the policy gradient method, and thereby improves its efﬁciency. The policy-search methods need a large number of samples to train and ensure the convergence of the algorithms. The cost of the algorithms limits their application in the real world. There are a set of algorithms that incorporates the advantages of value-function approaches and policy search methods, that is, they attempt to estimate a value function, meanwhile adopt the policy gradient to search in the policy space. The typical of these algorithms are Actor-Critic [37]. It combines the policy-based method (i.e., the actor) with the value-based approach (i.e., the critic) to learn the policy and value-function together. The framework of Actor-Critic algorithms is shown in Figure 2. The actor trains the policy according to the value function of the critic’s feedback, while the critic trains the value function and uses the TD method to update it in one-step. The one-step Actor-Critic algorithm replaces the full return of REINFORCE with the one-step return as [28] where w denotes the state-value weight vector learned by the Actor-Critic algorithm, which uses a learned state-value function ˆv(S, w) as the baseline. In recent years, many improved Actor-Critic algorithms have been developed, such as Asynchronous Advantage Actor-Critic (A3C) [38], Soft Actor-Critic (SAC) [39], Deterministic Policy Gradient (DPG) [40] and its variation Deep Deterministic Policy Gradient (DDPG) [41], as well as an improved DDPG algorithm called Twin Delayed Deep Deterministic policy gradient (TD3). Actor-Critic algorithms may solve the problem of sampling efﬁciency by experience replay. However, due to the coupling of value evaluation and policy updates, the stability of these algorithms is unsatisfactory. As can be seen in Table 1, a summary is given of these three kinds of RL algorithms. In a typical recommender system, suppose there are a set of users U and a set of items I, with R ∈ Rdenotes the user-item interaction matrix, where X and Y is the number of users and items, respectively. Let rdenote the interaction behavior between user u and item i. The general objective of the recommender system is to generate a predicted score ˆr, which learns the user’s preference for the item i. Generally, we can formulate the recommendation task as a ﬁnite MDP [44], [45], as shown in Figure 1(b). At each time step, the recommender agent interacts with the environment (i.e., the user and/or logged data) by recommending an item to the user in the current state. At the next time step, the recommender agent receives feedback from the environment and recommends a new item to the user in a new state. The user’s feedback may contain explicit feedback (e.g., purchase and rating) and implicit feedback (e.g., user’s browsing record from logged data). The recommender agent aims at learning an optimal policy with the policy network to maximize the cumulative rewards. More precisely, the MDP in recommendation scenario is a 5-tuple < S, A, P, R, γ >, which can be deﬁned as follows. the environment states, in which a state sis the representation of interaction history at time step t. recommended candidate items. An action ais to recommend an item i at time step t. In logged data, the action can be gotten from the user-item interactions. bility matrix. The state transits from s to saccording to the probability p(s, r|s, a) after the recommender agent receives the user’s feedback (i.e., the reward r). action a at state s, it usually obtains immediate reward or delayed reward r(s, a) in accordance with the user’s feedback. future rewards. We assume in online RL-based recommendation environment, the recommender agent recommends an item i to a user u, while the user provides a feedback ffor the recommender agent at the t-th interaction. Then the recommender agent obtains the reward r(s, a) associated with the feedback f, and recommends new item ito the user u at the next interaction. Given the observation on the multi-turn of such interactions, the recommender system generates a recommendation trajectory. The recommender agent aims to learn a target policy πto maximize the reward of the recommendation trajectory [44]. maxE[R], where R=γr(s, a where θ refers to policy parameters, and Rdenotes the expected reward with respect to the recommendation trajectory τ = (s, a, · · · , s). The recommender agent often suffers from the high cost to learn the target policy by interacting with real users online. An alternative is to employ ofﬂine learning, which learns a behavior policy πfrom the logged data. We should solve the policy bias to learn an optimal policy πwhen using the ofﬂine learning in recommender systems, since there is a noticeable difference between the target policy π and the behavior policy π. In this section, we summarize speciﬁc RL algorithms applied in ﬁve typical recommendation scenarios (i.e., interactive recommendation, conversational recommendation, sequential recommendation, explainable recommendation, and POI recommendation) following value-function, policy search, and Actor-Critic, respectively. An overview of related literature is shown in Table 2. 4.1 Interactive Recommendation Due to its nature of learning from dynamic interactions, RL-based recommendation approaches easily formulate the interactive recommendation process as an MDP, in which the agent constantly interacts with related users and learns the policy from their behaviors to improve the quality of recommendations [46], as shown in Figure 1 (b). Many studies have attempted to adopt different RL algorithms to solve various recommendation problems. 4.1.1 Value-function Approaches There is a common challenge of sample efﬁciency for RL algorithms, which may lead to inefﬁcient learning of the recommendation policy, and thereby reduces user satisfaction with recommendations. To address the limitation, authors in [51] incorporate Knowledge Graph (KG) within RL algorithms for the interactive recommender systems. They argue that KG can provide rich supplementary information, which well reduces the sparsity of user feedback. To this aim, KG enhanced Q-learning model is proposed to make the sequential decision efﬁciently. Instead of learning the policy by trial-and-error search, the model utilizes the prior knowledge of correlations among items learned from KG to guide the recommendation decision. Combined these information to learn the optimal policy, a DQN approach is designed with double-Q [79] and dueling-Q [80] to model the long-term user satisfaction. From the interaction history owith item iat time step t, the recommendation agent gets the reward rand then stores the experience in the replay buffer D. The goal is to improve the Q-network by minimizing the mean-square loss function as follows. L(θ) = E[(y− Q(s, i; θ)) where (o, i, r, o) refers to the learnt experience, and y denotes the target value according to the optimal Q. Besides, authors in [47] propose a DQN-based recommendation framework that incorporates Convolutional Neural Network (CNN) and Generative Adversarial Networks (GAN) [81], called DRCGR. It automatically learns the optimal policy by both positive and negative feedback. More precisely, in the DRCGR framework, the negative feedback (e.g., skipped items) and the user behaviors (e.g., click and order) in a state space are considered during the recommendation process. Firstly, a CNN model is developed to capture the user’s sequential preference for the positive feedback. Secondly, a GAN model is designed to get optimal negative feedback. Finally, the DQN generates an optimal action-value function based on the representations of positive/negative feedback. To optimize instant and long-term user engagement in recommender systems, Zou et al. [49] design a novel Qnetwork that has three layers named raw behavior embedding layer, hierarchical behavior layer, and Q-value layer. The hierarchical architecture well captures versatile information of user behaviors, such as instant feedback (e.g., click and purchase), and delayed feedback (e.g., dwell time and revisit). On the other hand, to ensure convergence in ofﬂine Q-learning, they propose an S-network as a user simulator, which simulates the environment and assists the Q-network learning. They later present PDQ framework [50] to ensure the stability of convergence and low computation cost of existing algorithms. The PDQ consists of two major components: a world model imitates user’s feedback from the historical logged data, and then generates pseudo experiences. While a recommendation policy based on Qlearning maximizes the expected reward by the pseudo experiences from the world model and logged experiences. Thus, the policy has a low bias by adaptively optimizing the world model and directly off-policy learning. The main idea in User-speciﬁc Deep Q-learning Network (UDQN) [48] is to resolve the multi-step problem in interactive recommender systems. The multi-step interactive recommendation is cast as a multi-MDP task for all target users. Compared to a single MDP task, it is difﬁcult to learn a global policy for the multi-MDP task, since the multi-MDP task of different users may vary greatly in state transitions. To solve this issue, user-speciﬁc latent states are constructed by matrix factorization. Then the UDQN framework evaluates the optimal recommendation policy based on the constructed states. Moreover, to model userspeciﬁc preferences explicitly, a biased UDQN is proposed by adding a bias parameter to capture the difference in the Q-values of different target users. 4.1.2 Policy Search Methods The idea in [5] is employing DRL and Recurrent Neural Network (RNN) to improve the accuracy of long-term recommendations, whereas the existing RL-based algorithms are often developed for short-term recommendations. Speciﬁcally, RNN is performed to simulate the sequential interaction between the environment (the user) and the recommender agent by evolving user states adaptively, which tackles the cold-start issue in recommender systems. On the other hand, to maximize the expected long-term recommendation reward, instead of applying the traditional REINFORCE algorithm directly, the authors split the complete interaction process into sub-episodes and reboot the accumulate reward of each sub-episode, which signiﬁcantly improves the effectiveness of policy learning. In text-based interactive recommender systems, user feedback with natural language usually causes undesired issues. For instance, the recommender system may violate user’s preferences, since it ignores the previous interactions and thus recommends similar items. To handle these issues, a reward constrained recommendation (RCR) model [52] is proposed to incorporate user’s preferences sequentially. To be speciﬁc, the text-based interactive recommendation is formulated as a constraint-augmented RL problem, in which user feedback is taken as constraint. To generalize from the constraints, A discriminator parameterized as the constraint function is developed to detect the violation of user’s preferences. The policy is learned using policy gradient with baseline (a general constraint), which can punish the policy for constraint violations. The user will give some feedback when she receives the recommended items. Then the recommender system incorporates constraints from these feedback into the next item recommendation. Most existing RL methods fail to alleviate the large discrete action space issue in interactive recommender systems, since there are a large number of items to be recommended. Chen et al. [53] present a Tree-structured Policy Gradient Recommendation framework (TPGR) to achieve high effectiveness and efﬁciency for large-scale interactive recommendations. Speciﬁcally, they conduct a balanced hierarchical clustering tree over items, in which each leaf node is mapped to an item, and each non-leaf node is connected with a policy network. Thus, recommending an item can be formalized by a path sought from the root to a certain leaf node. To maximize long-run cumulative rewards, the REINFORCE algorithm is utilized to learn the strategy of making recommendation decisions. 4.1.3 Actor-Critic Algorithms In recommender systems, major groups dominate recommendations, which leads to a higher proportion of opportunities, whereas minor groups (e.g., the new users and items) are usually ignored. This fairness concern is extremely challenging, since it is hard to deal with the conﬂict between fairness and accuracy. To address the challenge in interactive recommender systems, an RL-based framework (i.e., FairRec) [54] is proposed to dynamically achieve a fairness-accuracy balance, in which the fairness status of the system and user’s preferences combine to form the state representation to generate recommendations. Besides, a twofold reward is constructed in terms of both fairness and accuracy. The FairRec framework is divided into two parts: an actor network and a critic network. On the one hand, the actor network performs recommendations according to the fairness-aware state representation. The actor is trained from the critic, and updated by the DPG algorithm. On the other hand, the critic network estimates the value of the actor outputs. The critic is updated by TD learning. Cao et al. [8] try to address adversarial attacks in RLbased interactive recommender systems. They propose a general framework that consists of two models: an adversarial attack model and an encoder-classiﬁcation detection model. In the attack model, the agent crafts adversarial examples following Actor-Critic or REINFORCE algorithm with baseline. In the detection model, the agent detects potential attacks based on the adversarial examples, employing a classiﬁer designed by attention networks. The authors also demonstrate the effectiveness of the framework based on [73], in which actions of the agent are generated as paths over KG, and then the detection model classiﬁes action sequences to judge whether the recommender system is attacked. Online interaction in RL for interactive recommendation may hurt the user experiences. An alternative is to adopt logged feedbacks to perform ofﬂine learning. However, it usually suffers from some challenges, such as unknown logging policy, discrete stochastic policy, and extrapolation error. As a solution to deal with these challenges, Xiao and Wang [55] present an ofﬂine RL model. Firstly, the interactive recommendation problem is cast as a probabilistic inference, which learns discrete stochastic policy from logged feedbacks. Then, based on the probabilistic formulation, a stochastic Actor-Critic method is proposed to maximize long-run cumulative rewards. Finally, to reduce the extrapolation error, the authors present ﬁve regularization approaches: support constraints that combines a policy improvement loss to form the total loss function, supervised regularization that avoids pre-estimating the logging policy, policy constraints by introducing a lagrange multiplier to the policy improvement loss, dual constraints guide actions through the logging policy, and reward extrapolation for policy iteration. Such a combination of these approaches is critical to achieving a superior performance of recommendation in ofﬂine learning. In interactive recommender systems, utilizing multimodal data can enrich user feedback. Toward this end, Yu et al. [56] propose a vision-language recommendation approach that enables effective interactions with the user by providing natural language feedback. In addition, an attribute augmented RL is introduced to model explicit multi-modal matching. More speciﬁcally, the multi-modal data a(i.e., the action of recommending items at time t) and x(i.e., natural language feedback from users at time t) are leveraged in the proposed approach. Then a recommendation tracker, consisting of a feature extractor and a multimodal history critic, is designed to enhance the grounding of natural language to visual items. The recommendation tracker may track the users’ preferences based on a history of multi-modal matching rewards. The policy is updated via the Actor-Critic algorithm for recommending the items with desired attributes to the users. Different from most current works that design policy or learning algorithms to improve the recommendation performance, Liu et al. [57] focus on the state representation for the environment and develop an interactive recommendation framework based on Actor-Critic algorithm, in which four state representation schemes are designed to explicitly model the user-item interactions, and thereby guide the agent to make better recommendations. More precisely, a state representation module explicitly formulates the dynamic interactions between users and items to improve the recommendation quality. To alleviate huge computations caused by the complex interactions, by using an average pooling layer, all types of vectors are concatenated into a vector to deﬁne the state representation. In addition, they utilize the attention network to tackle the drawback of the average pooling layer. Thus, the actor network generates the recommendation scores for the recommended items. And the critic network estimates a state-action value, combining the novel state representation with the generated action by the actor network. 4.2 Conversational Recommendation Unlike the interactive recommendation whose users receive information passively (i.e., the recommendation system is dominant), conversational recommendation interacts with users dynamically, then explicitly acquires users’ active feedback, and thereby makes recommendations that users really like. To achieve this aim, conversational recommender systems usually communicate with users by real-time multiturn interactive conversations, based on natural language understanding and generation. In this case, there is a critical issue of trade-offs between exploration and exploitation [82]. Conversational recommender systems explore the items unseen by a user to capture the user’s preferences by multi-turn interactions. However, compared to exploiting the related items that already captured, exploring the items that may be unrelated will harm user experience. RL has been a hot topic to address the challenge. As described in Figure 3, the policy network stimulates the reward centers in conversational recommender systems, integrating exploration with exploitation in multi-turn interactions. 4.2.1 Value-function Approaches It is interesting that conversational recommender systems can use incremental critiquing as a powerful type of user feedback, to retrieve the items in line with the user’s soft preferences at each turn. From this point of view, it needs a related quality measure for the recommendation efﬁciency. To this aim, a novel approach was proposed to improve the quality measure by combining a compatibility score with similarity score [58]. To evaluate the compatibility of user critiques, exponential reward functions are presented by Monte Carlo and TD methods based on the user specialization. For the user’s critiquing satisfaction, a global weighting of user’s preferences is proposed to enhance the critiquing quality, which brings about faster convergence of the similarity. By using the combination of these two scores, the conversational recommender system improves its robustness against the noisy user data. The goal of the proposed conversational recommender system [59] is to improve the accuracy of the recommendations more efﬁciently. For this purpose, the authors propose an RL-based dialogue strategy that utilizes recommendation results based on user’s utterances. The proposed system consists of three main components. In the language understanding model, Long Short-Term Memory (LSTM) network is employed to extract the facet-values and estimate the user’s utterance intention. In the recommender system, Endto-End Memory Networks [83] are used to generate the recommendation results. In the policy network, a DQN estimates user’s utterances to achieve efﬁcient dialogues, aiming to maximize the action values of user’s utterances based on the facet-values, user’s utterance intention, and recommendation results. Existing conversational recommender systems usually utilize user feedback in implicit ways. Instead, to make full use of user preferred attributes on items in an explicit way, a Simple Conversational Path Reasoning (SCPR) framework [60] is proposed to conduct interactive path reasoning for conversational recommendation on a graph. The SCPR obtains user preferred attributes more easily, by pruning off irrelevant candidate attributes following user feedback based on a policy network. The policy network inputs the state vector s and outputs the action-value Q(s, a), referring to the estimated reward for asking action aor recommending action a. The policy is optimized by the standard DQN to achieve its convergence. Different from EAR [64], the SCPR uses the adjacent attribute constraint on the graph to reduce the search space of attributes, and thereby alleviates the pressure for decision-making. At each conversation turn of conversational recommendation, the policy learning usually aims at deciding what to ask, which to recommend, and when to ask or recommend. Authors in [61] present a UNIﬁed COnversational RecommeNder (UNICORN) that employs an adaptive RL method to cast these decision-making processes as a uniﬁed policy learning task. The UNICORN consists of four components, including a graph-based MDP environment that formulates the conversational recommendation problem, graphenhanced state representation learning that utilizes dynamic weighted graph to represent the state of the MDP environment, action selection strategy that introduces preferencebased item and weighted entropy-based attribute selection to improve the sample efﬁciency, and DQN that employs double-Q [79] with prioritized experience replay and dueling-Q [80] to conduct the recommendation policy learning. 4.2.2 Policy Search Methods For managing different phases of the dialogue system more efﬁciently, Greco et al. [62] adopt an HRL framework named Converse-Et-Impera (CEI), in which the main goal of the conversational recommender system is divided into simpler ones, and each goal is cast as a goal-speciﬁc representation. The framework is trained by a two-stage strategy. In the RL stage, a meta-controller understands the user utterance and then selects the relevant goal, whose policy is based on the REINFORCE algorithm. In the supervised learning stage, a controller utilizes the goal-speciﬁc representations to generate proper answers. To model the user’s current intention and long term preferences for personalized recommendations, Sun and Zhang [63] utilize deep learning technologies to train a conversational recommender system, in which a deep belief tracker extracts a user intention by analyzing the user’s current utterance, and a deep policy network guides the dialogue management by the user’s current utterance and long-term preferences learned by a recommender module. The framework of the proposed Conversational Recommender Model (CRM) is composed of three modules: a natural language understanding with the belief tracker, dialogue management based on the policy network, and a recommender designed by the factorization machine. More speciﬁcally, at ﬁrst, the belief tracker converts the utterance (i.e., the input vector z) into a learned vector representation (i.e., s) by LSTM network. Then to maximize the longterm return, they use the deep policy network to select a reasonable action from the dialogue state at each turn. Here the REINFORCE algorithm is adopted to optimize the policy parameter. Finally, the factorization machine is utilized to train the recommender module, and generates a list of personalized items for the corresponding user. The authors in [64] argue that a successful conversational recommendation should address three fundamental issues: 1) what questions to ask in terms of item features, 2) when to recommend related items, and 3) how to adapt with user feedback. To achieve this, they propose a uniﬁed framework called Estimation Action Reﬂection (EAR), in which a Conversation Component (CC) intensively interacts with a Recommender Component (RC) in the three–stage process. The framework starts from the estimation stage, the RC guides the action of CC by ranking candidate items. Then at the action stage, the CC decides what questions to ask in terms of item features and makes a recommendation. The conversation action is performed by a policy network, which is optimized via the REINFORCE algorithm. If a user rejects the recommended items that are made at the action stage, the framework moves to the reﬂection stage for adjusting its estimations. Extensive experiments demonstrate the proposed EAR outperforms the CRM [63] according to not only fewer conversation turns but also better recommendations, mainly because the candidate items adapt with user feedback in the interaction between the RC and CC. Existing conversational recommender systems only ask users for more interested information, hardly tackling users’ further concerns about the recommended items. On the other hand, two-way communications in task-oriented dialogue systems often suffer from the training issue, due to their high dependence in terms of system response generation and the recognition of user intention. To address these challenges, a Conversational Recommender System with Adversarial Learning (CRSAL) [65] is proposed to fully enable two-way communications, in which an adversarial training based on A3C algorithm makes the generated dialogue responses more human-like. The process of CRSAL is divided into three stages. In the information extraction stage, ﬁrstly, a dialogue state tracker infers the current dialogue belief state bfrom user’s utterances; then, a neural intent encoder extracts and encodes the user’s utterance intention z; ﬁnally, a neural recommender network derives recommendations from item features and dialogue states. In the conversational response generation stage, a neural policy agent performs the action in the current dialogue state, and a natural language generator updates conversational responses by the selected action. In the RL stage, the decision procedure of dialogue actions can be formulated as a Partially Observable MDP (POMDP). The agent selects the best action in each conversation round under the long-term policy. To this end, an adversarial learning mechanism based on the A3C algorithm is developed to ﬁne-tune the actions generated by the agent, which employs the discriminator to train the optimal parameters of the proposed model. 4.3 Sequential Recommendation Sequential recommender systems aim to predict users’ future preferences and recommend the next item successively under sequential user-item interaction. Some attempts have revealed that RL algorithms well deal with the sequential recommendation problems, as such problems can be naturally formulated as an MDP, and the recommender agent easily performs a sequence of ranking. 4.3.1 Value-function Approaches In sequential recommendation, it is essential to fuse longterm user engagement and user-item interactions (e.g., clicks and purchases) into the recommendation model training. Learning the recommendation policy from logged implicit feedback by RL is a promising direction. However, it faces challenges in implementation due to a lack of negative feedback and the pure off-policy setting. Xin et al. [44] present a novel self-supervised RL method to address the challenges. More precisely, the next item recommendation problem is modeled as an MDP, in which an agent interacts with the environment (i.e., the recommender system sequentially recommends items to related users) to maximize the cumulative rewards. To optimize the recommendation model more effectively, they deﬁne a ﬂexible reward function that contains purchase interactions, long-term user engagement, and item novelty. Based on this method, the authors develop two models: a self-supervised Q-learning model jointly trains two layers with the logged implicit feedback, and a self-supervised Actor-Critic model treats the self-supervised layer as an actor to perform ranking while the RL layer as a critic to estimate the state-action value. As a result, the model improves the recommendation performance with the pre-deﬁned rewards. Zhao et al. [66] argue that some recommended items that users skipped can inﬂuence the recommendation performance, which is usually ignored by the existing recommendation approaches. Hence, it is important to introduce such negative feedback for learning users’ preferences. They then propose a pairwise DRL for the recommendation framework, which simultaneously captures the positive and negative feedback to boost recommendations. To integrate the positive and negative feedback into the framework, the state can be deﬁned as s = {s, s}, where positive state s= {i, · · · , i} is the previous t items that a given user clicked or purchased, and negative state s= {j, · · · , j} denotes the previous t items that the user skipped. Thus, when the agent recommends item xin state s = {s, s} to a given user, if the user skip x, we should keep s= sand update s= {j, · · · , j, x}. If users click or purchase x, we should update s= {i, · · · , i, x} and keep s= s. The authors construct a novel DQN for the proposed framework, in which Gated Recurrent Unit (GRU) captures user’s sequential behaviors as positive state s, and negative state sis obtained by a similar way. The positive and negative signals are fed into the input layer separately, which assists the new DQN to distinguish contributions of the positive and negative feedback in recommendations. The radio channel recommendation is a special scenario, in which the user listens to the radio from multiple providers that provide radio channels, while a personal recommender system is installed on the client’s side to recommend relevant channel to the user. Moling et al. [67] leverage RL to develop a music recommender system named RLradio. It exploits both explicitly revealed channel preferences and user’s implicit feedback, i.e., the user actually listens to a music track that played in the recommended channel. The recommendation task can be formulated as an MDP, in which the channel recommendation agent learns user feedback to maximize the long-term cumulative rewards. The aim is to learn the optimal policy to select a relevant channel for recommending the next music track. To achieve this aim, the authors ﬁrst acquire a model of user’s listening behaviors by observing the interactions, while RLradio exploits explicitly revealed channel preferences with a channel selection policy. Then, the optimal policy is updated for the acquired model ofﬂine by the Policy Iteration method, and the RL agent implements the R-Learning [84]. Finally, during the continuous interactions between the agent and users, RLradio adapts its behaviors online using -greedy policy. 4.3.2 Policy Search Methods It is a non-trivial problem to capture long-term user’s preferences in sequential recommender systems, since the interaction between user and item may be sparse or limited, and it is unreliable for RL algorithms to learn user interests by using random exploration strategies. As a solution to overcome these issues, Wang et al. [68] develop a Knowledge-guidEd Reinforcement Learning (KERL) framework by adopting an RL model to make recommendations over KG. In the MDP modeled by KERL, an agent interacts with the environment at discrete time steps. The environment contains the information of interaction data and KG, which are useful for the sequential recommendation. In this case, the agent performs an action a in state sto select an item ifor recommendation. After each action, the agent gets an intermediate reward, which is deﬁned by integrating sequence-level and knowledge-level rewards. For the sequence-level reward, they estimate the quality of the recommendation sequence by the BLEU metric in machine translation. For the knowledge-level reward, the knowledge characteristics of user’s preferences are expected to be similar to the recommended sequences. The authors also present a pairwise learning method with simulated subsequences to handle the sparsity and instability in the RL model. Authors in [69] analyze user’s sequential learning, and point out that the attention-based recommender systems will perform poorly when the users enroll in diverse historical courses, since the effects of the contributing items are diluted by many different items. To remove the noisy items without direct supervision and thereby recommend the most relevant items at the next time, they design a proﬁle reviser by two-level MDPs, in which a high-level task decides whether to revise the user proﬁle, and a low-level task decides which item should be removed. Each kind of task is cast as a 4-tuple MDP < S, A, P, R >. The agent in Hierarchical Reinforcement Learning (HRL) framework [69] performs hierarchical tasks under the revising policy, which is updated by the REINFORCE algorithm, and then receives a delayed reward from the environment (i.e., the original dataset and recommendation model) after the last low-level action is performed. Finally, the proﬁle reviser is trained with the recommendation model based on attention networks to provide better recommendation results. In addition, for capturing users’ dynamic preferences in sequential learning behaviors, Lin et al. [70] propose a dynamic attention mechanism, which combines with the HRL-based proﬁle reviser to distinguish the effects of contributing courses in each interaction. 4.4 Explainable Recommendation The explainable recommendation aims to solve the problem of interpretability based on an explainable model in the recommender systems. The explainable recommender systems not only provide high-quality recommendations but also generate relevant explanations to give the reasons for recommendation results. The survey on explainable recommendation conducted by Zhang and Chen [85] has shown that the explanations contribute to the effectiveness, transparency, persuasiveness, and trustworthiness of the recommender systems, which also improve user satisfaction. The approaches are divided into ﬁve major classes: explanation with relevant items or users, feature-based explanation, social explanation, textual sentence explanation, and visual explanation. The emphasis of this section is to survey RL applied to the explainable recommendation, in which the approaches focus on textual sentence explanation and visual explanation. 4.4.1 Policy Search Methods In terms of textual sentence explanation, Lu et al. [71] propose a multi-task learning framework that simultaneously makes rating predictions and generates explanations from user reviews. The rating prediction is performed by a context-aware matrix factorization model, which learns latent factor vectors of the user and item. The recommendation explanation is employed by an adversarial sequenceto-sequence model, in which GRU generates personalized reviews from observed review documents, and adversarial training for the review generation is performed by the REINFORCE algorithm. It is noteworthy that the joint training of both models may be inefﬁcient, since the latent factor vectors and the textual features of reviews may be overlapped. To address this problem, an iterative optimization algorithm is introduced to reduce the dependency between these two models, i.e., it alternates the parameter update of the context-aware matrix factorization model with that of the adversarial sequence-to-sequence model. To control the explanation quality ﬂexibly, Wang et al. [7] design an RL framework to generate sentence explanations. In the proposed framework, there are couple of agents instantiated with attention-based neural networks. The task of the one agent is to generate explanations, while the other one is responsible for predicting the recommendation ratings based on the explanations. The environment mainly consists of users, items, prior knowledge, and the recommendation model treated as a black box. The agents interact with the environment to generate effective explanations. The ﬁrst agent will get the reward when it correctly predicts the output ratings of the recommendation model (i.e., model explainability). Based on some prior knowledge about desirable explanations (e.g., the desirable length of explanations), the second agent gets the reward from the environment if the explanation quality can be controlled ﬂexibly (i.e., good presentation quality of the explanations). The expected reward is optimized following the REINFORCE algorithm. Furthermore, to leverage the image information of entities, rather than focusing on rich semantic relationships in a heterogeneous KG, Tao et al. [72] present a Multimodal Knowledge-aware RL Network (MKRLN), in which the representation of recommended path consists of both structural and visual information of entities. The recommender agent starts from a user and searches suitable items along hierarchical attention-paths over the multi-modal KG. These attention-paths can reduce improve the accuracy of recommendations and explicitly explain the reason for recommendations. 4.4.2 Actor-Critic Algorithms Using item images as a way of interpretability may make users more satisﬁed, due to the intuition of visual images. And the visual explanation seems more intelligent when a recommender system conducts path reasoning over KG, because KG contains rich relationships between users and items for intuitive explanations. Figure 4 illustrates the common framework of the Actor-Critic model with KG for the explainable recommendation. The KG is cast as a part of the MDP environment, and the agent performs path reasoning for recommendations. However, the real-world KG is usually enormous. We should focus on ﬁnding some more reasonable paths in the graph. From this aspect, Xian et al. [73] propose a reinforcement KG reasoning approach called Policy-Guided Path Reasoning (PGPR), which performs recommendations and explanations by providing actual paths with the ActorCritic algorithm in a KG. Speciﬁcally, the recommendation problem is treated as a deterministic MDP over the KG. In the training stage, following a soft reward and userconditional action pruning strategy, the agent starts from a given user, and learns to reach the correct items of interest. In the inference stage, the agent samples diverse reasoning paths for recommendation by a policy-guided search algorithm, and thereby generates genuine explanations to answer why the items are recommended to the given user. Extending the PGPR approach, an ADversarial ActorCritic (ADAC) model [74] is proposed to identify interpretable reasoning paths for more accurate recommendations. By learning the path-ﬁnding policy, the actor interacts with the environment to get its search states over the KG and potential actions. The actor obtains the reward Rif the path-ﬁnding policy from the current state ﬁts the observed interactions. To integrate the expert path demonstrations, they design an adversarial imitation learning module based on two discriminators (i.e., meta-path and path discriminators). The discriminators are trained to distinguish the expert paths from the paths generated by the actor. When its paths are similar to the expert paths in the meta-path or path discriminator, the actor will obtain the reward (Ror R) from the imitation learning module. The critic merges these three rewards (i.e., R, R, and R) to estimate each action-value accurately. Then the actor is trained with an unbiased estimate of the reward gradient through the learned action-values. At last, the major modules of ADAC are jointly optimized to ﬁnd the demonstration-guided path, which brings about accurate recommendations. Although existing works (e.g., PGPR [73] and ADAC [74]) combine KG and RL to enhance recommendation reasoning, they are not suitable for news recommendation, in which a news article usually contains multiple entities. To address this challenge, a recommendation reasoning paradigm that employs Anchor KG path to make news recommendation, called AnchorKG [75]. The AnchorKG consists of two parts. An anchor graph generator captures the latent knowledge information of the article, which leverages k-hop neighbors of article entities to learn high-quality reasoning paths. On the other hand, An AC-based framework is developed to train the anchor graph generator. Finally, the model performs a multi-task learning process to optimize both the anchor graph generator and the recommendation task jointly. 4.5 POI Recommendation As an important service in Location-Based Social Networks (LBSNs), POI recommendation can suggest related locations that users prefer by exploiting various features from their check-in behaviors. Several studies have been made to leverage RL to pursuit better recommendation performance, since RL algorithms well model users’ check-in behaviors to mine their preferences. 4.5.1 Value-function Approaches To improve the novelty of the next-POI recommendation that gives rise to user’s interest, David and Ricci [76] propose a generalized user behavior model based on the Inverse Reinforcement Learning (IRL) algorithm [86]. They ﬁrstly cast the generation task of tourist trajectory as a ﬁnite MDP, in which a set of states refers to the visit trajectories in a speciﬁc context (e.g., weather and temperature), and a set of actions denotes the decision of moving to POIs. Then the trajectories are clustered by the Non-Negative Matrix Factorization technique, which groups the visit trajectories by using a common semantic structure. After that, two recommendation strategies are proposed to guide next-visit actions. The ﬁrst strategy (i.e., cluster behavior-based strategy) is performed for the few POI-visit action observations. In this case, the generalized user behavior in a cluster suggests the optimal action with the highest Q-value Q(s, a) of the current state. The second strategy (i.e., cluster behavior hybrid strategy) is performed for the more POI-visit action observations, in which the group generalized user behavior and the user speciﬁc preference are jointly employed to generate recommendations. To this aim, they design the reward consists of two scores. That is, one is the normalized Q-value Q(s, a), and the other is the user speciﬁc score R(s, a) according to a vector of Boolean features. By using these two recommendation strategies, the proposed model recommends the relevant and novel items to the target user. To improve the effectiveness of travel route planning, an RL-based Context-Aware Personalized POI sequence Recommendation (CAPR) [77] is proposed to exploit the external environment, such as the time and weather. More precisely, the POI popularity and user’s interests are modeled based on implemented weather and contexts on user check-in behaviors. Then the proposed method employs the Monte Carlo Tree Search (MCTS) algorithm [87] to generate personalized POI sequences in different environments (i.e., time and weather). In the MCTS algorithm, two similar trees (Tand T) are initialized with the root node as the starting POI. During the iteration process, these two trees record the number of related visits and the rewards of corresponding nodes to capture the user’s dynamic preferences. Authors in [78] propose an Adversarial POI Recommendation (APOIR) model to capture user latent preference, which learns the check-in distribution in an adversarial way. To be speciﬁc, they model the users and their check-in locations (the RL environment) in a generative way, and alternatively train the recommender and discriminator component to optimize the generative process. For training the APOIR model, based on the learned preference distribution, a set of POIs is sampled to train the recommender component by the REINFORCE algorithm, and maximizes the predicted probability that the POIs are potentially interested or unvisited. Subsequently, the discriminator component is trained by users’ true checkin distribution along with the POIs sampled from the updated recommender component. It distinguishes the recommended POIs from these true check-in distribution, and thereby provides relevant gradients to improve the recommender component with a rewarding method. The implicit feedback and the context of POIs are taken into the reward function, thus it is composed of three rewards: a reward from the geographical factor, a reward from the social factor, and the discriminator reward. These two synergistic components are jointly trained to maintain high ranking performance. As noted above, RL aims to maximize long-run cumulative rewards by learning the optimal policy from interaction between the agent and its environment. So it relies not only on the environment but also on prior knowledge. In the recommendation applications, RL approaches often suffer from various challenges, such as the explorationexploitation tradeoff and the curse of dimensionality, as well as learning bias. Toward this end, many researchers put forward relevant solutions according to different issues. In the following, we summarize related works from ﬁve aspects: environment construction, prior knowledge, the deﬁnition of the reward function, learning bias, and task structuring. In RL, the agent observes states from the environment, then conducts relevant actions under the policy, and receives a reward from the environment. Applied to recommender systems, the policy training in the environment is often confronted with many unpredictable situations, due to the need for exploration. In this case, environment construction is critical to learn the optimal recommendation policy. However, most existing works seldom focus on environment construction. 5.1.1 State Representation The state representation plays an important role in RL to capture the dynamic information during the interactions between users and items, since states of the environment are a key component of MDP. But most current RL methods focus on policy learning to optimize recommendation performance. To effectively model the state representation, authors in [57] propose a DRL-based recommendation framework termed DRR, in which four state representation schemes are designed to learn the recommendation policy (the actor network) and the value function (the critic network). Specifically, the DRR-p scheme employs the element-wise product operator to learn the pairwise local dependency between items. DRR-u scheme incorporates the pairwise interactions between users and items into the DRR-p scheme. DRRave scheme concatenates the user embedding, the average pooling result of items, and the user-item interactions into a whole vector to describe the state representation. In the DRR-att scheme, a weighted average pooling is conducted by attention networks. Therefore, the actor network conducts a ranking action a = π(s) according to the state representation s, and generates a corresponding q-value by the action-value function q(s, a). The critic network leverages a DQN parameterized as q(s, a) to approximate q(s, a). Based on DPG [40], the actor network can be updated by the policy gradient via where J(π) denotes the expectation of all possible q-values following the policy π, and N is the batch size. Once the actor network is trained, the critic network can be updated by TD method. Authors in [88] argue that environment reconstruction from the historical data is powerful in RL-based recommender systems. However, it is unrealistic to optimize the policy by directly interacting with the real-world environment, due to the sampling efﬁciency and huge computations. An alternative solution is reconstructing a virtual environment [89]. From this point of view, they present a DEconfounded Multi-agent Environment Reconstruction (DEMER) framework, combines a confounder embedded policy with the discriminator under Generative Adversarial Imitation Learning (GAIL) [90], to learn the environment for recommendations. Speciﬁcally, in the generator module, DEMER randomly samples one trajectory from the observed data, and then forms its ﬁrst state as the initial observation. It ﬁnally generates a policy-generated trajectory by the TRPO algorithm, considering the confounder embedded policy as a role of hidden confounders in the environment. On the other hand, a compatible discriminator is proposed to train the policies in the framework. In online communities, the heterogenous information network contains useful information, such as the user’s explicit/implicit feedback, as well as the connections among user and item nodes, which helps to provide accurate recommendations. Toward this end, Liang [91] put forward a user proﬁling technique with DRL for recommender systems, in which the agent interacts with an external heterogenous information network to learn the recommendation policy. The heterogenous information network is modeled as the external environment under a network schema. Each episode path is deﬁned as a sequence of nodes in the heterogenous information network and relation types in the network schema, and each state ﬁnds the agent’s position in the heterogenous information network. Meanwhile, translation-based embeddings (i.e., TransE [92] and TransH [93]) are utilized to learn the latent representations of these nodes and corresponding relation types. To efﬁciently train the policy of making recommendations, both data-speciﬁc and expert knowledge is adopted to generate meta-paths for user proﬁles. The existing bandit approaches for recommendations often assume that the environment is static. Pan et al. [21] propose Policy Gradients for Contextual Recommendations (PGCR) without the impractical assumption and prior knowledge, which employs the actor-critic algorithms to train the proposed framework. Particularly, the generalized setting is introduced to formalize as MDP with states and state transitions. At each time step, the agent observes the current state as well as contexts related to that state. Here the contexts are independently distributed conditioning on that state. To estimate the marginal probability of item selection, the authors extend Experience Replay [94] for the sampling procedure, which reduces the variance of policy gradients. It may be practical to formulate a simulation environment of recommendation as an RL gym. To achieve this aim, a PyRecGym [95] is designed for RL-based recommender systems to support standard test datasets and common input types. In the PyRecGym, the states of the environment refer to user proﬁles, items, and interaction features with contextual information. The RL agent interacts with a gym engine in the current state, and gets related feedback from the gym engine according to the reward. The PyRecGym contains three major functions: initialization function that initializes the initial state of the environment for the user, reset function that subsets the user state in the next episode and returns the initial state to the RL agent, and stepfunction that reacts to the agent’s action and then returns the next state. 5.1.2 Knowledge Graph Leverage Recent advances in KG have attracted increasing attention in RL-based recommender systems. On the one hand, the KGenhanced interaction fashion enriches user-item relations by the structural knowledge to improve the recommendation accuracy. On the other hand, the multi-hop paths in a KG contribute to infer causal relationships for the explainable recommendation. The KERL framework [68] incorporates KG into RLbased recommender systems, with the aim of predicting future user’s preferences and addressing the sparsity. Specifically, the task of sequential recommendation is formalized as an MDP. Firstly, the state representation is enhanced with KG information, in which an induction network is constructed to predict future knowledge characteristics of user’s preferences. Secondly, the reward function is designed by consisting of sequence-level and knowledgelevel rewards. Finally, the proposed model is trained by a Truncated Policy Gradient policy. For learning the user’s preferences from sparse user feedback, Zhou et al. [51] develop a KG-enhanced Q-learning model for interactive recommender systems. Instead of learning the policy by trial-and-error search, the model utilizes the knowledge of correlations among items learned from KG to enrich the state representation of both items and users, spreads users’ preferences among the correlated items over KG, and ultimately guides the recommendation decision to generate better candidate set. Xian et al. [73] make the ﬁrst attempt to leverage KG and RL for the explainable recommendation, which provides actual paths guided by the policy network in a KG. Towards this end, they develop a uniﬁed framework named PGPR that contains a soft reward strategy, action pruning approach, and multi-hop scoring function. To efﬁciently sample reasoning paths and candidate items for recommendations, they also propose a policy-guided search method to search for relevant paths over the KG. To further improve the convergence and interpretability of the PGPR approach, Zhao et al. [74] propose a demonstration-based KG reasoning framework, in which imperfect path demonstrations are extracted to guide path-ﬁnding, and the ADAC model aims to identify interpretable reasoning paths for accurate recommendations. Moreover, Cao et al. [8] develop an attackagnostic detection model to enhance the defence ability of interactive recommendation systems based on RL, in which actions of the agent are deﬁned as the detection data, and each action of PGPR is cast as heterogeneous graph path that starts from a given user. Based on these crafted data, the authors study adversarial attacks and detection on recommendation systems. Another interesting work is the negative sampling by KG Policy Network [96], which is incorporated into the recommendation framework to explore informative negatives over KG. The framework consists of three modules to perform the exploration operations: a graph learning unit for high-quality representations of users and items, a neighbor attention unit for performing path-ﬁnding to get knowledge-aware negative signals, and a neighbor pruning unit for reducing the search space. At ﬁrst, each negative item is sampled to pair one positive item. Then both of them are fed into the recommender based on matrix factorization. Finally, the recommender (i.e., matrix factorization) and the sampler (i.e., KG Policy Network) are jointly trained by the iteration optimization, in which the recommender parameters are updated by the stochastic gradient descent (SGD) method, and the sampler parameters are updated via the REINFORCE algorithm. 5.1.3 Negative Sampling As mentioned above, to discover informative negative feedback from the missing data, Wang et al. [96] propose a KG policy network for knowledge-aware negative sampling, which employs an RL agent to search high-quality negative instances with multi-hop exploring paths over KG. Speciﬁcally, the authors design an exploration operation to navigate from the target positive item, and select two sequential neighbours (i.e., two-hop path) to adaptively capture knowledge-aware negative signals. To this aim, a neighbour attention module is present to distinguish the different importance of one- and two-hop neighbours based on the positive interaction. The KG policy network recursively conducts such exploration operations to generate potential negative instances for target positive items. Thanks to the interaction knowledge, the KG policy network may alleviate false negative issues (i.e., some items sampled as negative during the training process are often positive in the test dataset). Authors in [97] argue that user exposure data - which records the history interactions in recommender systems based on implicit feedback - is beneﬁcial to train negative samples. From this point of view, they develop a recommender-sampler framework, in which the sampler samples candidate negative item as the output, and the recommender is optimized by the SGD method to learn the pairwise ranking relation between a ground truth item and a generated negative instance. After obtaining the multiple rewards, the sampler is optimized by the REINFORCE algorithm to generate both real and hard negative items. During the training process, the recommender uses highquality negative samples to predict user’s preferences. In order to improve the negative sampler, a Reinforced Negative Sampler (RNS) is proposed to sample high-quality negative instances (i.e., hard and real negative items), by the feature matching technique rather than selecting from the exposure data directly. To be speciﬁc, the adversarial training between the sampler and recommender is conducted to favor the hard negative items. Simultaneously, the sampler generates real negative instances that overlap with the noninteracted items in user exposure data. As a result, user’s preferences can be integrated into the hard negatives and exposure data. Traditional recommender systems often suffer from two main challenges, that is, cold start and data sparsity. A promising way of alleviating the issues is to model users’ preferences by leveraging social relations, which utilizes users’ social relationship to recommend their favorite items efﬁciently. Applied to RL-based recommendation, a Social Attentive Deep Q-Network (SADQN) [98] is developed to integrate social networks into the estimation of action-values. More speciﬁcally, a social matrix factorization method is proposed to structure the high-level state/action representations. The action-values are estimated by integrating two functions, in which the personal action-value function estimates the action-value in terms of user’s personal preferences, whereas the social action-value function estimates the action-value in terms of user’s social neighbors preferences, by employing the attention mechanism to capture the inﬂuence from different social neighbors. To learn more relevant hidden representations from personal preferences and social inﬂuence, the authors also develop an enhanced SADQN (i.e., SADQN++), which utilizes additional neural layers to summarize potential features from the hidden representations, and then predicts the ﬁnal action-value with the summarized features. As a result, the SADQN++ makes the agent more ﬂexible in learning the optimal policy than that of SADQN. Both SADQN and SADQN++ are trained by adopting the improved Q-leaning algorithm [99]. During the recommendation process, the user vector is updated by the SGD method based on the user’s real-time feedback and a sampled trust relation. A social recommendation can extract valuable relationships from social networks. Nevertheless, direct utilization of social relationships may reduce the recommendation performance, as users usually share various preferences with their social neighbors due to the randomness in the process of structuring social networks. To this end, a Social Recommendation framework based on Reinforcement Learning (SRRL) [100] is proposed to identify reliable social relationships for target users. The SRRL framework consists of two components: an agent samples user’s reliable social relations and transmits them to an environment, while the environment generates recommendations according to sampled social relations and provides rewards to the agent to learn the optimal policy. By using the REINFORCE algorithm to conduct end-to-end training, SRRL dynamically identiﬁes reliable friends who share similar preferences with the target user. In particular, SRRL adaptively samples the social friends to improve the recommendation quality with user feedback, since the reward is always real-time. 5.2 Prior Knowledge 5.2.1 Demonstrations The combination of imitation learning with RL can be named apprenticeship learning [101], which utilizes demonstrations to initialize the RL. In particular, the IRL algorithm [86] often assumes that the expert acts to maximize the reward, and derives the optimal policy from the learned reward function. In RL-based recommender systems, the reward signals are usually unknown, since users scarcely offer feedback. On the other hand, IRL algorithms are good at reconstructing the reward function for the optimal policy from users’ observed trajectories. Therefore, David and Ricci [76] adopt a novel IRL algorithm termed Maximum Log-likelihood (MLIRL) [102] to model the unknown user’s preferences based on the state features, and thereby improves the novelty of next-POI recommendation that boosts user’s interest. It assumes the knowledge of user’s preferences to estimate an initial reward function that justiﬁes her observed trajectories, and optimizes the user’s behavior by a gradient ascent method. To mine user’s interests in online communities efﬁciently, the author in [91] design a reinforced user proﬁling for recommendations by employing both data-speciﬁc and expert knowledge, in which the agent employs random search to ﬁnd data-speciﬁc paths in environment [103], while meta-paths contain expert knowledge and semantic meanings for searching useful nodes [104]. After that, a userbased CF technique based on the path-based user proﬁle is developed to generate recommendations. Unlike top-K recommendation with the target of ranking optimization, the novel exact-K recommendation focuses more on combinatorial optimization, which may be more suitable for addressing the recommendation problems in application scenarios. Towards this end, Authors in [105] design an encoder-decoder framework named Graph Attention Networks (GAttN) to learn the joint distribution of the K items and thereby output an optimal card, which contains these K items. To train the GAttN efﬁciently, the authors also present a Reinforcement Learning from Demonstrations (RLfD) method that integrates behavior cloning [106] into RL. In the GAttN framework, the encoder is represented as the state, while the decoder is represented as the policy, and the RLfD is performed as the agent in RL. Leveraging demonstrations speeds up the learning process. Nevertheless, the demonstration trajectories may be sub-optimal even noisy. To solve the issue, a supervise with policy-sampling is proposed to narrow the divide between training and inference of policy, and the reward function is optimized by the REINFORCE algorithm with hill-climbing. 5.2.2 Ofﬂine Policy In the interactive recommender systems based on RL, the interaction between the user and recommendation model by online policy learning usually brings about user negative experiences. An alternative solution is to adopt ofﬂine policy learning from logged data (e.g., logged feedback). For example, instead of interacting with the real user, the Pseudo Dyna-Q (PDQ) approach [50] utilizes a user simulator as a world model to simulate the environment, and then outputs simulated user experiences according to ofﬂine policy learning. During policy improvement, the user simulator is adaptively optimized under the recommendation policy. In this way, the PDQ approach can alleviate high computation cost and the instability of convergence. Moreover, the general ofﬂine learning framework [55] effectively learns interactive recommendation policy from logged feedback without online exploration, which combines some regularization methods to address the challenges of discrete stochastic policy and extrapolation error. Ofﬂine evaluation methods (e.g., importance sampling) usually need a large number of logged data, which results in the state dimensionality problem. Besides, in model-based RL, dynamic changes in the policy updates are peculiarly prone to the reduction of user satisfaction. To tackle these issues, authors in [107] employ GAN to model the interaction between the user and recommender system for ofﬂine policy learning, which fully uses the logged ofﬂine data to alleviate high interaction cost, and promotes robust policy learning by adversarial training. 5.3 Reward Function Deﬁnition The agent behavior in RL indirectly relies on the reward function. While in practice, it is a key challenge to deﬁne a promising reward function. Beyond the need to learn the policy by intermediate rewards, the reward deﬁnition according to the speciﬁc demands in different recommendation scenarios is necessary. For instance, many factors (e.g., multiple kinds of interactions between users and items such as watching, click, and purchase) in sequential recommendation should be considered. Thus Xin et al. [44] present a self-supervised RL for recommendation tasks, in which the RL part focuses on speciﬁc rewards in terms of these factors, and the self-supervised part updates the parameters by the cross-entropy loss. A creative work in [108] is leveraging GAN to model the dynamics of user behavior and learn relevant reward functions, which depends not only on the user’s history behavior but also on the selected item. The learned reward allows the agent to conduct actions in a principled way, instead of relying on the manual design. Based on the simulation environment using the user behavior model, a cascading DQN is proposed to learn the combinatorial recommendation policy. In recommender systems based on bandit methods [109], [110], [111], on the one hand, users usually get bored by a ﬁxed recommendation strategy. A better solution is to alternate different strategies appropriately. Thus Warlop et al. [112] extend the UCRL algorithm [113] to exploit the reward function of history actions, which introduces the long-term inﬂuence of the recommendations and constructs conﬁdence intervals to reduce the cumulative regret. On the other hand, it is difﬁcult to correctly estimate regret due to the dynamic environment in the real world. In this case, it leads to the biased estimation of the reward and highvariance. To tackle these issues, authors in [114] propose a stratiﬁed sampling replay method to address the sample problem, and design an approximate regretted reward that derives from the average reward in real time. In addition, they integrate both methods into a Double DQN to stabilize the reward estimation in a dynamic environment. For commercial recommendation systems, online advertising is frequently inserted into the personalized recommendation to maximize proﬁt. To this aim, a valueaware recommendation model [115] is put forward based on RL, which optimizes the economic value of candidate items to make recommendations. In the value-aware model, measuring by gross merchandise volume, the total reward is deﬁned as the expected proﬁt that is converted from all types of user actions (e.g., click and pay). Besides that, Zhao et al. [116] present an advertising strategy for DQN-based recommendation, in which the advertising agent aims at simultaneously maximizing the income of advertising and minimizing the negative inﬂuence of advertising on user experience. Thus the deﬁnition of the reward contains both the income of advertising and the inﬂuence of advertising on user experience. In practice, users focus not only on what they prefer but also on novel items. That is, the trade-off between accuracy and diversity should be essential in recommender systems, which is often neglected. Toward this end, Zou et al. [117] propose a fast Monte Carlo tree search method for diversifying top-K recommendations. Particularly, the diverse recommendation is formalized as an MDP. The goal is to generate top-K items via maximizing both diverse and accurate metrics. To be in line with the metrics, inspired by [118], the authors deﬁne the reward function with the diversity and accuracy gain that derives from recommending items at corresponding positions. 5.4 Learning Bias In the RL-based recommender systems, learning the recommendation policy from logged data (e.g., logged user feedback) often becomes a more practical solution, because it alleviates complex state space and high interaction cost with off-policy [119], [120]. Nevertheless, this learning method usually results in data or policy biases, which have attracted attention from researchers. 5.4.1 Data Biases There are lots of logged implicit feedback, such as user clicks and dwell time, available for learning user’s preferences. However, this learning method easily suffers from biases caused by only observing partial feedback on previous recommendations. In order to deal with such data biases, Authors in [121] propose a recommender system with the REINFORCE algorithm, in which an off-policy correction approach is utilized to learn from the logged implicit feedback, and incorporates a learned model of the behavior policy to adjust the data biases. Instead of the work by [121] that simply tackles data biases in the candidate generation module, Ma et al. [122] argue that scalable recommender systems should contain not only the candidate generation stage but also a more powerful ranking stage. Toward this end, they present a two-stage off-policy method to learn from logged user feedback and correct such biases by adopting inverse propensity scoring [123]. More precisely, the ranking module usually changes between the logged data (in behavior policy) and the serving of the candidate generation module (in target policy). If there are evidently different preferences on recommending between the two modules, the two-stage off-policy gradient can be conducted to correct data biases. Moreover, the variance is reduced by introducing a hyper-parameter to down-weight the gradient of the sampled candidates. 5.4.2 Policy Biases The direct ofﬂine learning methods, such as Monte Carlo and TD methods are subject to either huge computations or instability of convergence. To handle these problems, Zou et al. [50] propose the PDQ framework to tackle the selection bias between the recommendation policy and logging policy. More speciﬁcally, the ofﬂine policy learning is divided into two steps: in the ﬁrst step, a user simulator is iteratively updated under the recommendation policy via de-biasing the selection bias. In the second step, the recommendation policy is improved with Q-learning, by both the logged data and user simulator. In addition, a regularizer for the reward function reduces both bias and variance in the learned policy. Authors in [107] put forward a model-based RL to model the interactions with adversarial training, in which the agent interacts with the user behavior model to generate recommendations that approximate the true data distribution. For reducing bias in the learned policy, they employ the discriminator to rescale the generated rewards for the policy learning, and de-bias the user behavior model by distinguishing simulated trajectories from the real interactions. As a result, it improves the recommendation performance. Often the tasks of RL can be decomposed into basic components or a sequence of subtasks, which signiﬁcantly reduces the complexity of the learning task [13]. Applied to the recommender systems, the present study mainly focuses on Multi-Agent Reinforcement Learning (MARL), HRL, and Supervised Reinforcement Learning (SRL). We discuss such cases in the following subsections. 5.5.1 Multi-agent Reinforcement Learning In RL-based recommender systems, there are some inherent challenges (e.g., the exploration-exploitation tradeoff and the curse of dimensionality) when the single agent performs a task. An alternative solution is to leverage multiple agents with similar tasks to learn faster owing to parallel computation. MARL algorithms [124] can be classiﬁed according to fully cooperative, fully competitive, both cooperative and competitive, and neither cooperative nor competitive tasks. For instance, in the DEMER approach [88], the policy agent interacts with the environment agent to learn the policy of a hidden confounder jointly. This case can be considered as fully cooperative tasks. For addressing the sub-optimal policy of ranking due to competition between the independent recommender modules, a cooperative MARL approach is proposed to design a signal network [125]. It promotes the cooperation of different modules by generating signals for these modules. Each agent acts on the basis of its signal without mutual communication. Besides, an entropyregularized version of the signal network is developed to encourage exploration of the optimal global policy. For the i-th agent with the signal vector φ, given a Q-value function Q(s, a) and a shared signal network Φ(s), the objective function can be deﬁned as follows. where ξ and θ are the network parameters, D denotes the distribution of samples, and N is the batch size. The objective function for each agent is optimized according to SAC algorithm. In contrast, fully competitive tasks typically adopt the mini-max principle: each agent maximizes its own beneﬁt under the assumption that the opponents keep endeavoring to minimize it. In a dynamic collaboration recommendation method for recommending collaborators to scholars [126], the competition should be characterized as a latent factor, since scholars hope to compete for the better candidate. To this aim, the proposed method uses competitive MARL to model scholarly competition, i.e., multiple agents (authors) compete with one another by learning the optimized recommendation trajectories. Besides that, extending the marketbased recommender system [127] that employs fully competitive tasks, an improved market-based recommendation model [128] urges all agents to classify their recommendations into various internal quality levels, and employs Boltzmann exploration strategy to conduct these tasks by the recommender agents. 5.5.2 Hierarchical Reinforcement Learning In HRL, an RL problem can be decomposed into a hierarchy of subproblems or subtasks, which reduces the computational complexity. Some developed methods for HRL have solved the recommendation problems well. For example, using Hierarchies of Abstract Machines (HAMs) [129], Zhang et al. [69] formalize the overall task of proﬁle reviser as an MDP, and then decompose the task MDP into two abstract subtasks. If the agent decides to revise the user proﬁle (i.e., a high-level task), it allows the high-level task to call a lowlevel task to remove related noisy courses. Nevertheless, this HRL approach assumes that the user’s preferences are static. To improve the recommendation adaptivity and accuracy, a Dynamic Attention and hierarchical Reinforcement Learning (DARL) framework [70] is developed to automatically track the changes of user’s preferences in each interaction. Both of these methods adopt the REINFORCE algorithm to optimize the high- and low-level policy functions. HRL with the MaxQ approach [130] is a task-hierarchy that restricts subtasks to different subsets of states, actions, and policies of the task MDP without importing extra states. Applied in the recommender systems, a multi-goals abstraction based HRL [6] is designed to learn user’s hierarchical interest. More speciﬁcally, a high-level agent aims to catch long-term conversion interest, and introduces abstract goals to a low-level agent. The low-level agent tries to catch the short-term click interest following the abstract goals, and interacts with the real-time environment by recommending related items and receiving users’ feedback. In addition, Xie et al. [131] propose a novel HRL model for the integrated recommendation, in which the task of integrated recommendation is divided into two subtasks (i.e., sequentially recommending channels and items). A low-level agent works as a channel selector, which provides personalized channel lists in terms of user’s preferences. On the other hand, a high-level agent is regarded as an item recommender, which recommends heterogeneous items based on the channel constraints. Another prevailing approach is the options framework [132], which generalizes the primitive actions to include a temporally extended navigation of actions. For example, Greco et al. [62] design an HRL framework for the conversational recommendation. It manages each phase of a goal-driven dialogue, in which the agent goal is achieved by temporally extended actions. The options framework is formed of two modules, a meta-controller receives the user utterance and selects a goal, while a controller generates a corresponding answer under a goal-speciﬁc policy. 5.5.3 Supervised Reinforcement Learning In practice, the recommendation problems may be easier to resolve by utilizing a combination of RL and supervised learning rather than only RL, as supervised learning and RL can handle corresponding tasks according to their advantages respectively. For example, to meet the challenges of learning the policy from logged feedback in sequential recommender systems, a self-supervised RL model is proposed for the recommendation tasks [44]. The model has two output layers: one is the self-supervised layer trained with cross-entropy loss function to generate a list of ranking, and utilizes negative signals to update related parameters. The other is trained with RL based on a ﬂexible reward function, which performs as a regularizer for the supervised layer. Theocharous et al. [133], [134] focus on designing a good strategy for the user’s life-time value in personalized ad recommender systems. To achieve this goal, they propose an RL-based recommendation model, in which mapping from features to actions is learned by the random forest algorithm, which is a supervised learning method for classiﬁcation and regression. The recommendation model is trained by employing the random forest algorithm, -greedy policy, and an off-policy evaluation approach in turn. SRL is often applied to solve the adaptability issue of recommendations. For instance, to address the top-aware drawback (i.e., the performance on the top positions) that may reduce user experiences, a supervised DRL model [135] jointly utilizes the supervision signals and RL signal to learn the recommendation policy in a complementary way. There are two styles of supervision signals, in which they adopt the cross-entropy loss for classiﬁcation-based supervision signal, and employ pairwise ranking-based loss for the ranking-based supervision signal. The suitable supervision signals adaptively balance the long-term reward and immediate reward, and thereby overcome the top-aware drawback in the RL-based recommender system. Wang et al. [24] put forward SRL with RNN for dynamic treatment recommender systems. By combining the supervised learning signal (e.g., the indicator of doctor prescriptions) with the RL signal (e.g., maximizing the cumulative reward from survival rates), this approach learns the prescription policy to refrain from unacceptable risks and provide the optimal treatment. In particular, to deal with complex relations among various diseases, medications, and individual characteristics, the approach adopts an off-policy actor-critic algorithm, in which the actor is adjusted by both signals to guarantee effective prescription. Huang et al. [5] also propose a novel SRL with RNN for long-term prediction. More precisely, RNN is employed to adaptively evolve user states for simulating the sequential interaction between recommender system and users, and then generates the recommendation list. The framework effectively solves the user cold-start issue owing to the adaptive user states. To tackle the training compatibility among the components of embedding, state representation, and policy in RL-based recommender systems, authors in [45] develop an End-to-end DRL-based Recommendation framework namely EDRR, in which a supervised learning signal is designed as a classiﬁer of user feedback on recommendations based on the learned embedding. The authors also employ DQN and DDPG to elaborate how embedding component works in the proposed framework respectively. An overview of literature that contains RL issues in recommendation approaches is presented in Table 3. We have surveyed the RL-based recommender systems and realized that applying RL to recommender systems is not yet an outstanding achievement but rather requires certain improvements. The section ﬁrst discusses central issues faced by RL algorithms for recommendations. We then analyze practical challenges and provide several potential insights into successful techniques for RL-based recommender systems. 6.1 Open Issues In reality, RL algorithms are not applicable to a real-time recommendation, mainly due to their trial-and-error mechanism. Although the existing common alternative is to employ the ofﬂine evaluation, users always have high demand for the timeliness of recommendation. A key step in the application of RL algorithms is to improve efﬁciency and realize an intelligent design. Hence, some issues need to be overcome for making much progress in this ﬁeld. 6.1.1 Sampling Efﬁciency: Sampling plays a substantial role in RL, especially in RLbased recommender systems. Importance sampling has empirically demonstrated the feasibility of application in both on-policy and off-policy. Moreover, a few recent works [96], [97] have highlighted the superiority of negative sampling in recommender systems. Nevertheless, a bigger focus on the improvement in sampling efﬁciency is necessary, since the user feedback available to train large-scale recommendation models is scarce. We can leverage auxiliary tasks to improve sampling efﬁciency. For example, Chen et al. [136] develop a user response model to predict user positive or negative responses toward recommendations. Thus the state and action representations can be enhanced via these responses. Moreover, transfer learning may be qualiﬁed for the sampling task. For example, we can use the transfer of knowledge between temporal tasks [137] to perform RL on an extended state space, and concretize similarity between the source task and the target task by logical transferability. In addition, the transfer of experience samples [138] can be adopted to automatically estimate the relevance of source samples. Both transfer learning methods may be able to improve sampling efﬁciency in RL-based recommender systems. 6.1.2 Reproducibility: In many application scenarios, including recommendation, RL algorithms are often difﬁcult to reproduce because of various factors such as instability of RL algorithms, lacking open source codes & account of hyper-parameters, and different simulation environments (e.g., experimental setup, and implementation details). Especially for DRL methods, non-determinism in policy network and intrinsic variance of these methods leads to reproducibility crisis. Besides policy evaluation methods, there are several possible future directions for the reproducibility investigation with statistical analysis. To this aim, we can use signiﬁcance testing according to related metrics or hyper-parameters such as batch size and reward scale [139]. For example, the reward scale needs to check its rationality and robustness in speciﬁc recommended scenarios. And the average returns should be evaluated to verify whether it is proportional to the relevant performance. On the other hand, due to the sensitivity of RL algorithms changed in their environments, random seeds, and deﬁnition of the reward function, we should ensure reproducible results with fair comparisons, such as running the same experiment trials for baseline algorithms, each evaluation with the same preset epoch and different random seeds, while all experiments adopt the same random seeds [139]. 6.1.3 MDP Formulation: In principle, MDP formulation is essential to guarantee the performance of RL algorithms. However, the state and action spaces in recommender systems often suffer from the curse of dimensionality. Besides, the reward deﬁnition is sensitive to the external environment since user demand may be changed constantly, while interactions between users and items are random or uncertain. To address these issues, we can employ task-speciﬁc inductive biases to learn the representations of selection-speciﬁc action. As a result, the agent relies on better action structures to learn RL policies when the recommendation problem is formulated [140]. Another direction we can work on is to use causal graph and probabilistic graph models [141] to enable the MDP formulation, in which the causal graph structures the causal-effect relations among user-item interactions, and the probabilistic graph reasons the visual recommended paths. This joint process can improve the efﬁciency of agent search and tracks users’ interests over time. However, how to design MDPs for recommendation problems in a veriﬁable way remains an open issue. 6.1.4 Generalization: Model generalization ability is almost the pursuit of all applications. Limited by the shortcomings of different RL algorithms, it is difﬁcult to develop a general framework to meet various speciﬁc requirements of recommendation. On the one hand, model-based algorithms are only applicable to the speciﬁed recommendation problem, and fail to solve the problem that cannot be modeled. On the other hand, model-free algorithms are insufﬁcient in dealing with different tasks in complex environments. Fortunately, metaRL, such as Meta-Strategy [142] and Meta-Q-Learning [143], ﬁrst learns a large number of RL tasks to obtain enough prior knowledge, and then can be quickly adapt to the new environment in face of new tasks. In this case, it may enable the generalization of RL-based recommender systems. Apart from the transfer learning based on metaRL, we can use multi-task learning to handle related tasks (e.g., construction of user proﬁles, recommendation, and causal reasoning) in parallel. These tasks complement each other to improve the generalization performance via shared representation of domain information, such as parameterbased sharing, and joint feature-based sharing. 6.1.5 Autonomy: It seems easy to utilize RL for autonomous control [144]. But in fact, it is difﬁcult to achieve in online recommendation scenarios, since there are complex interactions between users and items, while it is not feasible to capture users’ dynamic intention by existing RL algorithms. Toward this end, one important work is feedback control of recommender systems by combining RL with LSTM or GRU, which enables the RL agent to be possessed of powerful memory that preserves the sequences of state and action, as well as different kinds of reward. Then, the historical information is encoded and transmitted to the policy network, and thereby improve the autonomous navigation ability of the RL agent. Another interesting open research direction is to improve the autonomous learning ability of RL from the ﬁelds of biology, neuroscience, and cybernetics. Thus, it is straightforward to use the learned knowledge to make better recommendations. 6.2 Practical Challenges The existing recommendation models based on RL have demonstrated superior recommendation performance. Nevertheless, there are many challenges and opportunities in this ﬁeld. We summarize ﬁve potential directions that deserve more effort from the practical aspect. 6.2.1 Computational Complexity: RL-Based Recommender Systems often suffer from huge computations due to the exploration-exploitation tradeoff and the curse of dimensionality. Apart from task structuring techniques, the IRL algorithm is an efﬁcient solution to the computation cost. Initializing RL with demonstrations can supervise the agent performing the action correctly, particularly for some speciﬁc recommendation directions. For example, apprenticeship learning via IRL algorithm [101] highlights the need for learning from an expert, which maximizes a reward based on a linear combination of known features. Speciﬁcally, learning the recommendation task is demonstrated by the expert. It aims to recover the unknown reward function that is close to the expert’s reward function. By directly imitating the expert, this algorithm only needs a small number of iterations to approach the recommendation performance of the expert. Besides, a promising method of driving route recommendation based on RL is to employ behavior cloning [106], which makes expert trajectories available and quickens the learning speed. Another feasible scheme is to improve the efﬁciency of agent exploration. For example, we can adopt NoisyNet [145] that adds parametric noise to its network weights, which aids efﬁcient exploration according to the stochasticity of the policy for the recommender agent. 6.2.2 Evaluation: Most existing RL-based recommender systems focus on the single goal of recommendation accuracy, with limitation of recommendation novelty and diversity [146] based on user experience. Beyond the need for new evaluation measures for multi-objective goals of recommendation, we should design standard metrics for other non-standardized evaluation measures (e.g., diversity, novelty, explainability, revenue, and safety). In general, these kinds of evaluation measures can be considered to be combinatorial optimization problems, naturally, maybe well achieved by multiobjective evolutionary algorithms [147]. Recently, some works have developed Pareto efﬁciency models for multiobjective recommendation [148]. For example, in the personalized approximate Pareto-efﬁcient recommender system [149], a Pareto-oriented RL module is proposed to learn personalized objective weights on multiple objectives for the target user. Nevertheless, it remains a challenge to balance different evaluation measures according to related metrics, since these evaluation measures have relevance and even conﬂicts. 6.2.3 Biases: In recommender systems, item popularity often changes over time due to the user engagement and recommendation strategy [150], thus long-tailed items are rarely recommended to users. The selection bias may hurt user satisfaction. Therefore, it is necessary to concentrate on the fairness work, which can combine with anthropology to analyze the differences of human behavior and cultural characteristics, or explore the user’s understanding and user-friendliness of the recommender system in humancomputer interaction. Besides, heterogeneous data often exists in online recommendation platforms, whereas most recommendation approaches are trained with a single type of data. The previous feature extraction and representation learning are crucial to deal with the data bias. Then MARL can be used to encourage different agents to execute multidimensional data simultaneously, and share parameters for a uniﬁed recommendation goal. This helps users to have a more comprehensive understanding of the recommendation results. 6.2.4 Interpretability: Due to the complexity of RL, post-hoc explanation may be easier to achieve than intrinsic interpretability [151]. Actually, the same is true in RL-based recommendation systems, that is, there is no intrinsic interpretable method for RL mentioned yet. How to explore causal inference theories and approaches of RL for more trustworthy recommendation is a promising line. Another research direction towards explainable recommendation is to provide formal guidance of recommendation reasoning process, rather than being concerned with a multi-hop reasoning process. Tai et al. [152] propose a user-centric path reasoning framework that adopts a multi-view structure to combine sequence reasoning information with subsequent user decision-making. However, they only focus on the user’s demand and do not provide theoretical proof of the rationality of reasoning. We should avoid any plausible explanation in the reasoning process. Toward this end, we can employ multitask learning to perform the recommendation reasoning process among multiple related tasks, e.g., representation of interaction, recommended path generation, and Bayesian inference for policy network. These related tasks jointly provide credible recommendation explanations by sharing the representation of internal correlation and causality. 6.2.5 Safety and Privacy: System security and user privacy are important issues, which are ignored by most researchers. For example, personal privacy is easy to be leaked when using RL and KG to perform the explainable recommendation, because the relationships among users and items are exposed. Differential privacy is widely applied to protect user privacy, and DRL can be employed to choose the privacy budget against inference attacks [153]. In addition, recent studies have found that Deep Neural Networks (DNNs) are vulnerable to attacks, such as adversarial attacks and data poisoning. For example, in DNNs-based recommender systems, users with fake proﬁles may be generated to promote selected items. To address this issue, a novel black-box attacking framework [154] adopts policy gradient networks to reﬁne real user proﬁles from a source domain and copy them into the target domain. Notwithstanding, We need to make more efforts from the research topic, such as utilizing safe RL [155] to guarantee the security of recommender systems (e.g., detecting error states, and preventing abnormal agent actions), and leveraging federated learning [156] to achieve privacy-preserving data analysis for MARL-based recommender systems, in which each agent is localized on distributed device and updates a local model based on the user data stored in the corresponding user device. Recommender systems serve as a powerful technique to address the information overload issue on the Internet. There has been increasing interest in extending RL approaches for recommendations in recent years. In this survey, we conduct a comprehensive review on RL-based recommender systems, using three major categories of RL (i.e., valuefunction, policy search, and Actor-Critic) to cover ﬁve typical recommendation scenarios, and restructure the general frameworks for some speciﬁc scenarios, such as interactive recommendation, conversational recommendation, as well as explainable recommendation based on KG. Then, the challenges of applying RL in recommender systems are systematically analyzed, including environment construction, prior knowledge, the deﬁnition of the reward function, learning bias, and task structuring. To facilitate future progress in this ﬁeld, we discuss theoretical issues of RL, analyze the limitations of existing approaches, and put forward some possible future directions. This work was supported by the National Natural Science Foundation of China [grant numbers 61977055].