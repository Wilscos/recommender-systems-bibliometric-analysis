Abstract—Graph Neural Networks (GNNs) bring the power of deep representation learning to graph and relational data and achieve state-of-the-art performance in many applications. GNNs compute node representations by taking into account the topology of the node’s ego-network and the features of the ego-network’s nodes. When the nodes do not have high-quality features, GNNs learn an embedding layer to compute node embeddings and use them as input features. However, the size of the embedding layer is linear to the product of the number of nodes in the graph and the dimensionality of the embedding and does not scale to big data and graphs with hundreds of millions of nodes. To reduce the memory associated with this embedding layer, hashing-based approaches, commonly used in applications like NLP and recommender systems, can potentially be used. However, a direct application of these ideas fails to exploit the fact that in many real-world graphs, nodes that are topologically close will tend to be related to each other (homophily) and as such their representations will be similar. In this work, we present approaches that take advantage of the nodes’ position in the graph to dramatically reduce the memory required, with minimal if any degradation in the quality of the resulting GNN model. Our approaches decompose a node’s embedding into two components: a position-speciﬁc component and a node-speciﬁc component. The position-speciﬁc component models homophily and the node-speciﬁc component models the node-to-node variation. Extensive experiments using different datasets and GNN models show that our methods are able to reduce the memory requirements by 88% to 97% while achieving, in nearly all cases, better classiﬁcation accuracy than other competing approaches, including the full embeddings. Index Terms—graph neural networks (GNNs), embedding layer, hashing, hierarchy, model compression, big data, scalability, dimension reduction In recent years graph neural networks (GNNs) have seen great success and have been widely applied to problems from computer vision [1] and natural language processing (NLP) [2], [3] to chemistry [4] and recommender systems [5]. GNNs compute node representations by considering both the topology of the graph and the nodes’ features in end-to-end training. When there are no features available or not enough features for training the model, GNNs use node identity features (one-hot encodings) and learn an embedding layer to compute node embeddings. Then they use these embeddings as input features. Learning an embedding layer for the onehot node features leads to an embedding table whose size is Computer Science & Engineering equal to the product of the number of nodes in the graph and the dimensionality of the embedding. This induces memory requirements that grow linearly with the size of the embedding table and for large graphs with hundreds of millions of nodes, the embedding dimension can range from hundreds to thousands of dimensions. In domains such as NLP [6], [7] and recommender systems [8]–[10], hashing-based techniques have been developed to reduce the size of the embedding table. The hashing trick [6] uses a hash function to randomly maps the IDs of the features to a smaller number of shared embeddings (hash buckets). However, this method suffers from collisions, as multiple IDs are mapped to the same bucket. Other methods build upon the hashing trick and reduce collisions by using multiple hash functions [9], [10] and learnable feature-dedicated importance weights [7]. A recent work, DHE [8] replaces onehot encodings with dense hash encodings and trains a deep feedforward network to get the ﬁnal embeddings. This method reports state-of-the-art performance compared to other hashbased techniques. Even though GNNs can take advantage of the above hashing-based methods, most real-world graphs have certain properties that can be exploited to develop better methods. One such property is network homophily, according to which similar nodes based on node attributes, more likely may attach to each other than dissimilar ones [11]. In most real-world graphs, this leads to nodes that are topologically close in the graph tend to have similar representations. In this work, we propose a family of methods for positionbased node embedding learning. Our methods offer a memory efﬁcient alternative to the expensive embedding table coming from the one-hot node features. The ﬁnal embeddings consist of two components: a position-speciﬁc component and a node-speciﬁc component. The ﬁrst is designed to capture the position of a node in the graph’s topology and exploit the fact that nodes which have similar positions (be close together in the graph or share the same set of neighbors) will most likely have similar embeddings due to homophily. The second is designed to model node-to-node variation and more localized signals. We developed two different approaches for computing the position-speciﬁc component. In the ﬁrst one, we perform a k-way graph partitioning to identify k partitions of nodes. Then, we learn a unique embedding for each partition. All the nodes that belong to the same partition are assigned the same embedding: the embedding of their partition. In the second one, we build a hierarchy of partitions. A partition higher in the hierarchy captures the relations and interactions of the partitions in the previous level. We learn a unique embedding for each partition at each level. The ﬁnal embedding of a node is the combination of the embeddings of the partitions it belongs to, along its hierarchical path. For the computation of the node-speciﬁc component, we developed two approaches based on Hash Embeddings [7]. In the ﬁrst one, we take into account the hierarchy of partitions. We distribute the embeddings equally among the partitions of the highest hierarchical level and nodes that belong to the same partition share a certain number of embeddings. The second one does not account for the hierarchy. The embeddings are shared among all nodes, irrespective of the partition they belong to. We evaluate our methods on three benchmark graph datasets provided by Open Graph Benchmark [12] for the task of node property prediction. For each dataset, we test two different state-of-the-art GNN models. As our experimental results showed, our methods perform better than existing approaches, including the one-hot full embeddings, in nearly all cases, and at the same time, they reduce the amount of memory required to compute the initial node embeddings by 88% up to 97% for the largest dataset considered. We summarize our main contributions in the following: for reducing the amount of memory required to compute the initial node embeddings. We evaluate the performance of various state-of-the-art hashing-based methods in settings where one-hot encodings are used. embedding learning. These methods offer a wide spectrum of model compression and can be used accordingly. We explore experimentally the importance of each method, as well as the beneﬁt we receive in performance. ity memory reduction by leveraging the nodes’ position in the graph. This constitutes them suitable and scalable to even extremely large graphs. We reduce the number of trainable parameters by 88% to 97% across the different datasets and models. based approaches and show that our methods perform better in almost all cases. compared to full embeddings, and at the same time reduce dramatically the number of trainable parameters, indicates that we do not need the full capacity of full embeddings to get high quality node representations. In Section II we discuss one-hot full embeddings in the context of GNNs and the basics from hashing-based embeddings. In Sections III and IV, we present our methods and extensive Fig. 1: Illustration of one-hot full embeddings for example node with id 2. The one-hot encoding for this node, vector uof dimension n, maps to the corresponding entry in the embedding table, W of size n × d, which contains the ﬁnal embedding of the node, v, a vector of dimension d. experimental evaluation. We continue with discussing related work in Section V and in Section VI, we conclude our work by summarizing the main points. In this section we discuss GNNs and one-hot full embeddings as well as hashing-based embeddings. The notation used throughout the paper is summarized in Table I. Graph Neural Networks (GNNs) are deep learning models that operate on graph structured data. They stack GNN layers to extract topological signals and learn node embeddings. Most of the layers in a GNN model can be expressed under a framework of message passing. Each node sends/receives messages to/from its neighbor nodes. A message is a function of three things: (i) the embedding of the source node, (ii) the embedding of the destination node and (iii) the edge features, if available. The message passing framework includes two phases: message passing and node update. Each node i updates its embedding by receiving messages from its neighbors: where N (i) is the set of neighbor nodes of i, his the node embedding of i in layer t, mis a message vector created by aggregating messages from i’s neighbor nodes, eis the edge feature associated with the edge between i and j, f(·) is a learnable function that maps embeddings of the sender and receiver as well as the corresponding edge feature to a message vector, g(·) is a learnable function that updates the node embedding by combining the incoming message and the embedding from the previous layer. In cases where there are no input features or these are of low-quality, the most straightforward way for GNNs to proceed is to use one-hot encodings and learn node embeddings as the input features. This leads to the full embedding table W ∈ R. Let vbe the embedding of node i. Then, we have where uis the one-hot encoding vector for node i, i.e., u∈ {0, 1}, with u(i) = 1 and u(t) = 0, ∀t 6= i, and u(t) is the t-th component of vector u. In Figure 1 we can see an illustration of the full embeddings for a node example. The size of the embedding table in this case is n ×d, which is not scalable for graphs with hundreds of millions or billions of nodes and for which cases d may range from hundreds to thousands of dimensions. Hashing-based techniques reduce the size of the embedding table by using hash functions to map feature values to shared learnable embeddings (hash buckets). Let B  n be the number of hash buckets, i.e., the number of rows of the embedding table W. The hashing trick [6] uses a single hash function and distributes the hashed values uniformly. The main drawback of this method is that it suffers from collisions as B  n. In order to reduce collisions, other methods use multiple hash functions [7], [9], [10]. Let u∈ {0, 1}be the vector we get after applying a hash function H to node i, with i ∈ {0, . . . , n − 1}. The hash function maps i to {0, 1, . . . , B − 1} and u(s) = 1 when s = H(i), and u(t) = 0, ∀t 6= s. When h hash functions are used, then uconsists of h component vectors, u= [u; u; . . . ; u] ∈ {0, 1}generated by the h hash functions. The ﬁnal embedding is the combination of the component vectors depending on the method. Let W ∈ R be the weight matrix of the embedding layer. Then, for the basic hashing-based methods, we have: The size of the embedding table in this case is B × d. The size of the embedding table in this case is again B × d. v= W(yu+ yu+ · · · + yu), (6) where yis the node-speciﬁc importance vector of node i, which controls the contribution of each of the h component vectors. The size of the embedding table in this case is B × d + n × h. Graph learning algorithms introduce relational inductive bias, which may potentially lead to similar node representations for nodes that are close together in the graph. In this work, we develop a family of methods, PosHashEmb, which leverages homophily by learning position-based embeddings. Instead of learning a single node-speciﬁc embedding that comes solely from the one-hot encoding of the node, PosHashEmb expresses the embedding of each node as the combination of two components: a position-speciﬁc component and a node-speciﬁc component. The position-speciﬁc term is designed to model homophily and the node-speciﬁc term models the node-distinct characteristics. Let pbe the position-speciﬁc component of node i and x the node-speciﬁc component. PosHashEmb computes the ﬁnal embedding for node i, v, as the sum of the two components, i.e., Following, we describe in detail each component. In order to leverage homophily, we perform a k-way graph partitioning to discover node communities. The number of partitions k, is controlled by a hyperparameter α, with α < 1, and is given by We explore two ways to capture position: either create a single level partitioning consisting of k partitions or a hierarchy of partitions. Fig. 2: Illustration of the position-speciﬁc component for node with id 2 in the case of multiple hierarchical levels. In this example, we assume the following: n = 625, L = 3, k = 5. We perform 5-way partitioning and we get the membership vectors z for the nodes. Since L = 3 and k = 5 we have m= 5, m component of node with id 2, from each level we retrieve the embedding of the partition the node belongs to, according to z. Note the different sizes of the three embedding tables. The embeddings of the coarsest level (level 0) are assigned higher embedding dimension (embedding table Pwith size m× d). The position-speciﬁc component of the node, p given by the summation of the three embeddings. 1) Single level approach: We learn a unique embedding for each of the k partitions. The position-speciﬁc component of a node embedding is the embedding of the partition it belongs to. In particular, we have P ∈ Rto be the embedding table containing the partitions’ embeddings. Then, for node i we have where zis a scalar indicating the partition id that i belongs to and P[z, :] is the z-th row of P. The size of the embedding table is k × d, where k  n and can be extremely small, as we will show in our experiments. We will refer to this method as PosEmb 1-level. 2) Hierarchical approach: The nodes themselves form the lowest level in the hierarchy, where one partition consists of a single node. The higher the hierarchical level, the coarser the communities are. Each level i in the hierarchy captures the relations and interactions of the communities in level i + 1. A hierarchy with L levels is constructed by applying recursive kway graph partitioning L times. We assign a number to each level, with the top level numbered 0, and the bottom level numbered L − 1. Level 0 is obtained by computing a k-way partitioning. Level 1 is obtained by partitioning each of them into k parts, leading to a total of kpartitions. Subsequent levels are obtained in a similar fashion recursively. Let m ∈ N be the number of total partitions across all levels. We have Again, we learn a unique embedding for each partition. This corresponds to learning strongly distinguishable representations as we move to higher levels in the hierarchy (coarser rep- = 25, m= 125. Then, for computing the position-speciﬁc resentations). The size of the embedding table then becomes m × d, where m  n. Another way to look at the case of multiple levels is to have L embedding tables, one for each hierarchical level. We construct each table in such a way that the coarsest level (level 0) is assigned higher learning capacity for each representation. The motivation behind that is the following. Each partition in this level includes the largest number of nodes which share the same embedding, compared to other levels. This means that there are more data samples available for training. As a result, these embeddings can be estimated more reliably. In order to balance the overall model compression, as we move to ﬁner levels, we decrease the embedding dimension. In particular, we have {P, P, . . . , P}, where P∈ Ris the embedding table containing the partitions’ embeddings of level i, m is the number of partitions in level i and dis the embedding dimension for level i with d = d> d> · · · > d. Let z∈ Rbe the membership vector for node i whose j-th component, z(j), contains the partition id that node i belongs to for level j. The vectors z for all nodes are the output of the partitioning. For the ﬁnal position-speciﬁc component of node i, we combine the corresponding hierarchical level representations by summing up all the partition embeddings across its hierarchical path. We have where P[z(j), :] is the row vector in the z(j)-th row of table P. Nodes that belong to the same partition share the same position-speciﬁc component of the ﬁnal embedding. Figure 2 presents an example of computing the position-speciﬁc component of a node when there are multiple hierarchical levels. Rather than ﬁtting a unique embedding vector for each node (as in the case of one-hot full embeddings), now each node’s embedding is selected from a shared pool of b embeddings (b hash buckets) by using hash functions to map nodes to the buckets. We use hashing in order to map the nodes to the shared embeddings and speciﬁcally, universal hashing for integers [13]. We follow the hash embeddings [7] paradigm as in Eq. 6. Speciﬁcally, we use h hash functions and for each node, we have h mappings to buckets, which correspond to h component vectors. Then, we learn node-speciﬁc importance weights to combine the h component vectors. In particular, the importance parameter vector yfor node i is represented as a row in an n × h trainable matrix Y. Each of the n rows is a vector of h scalar importance parameters. yis a scalar that corresponds to the importance weight of node i for the j-th component vector. Following, we describe two different ways to distribute the shared embeddings. 1) Intra-partition shared embeddings: The nodes that belong to the same partition in the coarsest level of the hierarchy (level 0) share c embeddings. There is a total of b shared embeddings and c = b/m, where mis the number of partitions in level 0. We have membedding tables, {X, X, . . . , X}, where X∈ Ris the embedding table containing the node-speciﬁc embeddings of partition i in level 0 (the coarsest level) and c is the compression factor and is equal to b/m. Then we have This approach can be combined only with the hierarchical approach, as far as the computation of the position-speciﬁc component is concerned. 2) Inter-partition shared embeddings: In this approach, following the hash embeddings approach [7], there is a set of b embeddings that are shared among all the nodes. We learn globally shared node embeddings, where we assign b shared embeddings for the nodes, irrespective of the partition they belong to. We have a single embedding table, X ∈ Rand we have The size of the embedding table for the node-speciﬁc computation is b × d, where b  n. We also need some additional space of size n×h for the importance weights, and typically, h = 1 or h = 2. In Algorithm 1, we describe our method PosHashEmb consisting of both the position-speciﬁc and node-speciﬁc components described in this section. We study the scalability of GNN models on large graphs for the task of node property prediction. We present our extensive experimental evaluation which aims to answer the following research questions: the performance? Algorithm 1 Position-based Hash Embeddings (PosHashEmb) Require: graph G, hyperparameter α, number of hierarchical levels L, embedding dimension d, number of nodes n, number of hash functions h. k-way metis partitioning. It returns a matrix Z with rows the membership vectors for all nodes, and a vector l ∈ R with values the number of partitions for each level. buckets of the node-speciﬁc term. ← d the position-speciﬁc component. node-speciﬁc component. importance weights of the node-speciﬁc component. performance of full embeddings? Is the combination of the two components more beneﬁcial than each of the components alone? the number of hierarchical levels? decrease the complexity of the node-speciﬁc part of the embedding? against other hashing-based methods? We use three datasets from Open Graph Benchmark (OGB) [12] for the task of node property prediction. The datasets’ statistics are presented in Table II. We use the default data splits provided by OGB and the same metrics to measure the performance. For ogbn-arxiv and ogbn-products datasets, the prediction task is multi-class classiﬁcation and accuracy is used as the performance metric. For the case of ogbn-proteins, the prediction task is multi-label binary classiﬁcation and the performance is measured by the average of ROC-AUC scores across the different kinds of labels. We consider the one-hot full embeddings (FullEmb) to be the method that requires the full-size amount of memory for the computation of the initial node embeddings. FullEmb gets as input a one-hot encoding for each node and learns a unique embedding for every node. We compare our method, PosHashEmb, against the following hashing-based approaches: method for handling large-vocab categorical features. It uses a single hash function to randomly map feature values into a smaller feature space (hash buckets). ters [14], Bloom Embeddings generate a binary encoding by using multiple hash functions. Then an embedding layer is applied to the encoding to retrieve the compact representation for the required feature value. use multiple hash functions and retrieve the corresponding entries from the embedding table (component vectors). HashEmb learns feature-speciﬁc weights that uses to control the contribution of each component vector for generating the ﬁnal embeddings. proposed method with non-one-hot encodings and a deep neural network (DNN) for computing embeddings. DHE ﬁrst encodes the feature value to a dense vector with multiple hash functions and then applies a DNN to generate the embedding. For each dataset, we choose two different GNN models implemented with Deep Graph Library (DGL) [15], which perform best based on the OGB leaderboard. We use the embedding method on top of the GNN model and we perform end-to-end training. For ogbn-arxiv, we use GCN [16] and GAT [17] models. For ogbn-proteins, we use MWE-DGCN and GAT [17]. Last, for ogbn-products, we use GRAPHSAGE [18] and GAT [17]. The training parameters for each model are set to those tuned by the DGL team. We implement all the methods using PyTorch and DGL [15]. We consider the case where we only use the identity features as the input features, i.e., one-hot encodings, and no additional node features, as the former are the ones that lead to large memory requirements increase coming from the size of the embedding table. For the embedding dimension d, we used 128 for ogbnarxiv and 100 for ogbn-products; the same as the dimension of the dataset’s original node features. For ogbn-proteins, where there were no node features originally, we tested values {30, 40, 50, 60, 40, 80, 90, 100, 150, 200, 250} and we report results for d = 200, where the full embeddings performed best. In the case of ogbn-proteins, we kept the 8-dimensional edge features. For PosHashEmb, we set α = 1/4, i.e.,lpmlpm k = n, c =n/kand b =n/kk, L = 3 and d= d, d= d/2, d= d/4, unless stated otherwise. For the graph partitioning we use METIS [19]. For PosHashEmb, HashEmb and Bloom we use h = 2. For DHE, we use h = 1024 as the number of hash functions used for the computation of the initial dense hash encodings and B = 10(this does not affect the size of the embedding table); both values proposed by the authors. For the computation of the embeddings, we used the default neural network architecture (equal-width MLP), which is the best performing according to the original paper [8]. The authors found that embedding networks with around ﬁve hidden layers perform better. However, this is not the case for our task, as we observed that a network of that depth performed poorly. To this end, we explored the depth of the network as well as the width of the hidden layers. For the number of hidden layers we tried values {0, 1, 2, 3, 4}, for the hidden width size (hidden dimensions) {500, 1000, 1500, 2000, 2500, 3000}, activation functions {relu, mish} (the two best performing activation functions proposed by the authors), and we tried every case with or without Batch Normalization [20]. We found small differences in performance regarding the two activation functions and inclusion of batch normalization or not. We report results for the best parameters: one hidden layer, hidden width size of 2000 and relu activation function. We run every experiment ﬁve times and we report the average performance and standard deviation. For ogbn-arxiv and ogbn-proteins datasets we perform full-batch training. For ogbn-products, we perform mini-batch training; we set the batch size equal to 1000 and we use as the sampling model the one where each node gathers messages from all its neighbors (full neighbor sampling). We used NVIDIA V-100 GPU for model training and inference. E. How does α affect the performance? (RQ1) In order to study how the number of partitions k affect the performance of our method, we only use the position-speciﬁc component for the computation of the node embeddings and we use a single level partitioning. We call this method PosEmb 1-level. We vary the value of k by controlling hyperparameter TABLE III: Performance comparison when we use the position-speciﬁc component for the computation of the node embeddings. α. We test the following values for α: {,,,,}. These correspond to the following values for k in each dataset: for ogbn-arxiv k = {5, 25, 125, 441, 9261}, for ogbn-products k = {7, 40, 343, 1600, 64000} and for ogbn-proteins k = {5, 25, 125, 400, 8000}. We present the results in Figure 3. In general, we need a large enough number of partitions to capture the positional relations of the nodes. Up to a certain point, as we increase k, either the performance does not signiﬁcantly change, as in ogbn-products dataset and ogbnarxiv in the case of GCN, or the performance deteriorates, as in the case of ogbn-proteins for both GNN models and ogbn-arxiv for GAT. One interesting observation is that in the case of ogbn-proteins dataset, even for the smallest number of partitions i.e., k = 5, the performance is better than full embeddings, and the amount of memory required to compute the initial node embeddings is reduced by 99% for the case of MWE-DGCN and 91% for GAT. F. Does the position-speciﬁc component improve the performance of full embeddings? (RQ2) In order to answer this, we start from using the full extent of the node-speciﬁc part of the embedding, i.e., the full embeddings (FullEmb), and we combine it with the positionspeciﬁc term of a single level partitioning (PosEmb 1-level). We call this method PosFullEmb 1-level. We also compare the performance with PosEmb 1-level, a method which consists solely by the position-speciﬁc component and this comes from a single level partitioning. As we can see in Table III, PosFullEmb 1-level improves the performance of FullEmb. This means that there is a beneﬁt from exploiting the position of nodes in the graph for embedding learning. We observe that PosEmb 1-level performs better than FullEmb in all cases, except for GAT on ogbn-arxiv where the performance is slightly worse. In addition, PosEmb 1-level not only performs better than FullEmb, but it also reduces the memory required to compute the initial node embeddings by 86% for ogbnproteins and GAT, up to 97% for ogbn-products and GAT. Further exploring the gains in performance coming from capturing the nodes’ position in the graph, we perform the following experiment. We compare the performance when using a community discovery partitioning (e.g., METIS) versus Fig. 3: Performance of PosEmb 1-level as a function of hyperparameter α for different number of partitions when using a single level partitioning. PosEmb 1-level consists of the position-speciﬁc component solely. Following OGB, for ogbn-arxiv and ogbn-products the performance is measured with accuracy, and for ogbn-proteins with ROC-AUC metric; the higher the values the better. a random partitioning. In particular, we compare PosEmb 1level with a method we call RandomPart that corresponds to the random partitioning. Essentially, RandomPart is a hashing trick with the number of hash buckets B to be equal to the number of partitions k. We present the results in Table III and we focus on the performance of RandomPart and PosEmb 1level methods. We can see that PosEmb 1-level leads to a better performance in all datasets and GNN models. G. What is the effect of hierarchy in the performance? (RQ3) We focus on the position-speciﬁc component and we explore how the performance of our method is affected by including multiple hierarchical levels. We start with a single level of partitioning (PosEmb 1-level) and we keep on adding levels up to three. We exclude entirely the node-speciﬁc component from the embedding computation. The results are presented in Table IV. We observe that as we increase the number of hierarchical levels, the performance either gets better or remains unchanged, except for GAT on ogbn-proteins in which case PosEmb 1-level performs better. At the same time, PosEmb 3-level reduces the amount of memory required to compute the initial node embeddings by 90% up to 99% across all datasets and GNN models. H. How much can we decrease the complexity of the nodespeciﬁc term? (RQ4) To answer this, we use both the components for the computation of the embeddings; we start from the full embeddings and we gradually decrease the complexity of the node-speciﬁc term, by keeping the position-speciﬁc component ﬁxed (all three hierarchical levels included). We test the two different ways of distributing the shared embeddings, discussed in Section III-B. For each of these ways, we further test two cases to compute the node-speciﬁc term of the embeddings with respect to complexity and number of learnable parameters. We end up with the following four ways: 1) Intra-partition shared embeddings: The nodes that belong to the same partition in the coarser level of the hierarchy share c = b/membeddings. We use hashing for assigning the nodes to the shared embeddings. We try the following cases: (i) a single hash function, h = 1, combined with learnable node-speciﬁc weights - PosHashEmb Intra (h = 1) method, (ii) two hash functions, h = 2, and learnable node-speciﬁc importance weights for the relative contribution of the h component vectors coming from the h hash functions - PosHashEmb Intra (h = 2) method. 2) Inter-partition shared embeddings: We learn globally shared node embeddings, where we assign b shared embeddings for the nodes, irrespective of the partition they belong to. Again, we test the two different ways mentioned above, which we call PosHashEmb Inter (h = 1) and PosHashEmb Inter (h = 2) methods, respectively. We present the results in Table V. As we can see, the performance is similar among the different ways to compute the node-speciﬁc component. In most cases, the performance is either the same or better compared to PosFullEmb, which induces memory requirements larger than the full size. This indicates that we do not need the full extent of the nodespeciﬁc component for good performance. At the same time, the PosHashEmb Intra and Inter methods, achieve to reduce the amount of memory required to compute the initial node embeddings by a range from 88% up to 97% across all datasets and GNN models. I. Performance comparison with the baselines. (RQ5) We compare the performance of competing methods against our method PosHashEmb. The position-speciﬁc component of PosHashEmb is computed with PosEmb 3-level and the nodespeciﬁc with Intra h = 2. We test the performance when the memory requirements are approximately equal to 1/2, 1/6 and 1/12 of the full size for ogbn-arxiv and ogbn-proteins, and equal to 1/2, 1/18 and 1/34 of the full size for ogbn-products as there is much more room for memory reduction in the case of this larger dataset. For the hashing-based methods, the memory requirements are determined by the value of B. For PosHashEmb, we control the memory requirements by adjusting the value of b of the node-speciﬁc component. When is needed, for the case of the smallest amount of memory, we use only the positionspeciﬁc component for computing the node embeddings, e.g., PosEmb 1-level with k selected accordingly, in order to match the desired memory requirements. For DHE, we control the required amount of memory by adjusting the number of hidden layers and their width. For the largest dataset ogbn-products, we were not able to run DHE with the same batch size of 1000 as all other methods, because of GPU memory limitations. While trying smaller batch sizes of 500, 300, 100, we observed that the performance signiﬁcantly deteriorated and thus, we do not report these results. The results are presented in Figure 4. As we can see, PosHashEmb performs better than FullEmb in all cases except for ogbn-arxiv and GAT. This is true even in the case of the smallest number of trainable parameters. According to our results, PosHashEmb performs better than all other approaches; this is true even in the cases where we have reduced the number of trainable parameters the most, with the exception of the ogbn-proteins dataset. Note that for the largest dataset, ogbn-products, PosHashEmb has the best performance and at the same time achieves up to 35 times less parameters or 97% memory savings compared to full embeddings. Another important observation is that the performance of PosHashEmb does not seem to vary signiﬁcantly as we modify the amount of memory required to compute the initial node embeddings across all datasets and GNN models. This indicates that we are able to achieve whatever memory savings required based on the available resources, while keeping the performance high. Hashing has been widely used for compression of the feature space in recommender systems and NLP applications. Hashing trick [6] is the simplest technique that uniformly maps feature values to a smaller number of shared hash buckets. Serra et al. [9] motivated by bloom ﬁlters [14], they TABLE IV: Performance results for the different levels of hierarchy for the computation of the position-speciﬁc component of the embedding. use multiple hash functions to generate binary encodings. Then a linear layer is applied to the encoding to recover the embedding for the given feature value. Hash embeddings [7] use multiple hash functions to retrieve multiple entries from the shared embedding table and then combine them to generate the ﬁnal embeddings. The main contribution of this method compared to others using multiple hash functions, is that it learns importance weights dedicated to each feature value which control the contribution of each entry to the ﬁnal embedding. In recommender systems, Zhang et al. [10] separate the features based on their frequency, as an indicator of their importance. They ensure that the most frequent ones will be assigned a unique embedding (zero collisions for those) and the rest will be hashed to shared embeddings using two hash functions (double hashing). In a recent work, Shi et al. [21] create a unique embedding for each category by composing shared entries from multiple smaller embedding tables. Again in the recommendation domain, Kang et al. [8] propose DHE that replaces one-hot encodings with dense vectors from multiple hash functions. Then it trains a feedforward network to produce the ﬁnal embeddings. While all the aforementioned methods are designed to reduce or eliminate collisions coming from hashing, there is another line of work, similar to ours, that uses hashing as a means to maintain a notion of similarity. Locality-sensitive hashing [22] hashes similar features into the same buckets and as such, aims to maximize collisions. HashRec [23] is a learning-to-hash method that learns preference-preserving binary codes for users and items in top-N recommendation. By using the hamming distance, it estimates preferences between users and items. In this work, we study the problem of node classiﬁcation using GNNs and we focus on the case when there are no available node features or the features are weak. In such cases, GNNs use one-hot encodings to learn an embedding layer and compute node embeddings that can then be used as input features; this can be memory expensive and is not scalable for large graphs. We present a family of methods that take into account the nodes’ position in the graph to compute efﬁcient node embeddings and reduce the number of trainable parameters signiﬁcantly. Our ﬁnal embeddings are generated by the combination of a position-speciﬁc component and a node-speciﬁc component; the so-called PosHashEmb method. The starting point for all our methods is to discover communities of similar nodes. For each partition we learn a unique embedding. Then, we build up the complexity of the embeddings by creating multiple hierarchical layers of communities and by including a node-speciﬁc term. For the latter, we use hashing and node-dedicated learnable weights to enhance the embedding with more localized, ﬁner, nodespeciﬁc signals. The complexity of the node-speciﬁc component can vary depending on the desired model compression. It could even Fig. 4: Performance as a function of the amount of memory required to compute the initial node embeddings, in all datasets and GNN models. The number of trainable parameters of FullEmb for ogbn-arxiv is around 22M, for ogbn-products around 245M and for ogbn-proteins is around 28M. The plotted values for the number of parameters correspond to memory size which is approximately equal to 1/12, 1/6 and 1/2 of the full size for ogbn-arxiv and ogbn-proteins, and equal to 1/34, 1/18 and 1/2 of the full size for ogbn-products. PosHashEmb uses PosEmb 3-level for computing the position-speciﬁc component and Intra h = 2 for the node-speciﬁc component. Following OGB, for ogbn-arxiv and ogbn-products the performance is measured with accuracy, and for ogbn-proteins with ROC-AUC metric; the higher the values the better. be excluded entirely when we need to induce extremely high model compression. In such cases, only the position-speciﬁc component is used and as we showed, PosEmb 3-level has better or similar performance with full embeddings, and at the same time achieves to reduce the amount of memory required to compute the initial node embeddings by 90% up to 99% across all datasets and GNN models. Our methods model homophily which is a strong characteristic of many real-world graphs, and as our experimental results showed, this is highly beneﬁcial for the performance. Speciﬁcally, our method PosHashEmb, that combines both components, performs better than both state-of-the-art hash-based techniques and full embeddings in almost all cases, and at the same time achieves great memory savings, reaching 88% up to 97% of the full size across the different datasets and GNN models. This work was supported in part by NSF (1447788, 1704074, 1757916, 1834251), Army Research Ofﬁce (W911NF1810344), and the Digital Technology Center at the University of Minnesota. Access to research and computing facilities was provided by the Digital Technology Center and the Minnesota Supercomputing Institute.