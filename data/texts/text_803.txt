Australian National UniversityAustralian National University zixian.cai@anu.edu.austeve.blackburn@anu.edu.au Abstract—Despite the long history of garbage collection (GC) and its prevalence in modern programming languages, there is surprisingly little clarity about its true overheads. Even when evaluated using modern, well-established methodologies, crucial tradeoffs made by GCs can go unnoticed, which leads to misinterpretation of evaluation results. In this paper, we 1) develop a methodology that allows us to place a lower bound on the absolute overhead (LBO) of GCs, and 2) expose key performance tradeoffs of ﬁve production GCs in OpenJDK 17, a high-performance Java runtime. We ﬁnd that with a modest heap size and across a diverse suite of modern benchmarks, production GCs incur substantial overheads, spending 7–82 % more wall-clock time and 6–92 % more CPU cycles relative to a zero-cost GC scheme. We show that these overheads can be masked by concurrency and generous provision of memory/compute. In addition, we ﬁnd that newer low-pause GCs are signiﬁcantly more expensive than older GCs, and sometimes even deliver worse application latency than stop-the-world GCs. Our ﬁndings reafﬁrm that GC is by no means a solved problem and that a low-cost, low-latency GC remains elusive. We recommend adopting the LBO methodology and using a wider range of cost metrics for future GC evaluations. This will not only help the community more comprehensively understand the performance characteristics of different GCs, but also reveal opportunities for future GC optimizations. Garbage collection (GC) is ubiquitous in software systems. Managed languages, such as C#, Java, and JavaScript, continue to grow in popularity due to their productivity and safety beneﬁts, which are in part provided by GC. On servers, many widely used web services, such as Twitter, GitHub, Shopify, and Alibaba, make extensive use of such languages. On clients, JavaScript engines are embedded in every web browser, and Java runtimes are embedded in every Android phone. Despite the ubiquity of GC, there is a surprising lack of clarity regarding its costs. In this paper, we address two key problems: a) understanding the overheads of GC algorithms relative to a notional zero-cost baseline, and b) mitigating the misinterpretation of GC evaluations, which is common even when modern, well-established methodologies are used. A lack of clarity regarding the overheads of a technology such as garbage collection can have profound impacts. For potential GC users, the decision of which GC algorithm to use—or even to use GC at all—hinges on a clear understanding of the true cost of GC algorithms. Without such information, a user might not realize how much a GC algorithm costs; such cost might not actually be acceptable to the user if they had known it. Furthermore, the decision of whether to use a Ohio State UniversityGoogle mikebond@cse.ohio-state.edummaas@google.com language with GC cannot be easily reversed, as switching to an alternative, such as C, C++, or Rust, is typically a huge undertaking. For GC developers, the importance of a clear understanding of GC overheads is twofold. First, a costly GC technique presented as cheap can mislead the community into overusing it, discouraging future optimizations, and dismissing cheaper alternatives. Second, identifying costly techniques can reveal opportunities for hardware acceleration and optimization across the stack [1], especially in datacenter settings. We now offer more detail regarding these two problems and outline our contributions. a) Unclear absolute overheads: When evaluating GCs (i.e., GC algorithms), it is rare to report the overheads of GCs relative to a zero-cost baseline, presumably because of the lack of a good methodology for establishing such a baseline. Hertz and Berger [2] attempted to measure these overheads. However, their approach relies on simulation and it is unclear whether the conclusions can be extrapolated to GCs running on real hardware. To address this problem, we introduce a lower-bound overhead (LBO) methodology to empirically estimate the absolute overheads of GC by approximating a zero-cost GC scheme. In contrast with Hertz and Berger [2], the LBO methodology can measure GCs running on real hardware, and does not require invasive runtime instrumentation. Due to its simplicity, the LBO methodology makes minimum assumptions about the GC implementation, and should therefore be easy to apply to different languages, runtimes, and GC algorithms. We systematically evaluate ﬁve production GCs in OpenJDK 17, the latest release of an industrial-strength, high-performance JVM. The LBO methodology offers clarity with respect to the true overheads of production GCs. We ﬁnd that these GCs impose substantial overheads to the program execution across a diverse set of workloads. Using a modest heap size, our LBO methodology estimates that by using GC, applications spend 7–82 % more wall-clock time and 6–92 % more CPU cycles relative to a zero-cost GC scheme. b) Misinterpretation of evaluation results: Not seeing the full picture of different collectors’ performance can lead to misinterpretation. Speciﬁcally, knowing the limitations of a GC can change our verdict on its suitability in certain use cases. In Fig. 1 and Fig. 2, we investigate three production OpenJDK collectors—Serial, G1, and Shenandoah—running the lusearch benchmark. We use four commonly reported (a) Total wall-clock time, normalized to the best value. Lower is better. G1 outperforms Serial for all but the two smallest heap sizes. Fig. 1: Two cost metrics for the total execution cost of Serial and G1 running the lusearch benchmark. The heap size is relative to the minimum required by G1. Each line and its shade represent the mean and 95 % conﬁdence interval over 20 invocations. (a) Average time (ms) per pause. Lower is better. Shenandoah consistently outperforms G1 for all heap sizes. Fig. 2: Two metrics for measuring the suitability of GCs on the latency-sensitive lusearch benchmark. The heap size is relative to the minimum required by G1. Each line and its shade represent the mean and 95 % conﬁdence interval based on 20 invocations. performance metrics. When looking at (parts of) each subﬁgure in isolation, we can arrive at different, sometimes diametrical, verdicts on the suitability of these collectors. 1)It is well known that GC is fundamentally a time–space tradeoff [3], [4]. If we chose to focus on a single heap size in Fig. 1a, we could arrive at two opposite conclusions. At1.9×heap, G1 is about 7 % slower than Serial, while at 3.0× heap, G1 is about 9 % faster than Serial. 2)G1 appears to be better if we optimize for the application throughput (Fig. 1a). However, its true cost is masked by parallelism, which we can only see when we measure the CPU cycles consumed. On a machine with less free hardware to spare, the performance of G1 might degrade due to the overall higher CPU consumption (Fig. 1b). Similarly, in a multi-tenant environment, extra cycles used by the GC lead to fewer cycles available for other applications on the same host (opportunity cost). 3)When optimizing for latency-sensitive workloads, such as lusearch, Shenandoah seems better suited because of shorter pause times(Fig. 2a). However, Shenandoah ends (b) Total CPU cycles, normalized to the best value. Lower is better. Serial outperforms G1 for all heap sizes. (b) 99.99th percentile metered query latency (ms). Lower is better. G1 consistently outperforms Shenandoah for all heap sizes. up delivering worse application latency than G1 (Fig. 2b) due to stalling the application when it cannot keep up with the allocation rate (discussed in detail in Section IV-C). The above examples highlight the risks of evaluating GC with limited metrics. In this paper, we measure ﬁve production GCs using multiple metrics including wall-clock time, CPU cycles, and application latency. Our results suggest that the tradeoffs made by different GCs can go unnoticed when employing na¨ıve evaluation regimes such as those shown above. Our surprising ﬁndings include situations where low-pause GCs (Shenandoah and ZGC) achieve worse application latency than simple stopthe-world GCs while also having much worse throughput. The LBO methodology is a simple way to estimate the absolute overheads of GCs, revealing the substantial costs incurred by widely used production GCs. We recommend using richer sets of metrics when evaluating GCs. This is important for revealing the tradeoffs and limitations of different collectors, which assists GC users in choosing the most appropriate GC for a plethora of existing and emerging workloads. Our ﬁndings about the overheads of production GCs invite future research. In this section, we review sources of GC overhead and describe the current, established evaluation methodologies. We highlight three sources of GC cost, which on their own are insufﬁcient to adequately characterize GC overheads. There are also a few attempts to measure the overall cost of GC, and we will point out their limitations. Finally, we will give an overview of the collectors that we study in this work, and how they contain these hard-to-measure sources of GC cost. A. Costs Tightly Coupled with Application Execution A GC can be considered in terms of three principal activities: allocation, identiﬁcation, and reclamation [9]. Allocation is performed by the application using space provided by the GC. Identiﬁcation establishes which part of the heap is live and which may be reclaimed. Reclamation makes unused space available for re-use. For performance reasons, some GC mechanisms such as allocators and read and write barriers are tightly coupled with application (mutator) execution, implemented using the fast-and-slow-path paradigm [10]. Fast paths are frequently executed, often inlined into the mutator code by the JIT compiler. Because they are so tightly integrated into the mutator, their costs are hard to measure. Blackburn et al. [4] placed an upper bound on the cost of a bump-pointer allocator by compiling the allocation fast-path out-of-line. Then, that upper bound was used to derive the cost of a free-list allocator using their relative performance. Barriers are code snippets that mediate mutators’ heap operations on behalf of the GC. Their cost has been investigated by prior work [11]–[14]. The methodologies used in prior work remove the requirement of barriers for correctness, e.g., through a full-heap trace for a generational GC, and then measure the execution with and without barriers. These approaches require deep understanding of the GC implementation and modiﬁcation of the collector and/or runtime, and consequently are not trivially applied to arbitrary language runtimes. Reclamation is typically performed directly by the GC but can also be performed by the mutator. For example, in the case of na¨ıve reference counting (RC) [15], the site that decrements the reference count of an object to zero can immediately reclaim the memory. Blackburn et al. [4] measured the cost of a deferred reference counting collector in terms of the mutator time. B. Indirect Costs/Beneﬁts Apart from the direct costs of GC, it can also bring indirect costs, and sometimes beneﬁts. A common source is mutator locality. GC impacts locality through sharing caches with the mutator. In the case of concurrent GC, GC threads contend with mutator threads when they share caches. In the case of stop-the-world pauses, GC code displaces the cache, leaving mutator threads resuming with a cold cache. However, GC can also improve the mutator locality through rearranging objects. A compacting or evacuating GC can improve the spatial locality by moving objects that are frequently accessed together to be spatially closer to each other [16]. Some of the locality impact might be observable through hardware performance counters, such as cache miss events (e.g., LLC and TLB). However, in general, it is hard to tease out the locality impact of GC running on real hardware, because that would require achieving the same GC effects (like compacting the heap) without affecting cache state. It may be possible to achieve such an effect under simulation, but it would be a major undertaking, likely requiring invasive changes to the runtime and signiﬁcant compute resources to evaluate. C. Absolute Overheads of Garbage Collection Hertz and Berger [2] attempted to quantify the overhead of garbage collection over an explicit memory management regime. By tracing the program execution, they constructed an oracle which guided the insertion ofmallocandfreecalls. However, they relied on simulation, the ﬁdelity of which is unknown. In addition, the algorithms and the workload used in the evaluation are dated. The LBO methodology measures GC running on real hardware and does not rely on an oracle to construct a baseline. Some work (such as [17]) has measured the cost of conservative GC in the context of explicitly managed languages. We focus on precise GC in managed languages. Numerous studies (such as [4]) measure GC pauses to quantify the GC costs. We show that this approach is problematic because some GC costs happen outside of GC pauses and are tightly coupled with mutator activities. In particular, for concurrent GCs, the GC pauses are shorter, and the majority of the work is shifted to run concurrently with mutators. D. Garbage Collection Algorithms in HotSpot Table I shows the production GCs studied in this work. The ﬁrst of these, Epsilon, does not actually collect garbage, but is used as part of our LBO methodology. The others can be divided into three groups: stop-the-world collectors (Serial and Parallel), concurrent tracing collector (G1), and concurrent copying collectors (Shenandoah and ZGC). A stop-the-world collector requires all mutators to be stopped while it is running, i.e., it does not exhibit any concurrency with respect to the mutator. A concurrent tracing collector performs garbage identiﬁcation via a trace which executes concurrently with the mutator. The trace does not modify the heap, but marks reachable objects as live. The correctness of the concurrent trace is typically protected by write barriers. A concurrent copying collector performs reclamation concurrently with the mutator by copying objects. This involves modifying the heap and ensuring that the concurrently executing mutator maintains a coherent view of the heap even when objects it references are moved. The correctness of concurrent copying is typically protected by read barriers. The production GCs we study are all based on tracing to establish liveness of objects. Apart from the STW collectors, where a mark-sweep-compact policy is used for the mature space, the concurrent collectors strictly rely on copying to reclaim memory, with G1 performing the copying in STW pauses, while Shenandoah and ZGC perform copying concurrently. Since the main design goals of the two concurrent copying collectors are to reduce the GC pause times and improve the responsiveness for latency-sensitive applications, we also refer to them as the low-pause collectors in the rest of the paper. As discussed in Section I, it is important to quantify the absolute overhead of GCs relative to a zero-cost baseline (with respect to some metrics). Knowing the overhead can, for example, help us assess whether GC optimizations can yield fruitful improvements to the overall performance of the application. In this section, we introduce the lower-bound overhead (LBO) methodology that gives a lower bound on the absolute overhead of GCs using empirical data. First, we sketch the intuitions underpinning LBO using an example with three production GCs running the h2 benchmark. This also serves as a running example throughout the section. Then, we give a formal deﬁnition of the LBO methodology. Finally, we discuss the advantages and the limitations of the LBO methodology. A. Intuitions It is easy to measure the relative overheads of different GCs. Table II shows the total cycle usage for three production GCs, Serial, Parallel, and Shenandoah, running the h2 benchmark. When normalized to the best value (Serial), we know that Serial is the cheapest GC in terms of cycles, with Parallel 0.2 % and Shenandoah 102.3 % more expensive. However, we do not know the absolute overhead of each collector. If an ideal GC existed, we could measure its cycle usage, and then use that to normalize the cycles of each real collector, giving us the absolute overheads. The ideal GC has all the beneﬁts of GC (such as improved locality through defragmenting the heap) but none of the costs (such as traversing the heap to identify live objects). Of course, the ideal GC does not exist. Our key insight is that we can establish an upper bound on ideal cycles using a real GC by subtracting cycles that we can ascribe to GC from its total cycles. TABLE II: The total CPU cycles consumed when running h2 with a generous3×heap (see SectionIV-A) using three different collectors. Lower the better. The cycles are also normalized to the best collector (Serial) shown in green. TABLE III: Further attribute the CPU cycles consumed in Table II to cycles used during stop-the-world (STW) pauses, and other cycles. The best other cycles is shown in green. This best value is used for the LBO calculation in Table IV. TABLE IV: The LBO value for each collector can be obtained by normalizing the respective total cycles by the smallest other cycles in Table III. TABLE V: A collector with a smaller other cycles (shown in green) is found. This allows us to derive tighter lower bounds, i.e., larger LBO values. As shown in Table III, one example of measuring the apparent GC cycles is to measure the cycles spent in stopthe-world (STW) pauses, where no mutator activities happen, and the cycles spent outside STW pauses (other). By deﬁnition, the other cycles of each GC is strictly greater than the ideal cycles. In other words, they yield upper bounds of the ideal cycles. The minimum of the other cycles among the three collectors gives the tightest upper bound of the ideal cycles for that set of collectors. We obtain the minimum using the 103.87 billion other cycles of Parallel. When normalizing the total cycles of each collector by this tightest upper bound on ideal cycles, we get a lower bound on the overheads (LBO) of the respective collector (see Table IV). The estimative nature of the LBO methodology means we can gradually reﬁne the estimation and obtain tighter lower bounds (numerically larger). As shown in Table V, when a collector (a hypothetical collector in this case) with a smaller other cycles is found, we have a better estimation of the ideal cost, which in turn allows us to have a better estimation of the absolute overheads of each GC, which is reﬂected as the larger LBO values for all collectors compared with Table IV. B. Deﬁnition Even though cycles is used in the previous section as an example, the LBO methodology does not depend on the choice of cost metric. Any notion of cost metrics, such as CPU cycles, the wall-clock time, or energy consumed, can be used as the Costin the formulae below. We use tilde to denote estimations. For example,gCostis an estimation of Cost. LetGbe the set of GC algorithms that we study. In the previous example,G = {Parallel,Serial,Shenandoah}. For some ﬁxed workloadWand hardwareH, we have the ideal costCostintrinsic to runningWonH. Then, for each g ∈ G, we deﬁne its absolute overhead by normalizing its total cost (i.e.,Costmeasured throughout the program execution) by the ideal cost: Overhead(g) = Cost(g)/Cost. In reality, we do not knowCost, and therefore, cannot calculateOverhead(g). However, for each GCg ∈ G, we can measure the apparent GC costgCost(g), e.g., by only counting theCostduring STW pauses. The insight is thatCost(g) andgCost(g)allow us to place an upper bound onCost. By deﬁnition,Cost(g) −gCost(g) > Cost. That is, the difference yields an upper bound ofCost. For the set of GCsGwe study, the tightest upper bound onCostcan be found byminCost(g) −gCost(g). We usegCost to denote this tightest upper bound. Note that the choice ofg that achieves the minimum depends on the workloadWand hardwareH. In the previous example, we obtain the minimum when g = Parallel (see Table II). Then, for eachg ∈ G, we deﬁne its lower bound overhead byLBO(g) = Cost(g)/gCost, analogous to the deﬁnition ofOverhead(g). SincegCostis an upper bound ofCost, LBO(g)is a lower bound onOverhead(g). For the previous example, this calculation is detailed in Table IV. C. Discussion The biggest advantage of the LBO methodology is its simplicity. It is agnostic to the particular GC algorithms and runtimes. The only requirement is to measure the apparent GC cost, which is conceptually easy, and should be possible with simple, non-intrusive instrumentations. In fact, for OpenJDK, this can be implemented using JVMTI callbacks [18], and for .NET, implemented on top of GCRealTimeMon [19]. However, the quality of the LBO estimation greatly depends on the ability to approach the ideal by identifying and attributing as muchCostto GC as possible. For example, if one only attributes theCostincurred using STW pauses to the GC, then the LBO estimation based only on concurrent collectors will be too low since the costs of concurrent GC tasks will not be attributed to the GC. To improve the estimation, the approach needed depends on the cost metric and how the apparent GC cost is captured. We give two speciﬁc examples. 1)When measuring the CPU cycles used, one can get per-thread readings from the performance monitoring units (PMUs), and then combine the activities of GC threads both during pauses and when running concurrently. This gives better estimations than measuring the wholeprocess cycles in and outside of STW pauses. 2)When measuring the wall-clock time, one can try to include GCs that are cheap outside STW pauses where possible. For example, a STW GC with no barrier is likely to give better estimation of the ideal time than a STW GC with a write barrier (e.g., a generational GC), which in turn is likely better than concurrent GCs. We also note that the magnitude of LBO values depends on the performance of GCs relative to the rest of the runtime. For example, with everything else constant, the same GC is likely to have a lower LBO value when paired with a simple interpreter than an optimizing compiler, because the application code is more expensive to execute with the interpreter (i.e., higher Cost). Furthermore, compiler optimizations that interact with allocations can also lead to lower LBO values with no change to the GC. For example, a compiler that performs escape analysis can reduce overall allocation, and therefore reduces the pressure on the GC. Such analysis allows objects to be allocated on the stack rather than on the heap when the objects do not escape. IV. CASE STUDY: COLLECTORS IN OPENJDK 17 Recall that we address two key problems: 1) the lack of good methodology to quantify the absolute overheads of GCs, and 2) the fact that GC evaluation results are prone to misinterpretation even when well-established methodologies are used. The previous section proposed the LBO methodology to estimate the absolute overheads of GCs. In this section, we perform a case study of GCs in OpenJDK 17, the latest release of a high-performance production JVM. First, we apply the LBO methodology on these GCs, and measure their wall-clock time LBOs (time LBOs) and total cycle LBOs (cycle LBOs). Second, we concretely show how the evaluation of these GCs can be misinterpreted when deploying only a few metrics. We then demonstrate how to mitigate this problem by using a richer set of metrics, including the LBO values. Our approach allows us to observe the following: 1)Garbage collection overheads: When deployed with a modest heap size, production collectors can incur substantial absolute overheads across workloads. This is revealed by both high time and cycle LBOs. A surprising trend is that in addition to being signiﬁcantly slower and cycle-intensive, the low-pause collectors fail to deliver better application latency for the evaluated workloads. 2)Misinterpretation of evaluation results: We highlight three important types of misinterpretation: a) not considering opportunity cost, b) not considering concurrency overhead, and c) measuring pause time instead of application latency. Based on our observations, we recommend: 1) that the LBO methodology should be used to report the absolute overheads of GCs, and 2) that more performance and cost metrics should be used in order to evaluate GCs holistically. TABLE VI: LBO total time overhead averaged over 16 benchmarks. The best value for each heap size is shown in green. Where a collector cannot run all benchmarks at a particular heap size, the entry is left blank. Parallel outperforms other collectors, except at1.4×, where G1 has the lowest cost. A. Methodology a) Benchmarks and latency measures: We use a snapshot release of the forthcoming Chopin release of DaCapo benchmark suite [20]. We exclude the benchmarks cassandra, h2o, and kafka, because they include depreciated Java features that are not compatible with JDK 17. DaCapo’s Chopin snapshot release includes a number of latency-sensitive benchmarks. Apart from the benchmark time, which is reported for all benchmarks, DaCapo reports two measures of latency, simple and metered, for the latency-sensitive benchmarks. Latency-sensitive services handle remotely issued requests (e.g., over a network) that arrive at some remotely determined rate. When such a system is unable to process a request immediately, it is placed in a queue. The latency of a request is impacted by three major sources of delay: the uninterrupted time taken to compute the request; the time taken inclusive of interruptions such as GC and scheduling; and the time taken inclusive of interruptions and queuing. DaCapo’s simple latency ignores queuing, while metered latency models requests coming at a metered rate with an arbitrary-sized queue. When an interruption such as a GC pause occurs, the metered measure reﬂects the delay that this imposes not only on the currently executing requests, but also on those that are enqueued during the delay. We use metered latency here because it more accurately models latency-sensitive services. b) Cost metrics: We use the Java Virtual Machine Tool Interface (JVMTI) [18] to measure the GCs. The interface provides callbacks for when a GC pause starts and when it ends. This allows us to break down the cost for the execution into the costs incurred in stop-the-world (STW) pauses and outside STW pauses. Our JVMTI agent captures metrics including the wallclock time, CPU cycles, instruction counts, cache misses, and Intel RAPL energy measurements. The performance counter readings are obtained from theperf_eventssubsystem in the Linux kernel. In this section, we focus on two important metrics, the wall-clock time and CPU cycles. c) JVM parameters: In this paper, we focus on the out-ofthe-box performance characteristics of GCs, and deliberately do not set any GC-related parameters except for the heap size. GC tuning is often speciﬁc to particular (classes of) workloads, whereas our concern is how well each GC handles a diverse set of workloads. Also, in general, GC tuning is an openended problem, outside the scope of this paper. We report the performance of each GC for different heap sizes, because the TABLE VII: LBO cycle overhead averaged over 16 benchmarks. The best value for each heap size is shown in green. Where a collector cannot run all benchmarks at a particular heap size, the entry is left blank. Serial consistently outperforms other collectors for all heap sizes. performance of GC is sensitive to the heap size [20]. Apart from Epsilon, which does not perform GC, we set the heap size relative to the minimum heap size required to run each benchmark (e.g.,2.0×means that the heap size is set to be twice as big as the minimum heap required for a particular benchmark). The minimum heap size for each benchmark is measured using G1 because it is the most space-efﬁcient GC among the ones we study. The only other JVM parameters we set are-server -XX:-TieredCompilation -Xcomp, to speed up the warmup of the JVM and reduce the experimental noise due to JIT compilation. We omit parameters-XX:-TieredCompilation -Xcompfor tradebeans and tradesoap, because these parameters cause these two benchmarks to crash for the version of OpenJDK we use. d) Execution methodology: For each conﬁguration, we invoke each benchmark for 20 times. We interleave invocations of different conﬁgurations to minimize bias due to systemic interference. In each invocation, the benchmark performs ﬁve iterations and we report results from the last iteration, with the ﬁrst four iterations serving to warm up the runtime. For each conﬁguration, we report the mean and the 95 % conﬁdence interval (CI) based on the 20 invocations. e) Hardware: All machines have Intel Core i99900K (Coffee Lake) CPUs (8 cores, 16 threads), with 4×32G DDR4-3200 memory. We disable the dynamic frequency scaling (i.e., Turbo Boost) to reduce experimental noise. f) Software: All machines run identical Ubuntu 18.04.6 LTS images with5.4.0-89-generickernels. We use the Eclipse Temurin (formerly AdoptOpenJDK)’s distribution of OpenJDK. For OpenJDK 17, we use theTemurin-17.0.1+12 release. We focus on OpenJDK 17 in this paper, as it is the latest LTS release as of writing and is supposed to bring many GC performance improvements (such as [21], [22]) since OpenJDK 11. We also measured OpenJDK 11 using the AdoptOpenJDK-11.0.11+9release, and the overall results were quite similar to those of OpenJDK 17. All benchmarks are executed on an otherwise idle machine, with as many background daemons and periodic tasks disabled as possible. B. Results: Garbage Collection Overhead First, we estimate the absolute overheads of the ﬁve garbage collectors other than Epsilon using the LBO methodology. Epsilon is only used in calculating the LBO values in the cases where it is able to run a benchmark without exhausting the memory available on our machine. TABLE VIII: Total time overhead at3×heap using LBO. Lower is better. xalan is excluded from the summary statistics due to ZGC failing to run, and the corresponding row is grayed out. The best results for each benchmark are shown in green (light green for xalan). Each LBO is the mean of 20 invocations. The 95 % CIs are all less than 2 % except for 8 % for pmd/ZGC. Parallel outperforms other collectors for most benchmarks. Table VI shows time LBOs of the collectors we study while Table VII shows cycle LBOs. Each table covers eight different heap sizes, ranging from a small1.4×heap to a very generous 6.0×heap (see SectionIV-Afor the multiplier notation). The LBO values for each conﬁguration (a collector at a heap size) is the geometric mean for 16DaCapo benchmarks. The production GCs incur substantial overheads. For a modest2.4×heap, production GCs on average spend 7–82 % more wall-clock time and 6–92 % more cycles relative to a zerocost GC. Even for a very generous6.0×heap, the overheads are as much as 23 % in terms of time and 34 % in terms of cycles. For all heap sizes shown, Serial achieves the lowest overheads in terms of cycles, while Parallel achieves the lowest overheads in terms of time for all but the smallest heap size. Table VIII and Table IX take a closer look at the3.0×heap size. This allows us to observe how different collectors behave when challenged with a diverse, modern workload. C. Analysis of Results We compare the performance trends among and within three groups of GCs (see Section II-D). a) Stop-the-world (STW) GCs vs Concurrent GCs: Overall, STW collectors (Serial and Parallel) are cheaper than concurrent collectors (G1, Shenandoah, and ZGC), both in TABLE IX: Cycle overhead at3×heap using LBO. Lower is better. xalan is excluded from the summary statistics due to ZGC failing to run, and the corresponding row is grayed out. The best results for each benchmark are shown in green (light green for xalan). Each LBO is the mean of 20 invocations. The 95 % CIs are all less than 2 % except for the following: 3 % for batik/ZGC, and 10 % for pmd/ZGC. Serial outperforms other collectors for most benchmarks. terms of the time and cycles. The exception is that Serial is uncompetitive in terms of time due to its lack of parallelism. At 3.0×heap, in terms of cycles, STW collectors are never more expensive than concurrent collectors.In terms of time, STW collectors are only more expensive than concurrent collectors for two out of 17 benchmarks: batik and zxing. b) Single-threaded vs multi-threaded STW GC: Between two stop-the-world collectors, Parallel is more expensive than Serial in terms of cycles. The difference is as much as 48 % for a small1.4×heap. However, the converse is true in terms of time. This is presumably due to the synchronization overhead of a multi-threaded collector. At3.0×heap, in terms of cycles, Parallel is only cheaper than Serial for three out of 17 benchmarks: lusearch, tradebeans, and tradesoap.In terms of time, Serial is only cheaper than Parallel for avrora. c) Concurrent tracing vs concurrent copying GC: Among concurrent collectors, the newer, concurrent copying collectors (Shenandoah and ZGC) are signiﬁcantly more costly than the concurrent tracing collector (G1). The difference is up to 75 % in terms of cycles (G1 vs ZGC at2.4×heap) and 78 % in terms of time (G1 vs Shenandoah at1.9×heap). This is presumably due to the use of costly read barriers TABLE X: Percent of time spent in STW pauses averaged over 16 benchmarks. Lower the better. The best value for each heap size is shown in green. Where a collector cannot run all benchmarks at a particular heap size, the corresponding entry is left blank. ZGC consistently outperforms other collectors where it runs. and the (un)timeliness of reclamation, which we will discuss below. At3.0×heap, G1 consistently outperforms Shenandoah and ZGC for all benchmarks in terms of time. In terms of cycles, Shenandoah/ZGC is only cheaper than G1 for batik, and xalan (not statistically signiﬁcant for sunﬂow). d) Pathological Modes of Concurrent Copying Collectors: We ﬁnd that the low-pause, concurrent copying collectors can deliver worse application latencies while incurring substantial costs. We observe two pathological modes of concurrent copying collectors when challenged with workloads that have high allocation rates (GB/s relative to the heap size). One stark result is for xalan, where Shenandoah has an enormous time LBO of 30.2, about ten times that of Serial/Parallel/G1. ZGC simply failed to run xalan with OOM errors. However, Shenandoah has a modest cycle LBO of 1.74, which is close to the 1.63 LBO of Parallel and even slightly better than the 1.78 LBO of G1. To understand this behavior, recall that Shenandoah and ZGC rely on tracing to establish liveness, and strictly rely on evacuation for reclamation (see SectionII-D). The consequence is a substantial delay between when an object becomes unreachable and when the unreachable object is reclaimed. The delay’s impact is ampliﬁed by the high allocation rates of benchmarks such as xalan and lusearch; because allocation would fail if reclamation did not keep up with the allocation rate, and the collector had to resort to STW collections. In this case, it might be more beneﬁcial to use a STW collector in the ﬁrst place and thus avoid the concurrency overhead (which we will discuss in Section IV-D). By examining the logs from Shenandoah, we observe two pathological modes. First, the untimeliness of reclamation causes allocation failures, and Shenandoah requires STW collection to ﬁnish an in-ﬂight concurrent collection (known as degenerated GCs in Shenandoah). Second, in order to avoid STW collections, Shenandoah throttles allocations by stalling the mutator at allocation sites (known as pacing in Shenandoah, or “allocation stall” in ZGC). Since sleeping threads do not contribute to the cycles consumed, but increase the wall-clock time needed to run a workload, this explains the much higher time overhead but modest cycle overhead. D. Misinterpretation of Evaluation Results Prior work [3], [4] points out common pitfalls and respective mitigations when evaluating GCs. We have applied these suggestions where possible (see SectionIV-A). For example, we TABLE XI: Percent of cycles spent in STW pauses averaged over 16 benchmarks. Lower the better. The best value for each heap size is shown in green. Where a collector cannot run all benchmarks at a particular heap size, the corresponding entry is left blank. ZGC consistently outperforms other collectors where it runs. control the heap size relative to the minimum heap size required to run a given workload. We reafﬁrm the fundamental time– space tradeoff is still applicable in a modern, diverse benchmark suite (such as shown in Table VI). However, these suggestions are not sufﬁcient to avoid misinterpretation of evaluation results, especially in light of modern hardware, concurrent GCs, and emerging latency-sensitive workloads. In this section, we highlight three important types of misinterpretation. In the next section, we make recommendations on mitigations. a) Opportunity cost: Comparing Table VI and Table VII, we notice that all collectors have higher cycle LBO than time LBO.The only exceptions are Serial, which is a single-threaded GC, and Shenandoah at small heap sizes due to pacing, which we discussed previously. In particular, the cycle LBOs of Parallel and G1 are about 0.3 larger than the respective time LBO at a small1.4×heap. In the extreme case, such as for batik at3.0×heap, Parallel has a mere 1.09 time LBO but a signiﬁcant 1.99 cycles LBO, the highest among all collectors studied. This difference reveals that substantial cycle overheads can go unnoticed when only the wall-clock time is reported. Historically, garbage collector performance has been mostly measured using wall-clock time only. This implicitly assumes that on machines with free cores available, these cores may be used by GC with no repercussions. On modern, massively parallel hardware, this assumption can mean that a signiﬁcant portion of the hardware is dedicated to the GC. This assumption, however, ignores the incurred opportunity cost. On multi-tenant hosts, which are increasingly common to increase utilization in datacenters, more CPU cores taken by GC threads lead to fewer cores available for other applications on the same host. That is, when heavily relying on parallelism, a collector will not only run slower when deployed in multitenant settings, because fewer cores are available to run GC, but also negatively impact other applications on the same host. Even when the server has only one application, fewer cycles required to run the application can reduce energy consumption. b) Concurrency overhead: A commonly used metric to tune GC for throughput is the fraction of time spent in GC (such as the-XX:GCTimeRatiosuggested in the GC tuning guide from the vendor [23]). Table X and Table XI show the fraction of time and cycles spent in STW pauses for different GCs. Similar to Table VI and Table VII, the results are grouped by heap sizes, and show the geometric means over 16 benchmarks. Fig. 3: GC pause time for lusearch in a3.0×heap. Each line and its shade show the mean and 95 % CI over 20 invocations. Compared with Table VI and Table VII, we can see that the classic methodology of estimating the GC overhead from the time/cycles spent in GC pauses is highly problematic, especially for the concurrent collectors. In particular, the two concurrent copying collectors spend a negligible fraction of time/cycles in GC pauses, but have enormous LBOs. With ﬁxed hardware resources, concurrent GC threads compete with mutator threads, e.g., for cache capacity and memory bandwidth; the effects are more severe effects for more parallel workloads. In other words, concurrent GC overheads are high not only from the expensive mechanisms they use, such as read and write barriers, but also from resource contention. c) Low pause6=low latency: A commonly used metric to tune GC for latency-sensitive applications is the maximum GC pause time (such as the-XX:MaxGCPauseMillissuggested in the GC tuning guide from the vendor [23]). Here, we show that the pause time is a poor metric to assess GCs for latencysensitive applications. Figure 3 and Fig. 4 show the distribution of pause times and query latencies (using metered latency; see SectionIV-A) of different GCs running the lusearch benchmark at3.0×heap. The low-pause collectors (Shenandoah and ZGC) indeed achieve better pause times in general, with ZGC consistently having the lowest pause time for all percentiles, while Shenandoah has lower pause times than the other three GCs under the 90th percentile. However, low pause times do not automatically confer low application latencies. Indeed, both Shenandoah and ZGC have worse (by factors of 10–100×) query latencies than the other three collectors. Application latencies are affected by GC pauses—both by their durations and frequency. Short but frequent pauses will not impact the distribution of pause times, but can certainly impact application latency. Importantly, application latencies are also functions of mutator performance. As discussed in previous sections, concurrent copying GC can affect mutator performance through expensive mechanisms, such as read barriers, and through the resource contention. In the case of the pathological modes we discussed, Shenandoah and ZGC throttle mutator threads when the GC cannot keep up with allocation. Such throttling indeed avoids Fig. 4: Metered latency for lusearch in a3.0×heap. Each line and its shade show the mean and 95 % CI over 20 invocations. triggering a STW collection, and keeping each GC pause short. However, it comes at great cost: if the mutators are sufﬁciently throttled, both the application latency and throughput will be worse than a simple STW GC, as evidenced in the above graphs and Table VIII. E. Recommendations Drawing on the observations, we make two recommendations for improving GC evaluation in future research. First, the LBO methodology should be used to report the absolute overheads of GCs. This helps us better understand the scale of the impact of GC on the program execution. Second, a richer set of performance and cost metrics should be used when evaluating GCs. At a minimum, both the wall-clock time and the CPU cycles used should be reported. Any additional metric (such as application latency, or an energy consumption model like Intel RAPL) can help us understand the performance characteristics of different GCs better. In this paper, we identify two important problems in empirical evaluation of GCs: unclear absolute overheads, and easy-to-misinterpret results presented using limited metrics. To address these problems, we ﬁrst devise the lower-bound overhead (LBO) methodology to empirically estimate the absolute overheads of GCs for any given cost metric. Then, we use the LBO methodology to study ﬁve production collectors in OpenJDK 17. We ﬁnd that these GCs incur substantial overheads: at least (lower bound) 7–82 % time overhead and 6–92 % cycle overhead relative to a zero-cost baseline for a modest heap size. We identify three important types of misinterpretation: opportunity cost, concurrency overhead, and using GC pause times as a proxy metric for application latency. These types of misinterpretation can be mitigated by including more metrics in the evaluation. Our ﬁndings reveal substantial overheads in production GCs, highlighting opportunities for future research on lowlatency collectors with low overheads. We recommend using more metrics when evaluating GCs, because this helps reveal tradeoffs and limitations of GCs that often go unnoticed. The LBO methodology is language and runtime agnostic, and thus can beneﬁt GC research on a wide range of managed languages and platforms, such as .NET (for C# and other CLI languages) and V8 (for JavaScript).