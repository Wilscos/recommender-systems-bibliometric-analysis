<title>Application of Deep Learning Technique to an Analysis of Hard Scattering Processes at Colliders</title> <title>1. Physics task</title> In the scope of high energy physics analysis the measurement of t-channel single top-quark production is used as a benchmark to calibrate the analytical tools and assumptions. Then the developed methods are used to measure deviations from the Standard Model – the Flavor Changing Neutral Currents (FCNC). The physics task is similar to the analysis of CMS collaboration [ ]. Neural networks are used extensively throughout the analysis to separate background and signal events. First neural network model is used to lter out the multi-jet QCD events as these events are hard to model with existing Monte-Carlo methods. This network uses only ve variables as its inputs and has relatively small number of trainable parameters. After the multi-jet QCD suppression a larger Standard Model neural network is used to identify top-quark events. These two dierent tasks allow us to test the performance of described methods in two separate instances. <title>2. DNN hyperparameter tuning</title> <title>arXiv:2109.08520v1  [physics.data-an]  14 Sep 2021</title> of hidden layers, number of neurons in each hidden layer, the learning rate, regularization constants, etc. In this case trainable parameters are the weights and biases of each neuron. Dierent hyperparameter combinations give the deep learning model varying degree of complexity and non-linearity. Therefore, hyperparameter tuning can help resolve overtting and undertting to a certain degree. Finding the best combination of hyperparameters can be a challenging task as the model’s performance can only be evaluated when the training is nished. Luckily, many hyperparameter optimization frameworks exist to automate this tedious process. One of the most established and well-known frameworks is called ]. It provides tools to tune any machine learning model and quickly visualize the results. Another useful feature of is the budget – the amount of time used to tune a single model. This can be useful to compare dierent models and still give both fair treatment, tuning them for the same time. However, we have opted to use ] for its tight integration with , allowing us to use hyperparameter tuning with minimal code modications and dependencies. After each trial this module generates a le containing all required information about a single run. If the user wishes not only get the best performing hyperparameter combination but also to explore the dependencies and tendencies of their deep learning model, the results can be parsed and visualized. The rst major step in setting up any hyperparameter tuning is dening all possible hyperparameter combinations – the hyperparameter space. Usually the ranges of numeric hyperparameters are dened either with a distribution or with a set array of values. The latter is done by dening the minimum and maximum values and setting the step parameter, the distance between two consecutive samples in the range. We have used this approach to visualize the relations between the model’s performance and the values of its hyperparameters. The non-numerical hyperparameters (the hidden layers activation function, for example) are chosen with the method. The second step is to dene a score variable – a metric to quantify and compare the model’s performance. In the default case this variable can be equal to model’s loss value (binary crossentropy in our binary classication task) or to any pre-made or user-dened metric. With the hyperparameter space expanding with each new variable, a suitable algorithm to navigate it is required. provides three basic Tuner algorithms, each with its own advantages and drawbacks: BayesianOptimization, Hyperband and RandomSearch. algorithm uses tuning with Gaussian process. This is the fastest built-in algorithm, however, it can only nd the local minima in the hyperparameter space. In our tests it converged within approximately 10% of the total combinations in the hyperparameter space. The parameters this algorithm converged on were adequate albeit not the overall best. algorithm uses the performance of the rst epochs to compare dierent hyperparameter combinations. We decided against this method as models with dierent learning rates will have dierent performance which will not reect their overall accuracy. tuning algorithm randomly samples hyperparameter combinations from the hyperparameter space. This method does not use any fancy logic, however, it reliably provides a near-best result when covering 40-50% of the hyperparameter space. The code covering the needed adaptations is available in the Appendix. After the tuning process is compete, all trials results are stored in les. The user can opt to use the best conguration without looking at other models, however, plotting the relations can provide useful insights into how the chosen model is performing. For general overview one can use Facebook’s ] utility (Figure 1). This interface allows the user to quickly analyse the trained models, sort them by their performance and check specic hyperparameter combinations. To further investigate the hyperparameter space, one can plot the relations between models’ performance and the values of used hyperparameters. We give two examples of such visualization in Figures 2 and 3. In the rst set of plots covering the tuning of larger Standard Model neural network we demonstrate the relation between the model’s performance and a certain hyperparameter value, averaging over the rest of hyperparameters. In the second set we used heatmaps to describe hyperparameter combinations for the smaller QCD suppression neural network. Having investigated the hyperparameter space for two typical High Energy Physics tasks, we can give broader recommendations for neural network design for this eld. First of all, using for hidden layers activation function is advisable. Standard and functions lead to worse performance in deeper, bigger networks. In both cases networks with one or two hidden layers performed better and more stable than their deeper counterparts. The number of nodes in the hidden layers varied depending on the amount of input features: for the bigger network with 50 input features the amount of neurons lied in range between 200 and 400, and for the smaller network with 5 input features it was closer to 120. <title>3. AutoML</title> AutoML approach covers nding the optimal machine learning model, training it, evaluating it and tuning it if its performance is insucient. In theory, given enough time and computational resources, this approach can yield an adequate model without investing researcher’s time into complex architecture tuning and feature engineering. High energy physics data is close in structure to tabular data. In other areas tabular data may contain text and categorical data, but in high energy physics data is primarily numerical and can be organized into columns, so the task is simpler in a certain way. Deep learning has shown comparable performance[ ] in tabular data classication to gradient boosting models ( ), which are much cheaper in terms of computational resources. However, our preliminary testing done using the package showed that ne-tuned neural network performs slightly better and overts less than a tuned model. Google’s ] package uses genetic algorithms to create a custom neural network structure for each machine learning task. This approach has a promising idea, however, the lack of support for weighted events limits its uses in high energy physics analysis where every event has a very specic weight value. We have opted for using ] library for automated machine learning. This library is easy to comprehend, has several performance modes (focused on data exploration, speed of inference or maximum accuracy of classication). It uses several machine learning algorithms (Linear, Random Forest, Extra Trees, LightGBM, Xgboost, CatBoost, Neural Networks, and Nearest Neighbors) for classication and then creates an ensemble of best performing models for nal classication. Here we present the results of two AutoML models in (runs in a dozens of minutes and performs Exploratory Data Analysis) and (maximum classication accuracy, needs more computational time, we have run it for a day) modes as the speed of inference is not crucial in the current analysis. The performance comparisons are shown in Figure 4. Both classication modes provided good accuracy and even outperformed the tuned neural network in terms of metric on the test dataset. However, this was done with a much higher degree of overtting, thus reducing the AutoML model’s predictive power. 3.4. Resume provided a good baseline in all our use-cases, with its maximum accuracy mode overtting a bit more that we would like it to. As it is much easier to control overtting inside Tensorow package through regularization and early stopping callback, for the time being we will continue to use it in our analysis, but this AutoML package came close to it in terms of classication accuracy. We will denitely monitor the development of this great tool and continue testing it. <title>4. DNN b oosting on errors</title> We have also tried boosting on errors. The concept of this method is simple: This method did not work as the results of classication worsened after each iteration. The illustration of this performance degradation can be found on Figure 5. The best explanation we have come up with was that deep neural network is not a weak learner, which were noticed to benet from such manipulations. <title>5. L-regularisation</title> Through experimentation with input features we have found out that certain features worked extremely well for rst-order classication, causing the model to increase its weights associated with these features, in turn lowering the importance of other, still needed, features. The proposed method included using l-regularization [ ] to limit high weights so that they will not overshadow other weights as much. We have conducted the l-regularization study by getting a good baseline model from the , xing its hyperparametes and varying only the regularization constant. This was done for L1, L2 and L12 regularization types. The results showed that when the regularization constant is chosen right, the discriminant distribution curve becomes smoother and general classication performance increases. However, when the regularization constant is too small, there are no perceivable improvements. High regularization constant values can be even more detrimental as they will outright decrease the classication performance of otherwise decent model. The illustration of these relations can be found in Figure 6. We have not noticed considerable dierences between the regularization types, L2 performed slightly better, but that disparity was within the margin of error. <title>6. Conclusion</title> We have demonstrated several approaches to improve the accuracy of classication based on a model of a Deep Neural Network in High Energy Physics. Here we provide a short summary of the methods described in the paper. DNN hyperparameter tuning is an eective method of improving the accuracy of the model, but it requires a lot of computational resources. The required computing time can be reduced by making hyperparameter space smaller and using a suitable optimization algorithm. We also provided recommendations based on our hyperparameter space exploration for typical high energy physics datasets. AutoML for tabular data can be used in High Energy Physics with relative success and little machine learning experience, however, the degree of overtting is hard to control. Boosting on errors is not advised to use with DNNs, and using any type of L-regularization is advisable if the regularization constant is chosen correctly. <title>References</title> [1] V. Khachatryan, et al. (CMS), Search for anomalous Wtb couplings and avour-changing neutral currents in t-channel single top quark production in pp collisions at 𝑠 = 7 and 8 TeV, JHEP 02 (2017) 028. doi:10.1007/JHEP02(2017)028. arXiv:1610.03545. [2] T. O’Malley, E. Bursztein, J. Long, F. Chollet, H. Jin, L. Invernizzi, et al., Keras tuner, https://github.com/keras-team/keras-tuner, 2019. [3] T. Akiba, S. Sano, T. Yanase, T. Ohta, M. Koyama, Optuna: A next-generation hyperparameter optimization framework, in: Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019. [4] D. Haziza, J. Rapin, G. Synnaeve, Hiplot, interactive high-dimensionality plots, https:// github.com/facebookresearch/hiplot, 2020. [5] Y. Gorishniy, I. Rubachev, V. Khrulkov, A. Babenko, Revisiting deep learning models for tabular data, 2021. arXiv:2106.11959. [6] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y. Liu, Lightgbm: A highly ecient gradient boosting decision tree, in: Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, Curran Associates Inc., Red Hook, NY, USA, 2017, p. 3149–3157. [7] C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, S. Yang, Adanet: Adaptive structural learning of articial neural networks, 2017. arXiv:1607.01097. [8] A. Płońska, P. Płoński, Mljar: State-of-the-art automated machine learning framework for tabular data. version 0.10.3, 2021. URL: https://github.com/mljar/mljar-supervised. [9] C. Cortes, M. Mohri, A. Rostamizadeh, L2 regularization for learning kernels, 2012. <title>A. Code for Keras Tuner adaptation</title> The function that returns the base model: The function that returns the model adapted to the Keras Tuner environment: