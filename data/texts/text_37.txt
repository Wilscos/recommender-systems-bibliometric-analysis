How can we predict missing values in multi-dimensional data (or tensors) more accurately? The task of tensor completion is crucial in many applications such as personalized recommendation, image and video restoration, and link prediction in social networks. Many tensor factorization and neural network-based tensor completion algorithms have been developed to predict missing entries in partially observed tensors. However, they can produce inaccurate estimations as real-world tensors are very sparse, and these methods tend to overÔ¨Åt on the small amount of data. Here, we overcome these shortcomings by presenting a data augmentation technique for tensors. In this paper, we propose DAIN, a general data augmentation framework that enhances the prediction accuracy of neural tensor completion methods. SpeciÔ¨Åcally, DAIN Ô¨Årst trains a neural model and Ô¨Ånds tensor cell importances with inÔ¨Çuence functions. After that, DAIN aggregates the cell importance to calculate the importance of each entity (i.e., an index of a dimension). Finally, DAIN augments the tensor by weighted sampling of entity importances and a value predictor. Extensive experimental results show that DAIN outperforms all data augmentation baselines in terms of enhancing imputation accuracy of neural tensor completion on four diverse real-world tensors. Ablation studies of DAIN substantiate the effectiveness of each component of DAIN. Furthermore, we show that DAIN scales near linearly to large datasets. ‚Ä¢ Computing methodologies ‚Üí Factorization methods;Neural networks; ‚Ä¢ Information systems ‚Üí Data mining. Tensor, Tensor Completion, Neural Network, Data Augmentation, Data InÔ¨Çuence, Recommender System, Deep Learning ACM Reference Format: Sejoon Oh, Sungchul Kim, Ryan A. Rossi, and Srijan Kumar. 2021. InÔ¨Çuenceguided Data Augmentation for Neural Tensor Completion. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ‚Äô21), November 1‚Äì5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637. 3482267 We observe various tensors on the Web, including images and videos, numerical ratings, social networks, and knowledge bases. Most real-world tensors are very large and extremely sparse [26] (i.e., a few observed entries and many missing values). Thus, tensor completion, i.e., the task of predicting missing values in a tensor, has been actively investigated in diverse areas including recommender systems [8], social networks [6], trafÔ¨Åc analysis [32], medical questionnaires [4], and computer vision [5]. For example, consider a movie rating tensor with three dimensions, namely users, movies, and time slices. Each tensor cell(ùëñ, ùëó, ùëò)contains the rating score given by userùëñto movieùëóduring time sliceùëò. The goal of tensor completion is to predict the rating scores of unobserved tensor cells. Tensor factorization (TF) is a popular technique to predict missing values in a tensor, but most methods [15,19,25,26,28,30] exhibit high imputation error while estimating missing values in a tensor. One of the reasons is that many TF models [15,19,25,30] regard missing values in a tensor as zeros. Hence, if those models are trained with a sparse tensor, their predictions would be biased toward zeros, instead of the observed values. Other TF methods [7,26,28] improve their accuracy by focusing only on observed entries; however, they suffer from overÔ¨Åtting as the tensor is very sparse. Neural network-based tensor completion methods [2,21,36,37] have been proposed to enhance the estimation accuracy. They have strong generalization capability [24], and capture non-linearity hidden in a tensor. However, these methods still suffer from data sparsity, and this can become a bottleneck for neural tensor completion methods, which require a large amount of data for training [35]. Moreover, these existing methods cannot generate new data points for data augmentation to solve the sparsity issue. In this paper, we propose a method that leverages the strength of neural tensor completion, and improves it through the utilization of data augmentation. Data augmentation increases the generalization capability of a model by generating new data points while training and has shown success in various applications of deep learning [29,33,40]; however, it has not been explored in the task of tensor completion. Here, we propose an inÔ¨Çuence-guided data augmentation technique called DAIN (DataAugmentation with INÔ¨Çuence Functions). First, DAIN trains a neural tensor completion model with the input tensor, and utilizes inÔ¨Çuence functions to estimate the importance of each training cell (e.g., the importance of a rating in a movie rating tensor) on reducing imputation error. Next, DAIN computes the importance of every entity by aggregating the importance values of all its associated cells. For example, to compute the importance of a userùëñ, the importance of all the ratings given byùëñare aggregated. The importance of an entity signiÔ¨Åes its impact in reducing the prediction error. Finally, DAIN generates new data Table 1: Comparison of our proposed data augmentation framework DAIN against existing methods. A checkmark indicates that a method satisÔ¨Åes a speciÔ¨Åc criterion. DAIN is the only data augmentation method satisfying all criteria. points by sampling entities proportional to their importance scores. Values of the augmented tensor cells are predicted via a trained neural tensor completion method. This inÔ¨Çuence-based sampling of entities augments data points using important entities, and thus, can lead to higher test prediction accuracy. We evaluate DAIN by conducting extensive experiments on four diverse real-world datasets and one synthetic dataset. Results show that DAIN outperforms baseline augmentation methods with statistical signiÔ¨Åcance, and that DAIN improves the prediction performance of a neural tensor completion method as the number of augmentation increases. Via thorough ablation studies, we conÔ¨Årm the effectiveness of each component of DAIN. We also show that DAIN is scalable to large datasets as illustrated by a near-linear relationship between runtime and the amount of augmentation. Finally, we demonstrate the sensitivity of the performance of DAIN with respect to hyperparameter values. Our main contributions are summarized by the following: ‚Ä¢ Novel problem.We propose a novel data augmentation technique DAIN for enhancing neural tensor completion. To the best of our knowledge, this is the Ô¨Årst data augmentation method for tensor completion. ‚Ä¢ Algorithmic framework.We propose a new framework for deriving the importance of tensor entities on reducing prediction error using inÔ¨Çuence functions. With the entity importance values, we create new data points via weighted sampling and value predictions. We also provide complexity analyses of DAIN. ‚Ä¢ Performance.DAIN outperforms baseline data augmentation methods on various real-world tensors in terms of prediction accuracy with statistical signiÔ¨Åcance. The code and datasets are available on the project website: https://github.com/srijankr/DAIN. Neural Tensor Completion.After recent advances in neural networks (NN), there have been many research topics applying NN to tensor completion frameworks. Neural tensor factorization (NTF) [36] is the Ô¨Årst model that employs MLP and long short-term memory (LSTM) architectures for a tensor completion task. Neural Tensor Machine (NTM) [2] combines the outputs from generalized CANDECOMP/PARAFAC (CP) factorization and a tensorized MLP model to estimate missing values. Moreover, Liu et al. [21] propose a convolutional neural network (CNN)-based tensor completion model, named COSTCO, which learns entity embeddings via several convolutional layers. However, as shown in Table 1, the above methods cannot augment new data points, do not generalize to multiple neural network architectures, and do not utilize inÔ¨Çuence functions. DAIN can provide high-quality augmentation for these methods while having these desirable properties. Note that we exclude tensor completion methods [22,26,28,41] that do not utilize neural networks since DAIN is designed for the neural methods. InÔ¨Çuence Estimation. Computing the inÔ¨Çuence (or importance) of data points on model predictions or decisions has been actively investigated [9,17,27,42]. InÔ¨Çuence has been mainly used to explain the prediction results by identifying useful or harmful data points with respect to the predictions. Firstly, Yeh et al. [42] leverage the Representer Theorem for explaining deep neural network predictions. Koh and Liang [17] introduce a model-agnostic approach based on inÔ¨Çuence functions. Amirata and James [9] propose a data Shapley metric to quantify the value of each training datum in the context of supervised learning. More recently, TRACIN [27] is proposed to compute the inÔ¨Çuence of a training example on a prediction made by a model with training and test loss gradients. In this paper, we decide to use TRACIN for the tensor data augmentation setting since it is scalable, easy to implement, and does not rely on optimality conditions as InÔ¨Çuence Functions [17] or Representer Theorem [42] does. Data Augmentation.Data augmentation is widely used to signiÔ¨Åcantly increase the data available for training models without collecting new data [12]. Basic image data augmentation methods include geometric transformations (such as Ô¨Çipping, cropping, rotation, and translation), mixing images by averaging their pixel values [13], and random erasing which randomly selects anùëõ √ó ùëöpatch of an image and masks it with certain values [45]. Augmentation Network model [20] Ô¨Ånds image augmentation that maximizes the generalization performance of the classiÔ¨Åcation model via trainable image transformation models and inÔ¨Çuence functions. Earlier data augmentation methods for text data are based on synonym replacement replacing a random word in a given sentence with its synonym by using external resources such as WordNet [23,44] and the pre-trained language model [14]. Yuning et al. [38] leverage machine translation to paraphrase a given text. There is a line of research that perturb text via regex-based transformation [3], noise injection [39], and combining words [10]. However, none of the above methods are applicable in a tensor completion task. To the best of our knowledge, ours is the Ô¨Årst work that enhances the performance of neural tensor completion via data augmentation. In this section, we deÔ¨Åne the preliminary concepts necessary to understand the proposed method. Table 2 summarizes the symbols frequently used in the paper. Tensors are multi-dimensional data and a generalization of vectors (1-order tensors) and matrices (2-order tensors) to the higher order. An ùëÅ -way or ùëÅ -order tensor has ùëÅ dimensions, and the dimension ùêºdimensionality/size of the ùëõdimension of X (ùëñ, ..., ùëñ ùëñentity of the ùëõdimension of X Œ©set of validation cells of X Œ©set of test cells of X ùõº, ..., ùõºentity importance Œòparameters of a tensor completion model at epoch ùë° ùúÇstep size at a checkpoint Œò Œò, ..., Œòùêæ checkpoints saved at epochs ùë°, ..., ùë° ùëá, ùëÄtime and space complexity of training entity embeddings ùëá, ùëÄtime and space complexity of training a value predictor ùëátime complexity of a single inference of a value predictor size (or dimensionality) is denoted byùêºthroughùêº, respectively. We denote anùëÅ-order tensor by boldface Euler script letters (e.g.,X ‚àà R). A tensor cell(ùëñ, ..., ùëñ)contains the valueX, and an entity of a tensor refers to a single index of a dimension. For example, in the movie rating tensor case, an entity refers to a user, movie, or time slice, and a tensor cell contains a rating. Tensor completion is deÔ¨Åned as the problem of Ô¨Ålling the missing values of a partially observed tensor. Tensor completion methods train their model parameters with observed cells (Œ©) and predict values of unobserved cells (Œ©) with the trained parameters. SpeciÔ¨Åcally, given an N-order tensorX (‚àà R)with training dataŒ©, a tensor completion method aims to Ô¨Ånd model parameters Œò for the following optimization problem. whereÀÜX= Œò(ùëñ, ..., ùëñ)is a prediction value for a cell (ùëñ, ..., ùëñ)generated by the tensor completion methodŒò. Neural tensor completion methods utilize different neural network architectures to computeÀÜX(see Section 4.1 for the multilayer perceptron case). Root-mean-square error (RMSE) is a popular metric to measure the accuracy of a tensor completion method [21,26,36]. SpeciÔ¨Åcally, we use test RMSE to check how accurately a tensor completion model predicts values of unobserved tensor cells. The formal deÔ¨Ånition of test RMSE is given as follows. Notice that a tensor completion model with the lower test RMSE is more accurate. We introduce the state-of-the-art inÔ¨Çuence estimator: TRACIN [27]. TRACIN calculates the importance of every training data point in reducing test loss. This is done by tracing training and test loss gradients with respect to model checkpoints, where checkpoints are the model parameters obtained at regular intervals during the training (e.g., at the end of every epoch). The inÔ¨Çuence of a training data pointùëßon the loss of a test data pointùëßis given as follows (please refer to Section 3 of TRACIN [27] for the details): whereŒò, 1 ‚â§ ùëñ ‚â§ ùêæare checkpoints saved at epochsùë°, ..., ùë°,ùúÇis a step size at a checkpointŒò, and‚àá‚Ñì (Œò, ùëß)is the gradient of the loss of ùëß with respect to a checkpoint Œò. InÔ¨Çuence estimation with TRACIN has been shown to have clear advantages in terms of speed and accuracy over existing methods [27], such as InÔ¨Çuence Functions [17] or Representer Point method [42]. We utilize TRACIN to create the Cell Importance Tensor in Section 4.2. In this section, we describe DAIN, a data augmentation pipeline using the importance of entities for efÔ¨Åcient tensor completion. Figure 1 shows our proposed method, and Algorithm 1 summarizes the overall data augmentation process of DAIN. We explain the steps of our method in the next subsections. First, we train a neural network model to learn embeddings for every entity in an input tensor (e.g., user, movie, and time embeddings in the movie rating tensor). The traditional technique of learning one embedding per data point is not scalable in tensors, as the number of data points in the tensor can be very large. Instead, we learn one embedding per entity of each dimension (i.e., in the movie rating tensor example, we learn one embedding for every user, movie, and time slice, respectively). Each tensor cell can then be represented as an ordered concatenation of the embeddings of its entities. For example, a tensor cell(ùëñ, ..., ùëñ)can be represented as [ùê∏,ùê∏,ùê∏, . . .], whereùê∏is an embedding of an entityùëñof theùëõdimension. The entity embeddings are trained to accurately predict the values of the (training) data points in the tensor. We train an end-to-end trainable neural network to learn the entity embeddings (line 1 in Algorithm 1). We choose a multilayer perceptron (MLP) model with ReLU activation since it shows the best imputation performance with data augmentation (see Figures 3(a) and 3(b)). Note that existing neural tensor completion models, such as NTF [36] and CoSTCo [21], can be also used to generate these embeddings. Our model‚Äôs prediction value for a tensor cell(ùëñ, ..., ùëñ) is deÔ¨Åned by the following. Figure 1: An overview of our proposed data augmentation method DAIN for a 3-dimensional tensor. DAIN Ô¨Årst learns entity embeddings with training data and computes training and validation loss gradients (step 1). Next, it obtains cell importance values by the inÔ¨Çuence estimator TRACIN and creates the Cell Importance Tensor (step 2). DAIN uniformly distributes cell importance values to the corresponding entities, and each entity calculates its importance by aggregating the assigned cell importance values (step 3). Finally, DAIN performs data augmentation to an input tensor by weighted sampling on those entity importance arrays and value predictions (step 4). Note that DAIN can use any neural tensor completion methods to learn entity embeddings (step 1) or predict values of sampled tensor cells (step 4). Please refer to each subsection in Section 4 for detailed information on each step. Note that[ùê∏, . . . , ùê∏]represents the embedding of a cell(ùëñ, ..., ùëñ) obtained by concatenating embeddings of entitiesùëñ, ..., ùëñ,ÀÜX is the imputed output value by the neural network for a cell(ùëñ, ..., ùëñ), andùëÄindicates the number of hidden layers.ùëä, ...,ùëäand ùëè, ..., ùëèare weight matrices and bias vectors, respectively.ùúô, ..., ùúô are activation functions (DAIN usesùëÖùëíùêøùëà). The model parameters ùëä, ...,ùëäandùëè, ..., ùëèare referred to asŒò. By minimizing the loss function(1)combined with Equation(4), we obtain trained entity embeddings as well as loss gradients for all training and validation cells (lines 2-3 in Algorithm 1). We utilize those gradient vectors to compute cell-level importance values in the next part. In Step 2, we create Cell Importance Tensor (CIT) which represents the importance of every tensor cell in reducing the prediction loss. In the movie rating tensor example, CIT stores the importance of ratings. Any inÔ¨Çuence estimator can be used to compute the CIT, but we choose TRACIN [27] introduced in Section 3.3 since it is scalable, easy to implement, and does not rely on optimality conditions. Moreover, we later show DAIN with TRACIN shows the highest prediction performance empirically compared to other inÔ¨Çuence estimation methods (see Figures 6(a) and 6(b)). We derive a value of a CIT cellùëßby the following steps. Computing the CIT is shown in step 2 in Figure 1. Equation(3)in Section 3.3 computes the inÔ¨Çuence of a training cellùëßon the loss of a test cellùëß. Since we cannot access the test data, we compute the inÔ¨Çuenceùú∂of a training cellùëßon reducing overall validation loss by the following (line 4 in Algorithm 1). ùú∂= |ùêºùëõùëì (ùëß, ùëß)| = |ùúÇ‚àá‚Ñì (Œò, ùëß) ¬∑ ‚àá‚Ñì (Œò, ùëß)| whereùêæis the number of checkpoints,ùúÇis a step size at a checkpointŒò, and‚àá‚Ñì (Œò, ùëß)is the gradient of the loss ofùëßwith respect to a checkpointŒò. We use an absolute value function to theùú∂ calculation since cells with negative inÔ¨Çuence can also be important. For example, cells with large negative inÔ¨Çuence contribute to increasing the validation loss signiÔ¨Åcantly. We can mitigate the loss increase by our data augmentation method, where the absolute function leads DAIN to create more augmentation for these cells. Identifying important entities is crucial. The output of the previous step identiÔ¨Åes the cell importance, but the importance of each entity remains unknown. For instance, in the movie rating tensor example, Algorithm 1: Data Augmentation with DAIN the cell importance score captures the importance of the rating on the prediction loss, while it does not reÔ¨Çect the importance of a user, a movie, or a time slice. If we can Ô¨Ånd the importance of every user, movie, and time slice, we can generate new inÔ¨Çuential data points to minimize prediction error by combining users, movies, and time slices that have high importance values. Here, we describe an aggregation technique to calculate the entity importance values from cell importance values (shown in step 3 of Figure 1 and line 5 in Algorithm 1). First, we uniformly distribute a cell‚Äôs importance value to its associated entities. For instance, given a training cellùëß = (ùëñ, ..., ùëñ), we uniformly distributeùú∂to itsùëÅ entities{ùëñ, ..., ùëñ}. After performing the allocation for all training cells, we compute the entity importance scoreùõºfor an entityùëñof theùëõdimension by aggregating cell importance scores as follows: Recall thatùú∂indicates the Cell Importance Tensor, andŒ© represents a set of training cells from an original tensor. In the movie rating tensor example, the above equation signiÔ¨Åes that a user‚Äôs entity importance is the aggregation of the importance scores of all the ratings the user gives. Similarly, a movie‚Äôs importance is the sum of the importance of the ratings it receives, and the importance of a time slice is the sum of the importance of all the ratings given during the time slice. Another way of computing the entity importance is applying rank1 CP (CANDECOMP/PARAFAC) factorization [18] on the Cell Importance Tensor. The output factor matrices from the CP model are entity importances. SpeciÔ¨Åcally, a value of each output array indicates the importance of the corresponding entity on predicting values in a training tensor. The loss function of the rank-1 CP model is given as follows. ùêø(ùõº, ..., ùõº) = Note thatùõº, ..., ùõºare entity importances,ùúÜis a regularization factor, and‚à•X‚à•is Frobenius norm of a tensorX. We choose the aggregation scheme to compute the entity importance as the rank-1 CP model can produce inaccurate decomposition results when a given tensor is highly sparse. By conducting extensive experiments, we later show that the aggregation method over the rank-1 CP model with respect to the prediction accuracy (see Figures 7(a) and 7(b)). In the Ô¨Ånal step, we identify the tensor cells and values for data augmentation using the entity importance scores and a value predictor, respectively. This is illustrated in step 4 of Figure 1 and lines 6‚Äì15 in Algorithm 1. A high entity importance score signiÔ¨Åes that the corresponding entity plays an important role in improving the validation set prediction. Thus, we create new cells using these important entities. We Ô¨Årst train a neural tensor completion modelŒòwith the original training cells (line 6), which will be used later for value predictions. After that, we conduct weighted sampling on every entity importance array (line 11) and select one entity from each dimension (line 12). Mathematically, an entityùëñof theùëõdimension (e.g., a user among all users) has a probabilityto be sampled. We combine the sampled entities from all dimensions to form one tensor cell (line 13). We repeat the process several times to create the required number of data points for augmentation (lines 8‚Äì13). Once indices of the cells are sampled, their values need to be determined (line 14). Trivially, one can use the overall average value√ç (i.e.,X) or Ô¨Ånd the most similar index in the embedding space and take its value. However, these heuristics can be inaccurate and computationally expensive, respectively. An advanced way of assigning the values of the augmented data points is by predicting the values using a tensor completion model (either the previously trainedŒòor training another modelŒò). SpeciÔ¨Åcally, we can use the trained embeddings fromŒòto predict the values, or we can train an off-the-shelf prediction methodŒò with the training data and predict the values with it. Reusing the trained neural networkŒòis computationally cheaper since it only needs to do a forward pass for inference, which is fast. However, this can cause overÔ¨Åtting in the downstream model since the resulting augmentation cell values are likely to be homogeneous to the original tensor. On the other hand, using another off-the-shelf methodŒò can increase the generalization capability of a downstream model by generating more heterogeneous data compared toŒò. Thus, we choose to train a new off-the-shelf modelŒò. We use the state-ofthe-art algorithm COSTCO [21] asŒòto predict the values of the augmented tensor cells, since COSTCO value predictor exhibits high prediction accuracy than other methods empirically (see Section 5.3 and Figure 5). After we obtain all tensor cell indices and values needed for augmentation, we add them to the input tensor to obtain an augmented tensorX(line 15). This augmented tensor can be used for downstream tasks. In this subsection, we analyze the time and space complexity of DAIN. Time complexity.The Ô¨Årst step of DAIN (Section 4.1) is training a neural tensor completion modelŒòto generate entity embeddings and gradients, and it takesùëÇ (ùëá)assumingùëÇ (ùëá)is the time complexity of trainingŒòas well as gradient calculations. The second step of DAIN (Section 4.2) is computing the cell importance ùú∂for each training cellùëß ‚àà Œ©. A naive computation ofùú∂ in Equation(5)for all training cells takesùëÇ (ùêæùê∑|Œ©||Œ©|), whereùêæandùê∑are the number of checkpoints and the dimension of the gradient vector, respectively. We accelerate the computation√ç toùëÇ (ùêæùê∑ (|Œ©| + |Œ©|))by precomputing‚àá‚Ñì (Œò, ùëß) for all checkpointsŒò, ..., Œòin Equation(5). The third step of DAIN (Section 4.3) is calculating the entity importance with the aggregation technique by Equation(6). It takesùëÇ (ùëÅ |Œ©|)since we distributeùú∂, ‚àÄùëß ‚àà Œ©to its entities and aggregate the assigned values. Finally, the data augmentation step (Section 4.4) takes ùëÇ (ùëá+ ùëÅ(ùëÅ log ùêº + ùëá)), whereùëÇ (ùëá)andùëÇ (ùëá)are the training and single inference time complexities of the value prediction modelŒò, respectively.ùëÇ (ùëÅùëÅ log ùêº )term indicates the time complexity of weighted samplingùëÅcells without replacement [34] fromùëÅdimensions (assumingùêº= ¬∑ ¬∑ ¬∑ = ùêº= ùêº). The Ô¨Ånal time complexity of DAIN isùëÇ (ùëá+ùëá+ (ùêæùê∑ + ùëÅ )|Œ©| + ùêæùê∑ |Œ©| + ùëÅ(ùëÅ log ùêº + ùëá)). Space complexity.The Ô¨Årst step of DAIN (Section 4.1), obtaining entity embeddings and gradients, takesùëÇ (ùëÄ+ ùêæùê∑ (|Œ©| + |Œ©|))space, assumingùëÇ (ùëÄ)is the space complexity of training a neural tensor completion modelŒò(including entity embeddings). ùëÇ (ùêæùê∑ (|Œ©| + |Œ©|))space is required to storeùê∑-dimension gradients of training and validation cells for allùêæcheckpoints. The second step of DAIN (Section 4.2), computing the cell importance ùú∂, takesùëÇ (|Œ©|)space since we need to store all cell importance values. The third step of DAIN (Section 4.3), calculating the entity importance with the aggregation method, takesùëÇ (ùëÅ ùêº)space since we need to store importance scores of all entities fromùëÅ dimensions (assumingùêº= ¬∑ ¬∑ ¬∑ = ùêº= ùêº). Finally, the data augmentation step (Section 4.4) takesùëÇ (ùëÄ+ùëÅùëÅ )space since we need ùëÇ (ùëÄ)space for training the value predictorŒòandùëÇ (ùëÅùëÅ ) space for storing the data augmentation withùëÅcells. The Ô¨Ånal space complexity of DAIN isùëÇ (ùëÄ+ùëÄ+ùêæùê∑ (|Œ©|+ |Œ©|) + ùëÅ (ùêº + ùëÅ)). In this section, we evaluate DAIN to answer the following questions. (1) Effectiveness of DAIN (Section 5.2).How much does our proposed data augmentation technique enhance the accuracy of neural tensor completion compared to the baseline augmentation methods? (2) Ablation studies of DAIN (Section 5.3).DAIN consists of three major components: training entity embeddings, generating new tensor cells, and predicting values of the new cells. How much does each component of DAIN contribute to boosting the neural tensor completion accuracy? (3) Comparisons of inÔ¨Çuence estimators (Section 5.4).How much do different inÔ¨Çuence estimators impact the prediction accuracy improvements of DAIN? (4) Comparisons of entity importance algorithms (Section 5.5). How much do different entity importance methods affect the prediction accuracy improvements of DAIN? (5) Scalability of DAIN (Section 5.6).Does the running time of DAIN linearly scale with the number of data augmentation? (6) Hyperparameter sensitivity (Section 5.7).How much do model hyperparameters of DAIN, such as embedding dimension and layer structures, affect the prediction accuracy? We Ô¨Årst describe the datasets and experimental settings in Section 5.1, and then answer the above questions. Table 3: Summary of real-world tensor datasets used in our experiments. 5.1.1 Datasets. We use four real-world tensor datasets to evaluate the performance of our proposed data augmentation method and baselines. As summarized in Table 3, we use MovingMNIST [31], Foursquare [43], Reddit [1], and LastFM [11]. MovingMNIST is a video tensor represented by (sequence, length, width, height; intensity), and we use 10% of the total data. Foursquare is a pointof-interest tensor represented by (user, location, timestamp; visit). Reddit includes the posting history of users on subreddits represented by (user, subreddit, timestamp; posted). LastFM contains the music playing history of users represented by (user, music, timestamp; played). For all tensors having timestamps, we converted their timestamps to unique days, so that all timestamps in the same day would have the same converted number. We randomly added negative samples with random indices and zero values to original data for Foursquare, Reddit, and LastFM tensors. 5.1.2 Baselines. To the best of our knowledge, there are no existing methods for data augmentation designed speciÔ¨Åcally for tensors. Therefore, we create a few baselines based on the broader literature in data augmentation by the following: ‚Ä¢ Duplication or Oversampling:This simple data augmentation method randomly copies tensor cells and values from the existing training data, and adds them to the original tensor; there can be multiple tensor cells with the same values. ‚Ä¢ Entity Replacement:This data augmentation method randomly selects existing tensor cells and replaces their indices one by one with one of the top-10 closest entities in the embedding space, while keeping their tensor values. Figure 2: Effectiveness of DAIN. Test RMSE of the neural tensor completion method (MLP) after applying various data augmentation methods on real-world datasets. For all tensors, DAIN outperforms all baselines and shows the lowest test RMSE value at the maximum augmentation with statistical signiÔ¨Åcance. ‚Ä¢ (Random, MLP) and (Random, COSTCO):These baselines generate data augmentation by randomly sampling missing tensor cells and predicting their values via the MLP and COSTCO [21] models trained with the original tensor, respectively. Recall that neural tensor completion methods such as COSTCO [21] and NTF [36] cannot generate new data points by themselves. Thus, these methods are not included in our baselines. The inÔ¨Çuence estimator TRACIN [27] also only computes the cell importance but cannot create new data augmentation. We exclude non-neural tensor completion algorithms [22,26,28,41] to test the data augmentation since DAIN is optimized for neural tensor completion methods. 5.1.3 Experiment Setup. We randomly split our datasets to use 90% data for training and 10% for test. We randomly sample 20% of training data as the validation set. We stop the training of a neural network when the validation accuracy is not improving anymore, with a patience value of 10. We repeat all experiments 10 times and report the average test RMSE. To measure statistical signiÔ¨Åcance, we use a two-sided test of the null hypothesis that two independent samples have identical average (expected) values. If we observe a small p-value (e.g., < 0.01), we can reject the null hypothesis. We Ô¨Åne-tune neural network hyperparameters of DAIN with validation data. SpeciÔ¨Åcally, embedding dimension, batch size, layer structure, and learning rate are set to 50, 1024, [1024,1024,128], 0.001, respectively. We use Adaptive Moment Estimation (Adam) [16] optimizer for the neural network training, and the maximum number of epochs for training a neural network is set to 50. To compute the Cell Importance Tensor, we set checkpoints and step size to every epoch and 0.001, respectively. We take gradients with respect to the last fully connected layer. We execute all our experiments on Azure Standard-NC24 machines equipped with 4 NVIDIA Tesla Figure 3: Ablation study of entity embedding generators of DAIN. We test the effectiveness of the MLP embedding model of DAIN on MovingMNIST and Foursquare tensors. The MLP model outperforms the COSTCO embedding model in terms of test RMSE at the maximum augmentation with statistical signiÔ¨Åcance. Figure 4: Ablation study of cell creation methods of DAIN. We test the effectiveness of the cell creation component of DAIN on MovingMNIST and Foursquare tensors. The entity importance-based cell creation of DAIN outperforms the random cell creation baseline in terms of test RMSE at the maximum augmentation with statistical signiÔ¨Åcance. Figure 5: Ablation study of value prediction methods of DAIN. We test the effectiveness of the value prediction component of DAIN on MovingMNIST and Foursquare tensors. The value prediction module of DAIN outperforms the MLP baseline in terms of test RMSE at the maximum augmentation with statistical signiÔ¨Åcance. We choose the MLP baseline since it shows higher prediction accuracy than the random one. K80 GPUs with Intel Xeon E5-2690 v3 processor. The data augmentation module is implemented in Python with the PyTorch library. To test the effectiveness of our proposed data augmentation method, we measure and compare test RMSE values of the neural tensor completion model (MLP) after applying DAIN and baselines on real-world tensors. Figure 2 shows their performance. We omit error bars since they have small standard deviation values. Both the baseline methods, Duplication and Entity Replacement, lead to a reduction in performance, i.e., an increase in test error, with any amount of augmentation. More augmentation leads to a greater reduction in performance. A potential reason is that the feature spaces of original and augmentation are very similar, so the augmentation cannot increase the generalization capability of a model. Although the other baselines (Random, MLP) and (Random, COSTCO) outperform Duplication or Entity Replacement, they have limitations in reducing test RMSE compared to DAIN. Our proposed method DAIN leads to improvements in performance across all datasets with augmentation. DAIN performs the best across all Ô¨Åve methods and presents the lowest test RMSE at the maximum augmentation with statistical signiÔ¨Åcance, which clearly demonstrates the effectiveness of DAIN. The performance improves as more data is added to the original one. A key reason for the high performance of DAIN is that it combines important entities from each dimension, which is leading to the augmentation of more inÔ¨Çuential and heterogeneous data points to the original tensor. At a high-level view, DAIN consists of three components‚Äîthe Ô¨Årst is training entity embeddings with a neural tensor completion methodŒò(step 1 in Figure 1), the second component is selecting new tensor cells for augmentation based on entity importance (steps 2 and 3), and the Ô¨Ånal is predicting values of the augmented cells using a neural tensor completion methodŒò(step 4). As mentioned previously, in DAIN, we use MLP, entity importance, and COSTCObased predictor in the three components, respectively.We perform ablation studies to investigate the contribution of each component in DAIN‚Äôs performance by replacing the component with the baseline component, while Ô¨Åxing all the others. Figures 3, 4, and 5 show ablation study results of the three components of DAIN on the two largest tensor datasets, namely MovingMNIST and Foursquare. In Figure 3, DAIN with MLP embedding generator presents lower test RMSE than a baseline of DAIN with COSTCO embedding generator at 50% augmentation ratio. A potential reason is that using COSTCO in both step 1 and step 4 (for augmented cell‚Äôs value prediction) can lead to overÔ¨Åtting. On the other hand, using MLP in step 1 and COSTCO in step 4 gives generalization capability to the model. The entity embedding module has a huge impact on performance improvements since entity embeddings and gradients generated by the module affect all the subsequent modules. In Figure 4, DAIN with entity importance-based cell creation beats the model with random cell creation. The result substantiates the usefulness of using entity importance in creating tensor cells for augmentation. In Figure 5, DAIN with COSTCO for augmented cell‚Äôs value prediction outperforms DAIN with a MLP-based value predictor. This is again because of overÔ¨Åtting where the MLP predictor produces homogeneous data points since the entity embedding model in step 1 Figure 6: Comparisons of inÔ¨Çuence estimators. We compare two inÔ¨Çuence estimators with respect to test RMSE improvements on MovingMNIST and Foursquare tensors. We Ô¨Ånd the TRACIN inÔ¨Çuence estimator is more suitable for our data augmentation framework. Figure 7: Comparisons of entity importance algorithms. We compare two entity importance algorithms with respect to test RMSE improvements on MovingMNIST and Foursquare tensors. We Ô¨Ånd entity importance calculation by the aggregation scheme is more suitable for our data augmentation framework. is an MLP as well. On the other hand, using a COSTCO-based value predictor produces heterogeneous data points giving generalizability. In summary, all of the components of DAIN prove useful by reducing test RMSE values signiÔ¨Åcantly compared to baseline components. In this subsection, we explore how different inÔ¨Çuence estimators affect the prediction accuracy improvements of DAIN on the two largest real-world tensors, namely MovingMNIST and Foursquare datasets. We compare two inÔ¨Çuence calculation methods: TRACIN [27] and Representer Theorem [42]. We exclude inÔ¨Çuence function [17] since it shows worse prediction accuracy than the Representer Theorem method. As shown in Figures 6(a) and 6(b), TRACIN consistently outperforms Representer Theorem with any amount of augmentation (with statistical signiÔ¨Åcance at 50% augmentation). One possible explanation for this gap is that TRACIN directly computes the inÔ¨Çuence of a training cell in reducing validation loss, while Representer Theorem cannot measure the direct contribution. Similar to Section 5.4, we conÔ¨Årm how different entity importance calculators affect the prediction accuracy improvements of DAIN on the two largest real-world tensors, namely MovingMNIST and Foursquare datasets. Between two entity importance calculators introduced in Section 4.3, the aggregation algorithm shows superior Figure 8: Scalability of DAIN. This plot measures the running time of DAIN at different augmentation levels. The sudden jump in total running time from 0% to 10% augmentation occurs due to the cost for the cell and entity importance calculation (see time complexity analysis in Section 4.5); the runtime increases linearly thereafter. performance than the rank-1 CP factorization (see Figures 7(a) and 7(b)) with extensive test RMSE differences at 50% augmentation. The main reason for this gap is that rank-1 CP factorization cannot decompose the Cell Importance Tensor accurately due to its high sparsity, while the aggregation algorithm does not suffer from the sparsity issue. In this subsection, we measure the runtime of DAIN on the largest real-world, namely MovingMNIST, and a synthetic tensor, namely Synthetic-1M. We construct the synthetic tensor with dimensionality(1000, 1000, 1000, 1000)and1, 000, 000non-zero cells. We randomly generated factor matrices and reconstructed a synthetic tensor from them. Note that the total running time for0% augmentation is equivalent to the neural network training time. On the other hand, our proposed data augmentation method involves four runtime-related components: (1) initial neural network training, (2) cell importance calculation, (3) entity importance calculation, and (4) weighted sampling and value inference. Therefore, there are static costs (i.e.,ùëÇ (ùëá+ùëá+ (ùêæùê∑ + ùëÅ )|Œ©| + ùêæùê∑ |Œ©|in Section 4.5) resulted from steps (1), (2), and (3), and linearly increasing costs (i.e.,ùëÇ (ùëÅ(ùëÅ log ùêº +ùëá))in Section 4.5) proportional to the amount of augmentation for step (4). Figure 8 exhibits the total running time of DAIN on MovingMNIST and Synthetic-1M tensors. As expected before, we Ô¨Ånd a sudden increase in total running time from 0% to 10% augmentation on both tensors due to steps (2) and (3) above. After that, the running time increases linearly. This shows that DAIN is highly scalable and can be applied to large datasets. In this subsection, we investigate how much model hyperparameters affect the prediction accuracy of a model. Our neural network hyperparameters include the length of entity embeddings, the number of hidden layers, the size of a hidden layer, learning rate, and batch size. We vary one hyperparameter while Ô¨Åxing all the others to default values mentioned in Section 5.1.3. Figure 9 shows the hyperparameter sensitivity of DAIN with respect to the prediction accuracy on the Foursquare dataset with 50% augmentation ratio. The RMSE improvement indicates how much validation RMSE values are enhanced after the augmentation (higher is better). As the embedding size increases, we observe Figure 9: Hyperparameter sensitivity plots. We vary one of the hyperparameters of DAIN, namely embedding length, hidden layer size, number of hidden layers, batch size, and learning rate, while Ô¨Åxing all the others to default values. We measure the validation RMSE improvements after 50% augmentation on the Foursquare dataset. We Ô¨Ånd medium-sized layers with large embedding dimensions, and proper learning rates are key factors for the prediction accuracy. performance improvements since large embeddings contain more useful information about training data. Regarding the number of hidden layers, 3 hidden layers are appropriate since 1 or 2 layers may not fully learn the augmented tensor, and 4 layers may overÔ¨Åt. Too small or too large of learning rates degrade the prediction accuracy since a small one leads to slower convergence, and a large one can Ô¨Ånd a low-quality local optimum. In this paper, we proposed a novel data augmentation framework DAIN for enhancing neural tensor completion. The key idea is to augment the original tensor with new data points that are crucial in reducing the validation loss. Experimental results on real-world datasets show that DAIN outperforms baseline methods in various augmentation settings with statistical signiÔ¨Åcance. Ablation studies of DAIN demonstrate the effectiveness of the key components of DAIN. We also verify that DAIN scales near linearly to large datasets. Future directions of this work include exploring the effectiveness of DAIN in downstream tasks such as anomaly detection and dataset cleanup. Deriving theoretical guarantees of the performance boost from DAIN is worth exploring as well. This research is supported in part by Adobe, Facebook, NSF IIS2027689, Georgia Institute of Technology, IDEaS, and Microsoft. S.O. was partly supported by ML@GT, Twitch, and Kwanjeong fellowships. We thank the reviewers for their feedback.