The construction of eective Recommender Systems (RS) is a complex process, mainly due to the nature of RSs which involves large scale software-systems and human interactions. Iterative development processes require deep understanding of a current baseline as well as the ability to estimate the impact of changes in multiple variables of interest. Simulations are well suited to address both challenges and potentially leading to a high velocity construction process, a fundamental requirement in commercial contexts. Recently, there has been signicant interest in RS Simulation Platforms, which allow RS developers to easily craft simulated environments where their systems can be analysed. In this work we discuss how simulations help to increase velocity, we look at the literature around RS Simulation Platforms, analyse strengths and gaps and distill a set of guiding principles for the design of RS Simulation Platforms that we believe will maximize the velocity of iterative RS construction processes. Additional Key Words and Phrases: recommender systems, simulations, machine learning, Feedback loops, Evaluation methods, Reinforcement learning, bandits 1 INTRODUCTION Recommender Systems are complex systems. They are usually composed of advanced statistical algorithms relying on huge data sets and large scale distributed software systems that need to operate in a low latency regime. Furthermore, they interact with humans, giving place to hard to foresee phenomena such as paradox of choice [ [8]. Recommender Systems are complex systems because they try to solve hard problems in complex environments. This implies that their construction process is therefore also complex. The main objective of most industrial recommender systems is to deliver value for the user as well as commercial gains for the provider which leads to iterative processes where the recommender system is continuously improved through new algorithms, user interfaces, etc. [ typically evaluated through Online Controlled Experiments (OCEs) [ new version on all the relevant metrics including user satisfaction and commercial gains among others. One of the most challenging aspects of this process is that given a current baseline system, it is not obvious which aspect (such as diversity, relevance, etc), nor which component (such as the ranker, candidate selector, serving infrastructure, etc.) should be the target of the next iteration. A common approach to address this is to develop well articulated hypotheses about issues (e.g. current serving approach has high latency leading to higher abandonment rate) or opportunities for improvement (e.g. point-wise ranker produces low diversity leading to very similar recommendations) in the current version, and use existing or new data to validate them. Unfortunately in many cases this is not possible, because the available data is not appropriate, or because gathering new data is too expensive. Once a concrete improvement opportunity is detected, the current system is modied accordingly, for example by changing a prediction algorithm, the user interface or the serving infrastructure. These are usually local interventions with measurable local eects. But they also have (and they are expected to have) global or system-wide eects, which are a lot harder to measure since they involve the system as a whole. On top of this, there is usually tension between various variables of interest such as relevance vs latency, or diversity vs relevance or adaptivity vs user satisfaction to name a few which further increases the importance of system-wide analysis. The ability to make good and principled trade-os is key to deliver a balanced and robust recommender system. In short, commercial recommender systems require a high velocity construction process capable of continuously increasing the delivered value. To achieve this, we identify two important challenges: • Opportunity Identication: in this stage we are interested in studying the current system to identify aws and/or improvement points which might lead to a better version of the system. • System Evaluation: Evaluate and estimate the impact of a new version of the system. In this work we study the role of Simulations as a tool to address them. We look at existing simulation platforms, identify gaps and propose a set of principles to maximize the velocity of the recommender system construction process. 2 IMPROVING THE RS CONSTRUCTION PROCESS WITH SIMULATIONS Simulations are good t for addressing both the Opportunity Identication and the System Evaluation challenges. 2.1 Opportunity identification In Opportunity identication, given a recommender system currently deployed in production, we want to nd areas of improvement by understanding it’s strengths and aws. Simulations allow us to manipulate any number of variables we consider important in order to understand their impact on system behaviour. For example, we can analyse the eect of feedback delay by creating multiple simulated environments introducing perturbations in the time it takes to observe user feedback, allowing developers to extract insights about the robustness of the current baseline to dierent user feedback delay regimes. Simulated environments can also be designed using modular components that can be combined and reused to produce new environments which further helps with velocity (e.g. a Delay Model can be combined with a User Preference Model or an Item Availability Model). Counterfactual Policy Evaluation (CPE) can also be used as a mechanism to manipulate variables, but their applicability is limited since for every single intervention a new counterfactual model must be constructed. Finally Online Controlled Experiments (OCE) allow to manipulate variables that are under full control such as recommendations, available items, user interface, etc. This technique is rather simple but it is costly and does not allow the manipulation of variables out of controls, such as user preferences or market conditions. In practice CPE and OCEs are mainly applied for the System Evaluation Challenge. Hence, we believe that simulations are a fundamental method to systematically identify opportunities to improve a recommender system. 2.2 System Evaluation In the System Evaluation challenge, we are interested in the causal eect of replacing the current baseline with a new recommender system on several variables of interest. The gold standard approach is Online Controlled Experiments, which provide unbiased causal estimates with strong guarantees under rather weak assumptions and with maximum reality faithfulness since actual users are exposed to the new system. At the same time have a few strong limitations aecting the velocity of the construction process of recommender systems, concretely: • Only a few experiments can be run concurrently, • each experiment has potentially high cost, •they provide limited external validity, which is of particular concern in non-stationary contexts such as the ones where recommender systems operate. Fig. 1. Comparison between Simulations, Counterfactual Policy Evaluation (CPE) and Online Controlled Experiments (OCE). Thus, OCEs are powerful but constrain the velocity of the recommender system construction process. At the opposite end of the spectrum, Counterfactual Policy Evaluation constructs estimates relying on data collected using the baseline system (or some other data-collection policy) [ parallel without even deploying the new system in production and therefore avoiding the associated costs. However, they also suer from limited external validity and they provide reasonable power only if the new policy is close to the data collection policy. More importantly, since the users are never exposed to the new system, they provide weak reality faithfulness and require very good counterfactual models which are usually very hard to build in a non-stationary environment. Evaluating a recommender system in simulated environments allows developers to run many experiments in parallel, without the costs of exposing users to the new system. Furthermore, it enables a trade-o between complexity and reality faithfulness: developers can trade reality faithfulness with environment complexity. OCEs and CPE are located at a xed point of the Environment-Complexity vs Reality Faithfulness trade-o plane (Figure 1), simulations are much more exible allowing to create environments of varying complexity and reality faithfulness. Figure 1 summarizes the trade os between CPE, OCEs and Simulations across dierent dimensions. Our main observation is that unlike CPE and OCEs which are constrained to small regions of the trade-o plane, Simulations provide much larger coverage resulting in high exibility allowing developers to strike a balance between Environment Complexity, Reality Faithfulness and Manipulability. 3 SIMULATION PLATFORMS We consider the literature concerned with the development of Simulation Platforms aiming at providing a general framework for running simulations at scale. We try to understand how they help to increase the velocity of the recommender system construction process, providing our remarks for each particular case. RecoGym [ studying sequential user interaction combining organic feedback with intermittent advertisement display, resulting in bandit feedback. While RecoGym supports sequential interaction and conguration of item/user dimensionality, it lacks exibility in conguration of user state transitions, dening multiple reward responses such as conversions, costs, bounce o, lifetime value, etc. These characteristics can be important for real world applications. 11] oers users with the capability to create congurable Reinforcement Learning (RL) environments for PyRecGym [14] extends the idea of RecoGym by allowing users to create more generic environments to accommodate a variety of recommender systems tasks instead of using synthetic data. It can support multiple input data-types and user-feedback functions. It can also ingest existing RS data-sets and use them to play out sequences of interactions between users and the RL system. PyRecGym allows exibility in its rewards and input but its architecture makes it hard to create reusable simulated environments. Another remark is that the user feedback is simulated by replaying the data-set which might introduce bias towards the data collection policy. RecSim [6] provides much more exibility in dening the environment. It allows the creation of new environments that reect particular aspects of user behavior (such as user preference in a document’s topic might increase/decrease over time) at dierent levels of abstraction. This kind of modular design encourages reusability of components across environments. This exibility, though, also means users of the platform are required to implement layers of abstractions to emulate specic aspects of the user behaviour. Thus, making it hard to simulate a complex system and in turn potentially creating a simulation-to-reality gap. MARS-Gym [12] is aimed more at directly using the production data, creating simulation events using it and provides evaluation metrics using CPE to overcome the bias introduced by the logging policy. It addresses the whole development pipeline: data processing directly from the production logs, model design and optimization, and multi-sided evaluation. Mars-Gym also simulates the dynamics of multi user marketplace instead of viewing each user session individually. However, it doesn’t allow manipulating parameters in the simulation module. This lack of exibility makes it dicult to test specic assumptions. Similar to RecoGym, MARS-Gym is limited in the kind of interactions and user feedback it supports. This also makes them unsuitable for multi-objective optimization. RS simulators based on logged data such as Mars-Gym, PyRecGym also suer from bias in logged data policy and require methods to bias-correct the simulated data [5]. Inspired by [14] Table 1 compares these platforms across several dimensions that we consider important. One general issue we detected is that most simulation platforms require the implementation of recommender algorithms or models for a specic run-time and complying with specic APIs which might not match the actual production run-time. This implies that the algorithms need to be re-implemented (specially the inference logic, but also the learning logic, for example in online learning such as in [1]) in order to be deployed to production which clearly hurts velocity and introduces potential gaps between the simulated version and the deployed version. 4 THREE PRINCIPLES FOR BUILDING EFFECTIVE SIMULATION PLATFORMS In this section we propose a set of principles for building a industry-ready Simulation Platform: (1) Simulation co de = Production code the algorithmic components) must be completely agnostic of the run-time environment where it runs, be it a simulated environment or a real production environment. (2) Minimal Input assumptions. Interventions and assumptions are specied by the user, everything else must be, as much as possible, inferred from real data. (3) Simulation = Reality + Assumptions + Interventions design simulated environments with reusable and extendable components giving developers the ability to manipulate variables and explicitly express assumptions and interventions. The Simulation Platform provides means to make good trade-os between reality faithfulness, environment complexity and manipulability. 5 FUTURE DIRECTIONS We further think that the following directions can help to make the platform more robust and generic and ultimately help bridge the simulation-to-reality gap: • Structural Causal Models to sit at the core of a Simulation Platform. They provide a modular language to express assumptions and interventions relying on the principle of independence of mechanisms [ learned from data [3]. • Integrated evaluation framework combination of tools which can help the developer throughout the journey, starting from doing a controlled study of specic aspects to gain understanding of algorithms, challenging assumptions by manipulating given parameter in the environment using simulator and oering capabilities to compare the desired test policy with current production policy using CPE. Once a policy has been chosen, developers should also have the capability to test the whole integrated system and deploy directly in production without much code change and eventually evaluate using OCEs. 6 CONCLUSIONS Simulations can be considered as a complementary tool to Online Controlled Experiments and Counterfactual Policy Evaluation. Although it can be a exible and powerful tool for oine analysis and evaluation, a complex and realistic system might require signicant development eorts. Thus, the tool needs to be generic to serve multiple use cases. Though there are some simulation platforms developed in the industry, we identied and described principles that constitute, based on our understanding, the fundamentals of an ideal simulation platform for production code development, of particular interest in the industry context.