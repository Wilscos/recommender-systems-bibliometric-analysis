GStatic graph neural networks have been widely used in modeling and representation learning of graph structure data. However, many real-world problems, such as social networks, ﬁnancial transactions, recommendation systems, etc., are dynamic, that is, nodes and edges are added or deleted over time. Therefore, in recent years, dynamic graph neural networks have received more and more attention from researchers. In this work, we propose a novel dynamic graph neural network, Eﬃcient-Dyn. It adaptively encodes temporal information into a sequence of patches with an equal amount of temporal-topological structure. Therefore, while avoiding the use of snapshots to cause information loss, it also achieves a ﬁner time granularity, which is close to what continuous networks could provide. In addition, we also designed a lightweight module, Sparse Temporal Transformer, to compute node representations through both structural neighborhoods and temporal dynamics. Since the fully-connected attention conjunction is simpliﬁed, the computation cost is far lower than the current state-of-the-arts. Link prediction experiments are conducted on both continuous and discrete graph datasets. Through comparing with several state-of-the-art graph embedding baselines, the experimental results demonstrate that Eﬃcient-Dyn has a faster inference speed while having competitive performance. Keywords: dynamic graph neural network, representation learning, link prediction Recently, static graph neural networks (SGNNs) have seen a notable surge of interest with the encouraging technique for learning complicated systems of relations or interactions [1]. Unfortunately, abundant cases indicate that SGNN is progressively diﬃcult to deal with the time-varied graph-based structure [2]. These kinds of graphs are challenging because their links and nodes may emerge and disappear along with moments. Since the dynamic graph neural networks (DGNN) append a new temporal dimension to accumulate the variation of embedding or representations, they become prevalent and powerful tools to employ in diverse ﬁelds, such as social media [3], bio-informatics [4], knowledge bases [5], brain neuroscience [6], protein-protein interaction networks [7], recommendation system [8], etc. In order to deal with the complicated time-varied graphs, it is necessary and crucial to preprocess the raw dynamic graph representations, which record all continuous evolution of the graph over time, such as node emerging/disappearing and link addition/deletion [9, 10, 11, 12, 13]. Current researches [14, 15, 16, 17, 18, 19, 20] reﬁne the raw dynamic representations to two main branches, dynamic continuous and discrete graphs. For the former graphs, the raw representations are projected to a single 2D temporal graph, storing most information in graph evolution. However, some temporal information is lost in this process of projection [15] and the corresponding dynamic continuous networks are considerably complex [13]. In terms of the latter, discrete graphs, the structural representations are sampled to graph snapshots at regular time intervals, such as one day, over time. Although the developing networks are easier than the continuous ones, the temporal information is lost much more [12]. More details about time encoding approaches are discussed in Appendix 7.1. We hope to ﬁnd a generalpurpose manner to encode the raw dynamic graph representations, which can alleviate the temporal information loss and simplify the evolved network in future representation learning. Thus, one of our primary contributions, Adaptive Data Encoding (ADE), is proposed to adequately project the temporal information into a sequence of event-based patches with equal amounts of temporal-topological structural patterns for avoiding information loss. Once the reﬁned temporal graphs are ready, the DGNNs extract and analyze patterns for graph learning along temporal dimension. It is crucial to have an eﬃcient and powerful network under speciﬁc tasks in this step. Some researches [16, 19, 20] utilize recurrent neural networks (RNNs) to scrutinize representations on the sequence of dynamic graphs. However, RNNbased DGNNs are more time-consuming and inadequately handle sequential time-dependent embedding with increasing moments of time-steps. Since the transformer-based approaches [14, 15] adaptively designate divergent and interpretable attention to past embedding over time, the performance on long time duration is increased than RNN-based DGNNs. However, because the standard transformer [21] contains fully-connected attention conjunction N, where N is the number of temporal patches, it causes the heavy computation on a time-dependent sequence [22]. We hope to simplify and eﬀectively convey temporal information along the time dimension and achieve acceptable performance under inductive and transductive link prediction tasks. Thus, a lightweight module in Eﬃcient-Dyn called Sparse Temporal Transformer (STT) is proposed. Our lightweight STT simpliﬁes the fully-connected attention conjunction and reduces the connections of patches to 2N, which leads to far lower costs and acceptable performance than current standard transformer-based DGNNs. Figure 1 illustrates the overall architecture of the EﬃcientDyn, which contains three main components: ADE for adaptive encoding the raw dynamic continuous graph-based data, graph structural attention (GSA) for investigation on local structural patterns on patches, and STT for graph evolution capture of global temporal patterns over the long-time duration. In order to evaluate our proposed model, we conduct experiments on two continuous datasets [20] under inductive link prediction tasks. The experiments demonstrate that the proposed dynamic network signiﬁcantly outperforms state-of-the-art networks on the continuous graph datasets. In addition, we also designed an abbreviated version of Eﬃcient-Dyn, which consists of only GSA and STT. This abbreviated version is then utilized to learn the representation on the discrete datasets under both inductive and transductive link prediction tasks. Experiments show that the abbreviated version is still faster, more efﬁcient, and has higher accuracy on four discrete dynamic graph datasets [23, 24, 25]. The contributions in this paper are summarized as follows: • An innovative general-purpose DGNN, Eﬃcient-Dyn, is proposed to trade oﬀ the accuracy and eﬃciency on the dynamic representations under link prediction tasks. • We recommend a new approach, Adaptive Data Encoding, to preprocess the raw dynamic graph representations. The ADE can alleviate the information loss in the process, and the corresponding developing networks are more eﬀective in the representation learning tasks • We propose a lightweight temporal self-attentional module called Sparse Temporal Transformer. The STT-based Eﬃcient-Dyn can substantially reduce the computation by comparing RNN-based and standard transformer-based solutions on both continuous dynamic graph datasets. GSA and STT, can also be utilized on the discrete dynamic graph datasets. The experiments consistently demonstrate superior performance for Eﬃcient-Dyn over stateof-the-art approaches under inductive and transductive link prediction tasks. The structure of this paper is organized as follows: Section 2 presents a review of the latest work on dynamic graph datasets and networks. Section 3 identiﬁes critical terminology and fundamental concepts used in this paper. Section 4 describes the detailed mathematical process of Eﬃcient-Dyn. The experiment design and results are discussed in Section 5. Finally, Section 6 presents our conclusion. In general, graph representations can be categorized into two distinct levels: static and dynamic. The former includes only structural information, while the latter includes another critical parameter: time. The dynamic continuous representation contains node interactions [26], and timestamped edges [12], where instantaneous events are recorded into the raw graph representations, such as creation and removal of nodes and edges. For representation learning, the raw representations should be prepossessed to dynamic graphs ﬁrst. Current researches mainly focus on two dynamic graphs: the continuous and discrete graphs [9, 10, 12, 13]. The dynamic continuous graphs store the most information by projecting the raw representations to a 2D temporal graph, which is also a speciﬁc static graph appended with temporal information [11]. However, some temporal information is lost in the project processing. TGAT [15] adds temporal constraints to each interaction on the graph. However, the network is complicated because it has to extract temporal information at each moment, which is the general issue of the dynamic continuous graphs. For the dynamic discrete graphs, the researches [9, 10, 11] group graph embedding with a certain temporal granularity over time. The discrete graphs include discrete equal time intervals, which can be represented with multiple snapshots along temporal dimension [27]. Because the temporal information is sampled at the discrete moments, such as one day/month, to several graph snapshots, the discrete representation is less complicated than a continuous representation [13]. However, this kind of graph tracking manner causes more information loss in the processing. Also, the distribution of events or interactions among diﬀerent temporal windows is not homogeneous, which leads to an imbalance of temporal information among divergence graph snapshots. In order to ﬁnd a general-purpose time encoding approach and trade-oﬀ accuracy and eﬃciency, we propose the eventbased ADE module to adaptively encode the temporal information and determine the optimum number of temporal patches on the time dimension. Unlike traditional representations, our temporal patches contain an equal amount of temporal-topological structural patterns, which is fair and eﬀective for future representation learning. Since dynamic graph representations append the time dimension on the static ones, the RNN-based DGNNs [18, 28] are considered to summarize temporal information over time. However, the computation of RNN-based DGNNs is expensive because RNNs need a large amount of graph data for training. Moreover, it scales poorly on a long-time temporal dimension [14]. In order to solve this issue, the transformer-based DGNNs [15, 14] are introduced to deal with the temporal information along the time dimension. TGAT [15] utilize a temporal graph attention layer to aggregate temporal-topological features on the continuous graph datasets. DySAT [14] generates a dynamic representation by joint self-attention of both structural and temporal information on discrete graph datasets. However, the common problem is that their computation is enormous on a long temporal sequence due to fully-connected attention conjunction of the standard transformer. Dynamic graph networks should achieve the desired trade-oﬀ between accuracy and eﬃciency under the graph representation learning tasks. In order to achieve this target, our proposed Eﬃcient-Dyn which contains a lightweight module, STT. Instead of the fully-connected attention conjunction, the information is only conveyed among 1-hop neighbors and the relay node. Experiments show that such a module signiﬁcantly reduces the inference time and still achieves a good or better performance than the state-of-the-art approaches on both continuous and discrete representations. This section illustrates the terminology and preliminary knowledge. Table 1 denotes various terminologies in this paper. Definition 1. Dynamic Graph Neural Network. In general, a dynamic graph, DG = (V, E; T ), contains two main aspects: structural messages (V, E), and temporal information T , where V and E are the sets of dynamic vertices and edges. Unlike static graph neural networks, nodes and edges emerge and disappear along the temporal dimension. Definition 2. Graph Link Prediction. Given a graphG = (V, E) with adjacency matrix A, the task is to estimate the link between nodes u and m by analyzing the aggregated information on both nodes. Definition 3. Inductive Learning. Given DG = (V, E; T ), the DGNN can only investigate the graph information from the beginning to time T − 1. The analyzed representations are utilized to predict the future links at time T . This task is prevalent and crucial since it makes predictions on unseen nodes and links in the future. Definition 4. Transductive Learning. Given DG = (V, E; T ), the DGNN can observe all nodes from the beginning to the end, T . The model learns representation and implements the task on each snapshot or moment. As shown in Figure 1, a general-purpose Eﬃcient-Dyn includes three components connected serially. In order to make the distribution of the events uniformly along a temporal dimension and alleviate the disturbance from irrelevant messages to crucial ones in the future graph learning, ADE adequately encodes the temporal information to a sequence of patches with an equal amount of temporal-topological structural patterns by investigating the frequency of events. The GSA module extracts the local structural representations with a self-attention layer on each temporal patch. The learned time-dependent structural representations are sent to the lightweight module, STT, to capture the global graph evolution along the temporal dimension. A continuous dynamic graph can be regarded as a particular static graph with an additional temporal dimension. Remarkably, all events are stored to the continuous representation along time, such as link addition, link deletion, node addition, node deletion, et al [11]. Recent researches [16, 29, 30] project the topological graph structures and node features from the continuous representation to the ﬁnal 2D temporal representations for future representation learning. In order to record the evolution of graph structure over time, TGAT [15] introduce temporal constraints on neighborhood aggregation methods. However, the continuous representation of these networks is far complicated. Also, the data processing of these networks is low-eﬃciency because they pay much attention to the inoperative information for the target node. We hope to ﬁnd a general-purpose approach to simplify the continuous representation along the temporal dimension and speed up the computation for time-dependent structural patterns. Thus, in this work, an ADE module is proposed to adaptively encode temporal information into a set of event-based temporal patches with an equal amount of temporal-topological structure. Figure 2 exposes the comparison of three encoding approaches: raw graph representation without encoding, uniform data encoding (UDE), and ADE. Based on the raw time-dependent representations with superabundant details, the UDE separates the graph patterns into several patches by the same temporal intervals, such as one day, one week, one month, et al. However, because the events have happened irregularly along the time dimension, the distribution of temporal-topological patterns is not homogeneous of the encoded patches. The computation on some patches is expensive because it takes more to deal with the complicated patch structure; meanwhile, the calculation is light on those patches with fewer events. Thus, it is ineﬃcient to process the patches with diﬀerent amounts of structural patterns in parallel computing. As shown in Figure 3, the raw data are divided into N eventbased patches along a temporal dimension by ADE. The amount of structural patterns of patch Pis equal to the one of patch P. In this procedure, it is crucial to balance the number of patches,N, and the quality of structural embedding of each patch along the time dimension. If N is too large, it is also ineﬃcient to analyze the graph structure due to minor embedding of each patch. A cost function of ADE is utilized to adaptively determine the number and the embedding of patches in Equation 1 and 2. Where E is the embedding of the raw graph representations, and τ is a constant parameter. The R is the number of total events, and N =is the number of patches. In Equation 1, the ﬁrst item is to minimize the diﬀerence between the quality of ﬁnal projected representations of all patches. Ideally, when N = 1, the raw graph representations indicate the diﬀerence is the least, which is equal around to 0. The second item is encouraged to increase a large value of N. By minimizing the cost function of L, an optimum balance between N and Ecan ﬁnally be obtained. In Equation 2, the function φ is the Pearson Correlation Coeﬃcient [31] to measure the linear correlation of embedding between each two patches. The  is a tiny constant parameter to ensure that each patch contains an equal amount of temporaltopological patterns. Since the raw graph representations are encoded to the optimal N time-dependent patches, the next step is to extract the local structural patterns on each encoded patch. The input of GSA is a set of node representations of the current patch. Inspired by GAT [32], a node self-attention matrix, α, is learned to determine the relevance between neighbors and the center node, u, on the time-dependent patch, as shown in Equation (3) and (4). Where eindicates the relevance of neighbor node m to the center node u. The σ is the exponential linear unit (ELU) activation function [33]. The A is the adjacency matrix of the patch to indicate the linking relation of current patch, The γ indicates the self-attention mechanism, and Wis the weight matrix of patch p. The xand xare the node representation of the center node u and neighbor node m. Once the attention coeﬃcients are received, the activation function ψ is applied to get the non-linear node representation of the current patch, as shown in Equation (5). Where ψ is the Gaussian Error Linear Unit (GELU) [34] for the ﬁnal output representations, h∈ Ris the patch embedding, and d is the updated feature dimension. Since the updated structural patterns of each patch are extracted, the next step is to add the position embedding of timedependent patches, which embed the absolute temporal position of each patch, as shown in Figure 4 Left. Thus, the ﬁnal output of GSA, p, contains both local structural patterns and temporal position information of the current patch. Since the GSA module extracts the structural patterns of time-dependent patches, the next step is to gather the global evolution of structural patterns along the time dimension. Current transformer-based DGNNs [15, 14] utilize the standard transformer to extract the temporal patterns and receive good accuracy on both continuous and discrete dynamic representations. However, due to the fully-connected attention conjunction as shown in Figure 4 Right, the computation is expensive of these transformer-based DGNNs on a long time sequence. In order to reduce the computation, we design a lightweight module, STT, instead of the standard transformer to deal with the global temporal patterns of time-dependent patches. Figure 4 Middle illustrates the architecture of the STT module, which consists of N time-dependent patches from GSA and a single relay patch. Each temporal patch is connected with two adjacent patches and the relay one on the STT. The functionality of the relay patch is to congregate and distribute the representation among all time-dependent patches. Thus, the STT module can learn global representations with the relay patch. By comparing the fully-connected attention conjunction, the advantage of our connection is that the computation to extract the temporal information is cut down by reducing the interaction times of patches. The input of STT is a sequence of representations for a center node u at all temporal patches. The embedding of the relay patch is initialized as the average of all time-dependent patches at the beginning. The context of c(t) of the patch i is updated by aggregating the representations from its two neighbor patch i −1 and i+ 1, the relay r(t −1), the state of itself at last moment z(t − 1), and the embedding pin Equation 6. c(t) =z(t − 1); z(t − 1); z(t − 1); p; r(t − 1)(6) The temporal self-attention function of the current state z(t) of patch i is deﬁned as in Equation 7. Where β(t) is the self-attention coeﬃcients of temporal patches, W, W, Ware learnable parameters, and d is the feature dimension of z. A layer normalization operation [35] is added after the transaction among all patches. Meanwhile, current state of relay patch r(t) gather all representations from temporal patches Z(t ), and the state of itself at last moment r(t − 1) in Equation 9. Where λ(t) is the self-attention coeﬃcients of relay patch, W, W, Ware learnable parameters, and dis the feature dimension of the relay. Both φand φare non-linear activation function. Similarly, the layer normalization operation is also added after the transaction on relay patches. Graph link prediction is one of the core graph tasks, whose purpose is to forecast the connection among nodes based on node representations. In the inductive task, the representation of T − 1 temporal patches are analyzed in the training processing. The network makes the link prediction to the unseen nodes on the ﬁnal predicted graph (PG). In this procedure, we utilize the deep walk [3] approach to sample some positive (connected links) and negative (unrelated links) on PG. A binary cross-entropy loss is to embolden positive cases to have similar representations while suppressing the negative ones in Equation 11. Where ϕ is the non-linear activation function, < · > is the inner-production, ωis a constant ﬁne-tuned hyper-parameter. The N(u) is the sampled positive cases in ﬁxed-length random deep walks on PG, while the P(u) is the sampled negative cases. In terms of the transductive task, the positive and negative cases are sampled at each temporal patch. Thus, the ﬁnal loss function is to calculate the sum of all costs on each patch in Equation 12. We experimentally validate the general-purpose EﬃcientDyn on six real-world dynamic graph datasets: two continuous and four discrete datasets. Table 2 and Table 3 summarizes the statistics of the details of these six datasets. Reddit and Wikipedia These two dynamic continuous graph datasets describe the active users and their editions on Reddit and Wikipedia in one month. The dynamic labels represent the state of the user on their editions. Reddit contains 10984 nodes and 672447 links, while Wikipedia contains 9227 nodes and 157474 links. Enron and UCI These two dynamic discrete graph datasets describe the network communications. Enron includes 143 nodes (employees) and 2347 links (email interactions), while UCI includes 1809 nodes (users) and 16822 links (messages). Yelp and ML-10M These two dynamic discrete graph datasets describe the bipartite networks from Yelp and MovieLens. The Yelp has 6509 nodes (users and businesses) and 95361 links (relationship), while ML-10M has 20537 nodes (users with the tags) and 43760 links (interactions). These experiments compare diﬀerent approaches on two continuous datasets under the inductive link prediction task. In these experiments, we compare our general-purpose networks with another four approaches as the baselines: GAT-T [32], GraphSAGE-LSTM [36], Const-TGAT [15], and TGAT. GATT concatenates the time encoding to the graph structural features when gathering the temporal information. GraphSAGELSTM considers Long Short-Term Memory (LSTM) to aggregate the temporal information over time. TGAT utilizes a temporal attention coeﬃcient matrix to aggregate temporal representations. Const-TGAT pays the same temporal attention to collect the temporal patterns. In order to compare the performance of STT and standard transformer to collect the temporal information, we modify the architecture of TGAT with STT instead of the standard transformer and name it as Eﬃcient-Dyn*. Table 4 shows the accuracy of these approaches under the link prediction task on two dynamic continuous datasets. It can be observed that the performances of transformer-based networks are better than RNN-based ones. With the temporal attention, the accuracy of TGAT can exceed 2.4% and 1.68% than the ones of Const-TGAT on Reddit and Wikipedia. By comparing with TGAT and Eﬃcient-Dyn*, the latter’s accuracy achieves 92.83% on Reddit and 87.46% on Wikipedia, which surpasses 2.15% and 2.04% than the former. Meanwhile, the inference time of Eﬃcient-Dyn* is less than TGAT in Table 7, which demonstrates STT is more eﬀective by comparing the fully-connected connection of the standard transformer. The Eﬃcient-Dyn’s accuracy is less than TGAT by 2.03% and 1.73% on Reddit and Wikipedia datasets because TGAT utilizes more details with temporal constraints of graph representations. However, our inference speed is only around 0.6 times that of TGAT on both continuous datasets, which is more competitive in practical usage. These experiments demonstrate the contribution of Eﬃcient-Dyn consisting of both EDA and STT on dynamic continuous representations under the link prediction task. The previous experiments demonstrate the power of EﬃcientDyn on dynamic continuous datasets. Our proposed generalpurpose network can also be utilized on discrete graph datasets. Since the discrete representations have several graph snapshots along temporal dimension, we compare our Eﬃcient-Dyn§, which only consists of GSA and STT, with another seven baselines: node2vec [4], GraphSAGE, GAT, Dynamic Triad [17], DynGEM [37], DynAERNN [18] and DySAT. The node2vec handles second-order random walk sampling to grasp node representations. Dynamic Triad combines triadic closure to preserve both structural information and evolution patterns. DynGEM utilizes a deep autoencoder to generate non-linear embeddings of snapshots. DynGEM constructs both dense and recurrent layers to investigate the temporal graph evolution. DySAT extract node representations via fully-connected self-attention on both graph structural and temporal patterns. Table 5 summarizes the results of these eight approaches on four dynamic discrete graph datasets. We ﬁnd that the accuracy of DySAT exceeds the other state-of-the-art approaches, except Eﬃcient-Dyn§, under the link prediction task, which beneﬁts from the fully-connected attention conjunction architecture of transform by extracting temporal patterns over time. This phenomenon demonstrates that the transformer-based DGNN outperforms the traditional graph learning approaches, includes RNN-based DGNNs. By comparing DySAT and Eﬃcient-Dyn§, we found the accuracy of Eﬃcient-Dyn§is 81.36%, 85.47%, 72.59%, and 94.28% on Enron, UCI, Yelp, and ML-10M datasets, which are better than DySAT. Meanwhile, the inference time of Eﬃcient-Dyn§is less than DySAT, which demonstrates that Eﬃcient-Dyn§is also competitive and eﬀective on dynamic discrete graph representations. Besides previous experiments, we also evaluate our generalpurpose network on dynamic discrete datasets under the transductive link prediction task. From Table 6, we observe the transformer-based DGNNs (DySAT and Eﬃcient-Dyn§) have better performances on four discrete datasets, which also prove the self-attention architecture is powerful for transductive graph learning. As a result, the accuracy of Eﬃcient-Dyn§achieves 87.94%, 87.53%, 72.01%, and 87.52% on Enron, UCI, Yelp, and ML-10M separately, which also demonstrates the improvements delivered by our innovative architecture of Eﬃcient-Dyn. This paper proposes a novel general-purpose dynamic graph neural network, Eﬃcient-Dyn, that trade-oﬀs the accuracy and eﬃciency under both inductive and transductive link prediction tasks. Eﬃcient-Dyn consists of three main components: ADE, GSA, and STT. The ADE module adaptively encodes temporal information into a sequence of patches with an equal amount of temporal-topological structure, which avoids the information loss in the projection processing due to adaptive generation with a more delicate time granularity. The GSA module learns the local structural representations on each encoded patch along the temporal dimension. The lightweight STT is utilized to extract global temporal patterns over time. Beneﬁted from the information delivery on the simpliﬁed architecture, the computation is further reduced compared with current state-of-the-art approaches. The Eﬃcient-Dyn is evaluated on two continuous and four discrete graph datasets. The result illustrates that our proposed network is competitive and eﬃcient on both inference speed and performance. Declaration of competing interest The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to inﬂuence the work reported in this paper.