XIAOWEN HUANG, JITAO SANG, and JIAN YU, School of Computer and Information Technology & Beijing Key Lab of Trac Data Analysis and Mining, Beijing Jiaotong University, China CHANGSHENG XU Sciences, China, School of Articial Intelligence, University of Chinese Academy of Sciences, China, and Peng Cheng Laboratory, China The cold-start recommendation is an urgent problem in contemporary online applications. It aims to provide users whose behaviors are literally sparse with as accurate recommendations as possible. Many data-driven algorithms, such as the widely used matrix factorization, underperform because of data sparseness. This work adopts the idea of meta-learning to solve the user’s cold-start recommendation problem. We propose a meta-learning based cold-start sequential recommendation framework called metaCSR, including three main components: Diusion Representer for learning better user/item embedding through information diusion on the interaction graph; Sequential Recommender for capturing temporal dependencies of behavior sequences; Meta Learner for extracting and propagating transferable knowledge of prior users and learning a good initialization for new users. metaCSR holds the ability to learn the common patterns from regular users’ behaviors and optimize the initialization so that the model can quickly adapt to new users after one or a few gradient updates to achieve optimal performance. The extensive quantitative experiments on three widely-used datasets show the remarkable performance of metaCSR in dealing with user cold-start problem. Meanwhile, a series of qualitative analysis demonstrates that the proposed metaCSR has good generalization. CCS Concepts: • Information systems → Personalization. Additional Key Words and Phrases: cold-start recommendation, meta-learning, graph representation, sequential recommendation ACM Reference Format: Xiaowen Huang, Jitao Sang, Jian Yu, and Changsheng Xu Recommender. ACM Transactions on Information Systems 1, 1, Article 1 (January 2021), 26 pages. https: //doi.org/10.1145/3466753 Recommendation systems (RS) intend to address the information explosion by nding a set of items for users to meet their personalized interests in many online applications, such as E-commerce websites [ since the rapid development of RS, many eective recommendation algorithms have been proposed: from the content-based methods, to the widely used collaborative ltering (CF) algorithm, to the 17], social networks [14], video-sharing sites [3] and news websites [36]. In the decades recently emerging deep learning-based approaches. Although these kinds of recommendation algorithms generally work well when sucient data is available, cold-start recommendation which address the sparseness problem is yet a dicult and urgent problem to be solved in practical applications. Cold-start happens when new users/items arrive on online platforms. Classic recommendation methods like CF assumes that there are sucient user-item interactions (browse, click, rate, etc.) for matrix factorization. However, new users/items usually contain extremely sparse data, so that we can infer ratings of similar users/items even if those ratings are unavailable. However, for new users/items, this becomes dicult because we have no or just only a few such interactive data for them. As a result, we cannot “ll in the blank” using typical matrix factorization techniques to achieve the desirable recommendation results due to the data sparseness issues. Content-based methods [ features (e.g., gender, location, nationality, religion) or item-specic properties (e.g., genre, publication year, actors, director in the case of movies). But in real-world scenarios, such information is usually missing due to the unavailability of data or user-privacy issues, which greatly reduces the eect of these methods. Some previous eorts [ tackle the cold-start problem with domain knowledge transfer. But they still require a large amount of shared samples across domains. Data sparseness issues, and the accessibility of additional side information remain key barriers to cold-start recommendations. This work addresses the cold-start issues in the sequential recommendation (SR) scenario. Compared with the traditional recommender, SR system holds the ability of capturing the evolution of users’ dynamic interests [ sequential behaviors. For example, when watching a movie or TV series, users usually watch the rst episode before watching the sequels; when shopping on the E-commerce websites, users usually buy a new computer, and then a mouse, keyboard, and other accessories. The sequential patterns are common to almost all users. It is very helpful to assist recommendation by transferring the prior common patterns to cold-start users. We notice that the user cold-start recommendation problem can be formulated as a few-shot learning problem, where the meta-learning method is a recognized solution. A popular metalearning method, Model-Agnostic Meta-Learning (MAML), provides a promising way to extract and propagate transferable knowledge of prior tasks and learn a good initialization for new tasks. In recent years, some work has introduced meta-learning algorithms into cold-start recommendation, but most of these algorithms need additional side information, and they do not model the temporal relationship of user behaviors, resulting in lacking the ability to model behavioral sequential patterns and the ability to capture user dynamic preferences. To consider the characteristics of sequential recommender and address the limitations of existing cold-start recommending methods, we propose the Recommendation framework (metaCSR), an end-to-end framework for user cold-start sequential recommendation (CSR), which takes the user-item interactions from warm-start (regular) users with adequate behaviors as input, and outputs “next-one” item prediction for cold-start (new) users with few behaviors. The key insight behind metaCSR framework is to learn the common patterns from regular users’ behaviors, facilitate the initialization of cold-start users so that the model can quickly adapt to new users after one or a few gradient updates to achieve optimal performance. There are three key components of metaCSR: (1) The Diusion Representer, which works on the user-item interaction graph, is proposed to learn the users’ and items’ high-order interactive representation. The purpose of this module is to learn more eective user/item embedding only through the information diusion on user-item interaction graph without using additional side information. (2) The Sequential Recommender, which is based on self-attention mechanism, is used 18] usually rely on additional side information, for instance, user-specic demographics to model the temporal dependencies of users’ sequential behaviors to capture users’ dynamic interests. (3) The Meta Learner, which is a model-agnostic meta-learning algorithm, is employed to learn common patterns of user behaviors so that it can quickly adapt to new users with only a few gradient updates. The contributions of this work are summarized as follows: • We propose a novel meta-learning based cold-start sequential recommendation framework, 1 RELATED WORK In this section, we briey review the related work that are most relevant to our work, including meta-learning methods, sequential recommendation methods and the existing recommendation approaches for the cold-start problem. 1.1 Meta-learning Meta-learning, also called “learning to learn”, is a recently popularized paradigm for training an easily generalizable model that can rapidly adapt to new tasks from only a few examples [ There are three main research directions of meta-learning, including metric-based, model-based and optimization-based meta-learning. Metric-based meta-learning, such as matching network model updates its parameters rapidly with a few training steps, which can be achieved by its internal architecture or controlled by another meta-learner model. Memory-augmented neural networks [27] and meta networks [ based meta-learning intends for adjusting the optimization algorithm so that the model can be good at learning with a few examples, including MAML [ These methods promise to extract and propagate transferable representations of prior tasks. 1.2 Sequential Recommendation Sequential recommendation (SR) problem is usually cast as sequence prediction problem. The sequence modeling methods mainly belong to Markov Chain based models [ based models [ Personalized Markov Chain (FPMC) combines the power of MF and MC to factorize the transition matrix over underlying MC to model personalized sequential behaviors for the problem of next-item recommendation given the last-N interactions of the user [ state from the current input in the sequence and the hidden state outputted by the previous time step. The recurrent feedback mechanism memorizes the inuence of each past data sample in the hidden state. It therefore makes RNN and its variants such as LSTM and GRU be able to model the temporal information for user behaviors in recommendation task [ an eective way to encode users’ behavior sequences, it still suers from several diculties, such as hard to parallelize, time-consuming, hard to preserve long-term dependencies. The emergence of the Transformer [ which is an end-to-end framework that can extract and propagate transferable knowledge of regular users and quickly adapt to new users. The incorporation of Diusion Representer and Sequential Recommender helps to better capture users’ dynamic preferences and achieve promising performance in dealing with user CSR problem without relying on any additional side information. We run extensive experiments on three real-world datasets. The promising results demonstrate the ecacy of our proposed metaCSR in addressing user CSR problem, while maintaining competitive performance in both warm-start and cold-start recommendation scenarios. ], aims to learn the similarity between samples within tasks. Model-based meta-learning 7,8,26,41]. Inspired by the great power of Matrix Factorization (MF), Factorized abandon the complex and time-consuming RNN structures, and instead construct the sequence model based on self-attention mechanism and apply it to SR system. ATRank [ in using self-attention structure for the SR and achieves encouraging results. CSAN [ a feature-wise masked self-attention to construct user preference representations for SR. EIUM [11] introducing knowledge graph into self-attention based Sequential Recommender models for providing explainable recommendations. 1.3 Cold-start Recommendation The research of cold-start recommendation mainly focuses on two aspects, named user cold-start recommendation [ new users who have no/few historical behaviors, or recommends new products that have just been added to the system to the suitable users. These methods suer from a common issue that they need user demographics or item attributes to assist in modeling to be applied to cold-start recommendation tasks. However, the missing of additional side information due to unavailability of data or user privacy issues will greatly reduces the eect of these methods that rely on attribute information. Actually, the cold-start recommendation task is a natural few-shot learning problem. Since meta-learning is a powerful way to solve the few-shot learning problem, in recent years, some research works have introduced the idea of meta-learning into the cold-start recommendation task. Vartak et al items arrive continuously. Bharadhwaj recommendation model based on the MAML algorithm to rapidly adopt new users with a few examples, which is named MetaCS and MeLU, respectively. Wei et al method which applicable to any dierentiable CF-based models like FISM [ learn a suitable model for initializing the adaption. SML [ which oers a general training paradigm, where a neural network-based transfer component can transform the old model to a new model that is tailored for future recommendations. Du et al [4]combine the scenario-specic learning with meta-learning for online-recommendation. Zhao et al. [45] parameters for user cold-start recommendation. Luo et al to facilitate user-level adaptive model selection in recommendation system. The major dierence between metaCSR and existing literature is that: we focus on the cold-start sequential recommendation task where common patterns of sequential behaviors are mined and learning through our meta-learning based algorithm. Moreover, Our proposed metaCSR is a general framework for CSR, which does not require any additional side information other than user ID, item ID, and interaction matrix of users on items, and can still achieve good results on the CSR task. 2 PRELIMINARIES 2.1 Problem Formulation General user behaviors can be interpreted using the binary relationship between a user and an item. We denote user set Hence, the historical sequential records of user Based on these preliminaries, we are ready to dene the sequential recommendation task: given user may interact with at time 𝑇 + 1. Scenario 1. Cold-start scenario. Our main purpose is to propose a universal framework to address user cold-start recommendation problem, so the main task of this work is to train the model on recommendation on new users 𝑢 ∈ U Scenario 2. Warm-start scenario. We also want to know whether it is eective in dealing with general recommendation task, thus we also conduct the experiments in user warm-start scenario, where we still make the predictions on regular users 𝑢 ∈ U 2.2 Notations The main notations of this work are summarized in Table 1. Fig. 1. Illustration of meta-learing based cold-start sequential recommendation framework (metaCSR). develop a two-stage meta-learning algorithm to learn xed and adaptive parts of model U.U = {U∪ U| U∩ U= ∅}.I = {𝑖, 𝑖, ..., 𝑖}as the set of items. , 𝑡 = 1, 2, ...,𝑇 }of a user towards items, the task is to predict the next item𝑖that the 3 METHODOLOGY In this section, we present the technical details of recommendation. The overall framework is illustrated in Fig. 1, where three key components are involved in, including Diusion Representer, Sequential Recommender and Meta Learner. 3.1 Diusion Representer There are many ways for the embedded representation of users and items, such as the simplest onehot representation, or low-dimensional dense vectors which are compressed from high-dimensional sparse vectors through an embedding layer (such as a fully connected network). Moreover, fusion representations by introducing auxiliary information, such as user’s attributes, item’s text and images, etc. Furthermore, there are semantic representations that contain semantic graph structure information, for example, introducing knowledge graphs into the representation learning of users and items. No matter what kind of representation manners, the ultimate goal is to represent users and items better. In recommendation tasks, mining users’ interest is the core means to improve task performance. The development of learning on graph-structured data, which is fundamental for recommendation applications, for example, to exploit user-item interaction graphs as well as social graphs [ Several recommendation models utilize user’s local neighbors’ preferences to alleviate the data sparsity issue. Graph Convolutional Networks (GCNs) have shown promising results by modeling the information diusion process in graphs that leverage both graph structure and node feature information. Thus, we construct a user-item interaction graph fusion eect in a more ne-grained way. We learn the users’ and items’ high-order interactive representations through aggregating neighbors’ information, and apply them to the downstream tasks. As illustrated in Fig. 2, the user-item interaction graph composed of users and items are connected through an interactive relationship. The Diusion Representer, which works on the user-item interaction graph, is to learn the users’ and items’ high-order interactive representation. The purpose of this module is to learn more eective user/item embedding only through the information diusion on user-item interaction graph without using additional side information. Certainly, additional auxiliary information (eg., textual and visual features of users and items) also can be seamlessly integrated into the graph. Inspired by SocialGCN [ two parts, one is the inherent feature vector of the entity itself, which can incorporate one-hot ID, attributes, context information, and so on. Let the latent feature vector obtained through information diusion, which contains the structural information of the interaction graph and the information supplement of neighbor nodes, which is represented by 𝑓 For each entity as: To simplify the description, we take 1-order convolution as an example. The procedure is detailed in Algorithm 1. The basic idea is that: step 1. step 2. step 3. step 4. The normalization makes the training more stable. A single convolution operation transforms and aggregates feature information from an entity’s one-hop graph neighborhood, and by stacking multiple such convolutions information can be propagated across far reaches of a graph. 3.2 Sequential Recommender Modeling users’ sequential behaviors, capturing the dependencies between the elements of behavior sequence, is conducive to learn users’ dynamic interests, thereby improving the performance of the sequential recommendation system. In order to simplify the structure of the model and improve the eciency of training, inspired by [ an element-wise self-attention module with two position encoding matrices model the sequential behaviors, in which we can benet not only from the ability in capturing the long-distance dependencies of the sequence with various lengths, but also from the capability in parallel computing for eciently learning. An aggregator𝐴𝐺𝐺.(e.g., mean pooling, max pooling, etc.) is applied on the neighbors ^𝑣 ∈ Nto aggregate the inuences from neighbors’ representations. The aggregated representationℎis transformed through an embedding layer to generate the latent feature embedding𝑓of entity𝑣, the purpose of which is to project the aggregated vector^𝑣into the same space as the inherent feature embedding𝑓. The inherent feature 𝑓is initialized via lookup operation. The aggregated representationsℎthrough an embedding layer to generate the latent feature embedding𝑓of entity𝑣, the purpose of which is to project the aggregated vector^𝑣 into the same space as the inherent feature embedding 𝑓. ← 𝑅𝑒𝐿𝑈 (𝑊ℎ+ 𝑏); Representer module. The masked self-attention network takes sequence to obtain the nal semantic representations of the entire sequence. They will be updated iteratively with the training of the model. Take the sequence correlations between the two elements: where the forward and backward matrices are dened as: where position of the two elements, the positional deviation between that there are two independent processes containing forward procedure and backward procedure. In the forward encoding process, 𝑖, ∞ otherwise. And vice versa in the backward encoding process. After obtaining attention scores over all elements, the output for 𝑖 Then we use a fully-connected layer to map the combination of and 𝐸 where [,] is a concatenation. preference embedding, which combines each independent behaviors and considers the dynamic dependency between them, can be used to predict probability of user predicting function 𝐹 , which can be inner product or H-layer MLP: All the weight matrices 𝜎 (·) is the sigmoid function. 𝑑= 𝑒𝑥𝑝 (|𝑚, 𝑛|),|𝑚, 𝑛| = 1if𝑖, 𝑖are adjacent, and so on. Taking into account the relative = {i, i, ..., i} ∈ Rto vector s∈ R: 3.3 Meta Learner metaCSR aims to excavate the common patterns for recommendation by reformulating coldstart recommendation as a few-shot learning problem and promote generalization ability of the trained model. Gradient (or optimization) based meta-learning has recently emerged as an eective approach for few-shot learning. We extend MAML into the metaCSR framework. The full algorithm is outlined in Algorithm 2. In the 𝑁users, each users provides quences as the query set learning of the Diusion Representer module. Then the sequences are sent to the Sequential Recommender to be encoded as the users’ preference embedding. We consider the model represented by a parametrized function of Sequential Recommender. 𝜃 In the sequential recommendation task, the ultimate goal is to rank the ground-truth item higher than all other items 𝑗 In the Recommender, by one or more gradient descent updates using 𝐷 The inner loop optimization across task is performed via SGD, such that the model parameters become 𝜃 The inner loop works for modeling the personalization of users to obtain personalized user preference embedding, in which we do not update the representation of users and items in Diusion Representer, because the users’/items’ representations are commonly shared. The outer loop optimization across task is performed via Adam, such that the model parameters and 𝜃 The purpose of this procedure is to learn the desired parameters of the whole model via metalearning in such a way as to prepare that model for fast adaption to new users after only a few local updates. In the and then the new model is applied to perform the nal recommendations. meta-train phase, let𝑝 (T )denotes the distribution over all tasks. Each taskTcontains inner loop optimizationprocedure, we update the task-specic model, i.e., the Sequential outer loop optimizationprocedure, we update all parameters by optimizing the perforusing 𝐷: are updated as follows: meta-test phase, a few samples of new users are applied to ne-tune the trained model, Input: 𝑝 (T ): distribution over tasks; 𝛼, 𝛽: step size hyperparameters; 𝜃: parameters of Diusion Representer; 𝜃: parameters of Sequential Recommender; 4 EXPERIMENTS We mainly focus on the cold-start sequential recommendation task. In this section, we conduct a comprehensive suite of experiments and evaluate the empirical performance of the proposed method in three public real-world datasets, and try to address the following ve major research questions (RQs): • RQ1 • RQ2: Can the proposed metaCSR model also perform well in warm-start scenario? • RQ3: How sensitive is the proposed metaCSR model to the amount of training data? • RQ4: How much does each module in the framework play a role in the user CSR task? • RQ5 4.1 Seings 4.1.1 Datasets. We adopt three widely-used datasets to conduct the experiments, including: , 𝜃randomly; : Can the proposed metaCSR model eectively solve the user’s cold-start sequential recommendation problem? : Does the proposed metaCSR model learn the common pattern of users’ sequential behaviors? recommendation task targeting for implicit feedback like previous eorts [6,31,42]. So the threshold of positive rating is set to 4. We dene the target value of user-item equals to 1 when user-item interaction is observed and the rating is above threshod, and 0 otherwise. The statistics of datasets are presented in Table 2. 4.1.2 Implementation Details. • General settings. • Meta-learning settings. • Implementation details. 4.1.3 Evaluate Metrics. A variety of widely used evaluation metrics are adopted to evaluate our approach: • AUC . It is an ocial song tag and song similarity dataset of the Million Song Dataset. We intercepted the latest one week data for the experiments. Only interactions are used in this study. timestamps on various products [21], is notable for its high sparsity and variability. We take "Video" sub-dataset for experiments. of interactions about 100, while Amazon-Video presents an obvious long-tail distribution pattern. Therefore, for the MovieLens and Last.fm datasets, we regard 80% of users with a large number of behaviors as regular users, and the remianing 20% as new users. In order to simulate real scenarios and evaluate the performance of the model, for all new users, we only keep at most 10 earliest behaviors, and the testing phase is carried out on these few behaviors. As for the Amazon-Video dataset, we regard users with behaviors in the range of 2-5 as new users, and users with behaviors greater than 5 as regular users. For each positive sample, we follow [6] and randomly sample 100 items that the user has not interacted with to generate negative samples that pair it. Given the user’s interaction sequence, we random select a short sequence containing𝑇interactions to predict the(𝑇 + 1)-𝑡ℎinteraction in the meta-train phase, where 𝑇 ∈ [2, 10]. regular users form a meta-task, each user’s support set contains 5 samples, and the query set contains 15 samples. In the meta-test phase, we hold out the last behavior of new users to build a cold-start scenario test-set, and the behavioral sequences before the last behavior are used to ne-tune the model to quickly adapt to unseen new users’ recommendation tasks. In the warm-start scenario, we hold out the last behavior of regular users to build the nal warm-start scenario test-set. search for the learning rate and nd𝛼 = 1𝑒with SGD optimizer and𝛽 = 1𝑒with Adam optimizer is the best. Weight decay =5𝑒. The batch size is set to 16. All the embedding dimension are set to 128. For the comparative methods, the parameters are set as suggested by the original papers. . We evaluate the ranking performance as practical recommender systems usually generate a ranked list of items for a given user. AUC [28], the area under the ROC curve, is 4.1.4 Comparison Methods. • metaCSR. Our proposed method. commonly used for evaluating the quality of a ranking list: where𝐽denotes the positive samples set, and𝐽means negative.𝛿 (𝑝> 𝑝)is an indicator function which returns 1 if(𝑝> 𝑝)is true, and 0 otherwise.𝑝is the predicted probability that a user𝑢 ∈ Umay act on𝑖in the test set. A higher value of AUC indicates better performance for ranking performance. The oor of AUC from random guess is 0.5 and the best result is 1. . Mean Average Precision computes the average value with considering the rank in the sequence of returned items, which is a popular performance measure in information retrieval. Mean average precision is dened as: where 𝑄 is the number of queries. . Hit Ratio considers whether the relevant items are retrieved within the top N positions of recommended list. It is 1 if any ground-truth items are recommended within the top N items, otherwise 0. We compute the mean of all users as the nal hit ratio score. positive and negative items within the top N of ranking list. It is a standard measure of ranking quality. . Bayesian personalized ranking [28] is a pairwise ranking framework that takes Matrix Factorization as the underlying predictor. Long short-term memory (LSTM) units are building units for the layer of a recurrent neural network (RNN), which are used to capture sequential dependencies and make predictions [43]. . Contextual self-attention network [12] is a sequential recommendation algorithm based only on feature-wise self-attention mechanism, which can speed up the training process and capture user dynamic interest. . MeLU [15] is a meta-Learned user preference estimator for cold-start recommendation, which is a MAML-based algorithm that can identify personalized preferences. linear regression model as the benchmark model. with deep neural network as the benchmark model. SML [44] is a sequential meta-learning method which oers a general training paradigm, where a neural network-based transfer component can transform the old model to a new model that is tailored for future recommendations. with meta-learning, which is applicable to any dierentiable CF-based models. We use MetaCFas the representative comparison method. Table 3. The next-one recommendation performance of all the methods across the evaluation metrics AUC and MAP in labeled with ‘ achieve w.r.t the best baseline. 4.2 Performance Analysis To demonstrate the ecacy of metaCSR, We conduct a comprehensive suite of experiments and evaluate the empirical performance of the proposed method. In this subsection, we perform the analysis of the following ve major research questions (RQs): 4.2.1 RQ1: Can the proposed metaCSR eectively solve the user’s cold-start sequential recommendation problem? Solving the user’s cold-start recommendation problem is the main task of this work, in which new users are not involved in the training of the model as invisible tasks, also called cold-start scenario in this paper. In this scenario, new users are held out at the beginning, and the model is ne-tuned only during the testing phase with a very small number of new user’s behavioral samples. Table 3 shows the next-one recommendation performance of all the methods across the evaluation metrics AUC and MAP on the three datasets in the cold-start scenario, and Fig. 3 shows COLD-START scenario. The best performance is boldfaced; the highest score in baseline is ∗’; the percentage in parentheses (+/-%) represents the relative improvements that metaCSR Fig. 3. The next-one recommendation performs of all the methods across the evaluation metrics NDCG@N and Hit@N in COLD-START scenario. 𝑁 ∈ [1, 20]. the performance on NDCG@N and Hit@N with N changes. It can be seen from the performance under dierent evaluation metrics: (1)BPR performs poor than other comparison methods in most cases, because it lacks the ability to model sequential dependencies so that they cannot obtain good performance on sequential recommendation tasks. The following four meta-learning based methods perform better than BPR due to the meta-learning framework they adopt. Table 4. The next-one recommendation performance of all the methods across the evaluation metrics AUC and MAP in labeled with ‘ achieve w.r.t the best baseline. (2)The sequence modeling methods like CSAN achieves good results in MovieLens-1M and Last.fm1week datasets, even better than three meta-learning based methods, MeLU, MetaCS-L and MetaCS-DNN, because this method can model sequential patterns, which is benecial to sequential recommendation tasks. CSAN performs better than LSTM in most cases, due to the ne-grained feature-wise self-attention architecture, which is able to capture the contextual dependencies of sequential behaviors. (3)SML and achieves better results because it adopts dynamic sub-graph sampling and incorporates potential interactions for facilitating generalization, which is designed for fast adaptions on new users without relying on user auxiliary information that suitable for our experimental settings. (4)In the cold-start scenario, metaCSR contiguously performs better than comparison methods, and there are signicant increases compared to the best baseline results. It demonstrates that our proposed metaCSR model can eectively solve the user’s cold-start sequential recommendation problem. WARM-START scenario. The best performance is boldfaced; the highest score in baseline is ∗’; the percentage in parentheses (+/-%) represents the relative improvements that metaCSR MetaCFperform better than other comparison methods in most cases.MetaCF Fig. 4. The next-one recommendation performs of all the methods across the evaluation metrics NDCG@N and Hit@N in WARM-START scenario. 𝑁 ∈ [1, 20]. 4.2.2 RQ2: Can the proposed metaCSR model also perform well in warm-start scenario? In addition to the quantitative evaluation of the eect of recommendation to new users in cold-start scenario, we also want to know whether metaCSR is also conducive to improve the performance of regular users. Table 4 shows the next-one recommendation performance of all the methods across the evaluation metrics AUC and MAP on the three datasets in warm-start scenario, and Fig. 4 shows the performance on NDCG@N and Hit@N with N changes. In the warm-start scenario, metaCSR achieves the best results in MovieLens-1M and Last.fm-1week datasets with high Fig. 5. Sensibility analysis: the trend of AUC and MAP metrics as the amount of training data increases of three datasets. increasing percentage, which demonstrates the eectiveness of metaCSR performing on regular recommendation task. We notice that CSAN gets better results than other methods in Amazon-Video dataset in warmstart scenario. The extreme data sparseness of this dataset makes most models can not well-t for user interests. We assume this is because CSAN is a feature-level attention network that owns the ability of capturing sequential dependencies at a ner-grain level, making up for data sparseness and performing better on regular users. In our sequential representer, we only conduct the element-level self-attention network to encode behavior sequence, which is an eective and ecient method in most cases, and it can also be exibly adjusted under special circumstances. If we face an extremely sparse dataset like Amazon-Video, we can improve performance by using a ne-grained attention network. Overall, the promising results demonstrate the ecacy of our proposed metaCSR in dealing with user-cold start problem, while maintaining competitive performance in both warm-start and cold-start recommendation scenarios. 4.2.3 RQ3: How sensitive is the proposed metaCSR model to the amount of training data? In order to verify the ability of our model to transfer knowledge and adapt quickly to new tasks, and to test the sensitivity of the model to the amount of training data, we conduct a series of experiments. We randomly divide the training data into 10 parts according to the number of users, start training the model from 10% of the data, and conduct a cold start recommendation test on new users. After every 10 % increase in training data, the model is trained to convergence and the test results are recorded. Fig. 5 shows the trend of AUC and MAP metrics as the amount of training data increases of three datasets. Among them, the dotted line represents the results of the eight baseline methods when the amount of training data is 100%. Dierent indicators measure dierent aspects of model performance. Although the values are biased, overall model performance tends to be the same. For all data sets, metaCSR only needs part of the training data, from 25% to 85%, to achieve the best performance of other baseline algorithms. This proves that our proposed model can learn enough knowledge to adapt to new tasks on less training data, so as to obtain better performance in cold-start recommendation tasks. 4.2.4 RQ4: How much does each module in the framework play a role in the user CSR task? We conduct some ablation experiments of removing each module from metaCSR framework separately for user CSR task on MovieLens-1M dataset. The results are shown in Table 5. There are some observations: (1) AUC decreases to varying degrees by removing any one module. (2)The decrease in performance of removing Diusion Representer (Index 1) indicates that user/item representations can be improved to some extent through the information diusion on the graph. (3)In the absence of other additional side information, the sequential dependencies of user behaviors are particularly important, which is helpful for capturing the user’s dynamic interest drifting, so as to achieve better results in the next-one recommendation (Index 2). (4)Meta Learner is the most important module in metaCSR. After removing Meta Learner, the performance dropped sharply (Index 3), indicating that Meta Learner actually learns some common knowledge from prior tasks and has a good generalization in new tasks. More analysis of Meta Learner can be found in RQ5. Fig. 6. AUC and Loss change with the number of iterations increasing. It is validated on MovieLens-1M dataset. 4.2.5 RQ5: Does the proposed metaCSR model learn the common paerns of users’ behaviors from previous tasks? Meta-learning extracts common knowledge from learning dierent tasks and transfers it for unseen tasks. In addition to the quantitative results as detailed described in RQ1, RQ2, RQ3 and RQ4, we also conduct some visualizations and qualitative analysis of the outputs of the model to verify the ecacy of metaCSR. The experiments of this section are validated on MovieLens-1M dataset. Faster convergence rate. Fig. 6 shows the changes of AUC and Loss with the number of iterations during meta-test phase in the user CSR task. Compared with “w/o Meta Learner” method (removing Meta Learner from metaCSR), metaCSR converge rapidly after a few updates and maintain good performance as the number of iteration increased. It demonstrates that metaCSR extract and propagate transferable knowledge of prior users and learn a good initialization for new users. The excellent ecacy of fast adaptation endows metaCSR algorithms the ability to run online. Better sample discrimination. As shown in Fig.7-8, we visualize the new/regular users preference embedding (outputs of the model) and items embedding in dierent situations via t-SNE. Users preference embedding and items embedding in metaCSR (Fig. 7-8 (a) compared to Fig. 7-8 (b)), especially on new users (Fig. 7 (a)), the embeddings have produced a degree of spatial convergence. As model training tends to pull the space of users preference embedding and items embedding closer, so as to achieve better performance when reaching the user-item match during the recommendation process. To further intuitively demonstrate the ability of our method to distinguish samples, we visualize the distribution of inner-product similarity of positive pairs [user-preference-embedding, positiveitem embedding] and negative pairs [user-preference-embedding, negative-item embedding] on MovieLens-1M dataset, as shown in Fig. 9. It is a commonly used and eective way to observe sample discrimination. If the overlap region (histogram intersection) between the two distributions is smaller, the dierence between positive and negative samples is greater, and the model can better distinguish between positive and negative samples. There are some observations: (1) Fig. 7. Visualization of NEW users’ preference embedding and items embedding. Fig. 8. Visualization of REGULAR users’ preference embedding and items embedding. The similarity of positive and negative sample pairs corresponding to new users is lower than that of regular users. For example, the similarity of positive pairs of new users under Fig. 9. Distributions of inner product similarity of positive pairs and negative pairs in dierent scenarios. It is validated on MovieLens-1M dataset. More consistent embedding space. Furthermore, as shown in Fig.10-11, we visualize the new/regular users preference embedding (outputs of the model) in dierent situations via t-SNE, we have the following main ndings: (1) (2) metaCSR model training (Fig. 9(b)) is distributed in the [-2,3] interval, mainly concentrated in the [0-1] interval. The positive sample pair similarity of regular users under the metaCSR model training (Fig. 9(d)) is distributed in the [0,3] interval, mainly concentrated in the [2-3] interval. This is because the model is trained on regular user data, thus the spatial consistency of which is higher. Compared with Fig. 9(a), the overlap region of the positive and negative sample pairs of new users in metaCSR (Fig. 9(b)) is smaller, which means that the dierence between the positive and negative samples is greater. Typically, a smaller overlap region indicates better matching performance, since it means more discriminating embeddings are learned. Meanwhile, the overall similarity of the samples in Fig. 9(b) is higher than Fig. 9(a). The above observations show that our method increases the dierence between positive and negative sample pairs, and can improve the retrieval accuracy in that sample space. Overall, the users preference embedding in metaCSR modal are both more evenly and regularly distributed than “w/o Meta Learner". Fine-tune is to further train on a pre-trained model to improve the generalization and performance of the model on the specic new task. We visualize the user preference embedding of regular users and new users without ne-tuning (Fig. 10) and with ne-tuning (Fig. 11). “without ne-tuning” case in metaCSR, the outstanding performance fully illustrates the high Fig. 10. Visualization of new/regular users’ preference embedding WITHOUT fine-tuning. Fig. 11. Visualization of new/regular users’ preference embedding WITH fine-tuning. The red “x” mark represents new users. We can observe that the distribution of new users’ preference of metaCSR changes very little from “without ne-tuning” to “with ne-tuning”, while the distribution through “w/o Meta Learner” model changes a lot. This observation shows that the metsCSR algorithm does not need to ne-tune to achieve good performance, that is to say, the pre-trained model has good generalization. Distribution of new users’ and regular users’ preference embedding have a high degree of mergence in metaCSR (Fig. 10-11 (a)). It demonstrates that the prior knowledge the model learned from regular users is eectively migrated to the new users. Especially in the generalization of the model. The analysis above proves that our metaCSR model has better generalization ability. 4.3 Parameter Analysis In terms of common sense, the next item that the user will interact with is aected more by the recent behaviors than by the older behaviors. Many behaviors with long intervals may introduce noise information into the system. Thus, our hypothesis is that the appropriate length of the user’s historical records as the input of the sequential recommender is helpful to accurately predict the user’s dynamic preference. To empirically investigate the impact of the length sequences on recommendation performance, we conducted the parameter analysis with 15, 20, 25, the results are shown in Fig. 12. It demonstrates the rationality of our hypothesis, and the appropriate input sequence length is 10. Also, since we regard the cold-start recommendation task as the few-shot learning task, choosing a shorter sequence length is more in line with the experimental setting of few-shot learning. Fig. 12. The performances of dierent input sequence length is validated on MovieLens-1M dataset in cold-start and warm-start scenarios. The best performance happened in T=10. 5 CONCLUSION AND DISCUSSION In this paper, we propose a meta-learning based cold-start sequential recommendation framework called metaCSR, which can learn the common patterns from regular users’ behaviors and optimize the initialization so that the model can quickly adapt to new users after one or a few gradient updates to achieve optimal performance. The extensive experiments on three widely used datasets show the remarkable performance of metaCSR in dealing with user-cold start problem. Meanwhile, in addition to quantitative analysis, we also qualitatively analyze the model ecacy. The analysis results show that our model has good generalization. Our proposed metaCSR is a general framework for cold-start sequential recommendations. It does not require any additional side information other than user ID, item ID, and user-item interaction matrix. Certainly, additional auxiliary information, such as item attributes, user proles, and even context information, multi-modal features, can be seamlessly integrated into our framework through entity representation or sequence modeling to further improve model performance. For eciency reasons, we use the self-attention-based model as the sequential recommender in this paper, but metaCSR has high applicability since our diusion representer and meta learner are universal modules, which can be adapted into any sequential recommendation model, even in traditional recommendation methods. The meta-learning way is naturally suitable for cold-start recommendation issues. In the future, we will focus more on the optimization of meta-learner to further improve optimization eciency and model generalization. ACKNOWLEDGMENTS This work is supported by the National Key R&D Program of China (2018AAA0100604), the Fundamental Research Funds for the Central Universities (2021RC217), the Beijing Natural Science Foundation (JQ20023), the National Natural Science Foundation of China (61632002, 61832004, 62036012, 61720106006).