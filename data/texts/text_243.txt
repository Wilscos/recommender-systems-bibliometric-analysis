<title>Achieving Counterfactual Fairness for Causal Bandit</title> <title>Abstract</title> In online recommendation, customers arrive in a sequential and stochastic manner from an underlying distribution and the online decision model recommends a chosen item for each arriving individual based on some strategy. We study how to recommend an item at each step to maximize the expected reward while achieving user-side fairness for customers, i.e., customers who share similar proﬁles will receive a similar reward regardless of their sensitive attributes and items being recommended. By incorporating causal inference into bandits and adopting soft intervention to model the arm selection strategy, we ﬁrst propose the d-separation based UCB algorithm (D-UCB) to explore the utilization of the d-separation set in reducing the amount of exploration needed to achieve low cumulative regret. Based on that, we then propose the fair causal bandit (F-UCB) for achieving the counterfactual individual fairness. Both theoretical analysis and empirical evaluation demonstrate effectiveness of our algorithms. <title>1 Introduction</title> Fairness in machine learning has been a research subject with rapid growth recently. Many different deﬁnitions of fairness have been designed to ﬁt different settings, e.g., equality of opportunity and equalized odds [ 14 ], direct and indirect discrimination [ 40 39 ], counterfactual fairness 20 31 36 ], and path-speciﬁc counterfactual fairness [ 37 ]. Although there are many works focusing on fairness in personalized recommendation [ 25 41 ], how to achieve individual fairness in bandit recommendation still remains a challenging task. <title>arXiv:2109.10458v1  [cs.LG]  21 Sep 2021</title> reward while achieving user-side fairness for customers, i.e., customers who share similar proﬁles will receive similar rewards regardless of their sensitive attributes and items being recommended. Recently researchers have started taking fairness and discrimination into consideration in the design of personalized recommendation algorithms [ 25 41 19 18 17 11 ]. Among them, [ 19 was the ﬁrst paper of studying fairness in classic and contextual bandits. It deﬁned fairness with respect to one-step rewards and introduced a notion of meritocratic fairness, i.e., the algorithm should never place higher selection probability on a less qualiﬁed arm (e.g., job applicant) than on a more qualiﬁed arm. The following works along this direction include [ 18 ] for inﬁnite and contextual bandits, [ 17 ] for reinforcement learning, [ 25 ] for the simple stochastic bandit setting with calibration based fairness. However, all existing works require some fairness constraint on arms at every round of the learning process, which is different from our user-side fairness setting. One recent work [ 16 ] focused on achieving user-side fairness in bandit setting, but it only purposed a heuristic way to achieve correlation based group level fairness and didn’t incorporate causal inference and counterfactual fairness into bandits. By incorporating causal inference into bandits, we ﬁrst propose the d-separation based upper conﬁdence bound bandit algorithm (D-UCB), based on which we then propose the fair causal bandit (F-UCB) for achieving the counterfactual individual fairness. Our work is inspired by recent research on causal bandits [ 21 32 22 23 27 ], which studied how to learn optimal interventions sequentially by representing the relationship between interventions and outcomes as a causal graph along with associated conditional distributions. For example, [ 27 ] developed the causal UCB (C-UCB) that exploits the causal relationships between the reward and its direct parents. However, different from previous works, our algorithms adopt soft intervention [ ] to model the arm selection strategy and leverage the d-separation set identiﬁed from the underlying causal graph, thus greatly reducing the amount of ex- and reward in the causal graph. As a comparison, the C-UCB achieves O( |P a(R)|· T ) where P a(R) is the parental variables of that is a trivial solution of the d-separation set. In our F-UCB, we further achieve counterfactual fairness in each round of exploration. Counterfactual fairness requires the expected reward an individual would receive keeps the same if the individual’s sensitive attribute were changed to its counterpart. The introduced counterfactual reward combines two interventions, a soft intervention on the arm selection and a hard intervention on the sensitive attribute. The F-UCB achieves counterfactual fairness in online recommendation by picking arms from a subset of arms at each round in which all the arms satisfy counterfactual fairness constraint. Our theoretical analysis shows F-UCB achieves O( cumulative regret bound where is the fairness threshold and denotes the maximum fairness discrepancy of a safe policy , i.e., a policy that is fair across all rounds. We conduct experiments on the Email Campaign data [ 27 ] whose results show the beneﬁt of using the d-separation set from the causal graph. Our D-UCB incurs less regrets than two baselines, the classic UCB which does not leverage any causal information as well as the C-UCB. In addition, we validate numerically that our F-UCB maintains good performance while satisfying counterfactual individual fairness in each round. On the contrary, the baselines fail to achieve fairness with signiﬁcant percentages of recommendations violating fairness constraint. We further conduct experiments on the Adult-Video dataset and compare our F-UCB with another user-side fair bandit algorithm FairLinUCB [ 16 ]. The results demonstrate the advantage of our causal based fair bandit algorithm on achieving individual level fairness in online recommendation. <title>2 Background</title> Our work is based on Pearl’s structural causal models [ 30 ] which describes the causal mechanisms of a system as a set of structural equations. Deﬁnition 1 (Structural Causal Model (SCM) [ 30 ]) A causal model is a triple M = hU, V, Fi where 1) is a set of hidden contextual variables that are determined by factors outside the model; 2) is a set of observed variables that are determined by variables in U ∪ V ; 3) is a set of equations mapping from U × V to . Speciﬁcally, for each V ∈ V , there is an equation ∈ F mapping from U × (V\V ) to , i.e., v = f (P a(V ), u , where P a(V ) is a realization of a set of observed variables called the parents of V , and u is a realization of a set of hidden variables. If all hidden variables in are assumed to be mutually independent, then the causal model is called a Markovian model; otherwise it is called a semi-Markovian model. In this paper, we assume the Markovian model when conducting causal inference. Quantitatively measuring causal effects is facilitated with the do -operator [ 30 ], which simulates the physical interventions that force some variable to take certain values. Formally, the intervention that sets the value of to is denoted by do(x) . In a SCM, intervention do(x) is deﬁned as the substitution of equation x = f (P a(X), u with constant X = x . For an observed variable other than , its variant under intervention do(x) is denoted by Y (x) . The distribution of Y (x) , also referred to as the post-intervention distribution of , is denoted by P (Y (x)) . The soft intervention (also known as the conditional action, policy intervention) extends the hard intervention such that it forces variable to take a new functional relationship in responding to some other variables [ ]. Denoting the soft intervention by , the post-interventional distribution of given its parents is denoted by (X|P a(X)) . More generally, the new function could receive as inputs the variables other than the original parents P a(X) , as long as they are not the descendants of . The distribution of Y after performing the soft intervention is denoted by P (Y (π)). With intervention, the counterfactual effect measures the causal effect while the intervention is performed conditioning on only certain individuals or groups speciﬁed by a subset of observed variables O = o . Given a context O =o , the counterfactual effect of the value change of from to x on Y is given by E[Y (x )|o] − E[Y (x )|o]. Each causal model is associated with a causal graph G = hV, Ei , where is a set of nodes and is a set of directed edges. Each node in corresponds to a variable in . Each edge, denoted by an arrow , points from each member of P a(V ) toward to represent the direct causal relationship speciﬁed by equation (·) . The well-known d-separation criterion [ 34 ] connects the causal graph with conditional independence. Deﬁnition 2 (d-Separation [ 34 ]) Consider a causal graph and are disjoint sets of attributes. and are d-separated by in , if and only if blocks all paths from every node in to every node in . A path is said to be blocked by if and only if: 1) contains a chain i → m → j or a fork i ← m → j such that the middle node is in , or 2) contains an collider i → m ← j such that the middle node m is not in W and no descendant of m is in W. <title>3 Achieving Counterfactual Fairness in Bandit</title> In this section, we present our D-UCB and F-UCB bandit algorithms. The online recommendation is commonly modeled as a contextual multi-armed bandit problem, where each customer is a “bandit player”, each potential item has a feature vector a ∈ A and there are a total number of items For each customer arrived at time t ∈ [T ] with feature vector ∈ X , the algorithm recommends an item with features based on vector which represents the concatenation of the user and the item feature vectors ( ), observes the reward (e.g., purchase), and then updates its recommendation strategy with the new observation. There may also exist some intermediate features (denoted by that are affected by the recommended item and inﬂuence the reward, such as the user feedback about relevance and quality. In bandit algorithms, we often choose an arm that maximizes the expectation of the conditional reward, = arg max E[R|x . The arm selection strategy could be implemented by a functional mapping from to , and after each round the parameters in the function get updated with the newest observation tuple. We advocate the use of the causal graph and soft interventions as a general representation of any bandit algorithm. We consider the causal graph , e.g., as shown in Figure 1, where represents the arm features, represents the user features, represents the reward, and represents some intermediate features between and . Since the arm selection process could be regarded as the structural equation of on we treat as ’s parents. Then, the reward is inﬂuenced by the arm selection, the contextual user features, as well as some intermediate features, so all the three factors are parents of . In this setting, it is natural to treat the update of the arm selection policy as a soft intervention performed on the arm features Figure 1: Graph structure for con. Each time when an arm selection strategy is learned, the textual bandit recommendation. corresponding soft intervention is considered to be conducted denotes the soft intervention conon while user features and all other relationships in the ducted on arm selection. causal graph are unchanged. There are several advantages of modeling arm selection learning using the soft intervention. First, it can capture the complex causal relationships between context and reward without introducing strong assumptions, e.g., linear reward function, or Gaussian/Bernoulli prior distribution, which are often not held in practice. Second, it is ﬂexible in terms of the functional form. For example, it can be of any function type, and it can be independent or dependent upon the target variable’s existing parents and can also include new variables that are not the target variable’s parents. Third, the soft intervention can be either deterministic, i.e., ﬁxing the target variable to a particular constant, or stochastic, i.e., assigns to the target variable a distribution with probabilities over multiple states. As a result, most existing and predominant bandit algorithms could be described using this framework. Moreover, based on this framework we could propose new bandit algorithms by adopting different soft interventions. Formally, let be the arm selection policy space at time t ∈ [T] , and π ∈ Π be a speciﬁc policy. The implementation of policy is modeled by a soft intervention. Denoting by R(π) the post-interventional value of the reward after performing the intervention, the expected reward under policy , denoted by , is given by E[R(π)|x . According to the -calculus [ ], it can be further decomposed as follows: where (a|x is a distribution deﬁned by policy . As can be seen, once a policy is given, the estimation of depends on the estimation of E[R(a)|x (denoted by ). Note that represents the expected reward when selecting an arm , which is still a post-intervention quantity and needs to be expressed using observational distributions in order to be computable. In the following, we propose a d-separation based estimation method and based on which we develop our D-UCB algorithm. For the ease of representation, our discussions in Sections 3.2, 3.3 and 3.4 assume deterministic policies but in principle the above framework could be applied to stochastic policies as well. Let W ⊆ A ∪ X ∪ I be a subset of nodes that d-separates reward from features (A ∪ X)\W in the causal graph. Such set always exists since A ∪ X and P a(R) are trivial solutions. Let Z = W\(A ∪ X). Using the do-calculus [30], we can decompose µ as follows. After taking the policy, we will have new observations on and . The sample mean estimator is then updated accordingly: We hypothesize that the choice of d-separation set would signiﬁcantly affect the regret of the D-UCB. To this end, we analyze the upper bound of the cumulative regret . The following theorem shows that, the regret upper bound depends on the domain size of d-separation set W. where |W| is the domain space of set W. δ = 1/T , it is easy to show that D-UCB algorithm achieves O( |W| · T ) regret bound. Please refer to Appendix B in the supplementary ﬁle for proof details. Remark. Determining the minimum d-separation set has been well studied in causal inference [ 13 ]. We leverage the algorithm of ﬁnding a minimum cost separator [ 35 ] to identify . The discovery procedure usually requires the complete knowledge of the causal graph. However, in the situation where the d-separation set to be used as well as the associated conditional distributions P (z|x are given, the remaining part of the algorithm will work just ﬁne without the causal graph information. Moreover, the assumption of knowing P (z|x follows recent research works on causal bandit. Generalizing the causal bandit framework to partially/completely unknown causal graph setting is a much more challenging but important task. A recent work [ 26 ] tries to generalize causal bandit algorithm based on causal trees/forests structure. causal algorithms exploit the knowledge of the d-separation set and achieves O( T ) regret, which implies a signiﬁcant reduction regarding to the regret bound if n << N . If the number of arm candidates is much smaller than the domain space of , our bound analysis could be easily adjusted to this case using a subspace of W that corresponds to the arm candidates. Now, we are ready to present our fair UCB algorithm. Rather than focusing on the fairness of the item being recommended (e.g., items produced by small companies have similar chances of being recommended as those from big companies), we focus on the user-side fairness in terms of reward, i.e., individual users who share similar proﬁles will receive similar rewards regardless of their sensitive attributes and items being recommended such that they both beneﬁt from the recommendations equally. To this end, we adopt counterfactual fairness as our fairness notion. Consider a sensitive attribute S ∈ X in the user’s proﬁle. Counterfactual fairness concerns the expected reward an individual would receive assuming that this individual were in different sensitive groups. In our context, this can be formulated as the counterfactual reward E[R(π, s )|x where two interventions are performed simultaneously: soft intervention on the arm selection and hard intervention do(s on the sensitive attribute , while conditioning on individual features Denoting by = E[R(π, s )|x ] − E[R(π, s )|x the counterfactual effect of on the reward, a policy that is counterfactually fair is deﬁned as follows. Deﬁnition 3. A policy is counterfactually fair for an individual arrived if = 0 . The policy is counterfactually fair if |∆ | ≤ τ where τ is the predeﬁned fairness threshold. To achieve counterfactual fairness in online recommendation, at round , we can only pick arms from a subset of arms for the customer (with feature ), in which all the arms satisfy counterfactual fairness constraint. The fair policy subspace Φ ⊆ Π is thus given by Φ = {π : ∆ ≤ τ}. Please refer to Appendix C of the supplementary ﬁle for the proof. which is derived based on the fact that the sum of two independent sub-Gaussian random variables is still sub-Gaussian distributed. Thus, the learning problem can be formulated as the following constrained optimization problem: where is deﬁned as the optimal policy in the policy space at each round, which is the same in D-UCB setting. The Assumption 3 in Appendix A gives the deﬁnition of a safe policy , which refers to a feasible solution under the fair policy subspace at each round, i.e., ∈ Π such that ≤ τ for each t ∈ [T ]. This optimization can be solved similarly by following the rule of OFU. Algorithm 2 depicts our fair bandit algorithm called the F-UCB. Different from the D-UCB algorithm, F-UCB only picks arm from at each time . In Line 5, we compute the estimated reward mean and the estimated fairness discrepancy. In Line 6, we determine the fair policy subspace , and in Line 7, we ﬁnd the optimal policy π = arg max [UCB (t)]. The following regret analysis shows that, the regret bound of F-UCB is larger than that of D-UCB as expected, and it is still inﬂuenced by the domain size of set W. Theorem 3 (Regret bound of fair causal bandit) Given a causal graph , let = 4|W|T δ and denote the maximum fairness discrepancy of a safe policy across all rounds. Setting = 1 and α , with probability at least 1 − δ , the cumulative regret of F-UCB is bounded by: ≤ ( + 1) × 2T |W|log(1/δ ) + 4 T log(2/δ ) log(1/δ τ −∆ Proof Sketch. Our derivation of the regret upper bound of F-UCB follows the proof idea of bandits with linear constraints [ 28 ], where we treat counterfactual fairness as a linear constraint. By leveraging the knowledge of a feasible fair policy at each round and properly designing the numerical relation of the scale parameters and , we are able to synchronously bound the cumulative regret of reward and fairness discrepancy term. Merging these two parts of regret analysis together leads to a uniﬁed bound of the F-UCB algorithm. By setting to 1/T we can show F-UCB achieves O( long-term regret. The detailed proof is reported in Appendix D of the supplementary ﬁle. Remark. In Theorem 3, and refer to the scale parameters that control the magnitude of the conﬁdence interval for sample mean estimators related to reward and fairness term respectively. Appendix D shows the numerical relation and should satisfy in order to synchronously bound the uncertainty caused by the error terms. The values taken in Theorem D is one feasible solution with α taking the minimum value under the constraint domain space. The general framework we proposed (Eq. (1) ) can be applied to any policy/function class. However, the D-UCB and F-UCB algorithms we proposed still adopt the deterministic policy following the classic UCB algorithm. Thus, the construction of = {π : UCB (t) ≤ τ} can be easily achieved as the total number of policies are ﬁnite. In this paper we also assume discrete variables, but in principle the proposed algorithms can also be extended to continuous variables by employing certain approximation approaches, e.g., neural networks for estimating probabilities and sampling approaches for estimating integrals. However, the regret bound analysis may not apply as |W| will become inﬁnite in the continuous space. <title>4 Experiment</title> In this section, we conduct experiments on two datasets and compare the performance of D-UCB and F-UCB with UCB, C-UCB and Fair-LinUCB in terms of the cumulative regret. We also demonstrate the fairness conformance of F-UCB and the violations of other algorithms. We adopt the Email Campaign data as used in previous works [ 27 ]. The dataset is constructed based on the online advertising process. Its goal is to determine the best advertisement recommendation strategy for diverse user groups to improve their click through ratio (CTR), thus optimize the revenue generated through advertisements. Figure 2a shows the topology of the causal graph. We use to denote three user proﬁle attributes, gender, age and occupation; to denote three arm features, product, purpose, send-time that could be intervened; to denote Email body template, ﬁtness, subject length, and user query; and to denote the reward that indicates whether users click the advertisement. The reward function is R = 1/12(I + I + I + A ) + N(0, σ where σ = 0.1 . In our experiment, we set δ = 1/t for each t ∈ [T ] . In Appendix E.1, we show the domain values of all 11 attributes and their conditional probability tables. Figure 2b plots the cumulative regrets of different bandit algorithms along . For each bandit algorithm, the online learning process starts from initialization with no previous observation. Figure 2b shows clearly all three causal bandit algorithms perform better than UCB. This demonstrates the advantage of applying causal inference in bandits. Moreover, our D-UCB and F-UCB outperform CUCB, showing the advantage of using d-separation set in our algorithms. The identiﬁed d-separation set (send time, ﬁtness, and template) and the domain space of (ﬁtness and template) signiﬁcantly reduce the exploration cost in D-UCB and F-UCB. Remark. Note that in Figure 2b, for the ﬁrst 2000 rounds, F-UCB has lower cumulative regret than D-UCB. A possible explanation is that fair constraint may lead to a policy subspace that contains many policies with high reward. As the number of explorations increase, D-UCB gains more accurate reward estimations for each policy in the whole policy space and eventually outperforms F-UCB. Table shows how the cumulative regret of F-UCB ( T = 5000 rounds) varies with the fairness threshold . The values in Table 1 (and Table 2) are obtained by averaging the results over 5 trials. The larger the , the smaller the cumulative regret. In the right block of Table , we further report the number of fairness violations of the other three algorithms during the exploration of T = 5000 rounds, which demonstrates the need of fairness aware bandits. In comparison, our F-UCB achieves strict counterfactual fairness in every round. We further compare the performance of F-UCB algorithm with Fair-LinUCB [ 16 ] on Adult-Video dataset. We follow the settings of [ 16 ] by combining two publicly available datasets: Adult dataset and Youtube video dataset. We include in Appendix E.2 detailed information about datasets and experiment. We select 10,000 instances and use half of the data as the ofﬂine dataset to construct causal graph and adopt the other half to be user sequence and arm candidates for online recommendation. The causal graph constructed from the training data is shown in Figure 3, where X = {age, sex, race, income} denote user features, A = {length, ratings, views, comments} denote video features. Bold nodes denote direct parents of the reward and red nodes denote the sensitive attribute. The minimum d-separation set for this graph topology is W = {age, income, ratings, views} The reward function is set as R = 1/5(age + income + ratings + views) + N(0, σ , where σ = 0.1 . Following previous section we set δ = 1/t for each t ∈ [T ] . The cumulative regret is added up through 5000 rounds. We observe from Table 2 a high volume of unfair decisions made by Fair-LinUCB under strict fairness threshold (nearly forty percent of the users are unfairly treated when τ = 0.1 ). This implies Fair-LinUCB algorithm can not achieve individual level fairness when conducting online recommendation compared to F-UCB. On the other hand, the cumulative regret for Fair-LinUCB is around 250 over 5000 rounds, which is slightly better than F-UCB. This is because we use the same linear reward setting as [ 16 ] in our experiment and Lin-UCB based algorithm will better catch the reward distribution under this setting. <title>5 Related Work</title> Causal Bandits. There have been a few research works of studying how to learn optimal interventions sequentially by representing the relationship between interventions and outcomes as a causal graph along with associated conditional distributions. [ 21 ] introduced the causal bandit problems in which interventions are treated as arms in a bandit problem but their inﬂuence on the reward, along with any other observations, is assumed to conform to a known causal graph. Speciﬁcally they focus on the setting that observations are only revealed after selecting an intervention (and hence the observed features cannot be used as context) and the distribution of the parents of the reward is known under those interventions. [ 22 ] developed a way to choose an intervention subset based on the causal graph structure as a brute-force way to apply standard bandit algorithms on all interventions can suffer huge regret. [ 23 ] studied a relaxed version of the structural causal bandit problem when not all variables are manipulable. [ 32 ] considered best intervention identiﬁcation via importance sampling. Instead of forcing a node to take a speciﬁc value, they adopted soft intervention that changes the conditional distribution of a node given its parent nodes. [ 27 ] proposed two algorithms, causal upper conﬁdence bound (C-UCB) and causal Thompson Sampling (C-TS), and showed that they have improved cumulative regret bounds compared with algorithms that do not use causal information. They focus on causal relations among interventions and use causal graphs to capture the dependence among reward distribution of these interventions. Fair Machine Learning. Fairness in machine learning has been a research subject with rapid growth and attention recently. Many different deﬁnitions of fairness have been designed to ﬁt different settings, e.g., equality of opportunity and equalized odds [ 14 38 ], direct and indirect discrimination 40 39 ], counterfactual fairness [ 20 31 36 ], and path-speciﬁc counterfactual fairness [ 37 ]. Related but different from our work include long term fairness (e.g., [ 24 ]), which concerns for how decisions affect the long-term well-being of disadvantaged groups measured in terms of a temporal variable of interest, fair pipeline or multi-stage learning (e.g., [ 12 10 ]), which primarily consider the combination of multiple non-adaptive sequential decisions and evaluate fairness at the end of the pipeline, and fair sequential learning (e.g., [ 19 ]), which sequentially considers each individual and makes decision for them. In [ 24 ], the authors proposed the study of delayed impact of fair machine learning and introduced a one-step feedback model of decision-making to quantify the long-term impact of classiﬁcation on different groups in the population. [ 15 ] developed a metric-free individual fairness and a cooperative contextual bandits (CCB) algorithm. The CCB algorithm utilizes fairness as a reward and attempts to maximize it. It tries to achieve individual fairness unlimited to problem-speciﬁc similarity metrics using multiple gradient contextual bandits. <title>6 Conclusions</title> In our paper, we studied how to learn optimal interventions sequentially by incorporating causal inference in bandits. We developed D-UCB and F-UCB algorithms which leverage the d-separation set identiﬁed from the underlying causal graph and adopt soft intervention to model the arm selection strategy. Our F-UCB further achieves counterfactual individual fairness in each round of exploration by choosing arms from a subset of arms satisfying counterfactual fairness constraint. Our theoretical analysis and empirical evaluation show the effectiveness of our algorithms against baselines. <title>Acknowledgments and Disclosure of Funding</title> This work was supported in part by NSF 1910284, 1920920, 1940093, and 1946391. <title>A Nomenclature and Assumptions</title> In our regret bound analysis of D-UCB and F-UCB algorithms, we follow several standard assumptions [29] to guarantee the correctness and the simplicity of the proofs. Assumption 1. For all t ∈ [T ] , both the error term of reward and the error term of counterfactual fairness discrepancy follow 1-sub-Gaussian distribution. Assumption 2. For all t ∈ [T] , both the mean of reward and the mean of counterfactual fairness discrepancy are within [0, 1]. Assumption 3. There exists a safe policy π , i.e., π ∈ Π such that ∆ ≤ τ for each t ∈ [T ]. The last assumption introduces the existence of a safe policy at each round, which plays an important role in the regret bound analysis of F-UCB. The nomenclature used for the proof part is shown in Table 3. <title>B Regret Bound of D-UCB</title> where |Z| denotes the domain size of subset , i.e., the difference between the d-separation set and A ∪ X in G. and the policy applied at each time t as π = arg max [µ ]. Let (t) = denote the count for a certain domain value of up to time . Further we deﬁne the mean of the reward related to a d-separation set domain value as = E[R|W = w] and its estimated value as ˆµ (t) = We also deﬁne the upper conﬁdence bound of the reward for each arm and the upper conﬁdence bound for each policy: As UCB (t) ≤ UCB (t) always holds due to OFU arm picking strategy, we have UCB (t) ≤ 0. We will use the following proposition called Azuma’s inequality to derive the bound of the ﬁrst term of Equation 14. Proposition 1. Suppose {M : k = 0, 1, 2...} is a martingale and |M −M | < c almost surely, then for all t ∈ [T ] and positive value  we have: The formula above gives a high probability bound of the ﬁrst part. Now we can combine the bounds of two parts in Equation 14 to derive the high probability bound of . Since P (E ) ≤ 2δT |W| applying union bound rule, with probability at least 1 − 2δT |W| − exp(− , the regret is bounded by: The above formula thus leads to O( |W| · T ) long-term expected regret. <title>C Upper Bound of Unidentiﬁable Counterfactual Fairness</title> E[R(a, s )|x ] ≤ P (i|x ) max {E[R|s , x \s , i]} P (z|x )P (i\z|, z, x ) max {E[R|s , w\s ]}, <title>D Regret Bound of F-UCB</title> Theorem 3 (Regret bound of fair causal bandit) Given a causal graph , let = 4|W|T δ and denote the fairness discrepancy of the safe policy . Setting = 1 and , with probability at least 1 − δ , the cumulative regret of the F-UCB algorithm is bounded by: ≤ ( + 1) × 2T |W|log(1/δ ) + 4 T log(2/δ ) log(1/δ τ −∆ Proof. Similar to the regret analysis of causal bandit, we decompose the cumulative regret into two parts. where (t) and (t) are the error term of the reward and counterfactual discrepancy. If the optimal policy belongs to the fair policy subspace, which means π , we can easily get: [µ ] ≤ E [UCB (t)] ≤ E [UCB (t)] [UCB (t)] − ρ τ −∆ [ρ (¯c +  (t))] − ρ τ −∆ τ −∆ + ρ (1 + α )E [β (t)] Denote as . From the design of the integrated policy ˜π we further have: > E [µ ] is satisﬁed if and only if: Next we will derive the bound of the second term in Equation 17. The result is given by the following proposition. [UCB (t)] − E [µ ] ≤ (α + 1)E [β (t)] Setting = E [β (t)|F ] − β (t) is thus a martingale sequence with |M − M | = |A | ≤ 2 2 log(1/δ). Thus applying Azuma-Hoeffding inequality implies: We denote the event that describes the results of the above inequality as . In the equation above, the sum of the adaptive scaling parameter could be decomposed as follows: Under event E, for each domain value of the d-separation set W we have: Since (T ) = T , using the fact that arithmetic mean is less than quadratic mean we have: which is exactly the result of Proposition 2. Finally, combining the theoretical derivation of the two parts above leads to the cumulative regret bound shown in Theorem 3. <title>E Experimental Settings</title> Our experiments were carried out on a Windows 10 Enterprise workstation with a 3.2 GHz Intel Core i7-8700 CPU and 64GB RAM. Table 4 shows attributes of Email Campaign data and their domain values. Table 5 shows the conditional probabilities of P (I = i|X , X , X . The following equations are the conditional distributions for the remaining variables. Following the setting of [ 16 ], we generate one simulated dataset for our experiments by combining the following two publicly available datasets. • Adult dataset : The Adult dataset [ ] is used to represent the students (or bandit players). It is composed of 31,561 instances: 21,790 males and 10,771 females, each having 8 categorical variables (work class, education, marital status, occupation, relationship, race, sex, native-country) and 3 continuous variables (age, education number, hours per week). We select 4 variables, age, sex, race, income, as user features in our experiments and binarize their domain values due to data sparsity issue. • YouTube dataset : The Statistics and Social Network of YouTube Videos dataset is used to represent the items to be recommended (or arms). It is composed of 1,580 instances each having 6 categorical features (age of video, length of video, number of views, rate, ratings, number of comments). We select four of those variables (age, length, ratings, comments) and binarize them for a suitable size of the arm pool. For our experiments, we use a subset of 10,000 random instances from the Adult dataset, which is then split into two subsets: one for graph construction and the other for online recommendation. Similarly, a subset of YouTube dataset is used as our pool of videos to recommend. The subset contains 16 video types (arms) representing different domain values of the 4 binarized arm features. The feature contexts used throughout the experiment is the concatenation of both the student feature vector and the video feature vector. Four elements in are selected according to domain knowledge as the variables that will determine the value of the reward. A linear reward function is then applied to build this mapping relation from those selected variables to the reward variable. In our experiments we choose the sensitive attribute to be the gender of adults , and focus on the individual level fairness discrepancy regarding to both male and female individuals. For the email campaign experiment setting, we construct the causal graph following the domain knowledge and one of the recent research works on causal bandit [ 27 ]. For the AdultVideo experiment setting, we construct the causal graph using a causal discovery software Tetrad (https://www.ccd.pitt.edu/tools/). <title>References</title> [1] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235–256, 2002. [2] Amanda Bower, Sarah N. Kitchen, Laura Niss, Martin J. Strauss, Alexander Vargas, and Suresh Venkatasubramanian. Fair pipelines. CoRR, abs/1707.00391, 2017. [3] Robin Burke. Multisided fairness for recommendation. arXiv preprint arXiv:1707.00093, 2017. [4] Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger. Balanced neighborhoods for multisided fairness in recommendation. In FaccT’18, 2018. [5] L Elisa Celis, Sayash Kapoor, Farnood Salehi, and Nisheeth K Vishnoi. An algorithmic framework to control bias in bandit-based personalization. arXiv preprint arXiv:1802.08674, 2018. [6] Silvia Chiappa and Thomas P. S. Gillam. Path-Speciﬁc Counterfactual Fairness. 2018. [7] Juan D. Correa and Elias Bareinboim. A calculus for stochastic interventions: Causal effect identiﬁcation and surrogate experiments. In AAAI’20, pages 10093–10100, 2020. [8] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. [9] Cynthia Dwork and Christina Ilvento. Fairness under composition. In ITCS’19, volume 124 of LIPIcs, pages 33:1–33:20, 2019. [10] Cynthia Dwork, Christina Ilvento, and Meena Jagadeesan. Individual fairness in pipelines. In FORC’20, pages 7:1–7:22, 2020. [11] Michael D Ekstrand, Mucun Tian, Mohammed R Imran Kazi, Hoda Mehrpouyan, and Daniel Kluver. Exploring author gender in book rating and recommendation. In RecSys’18, pages 242–250, 2018. [12] Vitalii Emelianov, George Arvanitakis, Nicolas Gast, Krishna P. Gummadi, and Patrick Loiseau. The price of local fairness in multistage selection. In IJCAI’19, pages 5836–5842, 2019. [13] Dan Geiger, Thomas Verma, and Judea Pearl. d-separation: From theorems to algorithms. In Machine Intelligence and Pattern Recognition. 1990. [14] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In NeurIPS’16, pages 3315–3323, 2016. [15] Qian Hu and Huzefa Rangwala. Metric-free individual fairness with cooperative contextual bandits. CoRR, abs/2011.06738, 2020. [16] Wen Huang, Kevin Labille, Xintao Wu, Dongwon Lee, and Neil Heffernan. Achieving user-side fairness in contextual bandits. arXiv preprint arXiv:2010.12102, 2020. [17] Shahin Jabbari, Matthew Joseph, Michael J. Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in reinforcement learning. In ICML’17, 2017. [18] Matthew Joseph, Michael J. Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Meritocratic fairness for inﬁnite and contextual bandits. In AIES’18, pages 158–163. ACM, 2018. [19] Matthew Joseph, Michael J. Kearns, Jamie H. Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In NeurIPS, 2016. [20] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In NeurIPS’17, pages 4066–4076, 2017. [21] Finnian Lattimore, Tor Lattimore, and Mark D. Reid. Causal bandits: Learning good interventions via causal inference. In NeurIPS’16, 2016. [22] Sanghack Lee and Elias Bareinboim. Structural causal bandits: Where to intervene? In NeurIPS’18, pages 2573–2583, 2018. [23] Sanghack Lee and Elias Bareinboim. Structural causal bandits with non-manipulable variables. In AAAI’19, 2019. [24] Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In ICML’18, 2018. [25] Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C Parkes. Calibrated fairness in bandits. arXiv preprint arXiv:1707.01875, 2017. [26] Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Causal bandits with unknown graph structure. arXiv preprint arXiv:2106.02988, 2021. [27] Yangyi Lu, Amirhossein Meisami, Ambuj Tewari, and William Yan. Regret analysis of bandit problems with causal background knowledge. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 141–150. PMLR, 2020. [28] Aldo Pacchiano, Mohammad Ghavamzadeh, Peter Bartlett, and Heinrich Jiang. Stochastic bandits with linear constraints. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2827–2835. PMLR, 2021. [29] Aldo Pacchiano, Mohammad Ghavamzadeh, Peter L. Bartlett, and Heinrich Jiang. Stochastic bandits with linear constraints. CoRR, abs/2006.10185, 2020. [30] Judea Pearl. Causality. Cambridge university press, 2009. [31] Chris Russell, Matt J Kusner, Joshua Loftus, and Ricardo Silva. When worlds collide: integrating different counterfactual assumptions in fairness. In NeurIPS’17, pages 6414–6423, 2017. [32] Rajat Sen, Karthikeyan Shanmugam, Alexandros G. Dimakis, and Sanjay Shakkottai. Identifying best interventions through online importance sampling. In ICML’17, pages 3057–3066, 2017. [33] Ilya Shpitser and Judea Pearl. Complete identiﬁcation methods for the causal hierarchy. Journal of Machine Learning Research, 9:1941–1979, 2008. [34] Peter Spirtes, Clark N Glymour, and Richard Scheines. Causation, prediction, and search, volume 81. MIT press, 2000. [35] Jin Tian, Azaria Paz, and Judea Pearl. Finding minimal d-separators. Citeseer, 1998. [36] Yongkai Wu, Lu Zhang, and Xintao Wu. Counterfactual fairness: Unidentiﬁcation, bound and algorithm. In IJCAI’19, pages 1438–1444, 2019. [37] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. PC-Fairness: A Uniﬁed Framework for Measuring Causality-based Fairness. In NeurIPS, 2019. [38] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness constraints: Mechanisms for fair classiﬁcation. In AISTATS, 2017. [39] Junzhe Zhang and Elias Bareinboim. Fairness in decision-making - the causal explanation formula. In AAAI’18, 2018. [40] Lu Zhang, Yongkai Wu, and Xintao Wu. A causal framework for discovering and removing direct and indirect discrimination. In IJCAI’17, 2017. [41] Ziwei Zhu, Xia Hu, and James Caverlee. Fairness-aware tensor-based recommendation. In CIKM’18, pages 1153–1162, 2018.