Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classiﬁcation, has recently found numerous applications in prediction of related searches and product recommendation tasks. The conventional usage of Convolutional Neural Network (CNN) to capture n-grams in text-classiﬁcation relies heavily on uniformity in word-ordering and the presence of long input sequences to convolve over. However, this is missing in short and unstructured text sequences encountered in search and recommendation. In order to tackle this, we propose an orthogonal approach by recasting the convolution operation to capture coupled semantics along the embedding dimensions, and develop a word-order agnostic embedding enhancement module to deal with the lack of structure in such queries. Beneﬁtting from the computational efﬁciency of the convolution operation, Embedding Convolutions, when applied on the enriched word embeddings, result in a light-weight and yet powerful encoder (INCEPTIONXML) that is robust to the inherent lack of structure in short-text extreme classiﬁcation. Towards scaling our model to problems with millions of labels, we also propose INCEPTIONXML+, which addresses the shortcomings of the dynamic hard-negative mining framework in the recently proposed LIGHTXML by improving the alignment between the label-shortlister and extreme classiﬁer. On popular benchmark datasets, we empirically demonstrate that the proposed method outperforms state-of-the-art deep extreme classiﬁers such as ASTEC by an average of 5% and 8% on the P@k and propensity-scored PSP@k metrics respectively. Extreme Multi-label Classiﬁcation (XML) involves classifying instances into a set of most relevant labels from an extremely large (on the order of millions) set of all possible labels (Agrawal et al. 2013; Jain, Prabhu, and Varma 2016; Babbar and Sch¨olkopf 2017). When these instances are short text documents, many successful applications of the XML framework have been found in ranking and recommendation tasks such as prediction of Related Search on search engines (Jain et al. 2019), suggestion of query phrases corresponding to short textual description of products on e-stores (Chang et al. 2020a) and product-to-product recommendation using only product title (Dahiya et al. 2021a,b; Saini et al. 2021; Mittal et al. 2021a; Chang et al. 2021). Technical challenges in XML : The XML task has two primary challenges: (i) extremely large number of possible labels which leads to both memory and computational bottleneck due to large number of parameters in the classiﬁcation layer, and (ii) a large fraction of these being tail labels paired with a handful of positive samples (Jain, Prabhu, and Varma 2016; Babbar and Sch¨olkopf 2017) which makes it challenging for the extreme classiﬁer to learn rich representations for these tail labels. Additional challenges from short-text queries: For extreme classiﬁcation over short textual input sequences consisting of three-to-ﬁve words, the above problems are exacerbated by further statistical challenges as : (i) the shorter context is insufﬁcient to learn meaningful representations for the classiﬁcation task, and (ii) the short-text queries in search and recommendation engines are typically plagued with non-standard phrases without due consideration to the word-order and noisy inputs (Tayal et al. 2020). InceptionXML: In this paper, (i) we develop INCEPagainst the traditional paradigm (Kim 2014; Liu et al. 2017) of convolving over the words dimension in favor of the embedding dimension, (ii) We add an embedding-enhancement module that projects the input to a word-order agnostic representation making our approach more robust to lack of structure in short-text queries. (iii) We also highlight and overcome the shortcomings of the dynamic hard-negative mining framework proposed in LIGHTXML by bring the label-shortlisting task closer to the extreme classiﬁcation task and scale our model to millions of labels. (iv) We further the state-of-the-art on popular benchmarks by an average of 5% and 8% on P@K and propensity-scored PSP@K metrics respectively. Towards balancing model complexity against computational complexity, INCEPTIONXML ﬁnds the sweet-spot between the two extreme ends of modern deep extreme classiﬁcation pipelines: (i) those which employ transformer-based encoders (Jiang et al. 2021; Ye et al. 2020; Chang et al. 2020a) for XML as a down-stream task, and as a result become unscalable to millions of labels, and (ii) those which are built on under-ﬁtting frugal architectures such as ASTEC (Dahiya et al. 2021b), which sacriﬁce accuracy to maintain easy scalability to millions of labels. Figure 1: INCEPTIONXML: Model Architecture. The convolution ﬁlters on the input data span only a subset of adjacent dimensions in the word embeddings while covering all the input tokens (‘who let the dogs out’). The Embedding-enhancement module (‘EM-module’) is shown in detail with its orthogonal self-attention layers followed by a projection layer. The use of extremely large transformer-based architectures like in LIGHTXML leads to much slower training and inference. Moreover, the real-world use cases of extreme classiﬁcation typically require very fast inference times and low memory loads. Under these application constraints, large transformer based models can be a suboptimal choice. We show that replacing the transformer encoder with our lightweight CNN-based encoder, combined with further improvements to dynamic hard-negative mining technique leads to better prediction performance apart from faster training and the ability to scale to millions of labels. This demonstrates that it is an overkill to use transformer based encoders for the problem of short-text extreme classiﬁcation. On the other end, by employing a much richer architecture in comparison to the low-capacity ASTEC model, we show that a single INCEPTIONXML model can even outperform an ensemble of three ASTEC learners on publicly available extreme classiﬁcation datasets by as much as 10% on P@k metrics. Even though signiﬁcant progress has been made in the domain of extreme classiﬁcation over the past years, the focus of a large majority of these works has been mainly on learning only the classiﬁer with ﬁxed features in the form of bag-of-words representation (Agrawal et al. 2013; Prabhu and Varma 2014; Bhatia et al. 2015; Jasinska et al. 2016; Jain, Prabhu, and Varma 2016; Babbar and Sch¨olkopf 2017; Yen et al. 2017; Prabhu et al. 2018a; Wydmuch et al. 2018; Babbar and Sch¨olkopf 2019; Khandagale, Xiao, and Babbar 2020) or with pre-trained features (Jain et al. 2019). With advances in deep learning and its success in learning effective representations for web-scale text and language datasets, the focus has shifted towards developing scalable deep extreme classiﬁers based on convolutions (Liu et al. 2017), attention mechanism (You et al. 2019), or pre-trained transformer models (Chang et al. 2020a; Ye et al. 2020; Jiang et al. 2021; Yu, Zhong, and Dhillon 2020). Short-text extreme classiﬁcation: In tasks where the inputs are short text queries, there has been a slew of works lately on designing deep extreme classiﬁers specialized to solve the problem. These include ASTEC (Dahiya et al. 2021b), DECAF (Mittal et al. 2021a), GALAXC (Saini et al. 2021), ECLARE (Mittal et al. 2021b), and SIAMESEXML (Dahiya et al. 2021a). Based on the availability of label meta-data, these can be divided into two categories: (i) ones which make no assumptions regarding label text, and (ii) others which assume that the labels are endowed with clean label text. While ASTEC and INCEPTIONXML belong to the ﬁrst category, the rest of the above mentioned algorithms identify with the second. Even though the additional label meta-data is useful, it is known for only a small subset of all labels. For instance, on AmazonTitles-3M, label text is available only for 1.3 million labels. On the other hand, the problem setup, which is the focus of this work, makes no assumption about label-text, and hence is a harder, more general and widely applicable problem. Hard negative-mining via label shortlisting: Due to the large output space, it is not possible to compute the loss over the entire label space for each data point (Reddi et al. 2018). While techniques have been studied for efﬁcient hard-negative label mining under ﬁxed representation of data points (Jain et al. 2019; Dahiya et al. 2021b; Yen et al. 2016, 2017), only recent algorithms (Jiang et al. 2021) have come up with dynamic hard negative-mining techniques. To bypass the computational and memory barrier of computing loss in the extreme setting, negative mining in recent XML algorithms works in two stages. The label space is ﬁrst clustered into a set of “meta-labels” or “label-clusters”. A meta-classiﬁer is trained on these meta-labels to limit the set of candidate labels for an input to a computationally tractable set of K shortlisted label-clusters. The second stage, or the “extreme classiﬁer”, uses the corresponding subset of the trained linear classiﬁers for the shortlisted labels to get the ﬁnal predictions for the input. While ASTEC ﬁrst learns a static meta-classiﬁer and then uses its predictions to learn the actual extreme classiﬁer in a separate stage, LIGHTXML uses an end-to-end trainable single-stage approach to learn both the tasks simultaneously. On looking closely at short-text queries, one can infer that the presence of a word in a query is more important than it’s exact position for extreme classiﬁcation tasks. For example, the queries “JBL wireless headphones 2021” and “2021 headphones JBL wireless” (both of which are common in real-world scenarios) should result in the same product search results on an e-commerce website (Tayal et al. 2020). In the conventional usage of CNNs over words in text classiﬁcation (Kim 2014; Liu et al. 2017), the intent is to capture the occurrences of n-grams for representation learning. Although, we argue, that this formulation is unsuitable for short-text classiﬁcation problems due to (i) as noted above, the implicit assumption in these methods of proper word-ordering in input sequences does not generally hold applicable for short-text queries, and (ii) as explained further, the average length of input sequences in this context presents statistical challenges against CNNs to convolve effectively over the inputs. In the datasets derived from Wikipedia titles, 98% documents have 8 or less words, while 82% documents have 4 words or less (Table: 1). Moreover, 70% of the data points in AmazonTitles-670K consist of 8 words or less (Figure: 4). This makes the convolutional ﬁlters spanning over 4 and 8 words (Kim 2014; Liu et al. 2017) behave analogously to a weak fully connected layer with very few hidden units for most of the input data, thus leading to feature maps with very few activations which are sub-optimal for representation learning tasks. In the context of the aforementioned problems, we hypothesize and demonstrate empirically that it is suitable to use CNNs to convolve over the embedding dimensions of our inputs instead of the word dimensions for short-text queries. For the remainder of this paper, we refer to these CNNs as “Embedding Convolutions”. Embedding Convolutions, by convolving over the embedding dimension, capture what we deﬁne as “coupled semantics”. Using a convolution over these embedding dimension in a stacked setting enables the model to detect correlations among different dimensions, or “semantics” in the embedding space. A convolution operation allows us to efﬁciently extract these coupled semantics by processing a limited subset of embedding dimensions at a time. Embedding convolutions create signiﬁcantly larger and enriched activation map outputs for input short-text queries as compared to traditional convolutional operations. Furthermore, our approach requires signiﬁcantly lesser parameters by using smaller ﬁlters of size Rwhere S is the maximum sequence length of the short-text query. We show empirically that this modiﬁed architecture works well for both short as well as “medium queries” of up to 32 words, significantly outperforming conventional CNNs (Liu et al. 2017; Kim 2014) for short-text XML task. Some readers might rightfully argue that using convolutions on the embedding dimension ideally should have several limitations. On one hand, a convolution operation through its spatial inductive bias, intrinsically assumes locality of information. While on the other hand, the pre-trained GloVe word embeddings (Pennington, Socher, and Manning 2014) have not been trained with any incentive for localizing semantic information in the embedding dimension. This means that the coupled semantics that our model tries to detect may initially occur across spatial distances that are too large for the convolution kernel to detect. To solve this problem, we enhance the word embeddings with self-attention layers before applying embedding convolutions. By applying self-attention on the embeddings dimension, information ﬂows across every pair of semantics, depending on their correlation, irrespective of the spatial distance between them. This transformation increases our convolutional feature extractor’s effective receptive ﬁeld and also makes it easier for it to detect the coupled semantics. Problem Setup and notation : Given a training set {x, y}such that the input instance xis a short-text query, the aim is to learn a classiﬁer that predicts y, the corresponding label set for the input query where y∈ {0, 1} and L denotes the total number of labels in the XML setting. It may be noted that even though the number of labels is large, an instance is annotated only with a handful of positives, i.e., ||y|| L for all i. The main body of our encoder, shown in Figure 1, consists of three modules that are applied sequentially on the word embeddings. These are (i) an word-order agnostic embedding enhancement module, (ii) embedding convolution layers and (iii) a linear classiﬁer. Embeddings For inputting data instance to our model, we stack the embeddings of the words comprising the instance, sequentially along the words dimension as opposed to taking their TFIDF weighted linear combination as used in some recent works (Dahiya et al. 2021b,a; Mittal et al. 2021a) or the more conventional bag-of-words representations approaches like (Prabhu et al. 2018b; Babbar and Sch¨olkopf 2017). The word embeddings are initialized with d-dimensional pretrained GloVe embeddings (Pennington, Socher, and Manning 2014) where d = 300. For a fair comparison, with the exact same size of vocabulary space as (Dahiya et al. 2021b) for all publicly available datasets and trained with the help of our encoder. Embeddings of words that do not exist in GloVe are initialize with a random vector sampled from the uniform distribution U(−0.25, 0.25). For tokenization, we use white space separated preprocessing function and remove the stop words and punctuation from the raw data (Liu et al. 2017). We ﬁnd empirically that our model works better without using sub-word tokenizers like word-piece or sub-word based embeddings like fastText (Joulin et al. 2016). Order-agnostic Embedding Enhancement Module The aim of this module is to enhance the initially stacked word embeddings which lack structure and contextual information, and make it word order agnostic. The module consists of two orthogonal self-attention layers applied sequentially on the word and embedding dimensions followed by a projection layer. These two layers encode global information not only on a word-level but also on a semantic-level, which helps increase the performance of our encoder (Figure 3). We use the SimpleSelfAttention (Doria 2019) variant of self-attention in our model, which is a simpliﬁcation of the self-attention operation described in Zhang et al. (2019). Here, G denotes a single self-attention operation where X and Y denote the key and query matrices respectively. The equivalent of the value matrix from Zhang et al. (2019) is represented by f (X) where f(·) is a convolution layer and γ is a learnable scalar parameter. Our sequential attention formulation is described in the equations below where E(x) denotes the stacked word embeddings for a sample text input x such that E(x) ∈ R. Attention is ﬁrst applied along the word dimension. Then, the output is transposed and a second attention operation is applied on the embedding dimension. Finally, we take this context-aware feature map and project each embedding dimension to a p-dimensional space where p = 32. This layer essentially performs a per-semantic learned addition of the enhanced word embeddings. Even though each projection in this layer is sensitive to order of words in the input query, the complete set of 32 projections forms a diverse set of linear combinations of words, speciﬁc to the distribution of the input queries in the dataset. The output feature map from this layer is effectively independent of the word order in shorttext queries, this makes our model better equipped to deal with the lack of structure in short-text queries and hence, more robust to noise. The ﬁnal output of this layer is denoted by SAwhere SA∈ R. Embedding Convolution Layers We employ three parallel branches of one-dimensional convolutional layers V, i ∈ [1, 2, 3] with ﬁlter sizes of wwhere w∈ [4, 8, 16] each with a stride of 4 along the embedding dimension and p output channels. Let hbe the result of applying Vover SA. We concatenate all resultant hrowwise before passing them on to the next layer. A ﬁnal embedding convolutional layer Vwith kernel size of 16 and stride of 4 is performed on the concatenated feature map, which is then ﬂattened to form the ﬁnal feature representation returned by the encoder. The maximum effective receptive ﬁeld of the second convolution layer is 76 embeddings. Given that we use d = 300, our encoder captures coupled semantics spanning upto 1/4of the input at a time. Linear Classiﬁer Head The ﬁrst layer R transforms the feature map from the encoder with a skip-connection while keeping the dimensions Training P@1 Figure 2: Progress of training (Precision@1) for the extreme and meta-classiﬁer of LIGHTXML and INCEPTIONXML+ framework for AmazonTitles-670K same. The next linear layer W has one-vs-all classiﬁers for each label in the dataset which projects the features to the label space. Loss: We use the binary cross entropy loss to train our model. Here y ∈ {0, 1}represents the ground-truth multihot encoded targets and ˆy are the model predictions. BCE(y, ˆy) = −(1 − y) log(1 − ˆy) + ylog(ˆy) (1) Our light-weight INCEPTIONXML described so far scales to datasets with hundreds of thousands of labels. However, scaling up to millions of labels in its existing form is difﬁcult as the loss computation in Equation (1) involves summation over all L labels, a very large majority of them being negative for the given instance. To overcome this, we now propose a scalable extension to INCEPTIONXML, called INCEPTIONXML+, in which the loss is computed over only for the hardest negative labels. Following the basic approach popularized by methods like (Dahiya et al. 2021b; Jiang et al. 2021), our model makes predictions in two stages: (i) shortlisting K label-clusters or “meta-labels” using a meta-classiﬁer, and (ii) employing a computationally feasible number of one-vs-all classiﬁers corresponding to the labels included in the shortlisted clusters to get the ﬁnal predicted labels and perform backpropagation. Label Clustering To perform label clustering, we construct Probabilistic Label Tree (PLT) using the labels’ Positive Instance Feature Aggregation (PIFA) representation over sparse BOW features of their training samples as done in (Jiang et al. 2021; You et al. 2019; Chang et al. 2020b). More speciﬁcally, we use a balanced 2-means clustering to recursively partition the label set until we have a mapping C from L labels to Lclusters where L L (Table:1). Drawbacks of LIGHTXML framework: When scaling our model using dynamic hard-negative mining as done in LIGHTXML, we notice that the performance of our encoder is bottlenecked by a poorly performing meta-classiﬁer. From the training metrics (in Fig: 2), we see a smooth increment in the P@1 values for the extreme classiﬁer (dashed blue) while the meta-classiﬁer is unable to catch-up (dashed red). This indicates that these two sub-tasks are not aligned well enough for the encoder to learn suitable common representations that work well simultaneously for both the sub-tasks. Our observations also indicate the fact that the extreme task is easier to learn on shortlisted labels than the meta-task on label clusters, and the model tends to learn representations that beneﬁt the extreme task at the expense of the meta-task. We make key changes to the dynamic hard-negative mining framework in accordance with these observations. These changes can be broadly grouped into two sets. The ﬁrst set of changes consist of architectural changes meant to bring the two tasks closer in order to enable the encoder to learn better common representations. In the second set of changes, we make modiﬁcations to the training loop in order to force the encoder to learn representations that improve the performance of the meta-classiﬁer while not compromising on the performance of the extreme task. Decoupled Architecture To bring the two prediction tasks closer, we give them similar structures by adding a linear layer with a residual connection before the meta-classiﬁer. We create a shortlistˆS of all the labels in the top K label clusters as predicted by the meta-classiﬁer using a label cluster to label mapping C. The extreme classiﬁer then predicts the probability of the query belonging to only these shortlisted labels, instead of all L labels. Architectural similarity of branches doesn’t ensure strong common representation learning. To help the encoder learn suitable common representations, we further bring the two branches closer by (i) increasing the fan out of label clustering, and (ii) adding spectral norm to the penultimate linear layers in both heads. Increasing the fan out of label clustering brings the meta-task closer to the extreme-task by increasing the “extremeness” of the meta-task. Addition of spectral normalization prevents the weights of the hidden layers of both task heads from drifting too far from each other (Dahiya et al. 2021b). Not only does this heavily improve upon the original implementation of dynamic negative-hard mining framework as proposed in (Jiang et al. 2021), but also inherently combines the task of the two stages of the DeepXML pipeline into an end-to-end trainable model. Even though we observe substantial gains from increasing the fan out, this comes at the cost of making the meta-classiﬁer heavier. So, in practice we aim to strike a balance (Table:3) between number of clusters and model efﬁciency for non-trivial gains in accuracy. Prediction: Even though we bring these tasks closer by the aforementioned changes, we truly decouple the architecture further by eliminating the dependence of the extreme classiﬁer on the meta classiﬁer for the ﬁnal predictions. While the ﬁnal predictions in Jiang et al. (2021) are generated by multiplying the logits of both meta and extreme classiﬁer, we only use the logits of the extreme classiﬁer to make ﬁnal predictions. This results in relative increment in performance ranging from 3% to 5% with our architectural changes. Loss: As with the unscaled version of our model, we use the binary cross entropy loss for training. The ﬁnal losses are given by: Algorithm 1: Training algorithm for INCEPTIONXML+ 1 for epoch in (1, epochs): 2 for x, y in data: ext_classifier) Table 1: Dataset Statistics. APpL denotes the average data points per label, ALpP the average number of labels per point. #W is the number of words in the training samples. For our scaled up model, L the value of K for top K clusters chosen per dataset while ALpC denotes the average labels per cluster. Detached Training To force the encoder to learn representations beneﬁtting the meta-task, we detach i.e. stop the ﬂow of gradients from the extreme classiﬁer head to the encoder (Algorithm 1), for the initial 1/4-th of the training loop. This results in shortlisting of harder negative labels for the extreme classiﬁer to learn during training time and ensuring higher recall during inference time. Detaching instead of simply removing the extreme classiﬁcation head has the added advantage of training the layers in this head keeping it in sync with the changing encoder representations without allowing it to affect the training of the meta-classiﬁer. This setting is possible because of the spectral norm applied to the weights of the penultimate layers in both the heads which ensures that the encoder learnt for the meta-task remains relevant for the extreme task when it’s gradients are re-attached. Datasets: To show the practical applicability of the frameworks, we evaluate on multiple publicly available benchmarks from the extreme classiﬁcation repository (Bhatia et al. 2016). The details of the datasets are given in Table 1, showing datasets with number of labels ranging from 350,000 for WikiSeeAlsoTitles-350K to 2.8 Million for AmazonTitles-3M. Evaluation on the Wikipedia-based datasets involves predicting tags and related pages from Wikipedia page titles and the Amazon-based datasets involves predicting items frequently bought together from just the names of Amazon products. Evaluation Metrics: As stated earlier, the main application of short-text XML framework is in recommendation systems and web-advertising, where the objective of an algorithm is to correctly recommend/advertise among the top-k slots. Thus, for evaluation of the methods, we use precision at k (denoted by P@k), and its propensity scored variant (denoted by P SP @k) (Jain, Prabhu, and Varma 2016). These are standard and widely used metrics by the XML community (Bhatia et al. 2016). For each test sample withb observed ground truth label vector y ∈ {0, 1}and predicted vector ˆy ∈ R, P@k is given by : where top(ˆy) returns the k largest indices of ˆy. and Top K denote the number of label-clusters and Table 2: Comparison of InceptionXML to state-of-the-art extreme classiﬁcation algorithms on benchmark datasets. The best-performing approach is in bold and the second best is underlined. ’-’ infront of a model implies that the model doesn’t scale for that dataset. Figure 3: Performance with and without self-attention layers in module 1 on WikiSeeAlsoTitles-350K & AmazonTitles-670K Since P @k treats all the labels equally, it doesn’t reveal the performance of the model on tail labels. However, because of the long-tailed distribution in extreme classiﬁcation datasets, one of the main challenges is to predict tail labels correctly, which are more valuable and informative compared to head classes, and it is essential to measure the performance of the model speciﬁcally on tail labels. By alluding to the phenomenon of missing labels in the extreme classiﬁcation setting and its relation to tail-labels, P SP @k was introduced in Jain, Prabhu, and Varma (2016) as an unbiased variant of original precision at k under no missing labels. This is widely used by the community to compare the relative performance of algorithms on tail-labels, and is also another metric used in our relative comparisons among various extreme classiﬁcation algorithms in Tables 2 and 3 for main results and ablation tests respectively. For a discussion on unbiased variants of binary loss and their convex surrogates, refer to Qaraei et al. (2021). Main Results The main results of our experimentats are shown in Table 2. Our proposed INCEPTIONXML module outperforms ASTEC, ASTEC-3, LIGHTXML and all other competing approaches on AmazonTitles-670K, WikiSeeAlsoTitles-350K, and WikiTitles-500K on all metrics with non-trivial gains in performance. We achieve state-of-the-art performance on all evaluation metrics on these datasets with our encoder either in its scaled or unscaled setting. Furthermore, the following observations can be made : • For most of the dataset-metric combinations, the proposed algorithms, INCEPTIONXML and INCEP- ASTEC and even its ensemble version ASTEC-3. Notably, our INCEPTIONXML module gains an average of 4.2% and 8.18% over ASTEC on all three datasets as measured by the P@1 and PSP@1 metrics We also outperform the previous CNN-based SOTA approach XMLCNN signiﬁcantly. • Much higher gains (upto 20% in some cases) are obtained compared to transformer based models such as LIGHTXML (Jiang et al. 2021), X-TRANSFORMER (Chang et al. 2020a), and APLC-XLNET (Ye et al. 2020). Notably, none of them scale to AmazonTitles-3M dataset, demonstrating the efﬁcacy and scalability of the proposed light-weight encoder. We also outperform the RNN based ATTENTIONXML (You et al. 2019) by a large margin across all datasets. • Our models outperform non deep learning approaches using bag-of-words representations such as the label-tree based algorithms like BONSAI (Khandagale, Xiao, and Babbar 2020) and PARABEL (Prabhu et al. 2018a), and DISMEC, (Babbar and Sch¨olkopf 2017) an embarrassingly parallel one-vs-rest implementation of Liblinear (Fan et al. 2008). • We note that INCEPTIONXML generally outperforms INCEPTIONXML+ on several benchmarks, especially on the PSP metric. We attribute this to the fact that the unscaled model always gets information about all negative labels instead of a small subset. This allows it to perform better on tail labels for which the label clusters may not be optimal. Training Time: Our model’s training time ranges from 7 hours with INCEPTIONXML on the WikiSeeAlsoTitles350K dataset to 31 hours on AmazonTitles-3M. We also observe ∼ 40% improvement in training time by using the INCEPTIONXML+ pipeline compared to the vanilla model INCEPTIONXML. As shown in Figure 3, using the self-attention layers improves InceptionXML’s performance by 2.6% and 3.7% on average on P@K and PSP@K metrics respectively across both AmazonTitles-670K and WikiSeeAlsoTitles350K datasets. Notably, our model generally outperforms ASTEC-3 and LIGHTXML (except for P@3 and P@5 on AmazonTitles-670K) even without the addition of a selfattention module. This proves our CNN-based encoder’s superior representation learning capability for short-text queries over transformer-based models like LIGHTXML and an ensemble of architectures like ASTEC-3. We also integrate our encoder with the DeepXML (Dahiya et al. 2021b) pipeline as used by ASTEC and ﬁnd it inﬂexible to improve upon due to the requirement of Table 3: Ablation results on AmazonTitles-670K for the impact of increasing fan-out of label clustering (L) for our encoder in different scaling up frameworks. ﬁxed representations for their shortlisting strategy. Moreover, our encoder’s performance degrades in terms of precision when using the DEEPXML pipeline for scaling, as compared to using our encoder as a drop-in replacement for LIGHTXML’s transformer model without other modiﬁcations (Table: 3 Column 1). We also note that our encoder in the DEEPXML pipeline performs better on PSP metric i.e. tail-labels as compared to the original LIGHTXML framework. We attribute this to the high dissimilarity between extreme and meta-classiﬁcation tasks in the LIGHTXML framework and low fan-out, leading to unsuitable clustering for tail labels and poorer hard-negative mining. It is clear from Table 3, that the changes we make to the dynamic hard-negative mining framework of LIGHTXML signiﬁcantly improves performance on both P@K and PSP@K metrics. We notice consistent improvement in results for our decoupled architecture, even without detached training, as we increase the fan-out of our PLT used for label clustering. We keep the the shortlisted labels consistent by doubling the number of shortlisted meta-labels as the fanout doubles. While our results keep increasing with increasing fan-out, the results of our encoder in the LIGHTXML framework results only show marginal improvement and a dip later across all metrics. We also notice that as the fan-out increases, our detached training method improves the results more prominently. This can be attributed to the fact that we bring the two tasks closer by increasing the fan-out and the representations learnt by the encoder for the meta-task become increasingly more relevant to the extreme-task when the gradients of the extreme classiﬁer are re-attached during training. In this work, we revisited the architecture of convolution networks for the task of short-text extreme classiﬁcation, which has wide-spread applications in search and recommendation engines. We recast the conventional convolutional architecture to capture coupled semantics along the embedding dimensions. We further make use of an Order-agnostic Embedding Enhancement module to make up for the lack of structure and noise in short-text data. On real-world shorttext classiﬁcation problems, we demonstrated that the proposed approach based on convolving ﬁlters along the word Figure 4: Sequence lengths of the input instance plotted against corresponding frequency for AmazonTitles-670K dataset. For this dataset, 70% of training instances have ≤ 8 words, and 30% have ≤ 4 words. embedding along with Self-attention leads to better performance than recent state-of-the-art methods. The resulting encoder, INCEPTIONXML, is light-weight compared to the transformer-based extreme classiﬁcation models, and yet powerful enough for learning rich feature representations for the short-text XML task. Furthermore, we also develop an extension to our model - INCEPTIONXML+ that scales to datasets with millions of labels.