Recently, multimodal sentiment analysis has seen remarkable advance and a lot of datasets are proposed for its development. In general, current multimodal sentiment analysis datasets usually follow the traditional system of sentiment/emotion, such as positive, negative and so on. However, when applied in the scenario of video recommendation, the traditional sentiment/emotion system is hard to be leveraged to represent different contents of videos in the perspective of visual senses and language understanding. Based on this, we propose a multimodal sentiment analysis dataset, named baiDu Video Sentiment dataset (DuVideoSenti), and introduce a new sentiment system which is designed to describe the sentimental style of a video on recommendation scenery. Speciﬁcally, DuVideoSenti consists of 5,630 videos which displayed on Baidu, each video is manually annotated with a sentimental style label which describes the user’s real feeling of a video. Furthermore, we propose UNIMO (Li et al., 2020) as our baseline for DuVideoSenti. Experimental results show that DuVideoSenti brings new challenges to multimodal sentiment analysis, and could be used as a new benchmark for evaluating apporaches designed for video understanding and multimodal fusion. We also expect our proposed DuVideoSenti could further improve the development of multimodal sentiment analysis and its application to video recommendations. Sentiment analysis is an important research area in Natural Language Processing (NLP), which has wide applications, such as opinion mining, dialogue generation and recommendation. Previous work (Liu and Zhang, 2012; Tian et al., 2020) mainly focused on text sentiment analysis and achieved promising results. Recently, with the development of short video applications, multimodal sentiment analysis has obtained more attention (Tsai et al., 2019; Li et al., 2020; Yu et al., 2021) and a lots of datasets (Li et al., 2017; Poria et al., 2018; Yu et al., 2020) are proposed to advance its developments. However, current multimodal sentiment analysis datasets usually follow the traditional sentiment system (positive, neutral and negative ) or emotion system (happy, sad, surprise and so on), which is far from satisfactory especially for video recommendation scenario of application. In the scenery of industrial video recommendation, content-based recommendation method (Pazzani and Billsus, 2007) is widely used because of its advantages in improving the explainability of recommendation and conducting effective online interventions. Speciﬁcally, the videos are ﬁrst represented by certain tags, which is automatically predicted by some neural network models (Wu et al., 2015, 2016; Rehman and Belhaouari, 2021). Then, a user’s proﬁle is constructed by gathering the tags from his/her watched videos. Final, it recommends candidate videos based on the relevance between current candidate video’s tags and the user’s proﬁle. In general, the types of tags are further divided into topic-level and entity-level. As show in Figure 1, for the videos talking about one’s traveling in The Palace Museum, they may relate to the topic-level tags such as ”Tourism” and entity-level tags such as ”The Palace Museum”. While in real application, the topic-level and entity-level are not afford to summary the content of a certain video comprehensively. The reason relies in that the videos, which share the same topics or contains similarity entities, usually gives different visual and sentimental perceptions to a certain user. For example, the ﬁrst video brings us a feeling of great momentum, while the second video brings us a feeling of Figure 1: Two videos displayed in Baidu, which record the authors’ travelling to The Palace Museum. Although, the two videos share the same topic-level and entity-level tags such as ”Tourism” and ”The Palace Museum”, they give different visual and sentimental perceptions to the audience. Speciﬁcally, the ﬁrst video gives us a feeling of great momentum, while the another one brings us a feeling of generic. generic, although they share the same topics. Based on this, we propose a multimodal sentiment analysis dataset, named baiDu Video Sentiment dataset (DuVideoSenti), and construct a new sentimental -style system, which consists of eight sentimental-style tags. These sentimental-style tags are designed to describe the users’ real feeling after he/she has watched the video. In detail, we collect 5,630 videos which displayed on Baidu, and then each video is manually annotated with a sentimental-style label selected from the above sentimental-style system. In addition, we further propose a baseline to our DuVideoSenti Corpus which is based on UNIMO. And we expect our proposed DuVideoSenti corpus could further improve the development of multimodal sentiment analysis and its application to video recommendations. In this section, we brieﬂy introduce previous works in multimodal datasets, multimodal sentiment analysis. To promote the development of multimodel sentiment analysis and emotion detection, a variety of multimodal datasets are proposed, including IEMOCAP (Busso et al., 2008), YouTube (Morency et al., 2011), MOUD (P´erez-Rosas et al., 2013), ICTMMMO (W¨ollmer et al., 2013), MOSI (Zadeh et al., 2016), CMU-MOSEI (Zadeh et al., 2018b), CHEAVD (Li et al., 2017), Meld (Poria et al., 2018), CH-SIMS (Yu et al., 2020) and so on. As mentioned above, current multimodal datasets follow the traditional sentiment/emotion system, which is not ﬁt to the scenario of video recommendations. In this paper, our DuVideoSenti dataset deﬁnes a new sentimental-style system which is customized for video recommendation. In multimodal sentiment analysis, intra-modal representation and inter-modal fusion are two essential and challenging subtasks. For intra-modal representation, previous work pay attention to the temporal and spatial characteristics among different modalities. The Convolutional Neural Network (CNN), Long Short-term Memory (LSTM) and Deep Neural Network (DNN) are representative approaches to extract multimodal features (Cambria et al., 2017; Zadeh et al., 2017, 2018a). For inter-modal fusion, numerous methods have been proposed recently. For example, concatenation (Cambria et al., 2017), Tensor Fusion Network (TFN) (Zadeh et al., 2017), Low-rank Multimodal Fusion (LMF) (Liu et al., 2018), Memory Fusion Network (MFN) (Zadeh et al., 2018a), Dynamic Fusion Graph (DFG) (Zadeh et al., 2018b). Recently, we follow the trend of pretaining, and select UNIMO (Li et al., 2020) to build our baselines. In this section, we introduce the constrcution details of our DuVideoSenti dataset. Speciﬁcally, DuVideoSenti contains 5,630 video examples, all examples are selected from Baidu. Besides, in order to ensure the data quality, all sentiment labels are annotated by experts. Finally, we release DuVideoSenti according to following data structure as shown in Table 1. Table 1 shows a real example in our DuVideoSenti datasets, which contains multi-regions such as url, title, video feaures and its corresponding sentiment label. For the video Table 1: Data structure of released dataset. (It should be noted that we have not provide the origin frame images in our dataset for the protection of copyright. Instead, we provided the visual features extracted from each frame image and its corresponding url for watching the video online.) features extraction, we ﬁrst sample four frame images from all the frame images of the given video at the same time intervals. Second, we use Faster R-CNN (Ren et al., 2015) to detect the salient image regions and extract the visual features (pooled ROI features) for each region, which is the same as (Chen et al., 2020; Li et al., 2020). Speciﬁcally, the image-region features are set as 100. In addition, we split DuVideoSenti into training set and test set randomly, the size of which are 4,500 and 1,130 respectively. The sentimental- (Hipsterism)”, “时尚炫酷(Fashion)”, “舒适温 馨(Warm and Sweet)”, “客观理性(Objective and Rationality)”, “家长里 短(Daily)”, “土里土 气 (Old-fashion)”, “呆萌可爱(Cute)”, “奇葩低俗 (Vulgar)”, “正能量(Positive Energy)”, “负能量 (Negative Energy)”, “其他(Other)” , the class distribution and examples of each sentiment label in our DuVideoSenti are shown in Table 2. In our experiment, we select UNIMO-large , which consists of 24 layers of Transformer block,as as our baseline. The maximum sequence length of text tokens and image-region features are set as 512 and 100, respectively. The learning rate is set to 1e-5 and the batch size is8. We set the number of epochs to20. All experiments are conducted on 1 Tesla V100 GPUs. We select Accuracy to evaluate the performance of the baseline systems. Table 3 shows the baseline performance in our dataset, speciﬁcally, UNIMO-large model achieves an Accuracy of 54.56% on the test set. It suggests that the by simply using a strong and widely used models failed to obtain promising results, which proposes the needs for further development of multimodal sentiemnt analysis, especially for videos. We are also interested in the affects brought by different number of image-region frames. We further test the performance of our baseline system when{1, 4, 20}frames are selected. The experiment results are listed in Table 3, which shows that the performance of decreases with less imageregion frames. It suggests that by improving visual representation, can further promote the classiﬁcation performance. Finally, we propose to evaluate the improve- Table 3: UNIMO baseline performance based on different number of key frames. ments brought by multi-modal fusion. We compare the accuracy performance among the systems Table 4: Baseline performance based on image and text. which use visual, textual, and multi-modal information respectively. The results are show in Table 4, which shows that by fusing the visual and textual information of the videos, it obtained the best performance. Furthermore, visual-only models performs better than the textual-only one, we suspect that is our deﬁned sentimental-style system is much more related to user’ visual feelings after he/she watched the video. In this paper, we propose a new multimodel sentiment analysis dataset named DuVideoSenti, which is designed for the scenario of video recommendation. Furthermore, we propose UNIMO as our baseline, and test the accuray performance of the baseline on a variety of settings. We expect our DuVideoSenti dataset could further improve the development for the area of multimodal sentiment analysis, and promote the applications to video recommendation.