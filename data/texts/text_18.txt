Abstract—Learning accurate low-dimensional embeddings for a network is a crucial task as it facilitates many downstream network analytics tasks. For large networks, the trained embeddings often require a signiﬁcant amount of space to store, making storage and processing a challenge. Building on our previous work on semi-supervised network embedding, we develop dSNEQ, a differentiable DNN-based quantisation method for network embedding. d-SNEQ incorporates a rank loss to equip the learned quantisation codes with rich high-order information, and is able to substantially compress the size of trained embeddings, thus reducing storage footprint and accelerating retrieval speed. We also propose a new evaluation metric, path prediction, to fairly and more directly evaluate model performance on the preservation of high-order information. Our evaluation on four real-world networks of diverse characteristics shows that d-SNEQ outperforms a number of state-of-the-art embedding methods in link prediction, path prediction, node classiﬁcation and node recommendation, while being far more space- and time-efﬁcient. Index Terms—Network Embedding, Quantisation, Semisupervised Learning, Path Prediction Etwork embedding methods learn an encoder system that converts nodes into low-dimensional vectors, aiming at preserving the original network information as much as possible. However, with the fast increase in network sizes, especially the number of nodes, it is increasingly challenging to store and retrieve such large-scale networks via the conventional embedding scheme, i.e. continuous embeddings, where each node is represented as a ﬂoat-valued vector. Another issue of continuous embedding methods is that the learned embeddings usually contain massive redundant information and many dimensions can be compressed [1], [2], [3]. The above two dilemmas motivate us to turn to more efﬁcient embeddings, such as hashing and quantisation. In the computer vision communities, these two strategies have been extensively studied for the task of image and video retrieval [4], [5], [6]. The main advantages of these techniques are that the discrete representations can signiﬁcantly reduce storage footprint and simultaneously improve retrieval efﬁciency. For storage footprint, the discrete vectors utilize a much smaller set of ﬁnite values. Speciﬁcally, hashing-based representations have only two possible values, zero and one, to denote each dimension, while quantisation has K different values for each dimension, also named as codewords in some works [7], [8], where K is usually greater than 2. Thus, each dimension can be encoded by logbits which is much less than the storage size required by conventional embedding methods, e.g., 32 bits or 64 bits for each dimension. For retrieval efﬁciency, due to the fact that each embedding dimension can only been selected from a ﬁnite set, when calculating the pointwise product and multiplication between two discrete vectors, the result of each dimension is also ﬁnite. Thus, we can pre-calculate all their possible ﬁnite results and store them in a ﬁxed-size table with the size of K. After that, the expensive online calculations of conventional methods can be sidestepped by efﬁcient lookups in the pre-calculated table. Additionally, the other issue of network embedding is the usage of label information. Unsupervised node representation learning methods are the most common. These include the well-known methods DeepWalk [9], LINE [10], node2vec [11], Graph2Gauss (G2G) [12], and NetMF [13], all of which completely disregard the valuable node label information, leaving signiﬁcant room for improvement on node classiﬁcation performance. In fact, it is impractical to train a model in a fully supervised fashion, as we cannot expect the availability of ground-truth label for each node, especially for large networks. However, discarding the available labels is not helpful for the model to learn semantically discriminative representations. Naturally, semi-supervised or weakly supervised methods can well mitigate this problem, as they are able to make use of limited label information. Many semi-supervised network embedding methods have been proposed, including the recent techniques based on graph convolutional networks (GCN) [14], [15]. GCN-based methods can be seamlessly assembled into a deep neural network in an end-to-end manner and facilitate many downstream applications, e.g. multi-label classiﬁcation [16]. A critical task to evaluate the quality of embeddings is link prediction [9], [10], [14], [17], which tests the capability of embeddings to reconstruct the original network’s neighbourhood structure, i.e., the ﬁrst-order proximity. On the other hand, it has been widely acknowledged that higher-order proximity has signiﬁcant impact on the quality of embeddings [1], [12], [18], [19]. While some existing methods [12], [14], [15] have claimed their strength in preserving high-order proximity, this capability is typically evaluated by indirect, alternative metrics, such as link prediction or node classiﬁcation. Moreover, from our empirical observations, these tasks do not consistently reﬂect the preservation of high-order proximity. Moreover, Zhang et. al [20] reported values of the objective function of high-order proximities preservation to measure it. However, those measurements are indirect too, as they do not empirically and explicitly demonstrate their ability to well embed proximity of higher orders. Therefore, a natural question arises, that Can we develop an explicit and uniﬁed metric for the evaluation of high-order proximity preservation beyond the indirect alternatives such as link prediction and node classiﬁcation? Toward this end, we propose a new metric, path prediction. Intuitively, the preservation of high-order proximity gauges the model’s capability of embedding hierarchical neighbourhood structure [21], [22], [23]. Building on this intuition, we view path prediction as a multi-neighbourhood classiﬁcation problem. That is, given a selected anchor node, we randomly sample its various high-order neighbours and train a classiﬁer to predict those neighbours’ order. We posit that if the highorder proximity is well-preserved in the embeddings, its neighbours will be readily classiﬁed. Additionally, another practical application of path prediction is that we can directly obtain the multi-hop relationship between two nodes, e.g. second-hop neighbours, beyond simply answering whether there exists a direct edge between them by link prediction. Although we could construct the original network by link prediction an then use graph search algorithms, such as Depth First Search (DFS) [24] or Breadth First Search (BFS) [25], to identify paths of exact orders between two nodes, it is time-consuming and even infeasible for large-scale networks. In this paper, based on our preliminary work [26], we propose a semi-supervised network embedding method through differentiable deep quantisation. Speciﬁcally, the embedding component incorporates both node labels, network proximity information, as well as node attributes, and preserves highorder proximity. Meanwhile, we deploy a direct differentiable quantisation module, built on a deep autoencoder neural network, to learn quantisation codes with a relaxation-free strategy. Our experiments on four real-world datasets with diverse characteristics show that our model outperforms existing stateof-the-art embedding methods on the tasks of link prediction, node classiﬁcation, node recommendation and path prediction. Comparing to well-known discrete embedding methods, we also achieve substantially better performance on these tasks while maintaining a comparable storage footprint and time cost for retrieval. In summary, our main contributions are threefold: pose a differentiable DNN-based quantisation method, named d-SNEQ, which can provide quantised codes with more hierarchical high-order information by a rank loss. Compared with SNEQ, d-SNEQ can bring about lower quantisation errors and improve its performance on the semantic and structural preservation by exploiting the merit of a deep autoencoder network. proximity preservation, path prediction, which more robustly measures the a model’s performance in preserving high-order proximity. classiﬁcation, node recommendation and path prediction, our method outperforms state-of-the-art network embedding methods, including both continuous and discrete embeddings. Note that a preliminary version of this paper appeared in AAAI 2020 [26]. In the preliminary version, we proposed a semi-supervised embedding scheme and a self-attentionbased quantisation compression strategy. In this expanded manuscript, we concentrate on the quantisation module and make the following extensions: a deeper neural network version by carefully designing an autoencoder network to more effectively quantise embeddings. force them to be equipped with consistent high-order proximities. to-end manner, we utilise a more effective and general technique, Gumbel-softmax, which shows a better performance than our previous self-attention-based strategy. prediction, for better evaluating a model’s performance on higher-order proximity preservation. ments to test the contribution of each component. Our work is closely related to learning to quantisation and network embedding, so we brieﬂy review the literature on those two tasks. Quantisation is a widely-used compression strategy to reduce amount of the redundant information in rich data, especially in the computer vision community [27], [28], [29], [30]. Generally, we can divide the mainstream high-dimensional data compression methods into two categories: binary hashing methods [4] and quantisation [29]. Speciﬁcally, hashing methods try to learn a hash function to map high-dimension data into a binary-valued space via multiple hyperplanes [31], [4], while the latter aims at quantising high-dimension data with a number of codewords, or centroids, so that the quantisation error is reduced as much as possible. Hence, in a wide range of empirical experiments, the performance of quantisation has been shown to be signiﬁcantly better than hashing by a large margin. However, the main advantage of hashing is that it generally outperforms quantisation in terms of time and storage efﬁciency, because binary hashing only requires the storage of the indices of each data point, whilst quantisation needs to store both the indices and the centroids. In this work, we concentrate on quantisation and brieﬂy review two quantisation techniques: Product Quantisation and DNN-based Quantisation. Product Quantisation (PQ) was proposed by J´egou [29] to quantise a vector into several subvectors, each of which is represented by a selected codeword via vector quantisation. Subsequently, many subsequent works based on PQ have been proposed. An optimized product quantisation was proposed [32], where authors introduced two optimization strategies: non-parametric and parametric methods, both of which can improve the performance of approximate nearest neighbour (ANN) search. Norouzi and Fleet [33] subsequently proposed a Cartesian K-means strategy with a compositional parameterization of cluster centroids that can boost the representational capacity of codewords and make it possible to quantise data using billions of codewords. Later on, nonorthogonal quantisations [34], [35] was proposed, with the core idea of summing M dependent codewords on the original space from M different codebooks to approximate the raw vectors instead of concatenating M subvectors. The beneﬁt of summation is that the compression quality can gain a significant improvement because M can be an arbitrary size and not limited by the vector size, leading to little reconstruction distortion. DNN-based Quantisation was investigated in some recent works [36], [37], [28], [38]. Gao et al [28] ﬁrst proposed Deep Visual-Semantic Quantisation (DVSQ) to jointly learn deep visual-semantic features and visual-semantic quantisation codes in an end-to-end fashion, and it is able to signiﬁcantly improve the quality of the learned compact codes and furthermore lift the similarity-based image retrieval performance. Shu and Nakayama [37] developed a deep compositional code learning framework based on multi-codebook quantisation (MCQ) in an end-to-end neural network to compress word embeddings in natural language processing and heavily reduce performance loss. Yu et al [36] developed a differentiable convolutional layer that is able to be plugged into an deep convolution feature learning network and made it possible to learn discrete codes in an end-to-end manner. Based on MCQ, Morozov and Babenko [38] proposed an unsupervised visual representation compression method which can be generally applied in a wide range of computer vision pipelines for compressed-domain retrieval. Network embedding has been an active research area in representation learning [39]. We can broadly divide network embedding methods into two categories: unsupervised and semisupervised. The unsupervised methods include DeepWalk [9], Line [10], NetMF [13], and those based on autoencoders, such as VGAE [40], and ARVGA [41]. Speciﬁcally, DeepWalk [9] ﬁrst proposed to use a sequence-to-sequence model to learn structural embeddings via a two-step strategy. Tang et al developed the inﬂuential two-step method LINE [10], which is able to simultaneously learn the ﬁrst- and secondorder proximity. Later on, NetMF [13] theoretically uniﬁes these methods under a matrix factorization framework. On the autoencoder-based unsupervised methods, VGAE [40] ﬁrst utilised an autoencoder framework to reconstruct the adjacency matrix, and subsequently ARVGA [41] devised a Variational Autoencoder (VAE) network that embeds each node representation into a Gaussian distribution and used Kullback–Leibler (KL) divergence to force the reconstructed distributions to be consistent with the original ones. Semi-supervised network embeddings has also gained extensive attention. Even through label information may be scarce, it is of potential value for the model to learn discriminative features, especially with the invention of graph convolution networks (GCN) [42], [43], [44]. TAE [42] is an end-to-end model-free framework based on metric learning and can not only preserve the topological structure of a network but also embed the discriminative semantic signals. SEANO [43] is a semi-supervised method and able to preserve three types of critical information: attribute afﬁnity, topological proximity and label similarity of nodes. GraphSAGE [44] based on GCN can inductively embed network structure by sampling and aggregating features from a node’s neighbours. In this section, we will present our network embedding and quantisation technique in detail. The overall architecture is shown in Fig. 1. Problem deﬁnition. Let G = (V, X, E, Y ) represent an attributed network. V = {v, v, . . . , v} is a set of N nodes. X ∈ Rrepresents D-dimensional node attributes. E = {e}denotes the edge set of the network. Specifically, e= 1 if vhas a connection to v, otherwise e= 0. Y = {y, y, . . . , y} is the label set of each node. To simplify the notation, we use an adjacency matrix A ∈ {0, 1}to represent the edges of the network. Our embedding goal is to embed the nodes of the network G into low-dimensional vectors Z ∈ R, where L  D. Simultaneously, the quantisation module is responsible for encoding the real-valued vectors Z into short and compact codes Q ∈ {0, 1, . . . , K}and a set of codebooks C = {C, C, . . . , C}, where C∈ R. M is the arbitrary size of codebooks in the embedding space Z, and K is the number of codewords in each codebook Cand usually set to the power of 2. The most important property of embeddings is the preservation of precise neighbourhood structure information of the original networks. To this end, we propose an adaptive margin based on metric learning as shown in 1. For an attributed network G, our model takes the attributes X as the input. We use a multi-layer perceptron (MLP) to embed each node v, initialised with node attributes x: where j denotes the j-th hidden layer and f (.) is the activation function. For notational convenience, the output of the last layer is denoted by Z. Intuitively, in a homogeneous network, the shorter the distance between two nodes is, the more similar they are. In this paper, we propose to use the shortest distance between two nodes (in terms of hops) to measure their structural similarity. Theoretically, we can regard embedding as a procedure to project the shortest distance information in the original network into the embedding space. Our motivation is how to precisely preserve the different shortest distance into the embeddings so that different distances of embeddings can simulate hierarchical information in the network. To this end, we adopt the widely-used metric learning technique, which has been successfully applied in computer vision tasks such as person re-identiﬁcation [45], face recognition [46], and hashing-based image retrieval [6], and leverage the triplet loss to learn node distances. The adaptive margin loss `is deﬁned as below: where Wdenotes the parameters of the embedding network; D(.) is the distance function in the embedding space, for which we chose Euclidean distance (Lnorm); and δ denotes the shortest distance from node vto node v. tr(.) is a triplet sampling function that, given an anchor node v, selects a positive node vand a negative node v. Concretely, given an anchor node v, vand vare sampled under the condition δ< δ. In Eq. 2, the adaptive margin, δ− δ, is the key term that deﬁnes the disparity between two shortest distances, that is, the anchor node to the positive node and the negative node. We adopt an adaptive margin to model the difference between a node’s neighbours. Speciﬁcally, the more distant neighbour is divided by a large margin while the closer neighbour is separated by a smaller distance. There are many different algorithms to calculate the shortest distance δ, such as Dijkstra’s and Floyd’s algorithms. In this paper we adopt a fast strategy based on matrix multiplication [47]. In many situations, a subset of the nodes in a network are assigned labels, and intuitively, nodes with the same label should be closer in the embedding space than otherwise. Based on this intuition, we explore a semi-supervised strategy to preserve label information into embeddings Z. The goal is to cluster same-label nodes with a small margin but separate those with different labels by a large margin. The general idea is illustrated in Figure 1, where the differently labelled nodes (different colors) should be separated by large margins. A key optimisation of our semi-supervised learning procedure is that we do not need to use all labelled nodes to train the model. This is due to the hypothesis that nodes with the small shortest distance are more likely to belong to the same class. Generally, as long as a small part of nodes in the same class are adequately separated, other nodes with small shortest distances to those separated nodes can also be separated. Thus, we only need to ensure that a margin, name as semantic margin used to separate differently labelled embeddings, is large enough because if it is too small, the adaptive margin will conﬂict with it and degrade node classiﬁcation performance. Furthermore, provided that the semantic margin is large enough, we can deem that it divides the embedding space into several discriminative subspaces, each of which stands for a community with the same class label, in which the adaptive margin is applied to preserve their neighbourhood structure information. The formulation of our semantic margin loss `is given as the following: where T is the sample size for semi-supervision; D(.) is the same distance function as in Eq. (2); and Srepresents the constant semantic margin and is formulated as below: where yis the set of labels of node v, and Mis a constant deﬁning the semantic margin. The aim of Eq. (3) is to force the same-labelled nodes to be close to each other, but nodes with different labels to be at a distance of M. It is worth noting that the value of T is important to node classiﬁcation performance, as a large T value generally improves node classiﬁcation result, due to more supervised information made available to the model. At the same time, a larger T could negatively affect graph structure learning due to the fact that some nodes which are supposed to be neighbours are separated by a large margin of M, leading to a degradation of the structural preservation. Hence, the choice of T is a trade-off between two competing factors: structural and semantic information. In the experiments, we test the impact of different T . As we mentioned previously, our quantisation strategy aims at compacting the continuous embedding vectors. Ideally, this reduction would improve both space and time efﬁciency while preserving task performance. Product quantisation (PQ) [29] is a well-known quantisation technique. PQ quantises embedding vectors Z into M codebooks C = {C, C, . . . , C}, and each codebook C∈ Rconsists of K centres (codewords) in the embedding space, which can be seen as K subspaces in the embedding space. For each codebook C, we use a 1-of-K indicator vector q∈ {0, 1}to assign one centre to z. It is worth noting that there is only one 1 in each q, which represents the closest centre to z, while all the other values of qare 0. The general quantisation loss `is calculated by: This loss function aims at reducing the reconstruction loss between the embedding vector z. A smaller `means better reconstruction performance from the quantisation codes. However, in our preliminary study [26], we observe that simply deploying M codebooks cannot adequately reduce the reconstruction errors. Inspired by some recently proposed bottleneck networks [6], [48], we deploy a deep autoencoderbased network to quantise zinto compact codes ˜z, where the bottleneck layer is the learned quantised codes and codewords, as shown in Figure 1. Generally, the enhanced quantisation loss can be rewritten as: where Wand Ware the parameters of the encoder E and the decoder of D, respectively. Noting that the output of E(z) is M binary vectors, denoting the selected index of codewords in the referring M codebooks, that is q= E(z). Nevertheless, the optimisation of Eq. (6) becomes a discrete code search problem, that is, we need to ﬁnd M binary codes q to approximate z as much as possible, similar to the learning to hash problem [4]. The direct optimisation is an NP-hard problem. Thus, many works [49], [3] turn to a relaxation strategy from hard (discrete) to soft (continuous). Similarly, we can rewrite Eq. (6) in a continuous form: u= E(z)∝ q, ∀j = 1, 2, . . . , N u∈ [0, 1], kuk= 1, ∀j = 1, 2, . . . , N q∈ {0, 1}, kqk= 1, ∀j = 1, 2, . . . , N The next question is how to choose a relaxation strategy. A dominant strategy in hash learning is to use sigmoid or tanh activation functions to anneal the relaxed vectors x to (0, 1) and then force them to be close to sign(x). A drawback of this approach is that it is hard to train the network when the relaxed vectors are heavily distributed in the margin of sigmoiod or tanh function, where the gradient is close to zero, leading to large quantisation errors. Inspired by the wide application of Gumbel-softmax [37], [50], [51], we deploy a differentiable discrete layer based on Gumbel-softmax so that the model can directly optimize quantisation codes in the training stage. Speciﬁcally, Gumbel-softmax can be presented as: where g= − log(− log(Uniform(0, 1))) denotes a standard Gumbel distribution and τ is a temperature factor as deﬁned in [51]. To obtain the discrete codes, we choose the maximum value in uas the selected codewords and then convert its index into a one-hot vector, which can be deﬁned as the below: where one hot(.) is a function to convert the index to a onehot vector; argmax(.) selects the index of the maximum value in a vector; [.] denotes the concatenation operation. Though we add the adaptive margin on the continuous embedding z, to furthermore boost the quantisation codes to be equipped with the neighbourhood proximity, we also deploy a rank loss over quantisation codes, as the below: `max(u· q− u· q+ 1, 0)(10) where · denotes the inner product of two vectors, tr(a, p, n) is the sample function same as in Equation (2), u is the probability output by Gumbel-softmax in Equation (7), and q is an one-hot vector in Equation (9). By optimising Equation (10), it will force the index of the maximum in uto be the same with q, that is, letting the value of the index be close to 1 but the others decrease to zero. On the other hand, for the negative points, it will reduce the value of uin the index of the maximum of qand increase the other dimensions’ values to force the index of the maximum in uto differ from q. Thus, the quantisation codes q is also equipped with the consistent structural information with the continuous embeddings. Our model learns network embedding and quantisation in an end-to-end architecture, which integrates adaptive margin loss (Eq. 2), semi-supervised semantic margin loss (Eq. 3), reconstructed quantisation loss (Eq. 7) and further a rank loss over discrete quantisation codes (Eq. 10) into a joint optimisation problem: where α and β are the balance parameters to trade-off the importance of each part. All the parameters in Eq. (11) are differentiable and trained by mini-batch stochastic gradient descent. Datasets. We evaluate our method on four real-world networks. Brief statistics of the datasets are shown in Table I. It is worth noting that cDBLP [52] is a large-scale network. We conduct experiments on the standard tasks of link prediction, node classiﬁcation and node recommendation. Besides, we evaluate the model on our newly proposed task, path prediction. Finally, we evaluate the efﬁciency and effectiveness of quantisation. Implementation Details. Our embedding method is a multilayer deep neural network consisting of three dense layers, each of which is followed by a batch-norm layer and activated by ReLu. The encoder network of E consists of two dense layers, also followed by a batch-norm layer and a ReLu layer, and the decoder D is symmetrical with the encoder. The dimension of the network embeddings is set to 128. The initial learning rate η is set as 0.001 and we adopt the One Cycle learning rate schedule [53] for faster model convergence, and batch size is set to 100. Hyperparameters α and β are not ﬁxed but tuned in an unsupervised way similar to [54]. Speciﬁcally, α =, where ω is a constant value 0.5, and µ is the training progress from 0 to 1, while β is set as 1.0−. The temperature τ is set to 1. For quantisation, the number of codebooks M is set to 8 and the size of codebook K is set to 256, so each quantisation code takes up 64 = (8 ∗ log) bits. The amount of labelled nodes T used in semi-supervised training (as deﬁned in Eq. (3) is set to 10% of |V | by default. For a fair evalutaion, the compared semi-supervised methods also use the same ratio of labelled nodes for training. For a fair comparison with learning to hash methods, their hash dimension is set as 64 bits, equal to our quantisation code length. All experiments were performed on a workstation with 256 GB memory, 32 Intel(R) Xeon(R) CPUs (E5-2620 v4 @ 2.10GHz) and 8 GeForce GTX 1080Ti GPUs. For each model a maximum approx. 100 GB of memory was allocated. To demonstrate the effectiveness and efﬁciency of our method, we select the following representative and stateof-the-art network embedding methods as our comparison baselines. The baseline models consist of both continuous and discrete embedding methods. Continuous network embedding models. We compare our continuous embedding model, denoted d-SNEQ, with the following methods: sequence model in natural language processing to learn neighbour structure proximity via a two-step strategy. use rich information as auxiliary features to improve the quality of network representation by factorising a referring matrix. attributed networks and can simultaneously embed three kinds of information: the topological proximity, label similarity of vertices, and attribute afﬁnity. embed a node as a Gaussian distribution, which is represented as a mean vector and a variance vector. developed a graph pooling mechanism by coarsening nodes into a hyper node to enlarge the receptive ﬁeld of the GCN, which enbles it to capture more global information. triplet loss to learn the topological structure and preserve more discriminative information. unsupervised, outlier-aware network embedding method, by minimising the effect of outlier nodes to improve the quality of the embeddings. Discrete network embedding models. We compare our discrete embedding model, denoted d-SNEQ, with the following methods: method for the approximate nearest neighbour search task. cipled hashing method able to tackle the challenging discrete optimization problem in hash learning and avoid large quantisation errors caused by two-step optimization. to embed trees in a graph, which can preserve information closer to the root node as much as possible. develops a Weisfeiler-Lehman proximity matrix that can preserve the dependence between node attributes and connections via combining the features from neighbouring nodes. Link prediction is a standard task to evaluate the performance of network embedding methods, aiming at measuring the preservation of the ﬁrst-order proximity. Speciﬁcally, we randomly select 5% and 10% edges as the validation and test set respectively, similar to Graph2Gauss [12]. Following the convention, we use AUC as the performance metric. Table II summaries the link prediction results of the continuous embedding methods (upper block) and of the discrete methods (lower block). N/A denotes the algorithms that did not ﬁnish within 24 hours or within 100GB memory. From the upper block of Table II, it can be observed that our method consistently outperforms other continuous network embedding methods, except lower than G2G by 0.41 percentages on the Cora ML dataset. However, on the large dataset cDBLP, ours exceeds G2G by 3.96 percentages. Note that our model obtains very close performance with SNEQ [26] in term of continuous embeddings. This is due to the fact that our modiﬁcation focuses on improving quantisation codes instead of network embeddings. Hence, we do not report the results of d-SNEQ on continuous embeddings on the other evaluation tasks. In the lower block of the table, it can be seen that our quantisation embedding is substantially superior to all the others, with at least 3∼4 percentage points better than the second best method BANE. The possible reason is that as the embedding dimension is relatively low, hash-based methods suffer larger loss on structure information without an additional schema (codebooks). In contrast, with the same dimension, the codewords of our quantisation are able to preserve more structural information. We can also observe another advantage of our method, that our model can handle large-scale networks such as cDBLP while some hashing methods, including DCF and BANE, cannot. This is because all of them depend on SVD, which incurs high memory usage. We test our method on the three attributed datasets: Citeseer, Cora ML, and DBLP. We use the one-vs-the-rest logistic regression as the classiﬁer, repeat the prediction for 10 times, and report the average of Macro-Fand Micro-Fresults. Figure 2 shows the results, where Figures 2 (a)–(c) are the results of the continuous embedding methods while (d)–(f) present the discrete results. In each experiment, a varying percentage (e.g. 2, 4, 6, 8, 10%) of labelled nodes are sampled for training the classiﬁer. From Figure 2, we can make the following observations. (1) From Figures 2(a)–(c), we can observe that our continuous embeddings are superior to the counterparts on all of the three datasets. Speciﬁcally, our method is about 1∼2 percentage points higher than the second best method G2G. H-GCN generally outperforms ONE since H-GCN performs a graph coarsening operation, grouping the nodes with same neighbours into one node. However, the node reﬁning operation in H-GCN highly depends on the initial network structure and the complexity or sparsity of networks. In contrast, our semantic margin module does not depend on the neighbourhood structure, and only considers the pairwise distance between different nodes, and thus can generalise better for more complex networks. (2) In the discrete embedding results, our quantisation shows a great advantage compared with the hashing-based methods. Speciﬁcally, our model exceeds the best hashing method BANE by about 4∼5 percentage points on all of the three datasets. The possible reason is that the short binary codes lead to large loss of information in hashing methods. Meanwhile, as node attributes are binary and can be regarded as hash codes, we also test the classiﬁcation performance of the raw attributes via a logistic regression classiﬁer, denoted as LR. It can be observed that LR exceeds many hash methods such as NetHash and DCF. This is because the dimensions reduce signiﬁcantly from ∼ 3, 000 to 64 bits and many meaningful features are abandoned. It is worth noting that SH is trained by our continuous embeddings. However it suffers a huge performance degradation, compared from the continuous embeddings, of nearly 30 percentage points. We conjecture that there are two main reasons: (1) the dimensions reduce to about one third, leading to signiﬁcant information loss; and (2) the procedure of learning to hash is not an end-to-end procedure, i.e., ﬁrst learning embedding vectors and then training the hash function. In contrast, our quantisation avoid these issues in two ways: (1) the codewords can preserve as much information as possible, even though the quantised code length are short; and (2) the learning to embedding and quantisation steps are uniﬁed in one network and jointly trained by the back propagation scheme. Though many methods incorporate the ability to handle higher-order proximity, they are not explicitly evaluated with a metric that is speciﬁcally designed to measure the preservation of higher-order proximity. Hence, we propose an extended link prediction task, dubbed path prediction, that is designed to not only measure whether there exists a connection between two nodes, but also to recognise which order it is between the two nodes, e.g., ﬁrst-order, seconder-order or even higher-orders. Formally, given two nodes, the path between them is deﬁned as the minimum hop length that connects them. For instance, if two nodes are immediate neighbours, the number of hops between them is 1 and their path is also 1. If two nodes are not immediate neighbours but are indirectly connected via another node, their path is 2. By that analogy, we can deﬁne any two nodes’ path by a adjacent matrix, denoted as P ∈ R. Note that if two nodes are disconnected, their path is deﬁned as inﬁnite. For evaluation purposes, we treat different path lengths as different categories and use the embeddings to train a classiﬁer for path prediction. Towards this goal, we treat the path prediction as a multiclass classiﬁcation problem. Theoretically, the more highorder proximity the embeddings are equipped with, the better performance the classiﬁer gains under the same training conﬁguration. In terms of experimental settings, we set the highorder range from the ﬁrst to the fourth, that is the classiﬁer is required to classify ﬁve classes, including the none class if two nodes do not have a path or their path length is greater than 5. Inspired by the relation recognition task in scene graph generation [61], [62], we use the multiplication of two embeddings, v∗v, as the fusion feature to train a one-vs-therest logistic regression classiﬁer under different training ratios of labelled points: 20%, 40%, 60%, and 80%. In addition, we randomly select 1, 000 pairs of nodes for each type of paths and report the average of Macro-Fand Micro-Fresults. Table III shows the comparison results of path prediction on the three attributed networks. Generally, we can obtain the following observations: (1) Our method consistently outperforms the other embedding models. Though G2G is able to preserve high-order information, ours is consistently better than it by about 2∼3 percentages in the three datasets. We deem that it is because G2G only employs a ﬁxed margin for graph structure preservation, leading to its inability to preserve more discriminative order information. By contrast, our model can learn different metrics for neighbours at different distances. (2) Furthermore, DW and TADW can also embed highorder proximity. However, both of them utilise a two-step strategy and thus are less effective than end-to-end models based on deep neural networks such as G2G and TEA. Similarly, d-SNEQexceeds the best discrete model BANE by approx. 3∼4 percentages, which demonstrates the advantage of quantisation over hash methods. (3) Although some methods, such as ONE and NetHash, perform well on the link prediction and node classiﬁcation tasks, their performance deteriorates substantially on the path prediction task. This result demonstrates that link prediction, the widely-used evaluation metric, cannot consistently reﬂect a model’s capability to preserve high-order proximity. For example, while ONE achieves competitive performance on the link prediction and node classiﬁcation tasks (see Table VI and Figure 2 respectively), it struggles on the path prediction task, especially on the Cora ML dataset. We believe this is because ONE only explores ﬁrst-order proximity for structure preservation, but not higher-order proximity. In contrast, methods with the capability of learning from high-order proximity, such as G2G and TEA, are superior to those that do not, e.g. ONE. Further, we test the path prediction performance (Fscore) of neighbourhoods of different orders (i.e. path lengths) on the Citeseer dataset, as shown in Figure 3, where “no” on the x-axis means path length is greater than 4 or no path exists between two nodes. It is worth noting that we randomly sample 80% of training samples as the training set and the remaining for testing. We can observe that some of the methods, such as DW and ONE, suffer a relatively larger performance decrease with the increasing orders of neighbours, as shown in Figure 3(a), possibly because the higher-order neighbours require more hierarchical information to be preserved, which these methods do not provide as they cannot learn the discriminative structural patterns for different types of neighbours. In contrast, our adaptive margin component can effectively model the hierarchical information by embedding differentiated shortest distances, and thus d-SNEQ does not suffer a large performance decrease on the higher-order neighbours. Moreover, we can observe a moderate performance climb for the “no” category for all methods. We conjecture the main reason is that the number of training samples for the “no” class is much greater than the others due to the fact that the class of “no” includes all the nodes with their path greater than 4 and those disconnected nodes, resulting in those models being sufﬁciently trained on this class. Similarly, with respect to the discrete models in Figure 3(b), d-SNEQalso gains the best performance with a 2∼3-point lift compared with SNEQ [26], the second best model. In the node recommendation task, given a node, an embedding model ranks all nodes according to a distance measurement and recommends the closest node. This task is widely applied in social and e-commerce networks, specially in the recommendation scenario. Following the settings of INHMF [1], 90% of neighbours of each node are used to train each embedding model while the remaining 10% neighbours are reserved for testing. We use NDCG@50 as the evaluation metric, and the ﬁnal results are averaged over 10 runs. Table IV shows the results of node recommendation, where we can obtain the following observations: (1) In terms of the continuous embedding methods, our method outperforms others on all the four datasets. Speciﬁcally, our method is superior to the most competitive methods H-GCN and G2G by about 2–3 percentage points. G2G adopts a rank loss to preserve neighbourhood information, but one difference from ours is that we learn adaptive margins for different neighbours: more distant neighbours will result in a larger margin, i.e., a larger penalty, while closer neighbours are separated by a relatively small margin. Hence, our method is more advantageous for the preservation of ﬁrst-order proximity. (2) In the second part of Table IV, our method shows even larger superiority with an average 4–9 percentage points higher than the best hash method NetHash. SH exhibits the worst performance, and the reason is that, as we stated above, hash codes are learned separately in a two-step way. It is worth noting that because of excessive memory usage by DCF and BANE, we did not obtain their results on the large network cDBLP. In this section, we will study the effectiveness of each component in our framework. Speciﬁcally, we will address the following four questions: 1) Does the adaptive margin improve the preservation of structural information compared with a ﬁxed margin? 2) Is the semi-supervised semantic margin effective for semantic embedding and what is the impact of different values of T ? 3) Can the rank loss `improve the performance of quantisation codes? 4) Is the differentiable module superior to our previous selfattention-based strategy in SNEQ? For the ﬁrst question, we modify the adaptive margin component to a number of different ﬁxed margins: 5, 50 and 100. The reason for the selection of the ﬁxed margins is that we ﬁnd the ﬁxed margin 50 shows the best result in our experiments and the margins close to 50 do not make a difference and hence we select two faraway margins 5 and 100 to represent a small margin and a large margin, respectively. Table V shows the link prediction results on Citeseer, Cora ML and cDBLP, where each row with FM denotes a setting of the ﬁxed margin and, Figure 4(a) shows the results of path prediction on the Citeseer dataset. Note that experimental settings are kept the same for all margins. From Table V, it is obvious to see that the adaptive margin outperforms the ﬁxed counterparts. In addition, we can observe that too large or too small a margin does not produce competitive results. The possible reason is that when the margin is too small, it is hard to separate nodes in the embedding space, but when it is too large, the loss function is more difﬁcult to converge. A problem of ﬁxed margins is that a node’s neighbours of different hops can be only coarsely divided by the same distance, losing discriminative neighbourhood information in the embeddings. In contrast, our adaptive margin can avoid the problem since it can learn different distances for neighbours at different hops, that is, hierarchical structure information. Speciﬁcally, those close to the anchor node are separated by a small margin while more distant neighbours by a large margin, which can more precisely model the original neighbourhood information. Figure 4(a) further demonstrates this observation, because our adaptive margin gains a higher improvement in terms of the path prediction task, which means that our model can preserve much more high-order or hierarchical structure information compared with different ﬁxed margins. The second question is addressed in Figure 4(b) and Table VI. Speciﬁcally, Figure 4(b) shows the effect of the semantic margin on node classiﬁcation on the Citeseer network. 0% stands for the model variant removing the semantic loss, i.e., no label information is provided to the model. We also evaluate the impact of different amounts of labelled nodes (T ) respect to all the nodes, i.e., 5%, 20% and 30%, used in the semantic margin loss. Noting that our full model has a T value of 10%. Obviously, with label information present, our model outperforms the variant with the supervision signal removed (0%). Also, it can be seen that a larger amount of supervision information results in improvements of node classiﬁcation performance. Link prediction results on the three datasets are shown in Table VI. Interestingly, in Table VI we can observe that as the value of T increases, link prediction performance deteriorates. From Figure 4(b) we have observed an opposite trend, that with the increase of T , node classiﬁcation performance improves. This is because as larger T brings more supervised information, it leads to more conﬂicts between the semantic margin and the adaptive margin, and thus negatively impacts the preservation of neighbourhood structure. Hence, to balance both tasks, we choose T =10% for our ﬁnal model. The third and fourth questions are addressed by Table VII, which shows the quantisation results of link prediction on the Citesser, Cora ML and DBLP datasets, where −`stands for the model removing the constraint of rank loss in Equation (10) and +SA means that we replace Eq.8 with our selfattention module in SNEQ [26]. With the rank loss removed, we can observe a performance dip of approx. 1–2 percentage points, which demonstrates `’s role in improving quantisation quality through equipping the model with structure information. Similarly, comparing +SA with the full model, a performance dip of about 1 percentage point occurs when we replace the Gumbel-softmax function with self-attention. However, it is worth noting that +SA still outperforms SNEQby one point, which conﬁrms that our DNN-based quantisation is better than non-deep neural network quantisation in SNEQ [26]. A main advantage of quantisation is the reduction in storage cost and retrieval time. We compare the efﬁciency of the continuous and discrete embeddings and we choose SH as the representative hash method. The space cost results are shown in Table VIII, where the ﬁrst row, Float, denotes the storage cost of continuous embeddings with dimension L = 128. The second row is the result of hash codes generated by SH with the same code length 64 bits, same as ours. The space and time efﬁciency results are shown in Table VIII and IX respectively. Empirically, it can be observed in Table VIII that, the larger the network, the more reduction in storage quantisation brings: for cDBLP, the original embeddings are approx. 50 times larger than ours. Compared to hashing methods such as SH, it can be observed that quantisation uses slightly more space as it stores M codebooks. However, the evidence in the previous three subsections demonstrates hash methods’ substantially inferior task performance to quantisation. In fact, the additional storage cost of quantisation over hashing is the codebooks, at approx. 512KB for ours. Therefore, for larger networks, hashing methods’ savings in storage are minuscule and negligible given modern hardware. Table IX shows the average retrieval time of node recommendation in milliseconds. For real-valued embeddings, we use the Euclidean distance to measure node similarity, while for quantised codes we use a pre-calculated table to look up their similarity, same as for our method. Quantisation achieves up to 70 × retrieval speedup over the real-valued embeddings. Hashing methods, represented by SH, are more time-efﬁcient than our model. However, as we noted above, this is achieved with substantially degraded task performance. Network embedding learns ﬁxed- and low-dimensional vector representations for nodes in a network, aiming at preserving node attributes and the structure information in the network. With the growth in network sizes, space- and time-efﬁcient embedding becomes an increasingly challenging problem. In this paper, we propose d-SNEQ, an end-to-end network embedding and quantisation framework. d-SNEQ is trained in a semi-supervised manner, which simultaneously preserves network structure and semantic information into the embedding space while compressing the embeddings by self-attentionbased product quantisation. Speciﬁcally, we incorporate a adaptive margin loss for preserving network structure information, a semantic margin loss for semantic space learning, and a DNN-based quantisation loss to learn compact codes. Moreover, we propose a new evaluation metric, path prediction to better and more directly evaluate a model’s ability to preserver higher-order proximity. In standard evaluation tasks on four diverse networks, our method outperforms state-ofthe-art network embedding methods. In addition, due to quantisation, d-SNEQ signiﬁcantly reduces storage footprint and accelerates query time for node recommendation. Compared to hashing methods, d-SNEQ achieves substantially superior task performance while maintaining comparable space and time efﬁciency.