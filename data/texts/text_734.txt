Keywords distributed training · adaptive · elastic · heterogeneous · neural networks Distributed training with multiple or even thousands of machines has already become a pervasive and effective approach to train large neural network (NN) models with massive data, which can signiﬁcantly improve their generalization ability and prediction accuracy [ into the framework design and implementation. First, models from various applications may need disparate parallel strategies based on their own characteristics. For example, a typical recommendation model usually consists of a large sparse distributed embedding lookup layer and several dense fully connected layers, where the former incurs high data access cost but the latter is computationally intensive. Therefore the parallel strategies for the recommendation models should be specially treated compared to other models. Second, the distributed training for one model may require a different parallel strategy when targeting on a new architecture cluster with different devices or topology. This situation Both authors contributed equally to this research. Distributed training has become a pervasive and effective approach for training a large neural network (NN) model with processing massive data. However, it is very challenging to satisfy requirements from various NN models, diverse computing resources, and their dynamic changes during a training job. In this study, we design our distributed training framework in a systematic end-to-end view to provide the built-in adaptive ability for different scenarios, especially for industrial applications and production environments, by fully considering resource allocation, model partition, task placement, and distributed execution. Based on the uniﬁed distributed graph and the uniﬁed cluster object, our adaptive framework is equipped with a global cost model and a global planner, which can enable arbitrary parallelism, resource-aware placement, multi-mode execution, fault-tolerant, and elastic distributed training. The experiments demonstrate that our framework can satisfy various requirements from the diversity of applications and the heterogeneity of resources with highly competitive performance. The ERNIE language model with 260 billion parameters is efﬁciently trained on thousands of AI processors with 91.7% weak scalability. The throughput of the model from the recommender system by employing the heterogeneous pipeline asynchronous execution can be increased up to 2.1 times and 3.3 times that of the GPU-only and CPU-only training respectively. Moreover, the fault-tolerant and elastic distributed training have been successfully applied to the online industrial applications, which give a reduction of 34.49% in the number of failed long-term training jobs and an increase of33.91%for the global scheduling efﬁciency in the production environment. becomes more common since the available computing resources for users vary widely and even an organization usually has different types or generations of training devices. Third, there exists a strong interdependence between the model and the underlying resources in the context of distributed training. Speciﬁcally, the design of an efﬁcient parallel strategy for one model should consider the topology and capability of used resources while the dynamic change of used resources during a training job may require a better new parallel strategy for the same model. There is no doubt that a single parallel strategy can not ﬁt into all the mentioned situations and it is also very challenging to manually design and implement individual strategies for all of them. So a good distributed training framework should address the following problem and provide the built-in adaptive ability for different scenarios: how to adjust the parallel strategy automatically to achieve efﬁcient distributed training for various NN models, diverse computing resources, and even the dynamic change of the used resources during a training job? Unfortunately, most of the existing studies have limited adaptive ability. First, lots of parallel strategies have been proposed to partition the computation of NN models [ effective for speciﬁc models and need to be re-implemented manually for others. Although automatic parallelization of them do not deal well with the heterogeneity of computing resources within a cluster or across clusters. Second, the device placement for the resulted partitioned tasks are often dealt with manually based on some heuristics from practice or the careful proﬁling results, so as the execution arrangement of the forward, backward and update computation. Even worse, there is almost no choice for users to select the appropriate distributed execution mechanism for their own workloads. Some studies [ best placement of execution policy, but they either only support very little parallelism or may fail to evaluate strategies more accurately with little consideration about the underlying hardware and the distributed execution runtime. Third, the existing frameworks usually delegate the resource management to external modules [ insufﬁcient awareness about it to make an adaptive and efﬁcient distributed training when the resources change during a training job, especially for complicated parallelism [ essential steps involved in the end-to-end distributed training process including resource allocation, model partition, task placement, and distributed execution, as shown in Figure 1. Figure 1: The essential steps of distributed training: 3assign each partition to a speciﬁed device, communication. In this study, we design our distributed framework from a systematic end-to-end view to satisfy the mentioned versatile adaptive requirements by considering all the essential steps of the distributed training in Figure 1. In the ﬁrst place, a uniﬁed distributed graph is employed to represent arbitrary parallelism including all the existing parallelism and a uniﬁed cluster object is used to describe homogeneous and heterogeneous resources within a cluster or across clusters. Based on these two uniﬁed representations for parallelism and resources, a global cost model is developed to evaluate the cost of a distributed graph training on a speciﬁc cluster. Driven by the global cost model, we can utilize a global planner to help us automatically choose better parallel strategies including partition and placement decisions according to the characteristics of the given NN model and the cluster. To further give users more options for the distributed execution, a new asynchronous executor based on the actor model [ synchronous executor for collective communication and the asynchronous push-pull executor for parameter-server (PS) communication. Moreover, our framework has native support to realize the fault-tolerant and elastic distributed training ,23,24] has been employed to minimize the efforts of developing parallelism for various NN models, most in case of dynamic resource change for large-scale and long-time industrial scenarios. Besides, we try to decouple these functions as many as possible and give advanced users the maximum ﬂexibility to select or conﬁgure the functions for their own sake. The main contributions are listed as follows: • The fault-tolerant and elastic distributed training are achieved through better built-in interaction between our Our adaptive distributed training framework is evaluated by various NN models on clusters of different architectures. The results show that our framework can train the popular GPT model with 146 billion parameters on 512 V100 GPUs and sustain efﬁciently trained on thousands of AI processors with 91.7% weak scalability. In addition to these two NLP models, the large-scale image classiﬁcation for face recognition can train 60 million classes on a single node with 8 NVIDIA V100 GPUs and offer competitive or better performance than other frameworks. We also test the two classic models from the recommender system by employing the heterogeneous pipeline asynchronous execution and can obtain up to 2.1 times and 3.3 times the throughput of the GPU-only (only using GPUs of the servers ) and CPU-only (only using the CPU servers) training respectively, which can be further improved by automatic partition based on the cost model. Finally, the fault-tolerant and elastic distributed training can give a reduction of 34.49% of failed long-term training jobs and an increase of 33.91% for the global scheduling efﬁciency in the production environment. In this study, three basic design principles are employed to handle the complexity of our framework to achieve adaptive and efﬁcient distributed training for requirements from different scenarios. First, the two uniﬁed representations for arbitrary parallelism and diverse resources are the core abstraction among all other implementations and optimizations. Next, different modules to implement the steps as mentioned in Section 1 are decoupled as many as possible to reach maximum ﬂexibility. Finally, the global end-to-end view for better overall performance is achieved by employing the global cost model and global planner. The architecture overview of our framework based on the design principles is shown in Figure 2. As we have pointed out in Section 1, extensive research has been done to parallelize the computation of NN models. To deal with all the existing parallelism and other parallelism in the future, we build a general and uniﬁed distributed graph representation, which can describe arbitrary parallelism theoretically. The distributed graph can be seen as an enhancement to the traditional computational graph by adding more complete parallelism semantics. One NN model can have different distributed graph representations, each of which encodes a unique parallel strategy but gives the same semantics as the serial computation. Ideally, the different distributed graphs only exert inﬂuence on the training performance without affecting the computation results. For now, most of the existing studies mainly focus on clusters with homogeneous devices or heterogeneous devices within a cluster. However, the available computation resources from an organization are usually much more heterogeneous and may spread across different clusters with different types or generations of devices. In addition, some special-purpose accelerators for deep learning are employed to achieve signiﬁcant performance improvements and power savings. These diverse heterogeneous resources can be represented by an abstract and uniﬁed cluster object in our framework, which is globally visible to all involved processes. Different devices are modeled in the same way and can be distinguished by their types and generations. Additionally, the cluster object can not only give the topology of all its connected devices but also abstract the ability of computation, storage, and communication in a uniﬁed quantitative approach. An improved distributed graph based on the traditional computational graph is employed by adopting three basic concepts: distributed tensor, distributed operator, and reshard transformation, which can represent arbitrary parallelism including all the existing parallelism. A uniﬁed cluster object is further built to represent the diverse resources for isolating the difference between them within a cluster or across clusters. Based on the two uniﬁed representations, a global cost model and a global planner are developed to select better resource-ware parallel strategies automatically. A new distributed asynchronous executor based on the actor model is added to enrich our existing distributed executors, which can automatically overlap the computation and communication as much as possible and support different granularity and complex control ﬂows. distributed training framework and the platform scheduler, which can improve the overall resource utilization and make the fault-tolerant and elastic training of complicated parallelism possible. 48.65%of the theoretical peak FLOPS. The ERNIE language model [38] with 260 billion parameters is To have the versatile adaptive ability, the modules of the distributed training framework should be decoupled and can be conﬁgurable or changeable based on the different requirements. First, the distributed speciﬁcation (i.e. the distributed graph) for a NN model is decoupled from the underlying implementation inspired by the work [ can conﬁgure how to partition the data and task since there is no universal partition policy for all NN models and they are familiar with the NN models they built. But it is the responsibility of the framework to provide necessary implementation mechanisms for performing the actual partition of NN models speciﬁed by users and automatically handle the necessary data movement and communication. So users can develop new partition strategies quickly without worrying about the tedious and error-prone implementation. And it is also very easy for us to improve and extend the distributed runtime mechanism while keeping the results consistent. In particular, the device-independent partition can be decoupled from the device-dependent placement. This can enable a speciﬁc partition to have a different placement policy not only on the same cluster but also a new one with different topology and devices. It can be observed that different placements for a partition, even on the same cluster, can exert a non-ignoble impact in terms of performance. For example, users can change the partition and placement for a better performance of the recommendation models based on their special model characteristics as mentioned in Section 1 and the given cluster. Unfortunately, only experts who know well about hardware architecture and parallel programming can make a good decision about the placement for a partition strategy. By decoupling these two aspects, most users can easily partition the NN model without considering the underlying hardware, while advanced users who care about optimal performance can replace the default placement policy with a tuned one. To achieve the maximum ﬂexibility needs to decouple the modules as many as possible, which is more likely to lose the global view and may lead to poor performance overall. However, it must be pointed out that the decoupling here is just a mechanism to realize the separation of concerned modules and should not hinder global optimization. Every module in our system is open to be conﬁgured or even replaced as long as users conform to the interface between different modules. This means users have full control over the system and can carry out application-aware or hardware-aware implementations or both of them to achieve a high-level performance depending on their knowledge of the application and the used resources. As shown in Figure 2, our distributed framework consists of modules covering all of the essential steps mentioned in Section 1 to achieve an end-to-end global view. While the most of existing studies focus on only parts of them, our work tries to provide a comprehensive solution by taking them all into consideration. A global cost model is employed to estimate the required resources or evaluation of a speciﬁc placement to help us choose better policies. Moreover, driven by the cost model, a global planner is implemented to automatically ﬁnd better policies. The uniﬁed distributed graph representation contains information about the partition and placement, and the abstract cluster object is also shared by the allocation and placement. Besides, the efﬁcient elastic and fault-tolerant training is co-designed with other modules within our framework to satisfy the requirements for long-scale and long-time distributed training in real industrial scenarios. This section details most of the modules of our distributed framework as shown in Figure 2. First, an improved distributed graph is constructed to represent arbitrary parallelism in a uniﬁed way. Then a cluster object is built to represent the diverse heterogeneous resources and a global cost model based on the two uniﬁed representations are developed to help us achieve resource-ware placement. After that, the multi-mode distributed execution including the new fully distributed asynchronous executor based the actor model is implemented to give users more options based on their own workloads. Finally, the fault-tolerant and elastic training is natively supported to adapt to the dynamic change of resources by better interaction between our distributed training framework and the platform scheduler. Figure 3: The transformation from the serial computational graph to the distributed computational graph based on the distribute attributes in Table 1 and Table 2. The produced tensors and the consumed requirements of the following operator are the same. To support all the existing parallelism and further explore potential ones, we reconsider the process of designing parallelism from a ﬁne-grained perspective. It is well known that the training process of a neural network can be described as a computational graph, where the computation at vertices are referred to as operators and the data that ﬂows along edges is referred to as tensors. The basic idea is that the whole neural network’s parallelism can be determined if the partition of each operator and tensor of it can be decided. To go a step further, all kinds of parallelism can be uniﬁed if the partition of operators and tensors can be described in a uniﬁed way. Fortunately, Some studies [ have already adopted this ﬁne-grained idea to achieve automatic parallelization. Based on these previous studies, we reformulate the traditional computational graph into a general distributed graph by introducing more parallelism semantics, which combines device-independent partition and device-dependent placement together. It is built on three basic concepts: distributed tensor, distributed operator, and reshard transformation. Figure 3 illustrates a complete example of how to transform the serial computational graph to our distributed graph by using these concepts. Each serial tensor in the original computational graph has one corresponding distributed tensor in the distributed graph, which is constructed based on its distributed attributes. It is worth emphasizing that the attributes here can represent an arbitrary partition of one tensor independent of the underlying hardware. As shown in Table 1, the attributes give complete information about how to partition and place each shard of a tensor. The partition information is speciﬁed by the anddims_mapping instead of the process_mesh the size of each shard along each dimension of the original tensor to enable more general parallelism such as the uneven parallelization. The last attribute shard. Like serial tensor, each serial operator also has a matched distributed operator. The behavior of a distributed operator is deﬁned by the distributed attributes of its input and output tensors. The actual local computation of one distributed operator in each process uses the local operator and the local shards of its inputs and outputs belonging to that process. Although using the same representation, the distributed attributes for each input and output tensor within the operator only specify the requirement, which can be different from the distributed attributes of the actual tensor. This decoupling can support more advanced scenarios such as the operator being partitioned on some devices while the used tensors can be on others. Note that the operator. The distributed tensor and operator across devices must preserve the semantics of their corresponding serial counterparts in a logical view. This enforcement will require necessary communications (including data movement) to be inserted properly in the distributed graph, which can be classiﬁed into intra-operator communication and inter-operator communication. The intra-communications are taken care of by the internal implementations of distributed operators to conform its serial semantic. The intra-communication is the responsibility of the reshard transformation to deal with the situation when a mismatch occurs between the required distributed tensor of a distributed operator and the actual distributed tensor. To put the above three concepts together, it can be concluded that the reformulated distributed graph can deal with arbitrary parallelism. Based on it, we can also apply optimization passes to further improve the training performance. Moreover, the distributed graph representation makes us easy to provide different parallelization modes for users to satisfy their personalized requirements: some users can annotate the distributed attributes of selected tensors or process_mesh,dims_mapping,shard_sizes. The ﬁrst two are very similar to thedevice_mesh device_meshto decouple the logical partition from the physical placement and each process of the owns one shard of the original tensor. Another extra attributeshard_sizesis introduced to specify operators and the rest ones will be automatically ﬁlled by our framework like [ automatically to ﬁnd the best distribute attributes for all tensors and operators. Figure 4: The distributed cost model based on the operator cost and the runtime simulation. Note that each tuple of the lists in the left panel is the static information required for querying (or ﬁtting) the computation operator cost from the performance database or computing the communication operator cost in an analytical way The information includes the operator type, the shapes of the inputs and outputs, the data types, the target device and the related processes, etc. As described in Section 2.2, a partition of one NN model can have multiple placements onto the same cluster and each of them may reveal different performance. Our system can realize a resource-ware placement to achieve a high-level performance by employing two basic mechanisms. The ﬁrst one is a cluster object to abstract the real underlying cluster for distributed training, which contains not only its topology information but also the quantitative measurements of its resources. The other mechanism we built is an accurate global cost model to evaluate the cost of a distributed graph with a speciﬁc device placement based on the cluster object. Users can make a good decision to realize the resource-ware placement by utilizing these two mechanisms. The challenge of constructing an abstract cluster object is that there are usually hundreds of machines connected through different topology networks from different clusters. This becomes more complicated when we want to train a NN model across clusters, especially with heterogeneous devices. Our cluster object is made up of some machine objects and each of these machines contains different components which can stand for any devices such as CPUs, GPUs, and link devices. This design allows a uniﬁed representation of the used clusters in different scenarios. Besides, we also store some quantitative measurements about the ability of computation, storage capacity, and communication inside a component object. For example, the computation can be measured by FLOPS while the memories capacity is also stored. The topology of the cluster is represented by a sparse adjacent matrix in a ﬂattened way without distinguishing components in a machine or across machines. Each row or column of the matrix represents a component. The sparse matrix has an element to record the bandwidth and latency between two components if there is a connection between them. In addition to the abstract cluster object, a global cost model is also developed, which can present the peak memory consumption and execution time at the same time. The overall distributed graph cost is reduced based on the operator cost and the execution simulation as illustrated in Figure 4. The computation operator cost is queried or ﬁtted from the benchmarking performance database without adopting an analytical way since not only the shape of the inputs and outputs but also the kernel implementation and hardware architecture can affect the cost and they are often intertwined with each other intricately. However, to benchmark the communication operator cost in a large scale, especially in complicated parallelism is impracticable because it needs to exclusively occupy all the resources for a long time. So the communication operators in our cost model are estimated analytically, which depend on not only the number of processes and message size but also the link topology and the utilization of these linkages from the built cluster object. After obtaining the cost of operators, the distributed graph cost can be inferred by applying the reduction rules to our distributed graph, which are similar to the studies [ like computation including the collective and peer-to-peer ones while the previous studies usually use the edge cost to represent them. As a result, these communication operators can be merged before applying the reduction. This merging process can easily retain the dependency among different ranks within the distributed graph and support more parallelism such as the intra-layer model parallelism. After merging communication operators, the key path will be identiﬁed by eliminating the branches and the ﬁnal cost is the sum of all operators along that linear key path. Moreover, the execution order and memory management from the underlying framework runtime are also simulated in our cost model to make it more accurate since they have a strong impact on the peak memory consumption and execution time. It is the responsibility of the executor in a deep learning framework to efﬁciently schedule and execute tasks on devices. To satisfy the different requirements, our framework has already implemented the synchronous executor for collective communication and the asynchronous push-pull executor for PS communication. Recently, many studies [ have shown that the scheduling order of forward and backward computation has a relatively large impact on both performance and memory utilization. In particular, it is extremely important to overlap computation and communication as much as possible for distributed training to achieve high-level performance. Users have to manually arrange the scheduling order of the forward and backward computation to maximize the parallel degree and the overlapping of computation and communication. Fortunately, the actor model [ century can automatically overlap the computation and communication as much as possible and inherently avoid race conditions. It has been ﬁrst introduced by OneFlow [ framework. To enrich our existing distributed executors, we also implement our own actor-like executor which can support ﬁne-grained task granularity and complex control ﬂows. With the help of our framework, users can select an appropriate executor based on the workload of their NN models and even can combine these executors together by exploring the characteristics of the different parts of one model. The new actor-like executor also uses actors as the basic process units and each actor works in a physical thread and is responsible to execute the tasks assigned to it. And all actors are always residing in the runtime until the end of the training. Meanwhile, they are busy receiving messages from upstream actors, scheduling ready tasks to their corresponding devices, and sending messages to downstream actors. It is often straightforward and easy to use one granularity to construct tasks such as each operator corresponding to a task. However, it is not suitable to use one granularity for all actors residing in different heterogeneous devices since the computation capabilities of these devices may vary widely. Our executor can bring users the ﬂexibility to assemble one operator or multiple operators as a task based on the device’s ability. It is also very convenient for us to deal with the complex control ﬂow such as loop and condition since we can make the operators within these control ﬂows into one big task. In particular, we also applied heterogeneous pipeline asynchronous execution to the industrial recommender system, e.g. training a large-scale CTR prediction, for using the heterogeneous computing resources efﬁciently. As mentioned in Section 1, it is well known that a typical recommendation model usually contains one part which incurs high data access cost to convert massive high-dimensional sparse data into dense features, and the other part which is computationally intensive. So we can partition the model into different pipeline stages to enable efﬁcient distributed training based on the characteristics of these layers, each of which may be a block of operators and serves as a task for the distributed execution. These tasks are assigned to heterogeneous devices for better performance. Some of them are executed by the asynchronous push-pull executor while the others use the synchronized executor. The partition strategy for the model and the used devices for each task can be conﬁgured by users as described in Section 2.1. The cost model mentioned in Section 3.2 can also be leveraged to guide the selection of the partition, and the device allocation policy to further improve the overall throughput. The limited awareness between the distributed training framework and the platform scheduler impedes the full exploration of the resource utilization in a large-scale and long-time training, especially on a multi-tenant industrial platform. To achieve efﬁcient and robust adaptive training, more interactions will be needed between the distributed training framework and the platform. Based on this idea, we provide the built-in support for fault-tolerant and elastic training, which is illustrated by Figure 5. The job scheduler from the platform usually tries its best to allocate resources in a more aggregated way when a resource request arrives. However, lots of fragments will also be produced if jobs with various occupancies are collocated in the same node over time. With the help of the fault-tolerant and elastic training from our framework, the job migration can be automatically and efﬁciently performed if the current available resources do not meet the desired one. In addition, a preemption operation will be more efﬁcient for other jobs with a high priority of resources allocation, which makes some of the low-priority jobs stop early and these jobs will be resumed later for better resource utilization. The basic fault-tolerant training can be implemented by reloading the recently saved checkpoint when there occurs a failure. However, we should make a trade-off between beneﬁt and cost about how to save and reload a checkpoint to achieve efﬁcient fault tolerance. In general, a checkpoint includes two parts, the weights of the model and the states of the training progress and hype- parameters, etc. Here we adopt a three-level checkpoint strategy to reduce the global overhead: 1) a save-before-exit hook triggered before shutting down when an exception is caught internally or a job is intended to restart manually; 2) a fast in-memory backup on each device for a fast partially recovery where the saving interval is calculated by the beneﬁt-cost ratio; 3) a snapshot of weights and states in the persistent storage taken by all devices with a larger interval, which can provide both partial and full recover capability with a much lower cost. Figure 5: Different cases supported by the fault-tolerant and elastic training: a) the original training with four devices; b) the fault-tolerant training replaces the failed device D2 with D4 without re-partitioning the graph; c) the elastic training releases the devices D2 and D3 with re-partitioning the graph; d) the elastic training with adding the devices D4 and D5 with re-partitioning the graph. This strategy is also optimized in several ways. First of all, the states are saved at a high availability key-value backend for fast sharing among all devices. Second, the checkpoint process takes place asynchronously for higher efﬁciency. Furthermore, each device uses an adaptive strategy based on the parallelization to reduce unnecessary saving and loading, especially in complicated parallelism. For example, the devices working as the data parallel role can save their weights in turn alternatively and the devices working as the model parallel role only save their own weights. The elastic training can make better utilization of resources for a multi-tenant platform. However, existing distributed frameworks provide limited support to realize the elastic training for complicated parallelism. Thanks to the decoupling of our uniﬁed parallelization and resource-aware placement, one serial graph can have different distributed graphs adapting to the available resources. For convenience, we deﬁne a distributed graph with speciﬁed resources as a scheme. Before training, the granularity of the change level about resources which will automatically trigger the elastic training needs to be conﬁgured. In the beginning, the distributed training job employs the initial scheme based on the available resources. A scaling up is performed if the next level of more resources for a candidate scheme is satisﬁed, while a scaling down is performed when the platform recycles resources of the current job to other jobs with high priority. In brief, the scaling in our elastic training between different schemes works in a discontinuous way gradually. Note that the hyperparameters such as learning rate and batch size may be coordinated to avoid causing harm to the model accuracy. And the saved weights may also need to be automatically converted when changing from one scheme to another especially using different parallelism. In this section, we try to give a complete evaluation of our adaptive distributed framework using experiments of the various NN models from different applications. First, the general parallelization is tested on GPT, ERNIE [ the classiﬁcation task for face recognition on large scale. Next, an MLP model and a multi-view DNN model for recommendation system by using heterogeneous pipeline asynchronous execution are compared with the CPU-only and the GPU-only training respectively. And these two models are further used to show the ability of our cost model. Last, the fault-tolerant and elastic training are evaluated for long-time distributed training in the production environment. GPT results on NVIDIA GPUs GPUs (32 GB) by combining data parallelism (DP), intra-model parallelism (MP) and pipeline model parallelism (PP). The results of the throughput with different conﬁgurations are shown in Table 3. From the table, it can be seen that our system can achieve a minimum of 44.61% of the theoretical peak FLOPS in all conﬁgurations. And the model with 146 billion parameters in the table can be trained by 512 V100 GPUs and sustains 48.65% of the theoretical peak FLOPS. ERNIE results on NPUs from Peng Cheng Laboratory model ERNIE [ transformer layers of ERNIE have smaller shapes, which results in a big performance difference between GPU and NPU. Fortunately, by conducting the resource-ware training, our framework takes on a parallel strategy different from GPU architecture, which can give from 1568 NPUs to 1920 NPUs as shown in Table 4. And all parallel strategies can achieve good weak scalability of throughput when we increase the DP degree and ﬁx the MP and PP degrees. Finally, ERNIE can be scaled up to 260 billion parameters on 1920 NPUs with 91.7% weak scalability by using the resource-ware conﬁguration. Table 4: Comparison between the two different parallel strategies when training ERNIE on NPUs. The one is the same as GPUs while the other one is resource-aware for NPUs. Image Classiﬁcation results on NVIDIA GPUs recognition [ with 8 NVIDIA V100 GPUs. The results on V100 and A100 GPUs are shown in Figure 6, where the backbone is ResNet50. In these tests, the batch size is 128 per process, the sample ratio is 0.1, and the number of classes is 93431. We can see that not only the results of FP32 and FP16 on V100 but also those on A100 deliver stronger performance than other frameworks. In particular, in the case of FP16 on A100, our framework can achieve about two times speedup compared to others at most. This experiment below is used to show the advantage of our heterogeneous pipeline asynchronous execution (Heter), especially for the recommendation models as described in Section 3.3. A MLP model and a multi-view DNN model for the click through rate (CTR) prediction problem [42] are trained by using an open dataset Criteo. We compare the costs of the GPU-only training, CPU-only training, Heter training conﬁgured with the cost model, and Heter training conﬁgured manually, where the costs are used for training in terms of the resource prices from our cloud platform. The manual Heter training partitions these models into two stages with an embedding layer in the ﬁrst stage and the remaining parameters in the second stage while the partition strategy searched by the cost model further splits the last 38] is trained on NPUs on a large scale by employing the hybrid parallelism. Unlike GPT, the last 41] also uses the hybrid parallelism of our framework and can support 60 million classes on a single node several fully connected layers. As shown in Figure 7, the partition strategies searched by the cost model for both models have a lower cost than the CPU-only, GPU-only, and manually conﬁgured Heter training. We also change the batch size and dense layers’ size to prove the advantage of the Heter training. Here we try to compare it with the GPU-only training and the CPU-only training where the former uses one GPU server with 8 NVIDIA V100 GPUs and the latter uses 10 CPU servers, where the price of one GPU server is roughly equal to the price of 10 CPU servers. The Heter partition the models into two stages with an embedding layer in the ﬁrst stage and dense layers in the second stage, where the former uses 5 CPU machines and the latter uses 4 GPUs (the price of those resources are equal to these of the GPU-only training and CPU-only training). First, We ﬁx dense layers’ size and increase the batch size to compare the training throughputs of the three training methods. Figure 8 shows that all the throughputs of the three methods are growing with the increase of the batch size, and the Heter training shows higher throughput than others in all batch sizes. When the batch size is 512, the Heter can achieve 2.1 times and 3.3 times the throughput of the GPU-only and the CPU-only training for the MLP model, while the Heter training can achieve 1.3 times and 1.4 times for the multi-view DNN model. Then the batch size will be ﬁxed to 512 and the throughputs are compared by using different dense layers’ sizes in the MLP model. As shown in Figure 9, the throughputs of all three types of training are decreasing when increasing the dense model size. However, the Heter has an apparent advantage Figure 8: Throughput comparison with different batch sizes in the MLP (left) and multi-view DNN model (right). because the communication can be well overlapped with the computation, which can give 2.2 times the throughput of the GPU-only training and 2.5 times the throughput of the CPU-only training at most. Without fault tolerance, all the resources that a job holds before being terminated will be wasted in case of failure. We estimate the fault-tolerant training of our framework under our production environment for one month. The results shows it can reduce queuing time. It is very hard for traditional job schedulers to make an optimal scheduling decision since they have little information about the training workload. With the help of our system, the job migration can be also easily conducted when it is suitable as mentioned in Section 3.4. It can improve the utilization efﬁciency by the production environment for a day long. In practice, most jobs are delayed for several minutes after being submitted even without restricting the resources occupation, while they can be scheduled immediately by performing the job migration (the line of optimized utilization), which can improve the efﬁciency of the global job scheduling. More speciﬁcally, as shown in the rectangle zone of Figure 10, the utilization rate is improved by applying the job migration so that enough resources are available for an immediate scheduling. In contrast, you may note that in some periods such Figure 9: Throughput comparison with different dense parameter sizes in the MLP model. as the center of the ﬁgure, the optimized utilization is lower than the real one, it also makes sense because the original queued jobs are consumed earlier which can result in a higher global throughput. Different Parallelism. large-scale distributed training. Horovod [ AllReduce And this idea is extended by ZeRO [ following ZeRO-Inﬁnity [ of GPUs. Megatron [ some layers, and Mesh-TensorFlow [ specifying parallelization strategies. Pipeline model parallelism is another common parallelization by splitting large models into different stages. GPipe [ without changing the strict synchronous optimizer semantics. PipeDream [ relax the synchronization by maintaining different versions of weights and updating stale weights in an asynchronous way to improve the pipeline efﬁciency. TeraPipe [ auto-regressive models. PipeTransformer [ some layers and allocating resources for the remaining active layers. Deepspeed [ parallelism, intra-layer model parallelism, and inter-layer pipeline parallelism to train extremely large models and the latter is optimized to give better performance. The uniﬁed distributed graph adopted by our framework can represent all the mentioned parallelism and can also support other parallelism in the future. Automatic Parallelization. can automatically search a fast scheme for a speciﬁc cluster by using an execution simulator to accurately predict the performance for a scheme. PipeDream [ parallelism conﬁgurations and the cost models of them are analytical methods based on proﬁling results. Dapple [29] further improves PipeDream’s planner by allocating different numbers of devices to each pipeline stage. Pesto [25] can fast ﬁnd the optimal placement and scheduling by formulating the problem as an integer program for model parallelism. Instead of adopting an analytical way, Mirhoseini et al. [ to automatically decide the placement for each operator of the computational graph. Autosync [ optimize synchronization strategies to lower the bar for data parallelism based on a model- and resource-dependent representation. With regard to operator cost, TVM [ implementations. Besides, the deep learning frameworks [ NN models to minimize efforts of developing complex parallelism. Our framework can also automatically parallelize NN models for arbitrary parallelism based on the uniﬁed distributed graph, and our search space is larger according to implementation. The sharded data parallelism [43] can shard the weight update computation across replicas. the distributed attributes, which combine the partition and placement information together. And the global cost model can synthesize the analytical and machine-learning methods as well as the runtime information at the same time. Fault tolerant and elastic training. clusters and in cloud environment have been studied in some works. Wu et al. [ layer between the cluster scheduler and the deep learning framework to enable elasticity with a simple API. Ma et al. [48] adopt the mixed-integer programming model to maximize the training progress in real a production environment. Saxena et al. [ in real-time. Hu et al. [ the volatility of parameters in the model and utilizing incomplete local updates. KungFu [ by Mai et al. introduces the ability to synchronize hyperparameters in order to make a distributed machine learning adaptive. Besides, several cluster schedulers are proposed in consideration of the NN workload. Gandiva [ the predictability of iterations in NN training to time-slice GPUs efﬁciently across multiple jobs, thereby delivering low-latency and improving cluster efﬁciency. GAI [ preempt resources occupied by other lower priority jobs. DeepSys [ to provide efﬁcient job scheduling based on a speed model and memory model. Pollux [ performance by adaptively co-optimizing inter-dependent factors both at the per-job level and at the cluster-wide level. In this study, we try to provide native support in our framework for better interaction with the cluster scheduler to achieve adaptive training. This study shows that our adaptive distributed training framework designed in the global end-to-end view can satisfy various requirements from the diversity of applications and the heterogeneity of resources and give a competitive and high-level performance. In the future, we will explore and support the adaptive requirement from the dynamic change of the NN model during a training job. The uniﬁed distributed graph will be improved to become a distributed intermediate representation, and the current manual optimization passes may be re-implemented by utilizing the existing compiler techniques. For now, the communication is implemented by selecting collective or peer-to-peer primitives from different backend libraries and can be also uniﬁed in the same interface, which can assure callers of the transparent use despite the underlying cluster. Based on the uniﬁed communication, it is much easier for the distributed training to explore the available resources from different organizations. Besides, we will further enhance our framework by collaborating more with the resource platform to achieve intelligent scheduling and transparent resource allocation. Part of this work is supported by Peng Cheng Cloud Brain II NPU Cluster in Peng Cheng Laboratory. We thank our colleagues who provided insight and expertise that greatly assisted the research. We would also like to express our gratitude to the colleagues for their comments that greatly improved the manuscript. 49] formulate the batch size selecting problem into a fast dynamic programming problem and solves it