School of Software Engineering, Xi’anCollege of Articial Intelligence, Figure 1: Screenshots of GeneAnnotator. (a) The user interface. The relationship between the subject, i.e. the red-masked car, and the object, i.e. the blue-masked car in the image region is dened by a directed graph, and described by editing or selecting from the recommended candidate relationships listed on the GUI. (b) To avoid trivial and irrelevant relationships, a “region” of interest, i.e. the yellow bounding box, could be set during the annotation. (c) Nearby cars can be annotated as a “cluster” in GeneAnnotator, sharing the same relationship with other cars or roads. (d) The real-time visualized scene graph according to annotations in (a). In this manuscript, we introduce a semi-automatic scene graph annotation tool for images, the GeneAnnotator. This software allows human annotators to describe the existing relationships between participators in the visual scene in the form of directed graphs, College of Articial Intelligence, Xi’an Jiaotong University liuyh@mail.xjtu.edu.cn hence enabling the learning and reasoning on visual relationships, e.g., image captioning, VQA and scene graph generation, etc. The annotations for certain image datasets could either be merged in a single VG150 data-format le to support most existing models for scene graph learning or transformed into a separated annotation le for each single image to build customized datasets. Moreover, GeneAnnotator provides a rule-based relationship recommending algorithm to reduce the heavy annotation workload. With GeneAnnotator, we propose Trac Genome, a comprehensive scene graph dataset with 1000 diverse trac images, which in return validates the eectiveness of the proposed software for scene graph annotation. The project source code, with usage examples and sample data is available at https://github.com/Milomilo0320/A-Semi-automaticAnnotation-Software-for-Scene-Graph, under the Apache opensource license. • Applied computing → Document metadata; Annotation. scene graph, trac knowledge representation, visual relationship description, autonomous driving dataset ACM Reference Format: Zhixuan Zhang, Chi Zhang, Zhenning Niu, Le Wang, and Yuehu Liu. 2021. GeneAnnotator: A Semi-automatic Annotation Tool for Visual Scene Graph. In MM ’21, October 20–24, 2021, Chengdu, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/1122445.1122456 Scene graph is a topological structured data representation of visual content. It is usually represented by a directed graph, the nodes of which represent the instances and the edges represent the relationship between instances. Specically, for a relationship, the starting node is called the subject, and the ending node is called the object. In recent years, scene graph has been applied to semantic image retrieval, image captioning, visual question answering and achieved reasonable performance. Large-scale scene graph datasets with precise manually annotated are the prerequisites for related research. The open-source annotation tool is relatively uncommon at present. Existing datasets cannot organize data into a unied data framework. It is not conducive to customize datasets. In order to ll these gaps, we propose the GeneAnnotator, a semi-automatic annotation software for scene graph generation by Python. It has the following major features:(1) Friendly user interactivity.Figure 1 shows the GUI of GeneAnnotator. Users can visually view the contents annotated and scene graph in real time. (2) Diverse information.In addition to the relationships between instances, it also allows users to annotate diverse information such as regions, clusters and attributes.(3) Semi-automatic annotation.GeneAnnotator can recommend the relationship according to the recommendation algorithm and rule system, which greatly improves the annotation eciency.(4) Highly customized.Users are free to dene the underlying data, including object categories, relationship lists and so on. It is helpful for researchers to build customize datasets.(5) Applying easily.The output dataset les are organized in VG150’s data format and can be ready to most existing models for scene graph learning. At the same time, we provide Trac Genome, a scene graph dataset with 1000 trac images built by GeneAnnotator. Trafc Genome contains information about the location, labels, relationships, attributes and other information of the elements in trac scenes. It is one of the earliest open-source trac scene graph datasets. In comparison to other related datasets, such as Visual Genome[2] and VG150[6], objects and relationships in Trac Genome are denser and the attribute coverage is highest. In this section, we present the system architecture of GeneAnnotator, as illustrated in Figure 2. We rst detail the functionalities and concepts in core modules and then introduce the annotation workow. Main Annotation Module is the most basic module. Its main functions include data input and output, data encoding, relationship annotation and scene graph visualization. This module allows users to freely dene the underlying data, i.e. object lists, object categories, object hierarchies, relationship lists, relationship categories, and relationship hierarchies. Object hierarchy and relationship hierarchy[8]:objects and relationships are hierarchical and instances connect to others within or across dierent layers to form a scene graph. Auxiliary Function Module allows researchers to annotate regions, clusters and attributes. Region:In order to make the scene graph directly express the visual perception of images, we suggest user located each visually meaningful region with a bounding box like Figure 1(b). The region is helpful to avoid annotating trivial and irrelevant relationships in scenes. Cluster:Users can regard objects nearby and attributes as a "cluster". For example, nearby cars in Figure 1(c) were annotated as a “cluster”. The practice has proved that clusters can make the annotation more ecient. Attributes:Attributes are an additional description of objects that can provide ner-grained nodes for the scene graph to better distinguish dierent instances. In GeneAnnotator, “orientation” attributes are constructed for the research content. The researchers can add the required attributes to software according to their own needs, such as speed, color, size and so on. Semi-automatic Annotation Module recommends the best relationship for each pair of instances according to the recommendation algorithm and rule system. Recommendation algorithm:Based on Simple Tag-based Recommendation algorithm, <subject, relationship, object> is represented as(𝑢, 𝑖, 𝑏).𝑢indicates a pair of <subject, object>, which has relationship𝑖and attribute𝑏.𝑏is a vector, each element in which is a visual attribute of the pair of𝑢, like “whether the prole touch” or “whether the dierent size of bounding boxes”. The equation of interest between a pair of <subject, object>𝑢and their relationship 𝑖 is given by: where𝑛indicates the number of times that𝑢has attribute 𝑏.𝑛indicates the number of times that𝑢with attribute𝑏has relationships 𝑖. The prior database consists of the annotated relationships. The parameters in Equation (1) can be quickly updated and relationships are recommended without training. Figure 2: System Architecture. In GeneAnnotator, there are three core modules: Main Annotation Module, Semi-automatic Annotation Module and Auxiliary Function Module. Each module is independent of one another. Removing one of the modules does not aect the use of the whole software, which makes it easy for developers to update and maintain. Rule system:The recommendation algorithm is based on a large number of prior information. In the stage of start in the sparse data, self-encoding rule are used to guess a relationship. The default rules in GeneAnnotator is to judge the relationship according to the contact and position of two instances. The workow of building a scene graph dataset is shown in the workow layer in Figure 2. First of all, the conguration les of the original image and other underlying data are read into GeneAnnotator. GUI shows the image and user on-demand region annotation and cluster annotation. Then the Semi-automatic Annotation Module recommends relationships, which users can modify or add to. The annotated relationships will be stored in the prior database to enrich the recommendation algorithm. Finally, the output les are the scene graph dataset. Trac Genome is a trac scene graph dataset comprised of 1000 trac scenes. The original images are selected from Cityscapes[1], which is a pixel-level semantic segmented dataset. We oer two dierent formats of Trac Genome. One format is following the structure of VG150, which covers all the scene graphs (1000 images). According to related work, most of the existing scene graph learning models use the database and metadata proposed in VG150. Therefore, les in this format can be directly used in existing models. However, it does not support users to modify or add relationships to a specied scene graph. Thus, we designed another format for per-image, containing image instances map, clusters annotations and other more detailed information. It is convenient to modify the specied scene graph. We also provide code to convert between the two formats. In Trac Genome, there are 34 semantic object classes (including "unlabeled") and 51 relationships. In comparison to related datasets, objects and relationships in Trac Genome are denser and the attribute coverage is highest. As shown in Table 1, there is a comparison among Trac Genome, Visual Genome and VG150. Object density:In Trac Genome, there are 25,147 annotated instances, of which 19,291 are involved in the scene graph. On average per image, Trac Genome has 19.29 instances were involved in a scene graph, which is very close to the 20.85 in Visual Genome and much larger than the 5.69 in VG150. At the same time, the percentage of instances involved in a scene graph in Trac Genome is about 76.71%, much larger than the 58.65% of Visual Genome and the 53.66% of VG150. It indicates that Trac Genome has richer and ner annotations, allowing more instances in the images to participate in the construction of the scene graph. Relationship density:We annotated 29,192 relationships in Trac Genome. On average, each scene graph has 29.19 relationships, much larger than the 14.17 in Visual Genome and the 5.76 in VG150. We also calculated the average of relationships involved per object. The results show that one object has relationships with another 3.02 objects in Trac Genome. The value in Visual Genome and VG150 is 1.36 and 2.02. It can be regarded as a measure of the sparsity of a scene graph. A denser graph has more edges involved per node. That means Trac Genome has a much denser relationship annotation than the other two datasets. Most of the relationships are focused on spatial relationships (such as “in left of”) and area relationships(such as “driving on”), 43.85% and 42.04% of the total relationships, respectively. The top 5 common combination of <Object – Relationship - Subject> are : “Person – Walking on - Sidewalk”, “Car – In front of/In back of Car”, “Car – driving on - Road”, “Car – Parking on - Road”, “Person – Walking on - Road”. Attribute coverage:Trac Genome annotated oriented attribute for each instance, i.e. "forward", "leftward", "rightward" and "backward". The attribute coverage is 100%, which is the highest of the three datasets. We use Trac Genome to experiment and evaluate the following scene graph generation models: •IMP[6]: The model solves the scene graph inference problem using standard RNNs and learns to iteratively improves its predictions via message passing. •Motif[7]: Regularly appearing substructures in scene graphs make object labels are highly predictive of relation labels. •VC-Tree[5]: Proposing a method to compose dynamic tree structures that place the objects in an image into a visual context. •VCTree-TDE[4]: Building a causal graph and making biased training with the graph. •EBM-Loss[3]: The additional constraint in the learning framework acts as an inductive bias and allows models to learn eciently from a small number of labels. Since Trac Genome was given the labels of ground-truth, we only evaluated the performance of predicate classication. Models successfully run with Trac Genome. As shown in Table 2, the predicate classication results of Trac Genome are very close to VG150. It even performs better in mR@100. By evaluating Trafc Genome in scene graph generation models, GeneAnnotator is proved to be a valuable tool for scene graph annotation. Table 2: The predicate classication results on dierent models. GeneAnnotator is released under the license of Apache 2.0 and its source code is openly available at: https://github.com/Milomilo0320/ASemi-automatic-Annotation-Software-for-Scene-Graph. In addition, Trac Genome will be made available at https://github.com/Milomi lo0320/Trac-Scene-Graph-1000. Contributions from the opensource community are welcome, via the GitHub issues/pull request mechanisms. In this paper, we have developed GeneAnnotator, a semi-automatic annotation software for scene graph generation by Python. It is one of the earliest open-source scene graph annotation tools. With modular designs, GeneAnnotator is easy-to-use and easy-to-extend. Furthermore, we provide Trac Genome, a scene graph dataset with 1000 trac images built by GeneAnnotator. By evaluating Trac Genome in scene graph generation models, GeneAnnotator is proved to be a valuable tool for scene graph annotation. This work was supported by the National Key Research and Development Program of China, No. 2018AAA0102504.