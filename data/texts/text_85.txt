Abstract—The sequential recommendation aims to recommend items, such as products, songs and places, to users based on the sequential patterns of their historical records. Most existing sequential recommender models consider the next item prediction task as the training signal. Unfortunately, there are two essential challenges for these methods: (1) the long-term preference is difﬁcult to capture, and (2) the supervision signal is too sparse to effectively train a model. In this paper, we propose a novel sequential recommendation framework to overcome these challenges based on a memory augmented multi-instance contrastive predictive coding scheme, denoted as MMInfoRec. The basic contrastive predictive coding (CPC) serves as encoders of sequences and items. The memory module is designed to augment the autoregressive prediction in CPC to enable a ﬂexible and general representation of the encoded preference, which can improve the ability to capture the long-term preference. For effective training of the MMInfoRec model, a novel multi-instance noise contrastive estimation (MINCE) loss is proposed, using multiple positive samples, which offers effective exploitation of samples inside a mini-batch. The proposed MMInfoRec framework falls into the contrastive learning style, within which, however, a further ﬁnetuning step is not required given that its contrastive training task is well aligned with the target recommendation task. With extensive experiments on four benchmark datasets, MMInfoRec can outperform the state-of-the-art baselines. Index Terms—sequential recommendation, contrastive learning, memory network Recommender systems play an important role in today’s online platform. To recommend proper items to a user, the essence is to predict the user’s preference. However, the preference will naturally shift as time goes on, which requires the recommender systems to capture the dynamic preference of a user. Recently, various sequential models have been developed, which capture these preference dynamics, which are mainly based on sequence modeling methods [2], [12], [13], [15], [24], [29]–[31], [33], [37], [40], [49], [51]. The most essential procedure of the sequential recommendation is to learn the sequential patterns of the user behaviors. Commonly, sequential recommender models rely on the next item for supervised training based on the assumption that if a model can predict exactly the user’s preference, the model can successfully capture the sequential patterns [12], [15], [40]. The more recent methods introduce auxiliary tasks to enhance the training process. For example, some recent methods choose to use a manually masked content prediction task as the extra Fig. 1: Illustration of latent representations of a sequence sand its positive sampleiand its negative sampleiof different objectives. (a) A typical BPR objective with one representation of the next item serves as the positive sample and one negative sample. (b) Vanilla NCE objective with one ground truth positive sample and multiple negative samples. (c) Multi-instance of semantically positive and negative samples could alleviate the sparsity issue. training signal, including masked item prediction [37] and masked attribute prediction [51]. With the contrastive learning, the self-supervised learning over the sequence representation is applied by recent methods to enable the model to learn from different supervision signals [24], [47], [50], [51]. Although these methods have obtained a comparative progress in sequential recommendation, there are two essential challenges remained: (1) The long-term preference of a user is difﬁcult to capture. The importance of the long-term preference is mainly indicated that the lone-term preference is related to the user’s general interest, which can affect the user’s behavior in addition to the user’s most recent interest. Existing methods usually rely on RNN or Attention architectures to focus more on the latest interactions [12], [15]. Under these situations, the long-term preference fails to be emphasized. (2) The supervision signal is too sparse to effectively train a model. As shown in Fig. 1 (a), the only objective directly related to the recommendation is still the next item prediction. Commonly, this objective is implemented by the cross-entropy loss [37] or Bayesian Personalized Ranking (BPR) loss [12]. In contrast, these proposed auxiliary tasks are all based on sequence understanding purposes and they are not aligned with the recommendation task. For example, it is not intuitive or straightforward how predicting a masked item, a masked short segment or a masked attribute in multiple previous steps can help predict the user’s current preference. Recently, there are two promising techniques that could solve the overcome the abovementioned challenges. Firstly, the external memory introduced in Neural Turing Machine [6] can explicitly store a set of memory slots for the long-term knowledge. The external memory has an effective reading and writing strategy for generating a comprehensive representation. Secondly, with the contrastive learning, the representation learning of different modals has achieved a great progress [1], [8]–[11], [14], [41]. In these methods, optimizing a contrastive objective on different views of the same sample or the semantically similar samples can improve the quality of representation learning signiﬁcantly by pulling the positive samples closer while pushing the negative samples further as in Fig. 1 (b). For a sequence in the recommendation, it is desirable to pull input sequence and the predicted target item closer. In addition, when different views of the target item are incorporated into the contrastive learning, the number of semantically positive samples can be enlarged with a high quality, which is illustrated in Fig. 1 (c). In this paper, we aim to develop an effective solution to alleviate the sparsity issue of the sequential recommendation with the contrastive learning. Speciﬁcally, a memory augmented multi-instance contrastive predictive coding model is proposed, denoted as MMInfoRec. In the sequence encoding procedure, a contrastive predictive coding (CPC) scheme serves as a sequence encoder. A memory module is developed to augment the CPC in order to provide a ﬂexible and general representation of the sequence representation to preserve the long-term preference effectively. For the design of the training objective, a multi-instance variant of the Noise Contrastive Estimation (NCE) is derived to provide a recommendation-related training signal to alleviate the sparse signal issue. The contributions of this paper are summarized as follows: The MMInfoRec model is proposed for the sequential recommendation using an end-to-end contrastive learning scheme, which alleviates the sparse training signal issue in sequential recommendation. A memory module is developed to provide a ﬂexible and general representation of the sequence, which can effectively preserve the long-term preference. derived for the sequential recommendation task in the batch training setting, which provides rich training signals aligned with the recommendation. Extensive experiments on four datasets demonstrate the superiority of the MMInfoRec model compared with the state-of-the-art baselines. This paper is organized as follows: In Section II, we will review the related literature. The detail of the proposed MMInfoRec is described in Section III, followed by our extensive experiments in Section IV to demonstrate the performance. With the successful usage of neural networks, many deep learning-based sequential recommender models have been developed [2], [12], [15], [20], [21], [24], [29]–[33], [37], [40], [47], [49]–[51]. They mainly use GRU [3] or Transformer [42] structures as the encoder. Some methods apply graph structure in sequential encoding such as FGNN [30], [31], GAG [33] and PosRec [29]. In terms of the training scheme, most of these methods follow the next-item supervised training style, for example, GRU4Rec [12], Caser [40], SASRec [15], and FDSA [49]. More recent methods apply masked token prediction by BERT4Rec [37], masked segment prediction by SRec [51], attribute prediction bySRec [51], or augmented sequences prediction by CL4SRec [47] as contrastive training objective. BERT4Rec [37] andSRec require a ﬁnetuning after the pre-training. Ma et al. [24] include the masked segment prediction as an extra objective as a component of multi-task learning along with the next-item prediction objective. The neural memory network is a network module that mimics the mechanism of the memory in computers to support the writing and the reading for learning the long-term knowledge [6]. Recent recommendation models apply the memory module with different purposes [4], [23], [39], [43], [45]. For example, NMRN [43] makes use of the memory network to store the historical information in streaming recommendation to mitigate the catastrophic forgetting. Similarly, MA-GNN [23] and DMAN [39] rely on the memory network to capture the long-term preference of the user. CMN [4] utilises the memory network to represent the user feature. MATN [45] designs a memory network to learn the multiple behavior patterns. Contrastive learning has been used by recent methods for the sequential recommendation [24], [46]–[48], [50], [51]. Masked content prediction is applied by a few methods, including the masked item prediction and the masked attribute prediction bySRec [51] and the segment encoding by CL4SRec [47]. These methods assume that all of the desired information is injected into the ID information through these pretext tasks. Contrastive learning attracts interest in many areas, such as computer vision [1], [8], [10], [11], [14], [44], natural language processing [18] and speech representation learning [41]. Recently, contrastive learning approaches achieve great success by discriminating the data itself among all other data [1], [10], [11]. These methods are based on estimating the mutual information between positive samples and negative samples. The mutual information is usually transformed into an optimizable bound [14], [28], [41]. For natural language processing and speech representation learning, pretext tasks include sequential prediction [41] and content prediction [18]. The sequential prediction objective requires the model to generate the following contents based on all the previous content. Based on the abovementioned NCE loss, we will describe how to derive the MMInfoRec model in CPC scheme with the NCE loss in detail. In SectionIII-A, we will provide the problem deﬁnition and mathematical notations. In SectionIII-B, the framework of the MMInfoRec will be described. The contrastive training objective is presented in SectionIII-C, followed by a discussion of the model in Section III-D. Fig. 2: Overview of MMInfoRec. Every item is ﬁrstly encoded into a latent feature. For example, sequence are encoded into a latent vectorzby an encoderg encoding model such as GRU or Transformer.cis the result of the encoding of the sequence before time step prediction stept + 1, a predicted resultˆzbased on the memory function vectorz, which is computed based on itemiand its attributes. The measurement of this comparison is MINCE, shown on the right for a two-step prediction. All of the positive samples (Pos.), the semantic positive samples (Sem. Pos.), the general negative samples (General Neg.) and the temporal negative samples (Temporal Neg.) are sampled from the training mini-batch. The semantic positive samplesz, . . . , zare calculated from different Dropout on the same sampleszare from other sequences in the same batch and the temporal negative samples With MINCE, the Pos. in the gray box will be omitted. If the Pos. is used instead of the Sem. Pos., then the MINCE degrades to NCE. During the test phase, the recommendation result is calculated based on the comparison among the latent representation In sequential recommendation, there are usually a user set Uand an item setI, whereu ∈ Udenotes a user andi ∈ I denotes an item.|U|and|I|are used to represent the number of users and items respectively. Chronologically, a useruhas a historically ordered interaction sequence with multiple items: [i, i, . . . , i], wherenis the number of interactions. Besides, there is an attribute setAincluding all attributes belonging to all items.|A|is used to denote the number of attributes appearing in the dataset. For every item, there are associated attributesA= {a, a, . . . , a}. Usually, different types of items have different attributes. For example, products have categories; songs are included in different albums; and videos are tagged with different genres. The purpose of a sequential recommender system is to predict the next itemia user will interact with based on[i, i, . . . , i]andAfor each item, where t indicates the current time step. The overview of the MMInfoRec is shown in Fig. 2. The ID and attributes of all itemsiin the sequence are ﬁrstly converted into dense feature vectors by two embedding layers respectively. For each item, its features are encoded by an attribute encoder ginto latent representationsz. The temporal aggregation modulegencodes the sequential information intoc, which is used to predict the futuristic latent representation by an auto-regressive prediction module g. 1) Embedding Layer: To convert IDs and attributes into dense vectors, two embedding matrices are applied: an item embedding matrixEmb∈ Rand an attribute embedding matrixEmb∈ R.drepresents the embedding size. For example, an itemiwith its attributes{a, a}, we can use a lookup function to obtain the dense vectors: x= Emb(i), a= Emb(a) and a= Emb(a). 2) Attribute Encoder: The attribute encoder aims to fuse all the information of the item, including the ID information and the side information, into a latent representation. After having the item embeddingxand its attribute embeddings {a}, an attribute encoder functiongcan compute the latent representation z of the item as: where x ∈ R, a∈ Rand z ∈ R. For the implementation of the functiong, a simple mean, or AutoInt [35], or a self-attention of the item embedding to all attribute embeddings are all applicable for this structure. 3) Temporal Aggregation: The temporal aggregation function is designed to aggregate the temporal information of the items before a certain time step. The function is deﬁned asg: where c∈ R. For the choice of the temporal aggregation, common choices in sequential recommender models, such as GRU or Attention are capable of performing this aggregation computation. 4) Memory Module: To enhance the representation ability of the model, a memory module is designed to calculate the predictive output of the each step based on the context vector. In the memory module, there is a memory bankM ∈ R withbmemory slots. In the memory addressing is deﬁned as: where MLP stands for the multi layer perceptron. The residual style is designed for a retain of the original prediction and an improvement of the gradient ﬂow in training. 5) Auto-regressive Prediction: For the multi-step prediction task, if the context is encoded into a vectorc, then the predicted latent representationzis expected to have strong semantic similarity withc. Therefore, we introduce the autoregressive prediction function g, wherecis the summary of the context from time step1 tot + 1, andˆzis the predicted feature of the time step t + 2. Similar to Seq2Seq style of prediction [38], [50], latent representations are predicted one-by-one in a sequential manner. 6) Recommendation: During the validation and test of MMInfoRec, the recommendation is conducted under a score ranking between the sequence representationcand all items in the item set. The score is simply computed with a dot-product between the sequence representationcat the current time step t and the latent representations z of all items in the item set. Noise Contrastive Estimation (NCE) [7] is a classiﬁcation objective that can distinguish real samples and noisy samples. Similar to [27], [41], the NCE loss can be directly applied to the item predictive task. In MMInfoRec, we extend this loss to a multi-instance variant, which can effectively alleviate the issue of the sparse training signal. 1) Vanilla NCE Loss: The vanilla NCE mainly makes a comparison between representations in the latent space to force the predicted representationˆzto be close to the ground truth representationz. In the forward rollout, the MMInfoRec ﬁrstly computes the predicted latent vectorˆzand the ground truth representationz. At time stepi, the latent representations with the same size are denoted asˆzandz. The similarity of the predicted vector and ground-truth vector pair is calculated by the dot product zz. The objective is: whereτis the temperature parameter andNis the negative sample set for itemi. Essentially, Eq. (5) is a cross-entropy loss between the positive pair and all other negative pairs. For a predicted vectorˆz, the only positive pair is(ˆz, z)because zis the corresponding ground-truth at time stepi. Except for the ground truth item, all other latent representationszfrom any other items consist of the negative pair samples. 2) Negative Sampling within Batch: Since there is a negative set in the training objective, a sampling procedure of this set is needed. Generally, there are two types of sampling methods for the negative set: memory bank [10] and batch sampling [1], [8], [11], [14], [41]. In MMInfoRec, the calculation of the NCE is inside a batch rather than sampling across the whole item set since a batch will contain enough negative samples. In the CPC methods for computer vision tasks [8], [11], [14], [41], the negative sample can be chosen from the channels of the same feature map or other feature maps from the same batch. Similarly, in sequential recommendation, we can use the feature vectorzof other items in the batch to construct the negative sample set Nfor every item i. a) General negatives: For items in other sequences of the same batch, they can generally serve as negative samples of the predicted preference. Since these items have a large potential to represent totally different preferences from other users, they can be considered as easy negatives inNof Eq. (6). On the right of Fig. 2, there is a General Neg. in the MINCE loss. b) Temporal negatives: For other items except for the ground truth positive sample in the same sequence, they are more difﬁcult for the model to discriminate. Because these items and the ground truth are in the same sequence, these other items have the tendency to represent a outdated and misleading preference of a user. Thus, it is important to include them as hard negatives to train the model, which could enable the model to discriminate the difference. On the right-hand side of Fig. 2, there is a Temporal Neg. in the MINCE loss. c) Number of negatives: As described above, the sampling of negative samples is conducted within the training mini batch. Assume there areDunique items in the training batch. According to the negative sampling strategy, the number of samples in Nis denoted as |N| = D − 1. 3) Multiple Instance Positive Sampling: For the predictive latent representationˆz, the most natural choice of positive pair is(ˆz, z), which chooses the ground-truth latent vectorz of the corresponding time step. In this section, we will extend this positive sampling strategy to a multiple instance scheme. To extend the vanilla NCE from single positive sample to multiple instances of semantically positive sample, the key challenge is to identify the semantically positive samples of the ground truth for the next item prediction. Since the sequential recommendation usually considers the next item as the label of prediction, it is generally difﬁcult to ﬁnd out the semantically similar items as the multiple positive samples in NCE. This difﬁculty is mainly due to the high unavailability of deﬁning the semantic similarity of sequences, which usually relies on counting on the nearest neighbor in the sequence level [5]. Different from data samples in computer vision research ﬁeld, e.g., images and videos, there is not widely applicable augmentation methods for the embedding representation of items to obtain the semantic positive samples [1], [10] nor meaningful labels to mine the semantic positive samples in a supervised learning style [16], [26]. To address this difﬁculty for semantically similar positive samples, a Dropout-based positive sample mining strategy is proposed in MMInfoRec. As described in SectionIII-B2, an item is encoded byginto a dense embedding vector. When there is any Dropout function insideg, a set of different yet semantically similar encoded vectors of the same item can be obtained by setting different Dropout masks ingon the same item. Assume there areqdifferent random Dropout functions. When these Dropouts are operated withgon the same item i, a set of different latent representations of the same item are denoted asP= {z, z, . . . , z}. These semantic positives are denoted as Sem. Pos. in Fig. 2. After generating the semantic positive set for every item in the batch, with thisPserving as the semantically positive set, the multi-instance NCE (MINCE) loss can include more than one term in the nominator: whereNis the negative sample set for itemiand naturally contains the theqvariants of the items in the negative set in Eq. (5). Thus, the size of the negative set in Eq. (6) is denoted as|N| = q(D − 1). In the training of MMInfoRec,`is chosen to be the training objective. Our MMInfoRec model provides a novel contrastive training scheme in sequential recommendation task. Fot the previous methods, the most common training scheme is the nextitem supervised training. Recent state-of-the-art methods, for example, GRU4Rec [12], SASRec [15], Caser [40], FDSA [49] and Seq2SeqRec [24], all applies this scheme to train their models. contrastive learning is introduced to this ﬁeld recently by adding a pretext task to train an encoder to learn a general understanding of the interaction sequence. BERT4Rec [37] and SRec both utilize the masked training from masked language models. They rely on representing the item with its neighbors. Such a training style in natural language understanding can improve the learned word embedding to generalize to different downstream tasks. However, in the sequential recommendation, there is only one desired task, the recommendation. Therefore, both of these methods need a further ﬁnetuning step to make the model target at the recommendation. In contrast, the proposed MMInfoRec have a consensus in the pretext task and the recommendation task by ranking the positive sample in a higher position than negative samples, which prevents from a ﬁnetuning after the contrastive pre-training. This vanilla NCE,`in Eq. (5) indicates the model to assign a higher similarity between representations from positive pairs than other negative samples. Under this situation, the model can distinguish the positive predictive result from the whole item set. In general,`is similar to the BPR loss [34] while using multiple negative samples. Compared with listwise TABLE I: Statistics of the datasets after preprocessing. losses,`focuses more on ranking the target item in a higher position than all the negative samples while listwise losses will pay attention to the ranking of negative samples. The MINCE loss`proposed Eq. (6) offers a novel perspective of the contrastive learning of sequential recommendation. The general NCE only considers the item itself as a positive sample. However, as pointed out in the literature of supervised learning with contrastive signals, when semantically similar samples are included into the contrastive training, the encoding result can be more accurate [9], [16], [26]. Our proposed MINCE shares the same spirit as these methods to avoid inappropriate categorization of positive and negative samples. In this section, we will describe the experiments to verify the efﬁcacy of the MMInfoRec. In SectionIV-A, the experimental setup is demonstrated. In the following sections, we will answer research questions (RQ) by different experiments: with the state-of-the-art sequential recommender models? (Section IV-B) the whole model? (Section IV-C) tial recommendation? (Section IV-D) training in the sequential recommendation? (SectionIV-E) the MMInfoRec model? (Section IV-F) In this section, the setup of experiments is presented, including datasets (SectionIV-A1) with the corresponding preprocessing (SectionIV-A2), metrics used to evaluate the performance (SectionIV-A3), state-of-the-art baselines (Section IV-A4) and the implementation (Section IV-A5). 1) Datasets: To evaluate the proposed MMInfoRec model, the following datasets are applied with a summary in Table I: [37], [51], we choose three subcategories of the Amazon dataset with their ﬁne-grained categories and the brands of the product as attributes. recommendation. Similar to [51], the transaction records after Jan. 1st, 2019 are used in our experiment. The categories of businesses are considered as attributes. 2) Preprocessing: For all datasets, we treat all the interaction records from the same user as a sequence. The order is given by the timestamp of the interaction. Following [37], [49], [51], item appearing frequencies less than 5 will be ﬁltered out. And if a sequence is shorter than 5, the sequence will also be removed. The maximum length is set to 50, which means that only the most recent 50 interactions will be kept if the sequence is longer than 50 interactions. The second last item in a sequence is used for validation and the last item is used for testing. The rest items are used for training. 3) Metrics: To evaluate the performance by fair comparisons, top-KHit Ratio (HR@K) and top-KNormalized Discounted Cumulative Gain (NDCG@K) are applied withKchosen from {5, 10}following previous methods [24], [37], [51]. These usually apply a sampling strategy for evaluation, which is proved to be unfair by recent work [19]. Therefore, we evaluate the ranking result over the whole item set. 4) Baselines: We choose the baselines according to current research [49], [51] and utilize the most popular and state-ofthe-art methods for comparisons. We omit shallow methods and non-sequential methods since they are proved to be unable to show competitive results with recent neural-based sequential recommender models [49], [51]. The following baselines are chosen to compare with the proposed MMInfoRec: sequence is treated as the input and the output hidden state serves as the sequence representation. Markov Chains by applying horizontal and vertical convolutional operations for the sequential recommendation. This structure is majorly different from usual sequence modeling methods in the choice of network type. sequential recommendation model, which uses the multihead attention mechanism to recommend the next item. It is a strong baseline in sequential recommendation since the attention mechanism is a powerful tool in sequence modeling, which is also demonstrated in NLP. similar to the masked language model. The backbone is the bi-directional self-attention mechanism. gating networks to capture long-term and short-term user interests. User information is used for both the gating network and the ﬁnal recommendation score. which leverages attributes. This model calculates the sequence representation in a parallel style. work [51], which concatenates the representations of items and attribute as the input to the model. feature level self-attention block to model the feature transition patterns. It is a two-stream model that calculates the representation of a sequence based on the ID information and side information of items respectively. Rec[51] applied masked contrastive pre-training similar to BERT4Rec. The mask is utilized on segments, attributes and single item. It serves as a strong baseline for sequential recommendation using side information of items. Recent Seq2SeqRec [24] method uses a similar next sequence prediction, which is a special case of SRec. The full ranking results of these baselines are from the updated resultprovided by SRec, which will be shown in Table II. 5) Implementation: In Section III, we provide the overall framework of the MMInfoRec. For the detailed choice of each module, we describe each module in this section. For the attribute encoderg, we choose to use the Transformer [42] module to perform a self-attention of the item embedding to all of its attribute embeddings. For the temporal aggregation g, another Transformer [42] is applied. Meanwhile, a learned positional encoding layer is added to the input ofg. For the auto-regressive predictive function g, a GRU network [3] is used for the multi-step predictive computation. In this paragraph, we will describe the hyper-parameter setting for the training and the network. The embedding size is set to64with all linear mapping function in the MMInfoRec has the same hidden size. The number of layer and head in the Transformer are chosen from{1, 2}. A Dropout [36] function with the ratio0.5is used on both the input of thegfunction and the Transformer module insidegto alleviate the overﬁtting issue. The training batch size is set to 256. We use the Adam [17] optimizer with the learning rate chosen from{0.0003, 0.001, 0.003, 0.01, 0.03}. The number of memory slotbis chosen from{5, 10, 32, 64, 128, 256}. The default number of the predictive step is chosen from{1, 2, 3, 4}. The number of different Dropout functions in Eq. (6) is chosen from{1, 2, 3, 4}. The temperature parameterτin Eq. (6) is chosen from{0.1, 0.3, 0.6, 1, 3}. An`regularisation is also applied along with the training, with the weight chosen from {0, 0.1, 0.01, 0.001, 0.0001, 0.00001}. In this section, we compare MMInfoRec with state-of-theart methods. We evaluate the performance on four datasets, Amazon Beauty, Amazon Sports, Amazon Toys and Yelp. The metrics used in this experiment are HR@5, NDCG@5, HR@10 and NDCG@10. The experimental result is shown in Table II. According to the result in Table II, the MMInfoRec achieved the best performance compared with all the baseline methods. The improvement percentage is signiﬁcant across all datasets. TABLE II: Overall performance. Bold scores represent the highest results of all methods. Underlined scores stand for the highest results from previous methods. The MMInfoRec achieves the state-of-the-art result among all baseline models. For sequential recommendation baseline methods without using the side information, they can achieve comparable results in certain situations. However, most of the time, they are far away from the highest performance. For example, GRU4Rec is the earliest neural-based method in the sequential recommendation. GRU4Rec utilizes GRU to encode the sequence. It generally has a less competitive result. Similar performance is achieved by Caser, which uses a convolutional neural network to compute the representation of a sequence. The backbone of these methods is not strong enough to help boost the experimental performance of models. SASRec and BERT4Rec use a single-directional and a bi-directional attention mechanism respectively to learn the dependence between items in a sequence. SASRec can serve as a strong baseline in this task since it can sometimes beat the methods that rely on extra item features. However, BERT4Rec is not as strong as SASRec, which could be because the indirect training scheme of BERT4Rec introduces unrelated information into the representation learning of items. However, they still prove that the Transformer structure is a strong backbone in the sequential recommendation. HGN includes the user information to support a structural gating network to learn the interaction between the user information and the item information. It can achieve comparable results with SASRec although not using a Transformer structure. This situation demonstrates that a carefully designed hierarchical gating network can model the relationship between items well. For the methods that using item attributes in the sequential recommendation, the performance of them is different. GRU4RecandSASRecare two extensions to the original non-feature-based methods GRU4Rec and SASRec respectively. They achieve similar results that outperform the basic GRU4Rec method. However, their performance is still interior to SASRec, which indicates that an inappropriate inclusion of the side information would hurt the performance of the model itself. A simple concatenation of the item’s ID representation and its attributes representations are unable to fuse the information effectively. FDSA is a method that mixes the parallel structure ofGRU4Recwith the Transformer network of SASRec to encode the ID embedding and attribute embeddings separately. The performance of FDSA is then similar to the featurebased methods since it uses the same next-item supervised training scheme with a similar network structure. The strongest baseline of the sequential recommendation is SRec, which uses a masked sequence modeling simultaneously on the ID embedding, attribute embeddings and segment representations. The contrastive training scheme beneﬁts the model a lot by injecting the side information and the prospective intention into ID embeddings. It achieves the best result in most of the situations compared with other baselines. It proves that contrastive learning could be a promising technique in sequential recommendation tasks. Given the overall performance, we can inspect the reasons for the improvement of the MMInfoRec over these baselines. (1) Compared with the traditional sequential recommender systems, GRU4Rec, Caser, SASRec and HGN, the MMInfoRec exploits the attributes of items to assist the learning of representations of items and sequences. (2) Compared with the feature-based sequential recommender systems,GRU4Rec,SASRec, FDSA and SRec, the MMInfoRec uses a self-attention structure to fuse the attributes of an item, which enables the model to learn the interaction between different attributes and the ID information. (3) Compared with the contrastive methods, BERT4Rec and SRec, the MMInfoRec focuses on a pretext task that is more similar to the recommendation task itself. In contrast, the pretext tasks in BERT4Rec and SRec are masked sequential prediction, which has a gap to our desired recommendation task. Therefore, the MMInfoRec has a more effective way to make use of the training signal. In this experiment, we conduct the ablation study to verify the efﬁcacy of each components in MMInfoRec, mainly for the memory modulegand the MINCE objective. When both of these two major components are removed from MMInfoRec, the model is degraded to the standard CPC scheme. To verify each TABLE III: Ablation study for the memory module and the MINCE objective. of these components, we addinggand MINCE separately to the basic CPC model, denoted as +gand +MINCE respectively. The experimental result is shown in Table III. According to the result, it can be seen that bothg and MINCE can consistently improve the recommendation performance. For the performance on the Beauty and the Sports datasets, the contribution of MINCE is more signiﬁcant than g. While for the Toys and Yelp datasets, the contributions of both components are close in terms of the performance. When combining them together into MMInfoRec, the highest performance can always be achieved across all datasets. In this experiment, we aim to investigate how the memory module improves the sequential recommendation. Firstly, the following variants ofgwill be used: (1) None: the baseline without any memory module; (2) FC-M: by removing the addition of the original context vectorcin Eq. (3); and (3) Res-M: the default memory module with the residual addition Fig. 4: Visualisation of the norm of the memory learned by MMInfoRec on Beauty with 64 slots and Sports with 10 slots. as Eq. (3) denoted. Besides, a visualization of the learned memory will be demonstrated. 1) Different designs ofg: We compare the performance of the variants above by conducting on Beauty and Toys datasets. The experimental results are shown in Fig. 3. It can be seen that Res-M can achieve the highest performance across all situations while the FC-M variant cannot have a comparative result with both the None variant and the Res-M variant. A proper way to use the module is required in the recommendation scenario, which maybe due to the embedding space of items is not similar to the feature-rich content space in computer vision. Solely relying on the memory module to accurately represent the feature space is too difﬁcult for a recommendation model. Therefore, a residual addition is a better choice for the memory. 2) Visualisation of memory: To illustrate the memory module, the norm of the learned memory is used to demonstrate what is learned within the memory module. The visulisation result is demonstrated in Fig. 4. It is worth noting that the learned memory bank is a sparse result. For example, in the experiment on Beauty with 64 memory slots, there only seven slots have signiﬁcant contribution while the rest has a nearly zero norm. Similar situation is shown on the Sports with 10 memory slots, which has six slots contributing profoundly to the model. In this section, we conduct experiments to evaluate the MINCE loss in Eq. (6) in the sequential recommendation. We mainly investigate the vanilla NCE loss in Eq. (5) and Bayesian pairwise ranking (BPR) loss. The result is shown in Fig. 5. From Fig. 5, MINCE can always achieve the highest score compared with NCE and BPR. NCE can consistently outperform BPR across all situations. The BPR loss computes the relative preference between the target item and a sampled item. Compared with NCE, BPR only considers one pair of positive and negative items. The estimation of the mutual information between the predicted result and the ground truth for NCE is theoretically proved to be a tight bound. MINCE can be seen as a multi-instance version for the estimation of the mutual information, where the samples are effectively used. Fig. 5: Performance of different ranking objectives. Fig. 6: Performance of different numbers of memory slot. The parameter b is chosen from {5, 10, 32, 64, 128, 256}. There are important hyper-parameters in the MMInfoRec model, for example, the number of memory slot,bin Eq. (3), the temperature parameter in Eq. (6) and the number of the prediction step. In this section, we will evaluate the parameter sensitivity on these hyper-parameters of MMInfoRec. 1) Impact ofb: In this experiment, the impact of the number of memory slot,bin Eq. (3) is evaluated. We choosebfrom the set{5, 10, 32, 64, 128, 256}. The result is presented in Fig. 6. From the result, it can be seen that the number of memory slots is not affecting the overall performance signiﬁcantly. It is worth noting that according to the visualisation result of the memory bank in Fig. 4, there are only a small number of memory slots are contributing to the model. This can explain that although the number of overall memory slots is changing vastly, the performance is still very stable. 2) Impact ofτ: In this experiment, the impact of the temperature parameterτis evaluated, which is chosen from the set{0.1, 0.3, 0.6, 1, 3}. According to the experimental results in Fig. 7,τneeds to set in a range to achieve a Fig. 7: Performance of different temperature parameterτ, which is chosen from {0.1, 0.3, 0.6, 1, 3}. Fig. 8: Performance of different numbers of prediction step. The number of steps is chosen from {1, 2, 3, 4}. reasonable performance. For example for Sports and Yelp datasets, whenτis too small, the prediction logits become closer to a deterministic distribution, which could not provide sufﬁcient training signals to train the model. 3) Impact of number of prediction step: In this experiment, the impact of the number of the prediction step is evaluated. This parameter controls how many steps the auto-regressive predictiongrolls out and how much of the futuristic information will be included into the training. From the result in Fig. 8, except for the Toys dataset, MMInfoRec cannot gain useful information from the futuristic steps on all other datasets. For the Toys dataset, MMInfoRec achieves the highest performance with a two-step prediction. In this paper, the MMInfoRec model is proposed with a memory augmented CPC training scheme and a novel MINCE loss for effective training. Speciﬁcally, the memory module is designed to provide a ﬂexible and general representation of the sequence to provide a comprehensive long-term preference. The MINCE introduces a multi-instance noise contrastive estimation objective by using the semantically similar samples under different Dropout functions with the same item encoder. In the experiment, we successfully demonstrate that the MMInfoRec achieves the best performance compared with state-of-the-art baselines by a large margin. (CE200100025, DP190102353, DP190101985, FT210100624).