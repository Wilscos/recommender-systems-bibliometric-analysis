Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list signicantly improves the performance. However, the method still has limitations in that1)it does not fully utilize the prediction errors of the student model, which makes the training not fully ecient, and2)it only distills the user-side ranking information, which provides an insucient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more ecient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be distilled. By doing so, DCD essentially provides the learning guidance tailored to “correcting” what the student model has failed to accurately predict. This process is applied for transferring the ranking information from the user-side as well as the item-side to address sparse implicit user feedback. Our experiments show that the proposed method outperforms the state-of-the-art baselines, and ablation studies validate the eectiveness of each component. • Information systems → Learning to rank; Collaborative ltering; Retrieval eciency. Recommender System; Knowledge Distillation; Learning to Rank; Model Compression; Retrieval eciency ACM Reference Format: Youngjune Lee,Kee-Eung Kim. 2021. Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3459637.3482093 In this era of information explosion, Recommender Systems (RS) are widely used in various industries to provide personalized user experience [3,12,16,18]. For achieving higher recommendation accuracy, the recommendation model has become very large to capture the complexity of personalized recommendations [11,17, 19,26,28,30]. However, large models incur correspondingly large computational cost as well as high latency for inference, which has become one of the major obstacles for real-time service [9,11,17]. To reduce the inference latency, early methods adopt hash techniques [13,20,21,30] or tree-based data structure [1]. However, they have problems such as easily falling into a local optimum or applicable only to specic models [9,28]. To address the problems, Knowledge Distillation (KD) has been actively studied for RS [9,10,17,19,26,28]. KD is a model compression technique that improves the performance of a small student model by transferring the knowledge of a pre-trained large teacher model [2,4,6,25]. During the distillation, the teacher model provides additional supervision which is not existent in the users’ feedback, so the student model can achieve a higher recommendation accuracy compared to the student model trained only on the original feedback. The state-of-the-art method, Relaxed Ranking Distillation (RRD) [9], formulates the distillation process as a ranking matching problem between the recommendation list of the teacher model and that of the student model. In other words, it utilizes the ranking orders among the items from the teacher model as additional supervision to guide the student model, and trains the student model to preserve the ranking orders of the teacher model. This ranking-distillation approach transfers the relative preference order among the user’s preferred items, which is the key knowledge directly aecting top𝑁recommendation accuracy. As a result, it signicantly improves performance over the previous methods [19,26] that do not directly utilize the ranking information [9]. Still, there are limitations in RRD. First, it transfers the knowledge without consideration of the prediction errors of the student model. As the student model gets more accurate in matching the prediction of the teacher model, repeatedly distilling the ranking information that the student model already correctly predicts cannot eectively enhance the student model and makes the training inecient. We argue that the knowledge to be distilled should be dynamically changed based on the student model’s prediction error, enabling “correction” for what the student model has not yet predicted accurately. Second, it only transfers user-side ranking information, i.e., ranking orders among the items. Previous studies have pointed out that learning only with the user-side ranking information degrades the quality of user representation [7] and provides a view insucient to fully understand the sparse implicit feedback [10,14]. Particularly in KD where the student model’s capacity is limited, these problems can be further exacerbated and severely degrade the performance. In this work, we propose a novel Dual Correction strategy for Distillation (DCD), which aims to address the aforementioned shortcomings. To this end, DCD rst computes discrepancy between the ranking list of the teacher model and that of the student model, then decides what knowledge to be distilled based on the discrepancy. By doing so, DCD provides guidance tailored to correct what the student model has failed to correctly predict, which helps to nd an eective path for the student model’s training. This process is conducted for dual-side ranking, i.e., for the user-side and the item-side, providing a comprehensive view to better understand both users and items [7,14]. We validate the superiority of the proposed method with extensive experiments on real-world datasets, and provide an ablation study showing the eectiveness of each proposed component. We focus on top-𝑁recommendation task for implicit feedback [8, 18]. Given implicit user-item interactions, a recommender system provides a ranked list of top-𝑁unobserved items for each user. The distillation process is conducted as follows: First, we train a large model (teacher) using the implicit feedback. Then, we train a small model (student) with the same feedback data along with the ranking list predicted from the teacher. The ranking information reveals the detailed preference orders among the unobserved items, which helps the training of the student. Our goal is to design a distillation strategy that allows the student to eectively follow the teacher’s ranking list. We denote the teacher by𝑇and the student by𝑆.𝑅and𝑅 denote the user-side ranking list for user𝑢(i.e., the list of the unobserved items) predicted by the teacher and the student, respectively. 𝑅(𝑖)denotes the rank of item𝑖in the ranking list where a lower value means a higher ranking position, i.e.,𝑅(𝑖) =0 is the highest ranking. For the item-side ranking list, we simply reverse the notation of the user-side. Concretely,𝑅and𝑅denote the itemside ranking list for item𝑖(i.e., the list of the unobserved users) predicted by the teacher and the student, respectively, and𝑅(𝑢) denotes the rank of user 𝑢 in the ranking list. The ranking distillation (RRD) [9] formulates the distillation as a ranking matching problem between the ranking list of the teacher and that of the student (i.e.,𝑅and𝑅). Specically, the method trains the student to preserve the orders of ranking in𝑅by using a variant of ListMLE [29]. The core idea is to dene a permutation probability based on the the student’s ranking scores, and train the student to maximize the likelihood of the teacher’s ranking 𝑅. To make the student better focus on top-ranked items, we also adopt relaxed permutation probability [9] that ignores the lowranked items’ detailed orders. Formally, let𝑅is decomposed to two sub-ranking lists𝑅= [𝜋;𝜋], where𝜋includes a few top-ranked items and𝜋includes the remaining items (𝑢and𝑇are omitted for simplicity). The relaxed permutation probability of the ranked list 𝜋 for the student 𝑆 is dened as follows: 𝑝 (𝜋 |𝑆) =exp 𝑆(𝑢, 𝜋)ÍÍ where𝜋is the𝑘-th item in𝜋,𝑆 (𝑢, 𝜋)is the score of the user-item interaction predicted by the student. By maximizing the probability, the student learns the detailed ranking orders in𝜋while lowering all the ranks of items in𝜋below the lowest rank of items in𝜋, which allows the student to focus more on top-𝑁ranking orders. The student is trained by the ranking knowledge distillation (RKD) loss as follows: whereLis the loss for training the base model using the im-Í plicit feedback data, andL= −log 𝑝 (𝜋|𝑆)is the permutation loss dened for the users in mini-batch 𝐵. We present Dual Correction strategy for Distillation (DCD) that adaptively assigns more concentrations on training instances that the student fails to predict correctly, unlike the prior methods such as RRD that generate training instances solely based on the teacher’s predictions. This correction is used for transferring the ranking information from the user-side as well as the item-side, providing a comprehensive view to understand both users and items. 2.3.1Identifying discrepancy between Teacher model and Student model. DCD rst identies discrepancy between𝑅and 𝑅to decides what knowledge to be distilled. We dene two types of discrepancy: 1) underestimation error and 2) overestimation error. The underestimation error means that the student predicts a low ranking position whereas the teacher predicts a higher ranking position, i.e.,𝑅(𝑖) > 𝑅(𝑖). Thus, the student needs to be corrected to give a lower rank value for(𝑢, 𝑖). The overestimation error means the opposite, the student predicts a high ranking position whereas the teacher predicts a lower ranking position, i.e.,𝑅(𝑖) < 𝑅(𝑖), which needs to be corrected to give a higher rank value. The userside errors are computed as follows: We use𝑡𝑎𝑛ℎ, which is a saturated function, to treat the errors above a certain threshold equally, allowing the student to learn the teacher’s knowledge on most of the discrepant predictions.𝜇is a hyperparameter that controls the sharpness of the tanh function. Using the computed errors, we identify the discrepant predictions that need to be corrected. Concretely, we sample𝑀underestimated items and𝑀overestimated items. Both sampling probabilities are proportional to the degree of a discrepancy. where𝑝(𝑖)is the sampling probability of underestimated items and𝑝(𝑖)is the sampling probability of overestimated items for user𝑢. These discrepant items are dynamically changed based on the prediction errors of the student during the training, and will be corrected by the correction loss (Sec. 2.3.2). DCD also provides the corrections for discrepancy in terms of the item-side ranking. As pointed out in the previous work [7,14], learning only the user-side ranking degrades the quality of user representation [7] and also provides a restricted view insucient to understand the sparse implicit feedback [14]. Especially, in KD where the student’s capacity is highly limited, these problems can be further exacerbated, which leads to degraded performance. Similar to the user-side, we identify the discrepant predictions on the item-side. We sample𝑀underestimated users and𝑀overestimated users based on the discrepancy between𝑅and𝑅. The sampling probabilities are as follows: where𝑝(𝑢)is the probability of underestimated users and𝑝(𝑢) is the probability of overestimated users for item𝑖. Without loss of generality, 𝐷(𝐴, 𝐵) = 𝑡𝑎𝑛ℎ(𝑚𝑎𝑥 (𝜇(𝑅(𝑢) − 𝑅(𝑢)), 0)). 2.3.2Dual Correction Distillation Loss. Now, we correct the discrepant predictions in the user-side (summarized by𝑀and𝑀) and the item-side (summarized by𝑀and𝑀). From the points of the teacher, the underestimation errors contain the predictions that should be higher-ranked, whereas the overestimation errors contain the predictions that should be relatively lower-ranked compared to the former. As consistently shown in the existing distillation work [9,19,26], the student takes a huge benet by learning the teacher’s knowledge with a particular emphasis on the high-ranked items, because it directly aects the top-𝑁recommendation accuracy. In this regard, we design the correction loss that corrects the ranks of the underestimation errors in detail and lowers the ranks of the overestimation errors overall. Let𝜌denote the sorted lists of𝑀by the original order in 𝑅, and𝜌denote the sorted lists of𝑀by the order in𝑅. The user-side correction distillation (UCD) for user𝑢is conducted by maximizing the following relaxed permutation probability: 𝑝 (𝜌|𝑆) =ÍÍ, (7) where𝜌is the𝑘-th item in𝜌. UCD is applied for the users inÍ mini-batch𝐵, i.e.,L= −log 𝑝 (𝜌|𝑆). Analogously, item-side correction distillation (ICD) for item𝑖is conducted by maximizing the following relaxed permutation probability: ICD is also applied for correcting errors with respect to the items inÍ the batch, i.e.,L= −log 𝑝 (𝜌|𝑆). Finally, the proposed DCD trains the student with the following loss function. where𝜃is the learning parameters of the student.𝜆and𝜆 are hyperparameters controlling the user-side and item-side corrections, respectively. Note thatLis computed for the same ground-truths regardless of the discrepancy during the training. Finally, our dual correction loss provides dynamically changing guidance to correct the student errors for more eective training. Table 1: The number of parameters and inference time for generating recommendation list for every user. We closely follow the setup of the state-of-the-art method, RRD [9]. Specically, datasets, base models, evaluation protocol, and the metrics are the same as [9]. Due to the limited space, we omit the detailed explanations of the setup. Please refer to [9]. Datasets.We use CiteULike [27] and Foursquare [22] which are public real-world datasets. After the preprocessing [9], CiteULike has 5,220 users, 25,182 items, and 115,142 interactions. Foursquare has 19,466 users, 28,594 items, and 609,655 interactions. Base models.We use two base models for the top-𝑁recommendation: BPR [24] and NeuMF [5], which have dierent architectures and optimization strategies. For both models, the dimension of user/item representations are set to 200 for the teacher model, and 20 for the student model. Following [9], we denote the student model trained without distillation as "Student". Table 1 presents the number of parameters and inference time. The inferences are made using PyTorch with CUDA from TITAN Xp GPU and Intel i7-4770 CPU. It shows that the smaller model has lower inference latency. Evaluation protocol and metrics.We use the leave-one-out evaluation protocol whereby two interacted items for each user are held out for test/validation, and the rest are used for training [9]. We adopt two top-𝑁ranking evaluation metrics, namely Hit Ratio (H@𝑁) and Mean Reciprocal Rank (M@𝑁). We compute the average score of those two metrics for each user. Finally, we report the average of the ve independent runs. Baselines.We compare DCD with the state-of-the-art ranking distillation method, RRD [9]. Note that we do not include the previous methods distilling point-wise information (e.g., RD [26], CD [19]), because RRD already outperforms them by a huge margin [9]. Implementation details.We use PyTorch [23] for implementation and train all models with Adam optimizer [15]. For each base model and dataset, we tune the hyperparameters by grid search on the validation set. We tune learning rate and L2 regularizer ∈ {10,10,10,10,10,10}. In the case of RRD-specic hyperparameters, we tune them in the ranges suggested by the original paper. For the dual correction loss, we tune𝜆, 𝜆∈ {1,10,10,10,10,10}, and conduct the sampling process every 5 epochs. The number of discrepant users/items (𝑀,𝑀) is set to 40, but it can be further tuned. Lastly, 𝜇 is set to 10. 3.2.1Overall Evaluation. Table 2 presents top-𝑁recommendation accuracy of the methods compared. DCD achieves significantly higher performance than RRD on both datasets and both Table 2: Performance comparison. improve.r denotes the improvement of DCD over RRD and improve.s denotes the improvement of DCD over Student. * and ** indicate 𝑝 ≤ 0.005 and 𝑝 ≤ 0.0005 for the paired t-test of DCD vs. RRD on H@5. NeuMF base models. Also, in terms of the number of recommended items (𝑁), DCD shows larger improvements for H@5/M@5 compared to H@10/M@10. Namely, DCD has a better performance at predicting the top-ranked items than RRD, which is practically advantageous for real-world RS, which gives the users the most preferred items. 3.2.2Ablation Study. We provide ablation study of the key components of DCD in Table 3. We compare the following ablations: 1)w/o Correctiontransfers the user-side and item-side ranking information without the correction strategy, i.e., RRD + item-side RRD. 2)w/o Item-sideandw/o User-sideablate ICD and UCD from DCD, respectively. 3)w/o Samplingdeterministically selects items/users with the largest underestimation error and overestimation error without the sampling process (Sec. 2.3.1). We observe that each proposed component is indeed eective in distilling the ranking information. This result supports our claim that the supervision from the teacher model should be dynamically changed based on the student’s errors (w/o Correction) and distilling the single-side ranking is insucient (w/o Item-side and w/o User-side). Also, the comparison with “w/o Sampling” shows that a certain degree of exibility is benecial in choosing the discrepant predictions for the correction strategy. 3.2.3Further Analysis. We provide further analysis on DCD. For the sake of the space, we report the results of BPR on CiteULike. First, Figure 1a presents the average ranking-discrepancy of various methods. In specic, we compute the discrepancy as |𝑅(·) − Table 3: Ablation analysis on Foursquare dataset. BPRw/o Correction 0.5318 0.3408 0.6710 0.3594 NeuMFw/o Correction 0.5147 0.3214 0.6704 0.3424 Figure 1: Eects of DCD. (a) The average discrepancy from Teacher, (b) H@5 with varying 𝜆and 𝜆. 𝑅(·)|for the user-side (and for the item-side) top-50 recommendation list produced by the teacher model. We compute it for all users (and for all items), then report the average value. We observe that the proposed correction strategy eectively reduces the discrepancy between the teacher model and the student model. All the correction-based methods (i.e., ICD, UCD, and DCD) achieves lower discrepancy than RRD. Also, DCD achieves the lowest discrepancy in both user-side and item-side, which supports its superior recommendation performance. This also again shows the importance of the dual-side ranking correction. Lastly, Figure 1b shows the eects of𝜆and𝜆. Note that𝜆=0 &𝜆=0 corresponds to RRD. We again observe that both user-side and item-side corrections are indeed eective. The best performance is achieved when 𝜆is around 10-10and 𝜆is around 10-10. We propose DCD, a dual correction strategy for ranking distillation in top-𝑁RS. Unlike the existing method based on unilateral distillation, DCD provides guidance designed to correct the errors that the student model has failed to learn. By considering the prediction errors of the student model, DCD helps to nd an eective path for the student model’s training. DCD also considers the user-side ranking and item-side ranking simultaneously, providing a comprehensive view to understand both users and items. We validate the eectiveness of DCD with extensive experiments on real-world datasets. Also, we provide in-depth ablation study to ascertain the validity of each proposed component. For future work, we will investigate the eects of DCD on various base models. Acknowledgement.The authors thank SeongKu Kang for contributing to the implementation and improvement of DCD. This work was supported by IITP grant funded by the MSIT: (No.20190-00075, Articial Intelligence Graduate School Program(KAIST)) and the ETRI: (Contract No. 21ZS1100).