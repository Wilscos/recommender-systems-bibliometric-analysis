Industrial recommender systems are frequently tasked with approximating probabilities for multiple, often closely related, user actions. For example, predicting if a user will click on an advertisement and if they will then purchase the advertised product. The conceptual similarity between these tasks has promoted the use of multi-task learning: a class of algorithms that aim to bring positive inductive transfer from related tasks. Here, we empirically evaluate multi-task learning approaches with neural networks for an online advertising task. Specically, we consider approximating the probability of post-click conversion events (installs) (CVR) for mobile app advertising on a large-scale advertising platform, using the related click events (CTR) as an auxiliary task. We use an ablation approach to systematically study recent approaches that incorporate both multitask learning and “entire space modeling” which train the CVR on all logged examples rather than learning a conditional likelihood of conversion given clicked. Based on these results we show that several dierent approaches result in similar levels of positive transfer from the data-abundant CTR task to the CVR task and oer some insight into how the multi-task design choices address the two primary problems aecting the CVR task: data sparsity and data bias. Our ndings add to the growing body of evidence suggesting that standard multi-task learning is a sensible approach to modelling related events in real-world large-scale applications and suggest the specic multitask approach can be guided by ease of implementation in an existing system. CCS Concepts: • Computing methodologies → Multi-task learning; Neural networks; Batch learning. ACM Reference Format: Conor O’Brien, Kin Sum Liu, James Neufeld, Rafael Barreto, and Jonathan J Hunt. 2021. An Analysis Of Entire Space Multi-Task Models For Post-Click Conversion Prediction. In Fifteenth ACM Conference on Recommender Systems (RecSys ’21), September 27-October 1, 2021, Amsterdam, Netherlands. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3460231.3478852 Many industrial recommender system applications, particularly in the online advertising space, have tasks whose representations or labels are meaningfully related to other tasks. Sometimes this manifests as a strict causal relationship, such as a purchase event conditional on an add-to-cart action. Other times the tasks are merely correlated, for example, replying to and “favoriting” a social media post. The existence of these similar events naturally raises two questions: (1) how should the objectives be modelled and (2) what is the best way to elicit positive transfer between tasks. The answers to these questions are, of course, dependent on the problem at hand. In this paper, we restrict our focus to a specic, but important, real-world scenario: predicting post-click conversion rates (CVR) for the purpose of online advertising on Twitter. In the most straightforward setup, ad click-through rate (CTR) and CVR prediction are treated as separate supervised learning problems with two models trained independently. Of these tasks, CVR prediction is usually more challenging for two reasons. The rst reason isdata sparsity: every impression shown to a user generates training data for a CTR model whereas only impressions which result in a click generate training data for a CVR model. The number of impressions that generate ad-click engagements are typically a small fraction, sometimes less than 1%, so the CVR model must be trained with signicantly less data. This challenge is exacerbated by the fact that exploration data is expensive to obtain as there is opportunity cost associated with each served ad impression. Put dierently, serving random trac for better exploration comes with a signicant nancial disincentive. The second reason isdata bias: the CVR model needs to make predictions over all impressions, however, only impressions which resulted in a click are used as training examples. That is, for an impression which did not result in a click, we lack the counterfactual information about whether this would have resulted in a conversion had the user clicked on the ad (see Figure 1). Recent work [6] introduced an approach to modeling CVR they named Entire Space Multitask Model (ESMM) which has two key ideas: (1) sharing parameters for representation learning between the CVR and CTR problem and (2) modeling the CVR unconditionally which allows training the CVR model on all impression samples (they term “entire space” modeling). We expand on these descriptions below. Here we systematically investigated, through the use of ablation studies, the mechanisms behind the good performance of the ESMM model. We reproduced the ndings of [6] that ESMM outperforms modeling CVR and CTR as separate models on a dierent, industry scale dataset. However, we also found that a similar level of performance can be obtained by approaches which incorporated only one aspect of the ESMM model. That is, models which use only parameter sharing between CVR or CTR, or only “entire space” training. 1.1 Problem Formulation We consider the conversion prediction problem under standard supervised learning assumptions. That is, we are assumed to be presented with an ad context, denoted𝑥, that represents the attributes of the ad placement, user request, and the ad itself, drawn i.i.d. from some stationary distribution,𝐷. If this ad candidate is presented and observed by the user (an "impression") then the user will elect to click on the ad, denoted𝑦 =1, with probability𝑝 (𝑦 =1|𝑥). Additionally, the user may also elect to convert by installing the advertised application or purchasing the product, denoted𝑧 =1, with probability𝑝 (𝑧 =1|𝑦 =1, 𝑥). By construction, a conversion is only possible if the user has clicked: that is,𝑝 (𝑧 =1|𝑦 =0, 𝑥) =0. The goal is to produce a classication function,𝑓 (𝑥;𝜃) → [0,1], that minimizes the expected (cross-entropy) loss for any new example drawn from the same distribution. That is, we aim to nd model parameters, 𝜃 , where arg minE[𝐿(𝑓 (𝑥; 𝜃), 𝑧)], for 𝐿(𝑝, 𝑧) = −(𝑧 · log(𝑝) + (1 − 𝑧) · log(1 − 𝑝)). 1.2 Related work Deep learning based models have been widely studied for use in multi-task and transfer learning [2,3,5,10,13,15]. One common approach to transfer learning is to share neural network parameters between related tasks, until the nal hidden layer of a deep network [14]. The general consensus from this body of work is that relatively straightforward techniques often work well in practice and can greatly reduce the amount of time or data required to learn a new task Fig. 1. Event flow from impressions to clicks ( compared to those without ad-clicks, that is, missing counterfactuals that cause data bias. Conversion models trained only on clicked data, predicting aected by both the sparsity and bias when used for conversion inference on the space of all impressions. (see survey: [ done naïvely [4, 11]. Regarding the specic task of conversion prediction for online advertising, the expense of obtaining labeled conversion data and the inherent rarity of successful advertising-driven conversions has encouraged the development of multi-task learning approaches in numerous industrial contexts. While the business-sensitive nature of this application does dissuade publication of production systems, there are some representative examples in the literature. For instance, as early as 2014 hierarchical multi-task learning (MTL) conversion models were deployed at scale at Yahoo [ described a multi-task feature engineering approach for online advertising. The approach discussed in [ the problems of data sparsity in the post-click conversion task through a proposed a multi-task model sharing parameters between CTR and CVR tasks. Additionally, the authors aim to address the dataset bias issue by predicting the joint probability of click and conversion – treating the marginal CTR prediction as an auxiliary task. This work demonstrated improved prediction performance over baselines. Also, notably, [ specic focus towards the issue of delayed feedback; while interesting, the challenge of delayed feedback falls outside the scope of this work. 2 METHODS As detailed above, the main quantity of interest for ad ranking systems is the user-ad conversion rate, are a number of ways to decompose this prediction, which result in dierent characteristics and may allow for dierent MTL approaches. For instance, choosing to ignore the decomposition of leaves only a single prediction, and hence a single-task deep neural network (DNN) architecture training in the impression space. In this work, we test 6 dierent approaches, including the naïve choice just described. Although MTL models have the potential to become complex we constrain our analysis to the use of (1) hard parameter sharing, (2) careful selection of training spaces and prediction heads, and (3) conditionally aware CVR prediction. We describe the 6 modeling approaches below We denote the baseline approach Independent Prediction (IP) which treats CTR and CVR as separate tasks: two multilayer perceptron (MLPs) with no shared parameters. CTR prediction, 14]). However, transfer learning can be challenging to do well, and can easily result in negative transfer if and also present this information for direct comparison in Figure 2. click data and CVR prediction,ˆ𝑝 (𝑧 =1|𝑦 =1, 𝑥), is trained using impressions that were clicked,𝑦 =1. The nal prediction is constructed as the product of those two predictions,ˆ𝑝 (𝑧 = 1,𝑦 = 1|𝑥) =ˆ𝑝 (𝑧 = 1|𝑦 = 1, 𝑥) ·ˆ𝑝 (𝑦 = 1|𝑥). The primary approach introduced in [6] is to train a model to directly predictˆ𝑝 (𝑧 =1, 𝑦 =1|𝑥)along with predicting ˆ𝑝 (𝑦 =1|𝑥), and constructing the network such thatˆ𝑝 (𝑧 =1, 𝑦 =1|𝑥) =ˆ𝑝 (𝑧 =1|𝑦 =1, 𝑥) ·ˆ𝑝 (𝑦 =1|𝑥). That is, there is an internal node in the network that can be considered as a prediction ofˆ𝑝 (𝑧 =1|𝑦 =1, 𝑥), but there is no loss directly optimizing this prediction. We refer to this approach as the Entire Space Multitask Model (ESMM), the name used by [6]. Our model is conceptually equivalent but the specic architecture is dierent (see Section 2.3), in that we used hard parameter sharing in early DNN layers, as opposed to just the feature embeddings. The ESMM approach introduces 3 characteristics distinct from the baseline (IP) approach: • ESMM uses hard parameter sharing between the CTR and CVR task. (Shared Parameters) •ESMM trains the install prediction over the entire space of impressions by predictingˆ𝑝 (𝑧 =1, 𝑦 =1|𝑥)rather thanˆ𝑝 (𝑧 = 1|𝑦 = 1, 𝑥). (Entire Space) • ESMM implicitly weights the install prediction’s loss by the click prediction. (Weighted CVR) In order to separate the impact of these characteristics and understand their individual and combined eects we tested several variants of ESMM. Entire Space Multitask Model - No Shared (ESMM-NS) uses the same losses as ESMM Fig. 2. Dierent modeling approaches evaluated here. Independent Predictions (IP) serves as the baseline which models CVR and CTR as independent tasks. Entire Space Multitask Model (ESMM) includes all 3 characteristics, namely Shared Parameters, Entire Space prediction and Weighted CVR. The other models incorporate only some of these characteristics, allowing us to study their contributions to performance. The MLP labels (CTR, CVR) indicate the space of data used in training. CTR is all training examples (entire space), CVR is the subset of examples where𝑦 =1. (e.g. the ESMM-NS has two MLPs, both of which are trained on the CTR data). See Section 2 for a description of each model. but has no shared parameters between the CVR and CTR prediction tasks. The ESSP-Split model uses the same losses as the ESMM model and Shared Parameters, but the two predictions, independent heads with no constraint on their relationship same approach and losses as the IP model (that is, it is not an Entire Space model) but shares parameters between the CVR and CTR prediction. Finally, Entire Space Prediction (ESP) just predicts training over the whole space, but makes no use of the CTR task. 2.1 Dataset and Training Setup The evaluation dataset for this paper is comprised of real click and conversion data for digital mobile app install ads served on Twitter, as well as MoPub, Twitter’s mobile display network (e.g. in-game ads). While this real-world dataset allows us to evaluate the performance of these technologies on a truly representative problem the dataset itself is not publicly available due to numerous user privacy and business-sensitive constraints. Specically, in each of the evaluations below a xed dataset of click and conversion events collected during a consecutive number of days in mid 2020 were used for model training and evaluation. The raw data consisted of over 5 billion ad impressions (later down-sampled, as discussed below), over 50 million ad clicks, and several million conversion events below, evaluation hold-out sets for these experiments always ensure past vs. future evaluation, such as training on the previous 14 days of data and testing on the 15th day. Also note, when training on the rst shued to make the data approximately i.i.d. Below, the results reported are for a single evaluation day. However, the robustness of these modeling approaches to temporal shift, i.e. how prediction performance changes as the model is tasked to make predictions further into the future without the benet of retraining, were also evaluated. While this is a particularly interesting, and practically relevant, aspect of this problem we ultimately did not observe a noteworthy dierence between the approaches in this regard. 2.1.1 Negative downsampling. Imbalanced datasets are a common problem in advertising datasets. We downsampled negative examples by some factor, factor,𝑓. Note that all samples where The evaluation dataset was generated identically, with the same downsampling and upweighting procedure applied to negatives for the click task. 2.2 Metrics Ultimately, for the purpose of ranking potential ads and valuing impressions, we are interested in the probability that an impression leads to a conversion, well-calibrated so we focused on the cross-entropy loss (we report PR-AUC in Appendix B). We report our scores as relative percentage performance improvements versus the baseline model. 2.3 Model architecture Since the models each have slightly dierent characteristics the exact architectures vary; however, the number of trainable parameters was kept comparable across all MLPs. The multi-task models (except ESMM-NS) had two shared layers after the feature embeddings, followed by two layers per head as the model branched. Models using a "Weighted CVR", e.g. ESMM, had the two branches reconnect with no trainable parameters after the𝑝 (𝑦 =1|𝑥)entity and the implicit𝑝 (𝑧 =1|𝑦 =1, 𝑥)entity, as in [6]. Models without this characteristic, e.g. IPSP, had a single entity at the root of each branch. We experimented with larger models, in terms of both wider layers and greater depth (more layers) for both the shared and branched parts of the network, but this did not bring any benet. Larger models were also trained with batch normalization layers both included and excluded. The lack of benet might be explained by lack of sucient training data; though this is just another example of the wider open question about why larger models do not consistently perform better on recommendation tasks [8]. 3 RESULTS We manually tuned all the models for similar numbers of experiments to nd the best hyperparameters. In general, the models were fairly robust to hyperparameter choices, with the exception being the ESMM model which did have slightly more varied performance as a function of hyper-parameter values, discussed below. Figure 3 gives a summary of the key results from our experiments. They provide clear evidence that a meaningful decomposition of the prediction task has clear benets, shown by ESP performing 2% worse than IP. This naïve approach to training on the entire space of predictions leaves the model susceptible to learning noise when the positive install labels are so relatively infrequent, unaided by the useful signal that the click labels can provide. There is then a performance jump to the ESSP-Split model, with a marked increase versus ESP. This comparison highlights the utility of hard parameter sharing. This is the ‘classical’ benet of MTL - which is often discussed in terms of "shared representations" or additional “regularization” [9]. This same impact is demonstrated by IPSP, Fig. 3. Model performance by multi-task setup. Performance gains normalized by the mean score of the baseline IP model. The standard error for each model’s performance dierence (against IP mean) is calculated across at least 10 runs. The results show that there are multiple mechanisms for inducing positive transfer, but that hard parameter sharing alone (IPSP) may be optimal. Beer than column indicates models that this model outperforms 𝑝 < 0 .01, 2-sided t-test. the best performing model, which kept the tasks as independent heads but leveraged combined early layer feature transformations. The benet of the signal from the CTR task through shared parameters provided all of the gains in performance seen in alternative model designs. Surprisingly, ESMM-NS performed very competitively. By simply weighting the loss on the CVR head by the click prediction, signicant). We suggest that what this highlights is the extent of the data bias problem. If training on the entire space then there has to be some mechanism to assign ‘relevance’ to the CVR samples – otherwise, you get the poor performance of ESP. This, seemingly small detail, is (empirically) more important than any classical transfer learning arguments (since ESMM beats ESSP-Split). We do note that ESMM-NS increases the size of the model compared with all the other MTL designs, since, like baseline, it has two separate embeddings which is where most parameters exist even in very deep RecSys models. However, we don’t suggest that the extra parameters are really helpful in this instance. In fact, given (over)tting biased data seems to be an issue, more parameters alone would be likely to make things worse. Finally, ESMM had similar performance to IPSP, albeit with greater variability. We tentatively suggest that with increased eort it may be possible to get consistent, larger performance gains from a well-tuned ESMM model. That is, we posit that the marginal benet from increased model tuning for ESMM is much greater than for any of the other models. This would make sense given its design allows for the most complex learning interactions. But it is also a weakness in that simpler approaches may require less tuning. An alternative view is that IPSP not training on the entire space may be positive since this avoids directly optimizing 𝑝 (𝑧 = 1,𝑦 = 1|𝑥), which is arguably the most dicult training objective (i.e. having the worst signal-to-noise ratio). 4 CONCLUSION We provide clear evidence that simple MTL methods can improve conversion model performance. Our experiments show that hard parameter sharing alone (IPSP) might be optimal for improving performance with signicant, and relatively easy, wins versus a factored (IP) or naïve (ESP) baseline. We also establish the importance of counteracting the data bias problem that occurs when trying to predict installs on the entire space of impressions. The surprisingly simple solution of a weighted conditional install prediction tackles the bias well. However, we note that the gains from these two characteristics do not seem to be additive when combined. Whilst we studied this problem in the context of clicks and conversions, we suggest that this simple methodology can be used to explore other conditionally dependent tasks, as the methods to counter the fundamental problems of data sparsity and data bias should generalise well. 5 ETHICAL CONSIDERATIONS The research in the submitted paper has been reviewed as part of our organisation’s research and publishing process. This includes privacy and legal review to help ensure that all necessary obligations are satised. As with many companies that rely on advertising to fund free and open access to products and services, our platform utilizes algorithms that recommend personalized content, including ads. Recommender systems are imperfect, and automated decision systems may not treat all people equitably. The identication and prevention of inequity and bias in ML is a growing eld of research that we closely follow. Despite ongoing eorts to detect and prevent algorithmic amplication of bias, inequality still exists in society and therefore may impact the source data used to train many models. The authors of this paper are not aware that the experiments conducted resulted in any positive or negative impacts on the inherent bias that exists in recommender systems. ˆ𝑝 (𝑦 =1|𝑥), the model was able to perform better than baseline, and even beat ESSP-Split (not statistically