Deep Learning Recommendation Models (DLRM) are widespread, acco unt for a considerable data center footprint, and gr ow by more than 1.5x per year. With model size soon to be in terabytes range, leveraging Storage Class Memory (SCM) for inference enables lower power consumption and cost. This paper evaluates the major challenges in exten ding the memory hierarchy to SCM for DL RM, and presen ts different techniques to improve performance through a Software Deﬁned Memory. We show how underlying technologies such as Na nd Flash and 3DXP differentiate, and relate to real world scenarios, en abling from 5% to 29% power savings. Recommendation models are ubiquitous across web compan ie s (Gomez-Uribe & Hunt, 2016; Covington et a l., 2016; Cheng e t al., 2016; Smith & Linden, 2017; Ma et al., 2020; Lopez et al., 2021), with ranking and click throug h rate (CTR) prediction (Hazelwood et al., 2018; Park et al., 2018; Gupta et al., 2020) being among the widely deployed use cases. Such use cases account for a considerable demand in infrastructure resource (Zhao et al., 2019; Gupta et al., 2020; Naumov et al., 2020) and rapid increase in data-center s footprint (Anderson et al., 2021). Deep learning recommendation models (DLRMs) are often composed of sets of fully connected layers (MLPs) and embedding tables (Naumov et al., 2019), and tend to be very large with up to trillions of parameters. One of the main reasons for such high numb e r of parameters is that more sparse features (materialized through embedding tables) usually result in better mo del quality (Park et al., 2018). Hence the model size is mainly dictated by the emb edding tables, which could accoun t for 100s of Gigabytes at the time of serving ( inference), and increases rapidly year over year (e.g. 1.5x per year (Jouppi et al. , 2021) or more). The massive size of DLRM models requires considerable amount of memory ca pacity to serve. Relying on DRAM is expensive. Interestingly, no t all such capacity is required Facebook, Menlo Park, USAGeorge Mason Universi ty University of Illinois Chicago. Correspondence to: Ehsan K. Ardestani <ehsanardestani@fb.com>. Figure 1.Embedding Table Size (x-axis) and Bytes per query (y-axis) in a 140GB model. The model has 734 tables, out of which 445 are user tables accountin g for 100GB. Majority of tables, and hence model capacity, requires l ow BW. at the same memory bandwidth (BW). There is a high variation in BW and Size among the embedding tables with some being a ccessed many times per query (e.g. 1000 accesses per query, hence requiring high memory BW), while others do not (e.g. 1 access to a row hence low BW requirement). The inherent difference b e tween batched accessing in user related e mbedding tables and item related embedding tables (explaine d in Section 2) further skews such BW requirement, resulting in majority of capacity to require much sm aller BW compared to a subset (mainly item related ones) requiring high BW. Figure 1 shows an example of such skew. Presence of locality accessing the embedding tables (Section4.2) would further allow for leveraging slower, but denser memory through caching (Eisenman et al., 2018). Extending the memory hierarchy beyond DRAM to include slower mem ory technologies, such as Storage Class Memory ( SCM) , provides a cheaper and more power efﬁcient approa c h to increase memory capac ity per host. Considering the scale of deployment, the power saving could be in the order of 10s of Mega Watts. Given the importance of power in serving such m odels, it is becoming increasingly appealing to leverage a tiered memory. However, given the latency and BW requirement, and access granularity issues, deploying such a solution is challenging. This paper presents a software deﬁned me mory system which extends the m e mory hierarchy to SCM to accommodate the ever inc reasing memory capacity needs of massive DLRM models at inferenc e . To our knowledge, this is the ﬁrst paper that not only entertains the possible so lutions to some aspects of enabling such technology for inference, but also evaluates all the challenges that need to be addressed by pushing the solu tion all the way to real world datacenter deployment, and evaluating h ow the solution adds value for the end to end warehouse scale usecase. The contributions of this pa per are as follows: • Extends memory available to DLRM using SCM through a Software Deﬁned Memory stack, which can leverage different underlying technologies suc h as Nand Flash and Optane SSD. • Enable s smaller granularity of read acc ess, down to dword, fo r NVMe devices, which saves latency and BW, and avoids read ampliﬁcation. • Evaluates pooled embedding cache to improve performance by bypassing dequan tization an d pooling when possible, and consider s a range of trade offs with the cheaper capacity in slower m e mory to gain further performance when possible, namely d e-quantization and de-pruning at load time. • Presents end to end results for running the usecases, and discusses the add ed value of the solution in realistic warehouse scale deployment scenarios. Recommendation models rank a set of items accorduses the recommendation models for selecting items in its catalog (Smith & Linden, 2017; Ma et al., 2020; Lopez et al., 2021), Netﬂix for showing movie options (Gomez-Uribe & Hunt, 2016), Google for displaying personalized advertisements (Cheng et al., 2016), and Facebook for ranking and click through rate (CTR) prediction (Hazelwood et al., 2018; Naumov et al., 2020). Deep Learning Recommendation Models (Naumov et al., 2019) a re often composed of two main components: 1. Embeddings, wh ic h map the categorical features (e.g. what subject a user has shown interest in) into dense representations. Different categorical features have varying cardin a lity, and hence require different size when materialize d through emb e dding tables. The embedd ings could be further divided into user embeddings (materializing categorical features for users) and item embeddings (materializing categorical features f or items to be recommended such as news and movies). The embeddings are typically memory intensive. 2. Interaction, which a ggregates continuous features a nd the dense representation of categorical features (e.g. by concatenation), and ca ptures their complex interaction (e.g. by multi-layer perceptrons (M LP)). The interaction components are typically compute intensive. Figure2 depicts the high-level architecture of DLRM models. Botto m MLP reprojects the continuous features (e.g. age of the user) to d ense ones. Th e embeddings com ponents convert the categorical features to dense representation, and the top MLP c aptures the interaction of all the features. BW requirement for embeddings can be deﬁned as where QPS is Query Per Second, which is the rate at which the inference queries are expected to b e processed in a given host, pis the pooling factor (number of embedding rows which needs to be looked up per query) f or table i, and dis the embedding dimension for table i . T is the number of embedding tables in the model. Note that the number of rows in the tables doe s not imp act BW. For infe rence, several items will be evalua te d (ranked) to arrive at the top item s for recommendatio n. Hence an inference query could access the u ser embeddings once for that user, while accessing the items’ embeddings for a batch of items. The user side embedding results could be broadcasted to all the items for Top MLP computation. It’s worth nothing that this is different from training w here on e input sample consists of one user and one item. As a result, the BW requirement per query for user embe ddings is much lower than that of item embeddings. We can rewrite Equation1 as follows: BW = QP S ∗(B(p∗d)+B(p∗d)), i ǫ T, j ǫ T where the batch size for Items and Users (Band B, respective ly) is separated . Tand Tdenote the number of item and user embedding tables, respectively. Given the latency sensitivity of inference queries, Bis typically 1. Bcould in in order of 10s or 100s of items (depending on how fast they can be processed). Our observation shows that more than 2/3 of the model capacity are contributed by the user embeddings. T his could be due to the fact that there is a wider set of categorical features to describe the users, resulting in more user embeddings being used in th e model. The implication is that the bigger portion of model size have lower BW requirem ent. Another impor tant observation is that the execution of user and item embeddings are independent, while the Top MLP has dependency on both. Assuming the user embeddings are the prime candidate to be accommodated by slower memory, as long a s the access time for user embedding is still smaller than tha t of item embeddings, the slower access due to slower memory is not exposed in end to end latency. Equation3 formulates, at high level, the time bud- Large item tables with low pooling factor could be considered for placement on the sl ower memory. However, without lack of generality, in this work we primarily focus on placing user embeddings on slower memory. get for the slower mem ory. time(UserEmbeddings) = time(ObjectEmbeddings). which could be elaborated further as follows: BW(user) refers to the BW requirement at a g iven query for user embe ddings, and BW(items) denotes that of item embeddin gs. Latency and Throughput: The inference of DLRMs a re both latency and throughput sensitive. The latency sensitivity is derived from real time user interaction, requiring the la te ncy in 10s of milliseco nd range for the ranking . At the same time, queries at Data Center level need to be processed within the expected throughput. Given QPS per host at a given target latency, the total throughput (e.g. in a DC region) will tra nslate into a set number of hosts (Equation5-7). Note that th e latency requirement varies across different model/u secases. For example some mod els have strict p99latency requirement with active load balancing to ensure the latency re quirement across the ﬂeet. Other models/usecases could have de sired p95 latency w hich is achieved through static allocation of resources. QP S(HW ) ∝ min(BW (HW )/BW, Comp(HW )/Comp) Latency(HW ) ∝ sum (BW/BW (HW ), Comp/Comp(HW )) Scale Up vs Scale Out: A s the model size increases, either the m emory p er host needs to increase (scale up) or the model needs to be sharded to scale the memory by leveraging multiple hosts (scale out, e.g. see (Lui et al., 2021)). Extending the memory to SCM could be considered as a scale up only approach, or applied to the hosts involved in scale out to reduce the fan out. It needs to be m entioned that increa se in model size usually is acc ompanied with increase in compute intensity of the model as well. So the relevant approac h to serve the model would depend on the compute, memory BW and memory capacity req uirement and their relative ratio. Powe r Boundness: DLRM models keep increasing in their complexity and size faster than the rate new data centers could be developed (e.g. see (Anderson et al., 2021)). Furthermore, DLRM applications account for a considerable portion of infrastructure resources (Gupta et al., 2020; Here, p99 denotes 99 percents of queries needs to be processed within the latency requirement, similar for p95. Zhao et al., 2019). This leads to power boundness of the usecase, with quer y/watt at the acceptable latency being the primary metric to solve for at scale. Tier ed memory directly helps with this top line metric by 1) leveraging more power efﬁcient pe r GB memory when possible, and 2) allowing for better system solution, e.g. not scaling out. TCO: Another important factor impacted by the choice of tiered memory is the Total Cost o f Ownership (TCO). Cheaper memory per GB reduces the total TCO. Furthermore, higher memory capacity per host (scale up) is not always possible, which could require tiered memory or scaling out solution. Such choices shape the end to e nd system serving the model, with different overhead and efﬁciency, and consecutively impact the power provisionin g and TCO beyond component level. Extending the memory hierar chy to SCM ca n b e deployed regardless of the choice of accelerators (e.g. using GPU for inference ), and the hierarchy of faster memo ries (e.g HBM + DRAM). Hence, w e refer to the the last level memory with SCM as SM (for slow memory) and the ﬁrst level(s) of memor y as FM (for fast memory). There is a range of technologies that could be used for SM . Table1 lists some of the currently more readily options. We track the following key parameters for ea ch technology: IO Per Second (IOPS). The access patterns to the emb edding tables are random. We track IOPS instead of GB/s, because the embe dding rows, and hence the access granularity to the SM , is typically much smaller than 4KB block size (read ampliﬁcation). The inference access is read only, with non-fr e quent writes only during model update. Access Granularity. The quantize d embedding rows, while growing, are in 128-256B range. IO Read with higher granularity (e.g. 4KB) will result in read ampliﬁcation and wasted BW. Latency. This is the loaded access latency for a blo ck of data. Different technologies show different curve as the load increase from low to high. Given latency sensitivity of the usecase, we need to oper ate on a latency region that is in order of up to a few 10s of us. Write BW. The only write access happens during model update. In general more symmetric read and write BW becomes more important as the update frequency increase. For example given limited number of DI MM slots per CPU (e.g. 6), with a maximum capacity per DIMM (e.g. 128GB), there is a max DRAM amount (e.g. 768GB) that could be deployed. Figure 3.IOPS and latency for Nand Flash and Optane SSDs. Given each q uery to a table involves multiple lookups (pooling factor), we benchmark each device with average of 20 lookups per IO. The latency is for the batch of 20 lookups. As the results show, Optane SSD provides much lower l atency and higher IOPS than the Nand Flash. 365 ∗ ModelSize/(pDW P D∗ SM Capacity) Sourcing. How many vendors offer the tec hnology. The higher, the better. Nand Flash provides the cheapest op tion, with multiple vendors offering the technology. However, it suffers from two drawbacks. Low rand om IO per second (IOPS), a nd in creased latency as the IOPS increases (Lower endu rance can be offset by capacity). This limits the u seca se to models with low BW requirement. PCI3 3DXP (Optane) provides a good random I OPS (4M at 51 2B) and considera bly better la te ncy proﬁle compare d to Nand ﬂash (O(10) u sec). The endurance is also high enoug h to accommod ate frequent updates. As a result, Optane SSD can e nable tiered memory for the frontier of the models with high capacity and BW requirements. Figure3 Table 1.Different options for the slow memory (SM ). The values are based on public information. Cost is relative to DRAM shows the IOPS and latency proﬁle for Nand Flash and Optane SSD. PCIe ZSSD offers better latency compared to Nand Flash, but does not offer high enough IOPS to set it considerably apart from Nand Flash. DIMM 3DXP impacts the available memory BW to the CPU which is point of concer n. CXL 3DXP would provide the best performance in the set, without having the negative side effect of DIMM 3DXP. But still not as r e adily available as other techn ologies listed here. The c hoice of technology for SM depends on speciﬁc usecase and model characteristics. As the models scale size and BW, the higher BW options become mor e relevant. We observe tha t Nand Flash and Optane SSD enable tiered memory for a wide range of DLRM models (lower-end with less strict p99 latency, and higher-end of BW requirement, respectively). As the model’s cap acity and BW scale overtime, CXL based solution would become more relevant. We evaluated several different design choices for the software stack. Given the scheme could be used fo r a wide range of model co nﬁguration and underlying HW, we evaluate the design choices, such as cache organization, by evaluating a wide range of target models beyond what presented in the results section. We also consider both Inference as well as Inference Eval (see Table2). This is to avoid over designing for a particular usecase. Sever a l tuning options are provided such that the desired serving conﬁguration could be decided at model deployment time (e.g. through an auto-tuning tool). Su ch tuning options are highlighted as Tuning API in each subsection. Most of the relevant SM technologies currently are block devices with NVMe interface. As the BW requir ements of the model grow, the IOPS requirement consequently g row (Equation8). However, IO through NVMe stack is still an expensive operation . Performing multi-million IO per InferenceEval is similar to eval after training, but model has gone through inference speciﬁc transformation. second could required prohibitive amount o f computing resource (CPU). We h ave chosen to use io-uring (Axboe) due to its lower overhead per IO. Figure 3 shows the performance characteristics of PCIe Nand Flash and PCIe Optane using io-uring. IOP S ∝ QP S ∗ One particular design choice was mmap vs DIRECT − IO. Due small access granularity and lack of considerable spacial locality ( Section4.2), we observed that mmap would not provide the best use of FM space, and results in higher access latency (by 3x. e.g. reading in and maintaining 4KB into memory for a 128B request). Hence w e opted for DIRECT − IO with an application level cach e . Given different technolo gies could be used for SM , we realized some optimiza tions ar e technology sp eciﬁc. For example with Nand Flash, we need to smooth out the bursts by limiting the maximum outstanding requests to the SSD because SSD controllers typically try to serve all possible outstanding requests which results in extra latency. Tuning AP I: Total number of outstanding IOs per table and total number of tables that can be processed at given time. Sub-block (e.g. 4KB) reads is not normally supported by an o perating system. T he higher access granula rity to the SCM device, given the lack of spacial locality (Section4.2) has three adverse implications: 1) higher latency due to read ampliﬁcation a s more data need to be transferred fro m device to the host; 2) more pressure on the interconne ct (PCIe) in the system, which might require provisioning more PCIe lanes, and hence increased system power and cost; 3) requir ing extra mem ory copy to handle extracting row data from block data and copying it into the cache. Given that majority of tables have embedding dimension smaller than 512B at inference (due to quantization (Guan et al., 2019)), we have enabled arbitrary access granularity, down to DWORD, with NVMe. A two legged approa c h is taken to a c hieve this goal. • Linux Kernel: Linux kernel is updated to allow a custom command over the io-uring (Axboe) application transport that allows down to 4B granularity reads. • NVMe Driver: The NVMe Scatter Gather List ( SGL) Bit Bucket is used to communicate th e desired portion of a block. This allows full ﬂexibility as to in which parts of a request the host is interested, hence only transferring the necessary parts of a read over the bus. By only re ading the parts of a block that is necessary, we save aroun d 75% of the bus ba ndwidth and reduce the time needed to transfer this data. This reduces the observed latency of a given read by 3-5%. The savings at the ap plication level are more given removal of the extra memcpy (see Section4.3 for more details). Both of these features will be submitted for the upstream kernel, an d will be publicly available. Locality is an important c haracteristic of accessing embedding tables as it could allow for providing a higher effective BW for the data in SM by a cache in FM . Figure4 captures temporal locality through the cumulative distribution of a range of ca tegorical features. Majority of the features show a power law distribution, with a small subset of embedding rows accounting for majority of accesses, hence high temporal locality. We separate User and Item embeddings sin ce we observe a meaningful difference in the distributions (item embeddings show more locality). This motivates the use of a Software Managed Cache in FM to cache the hot portio n of embeddings place d in SM . Note that the temp oral locality observed from a host also depends on the serving system. Inference qu eries will go through a scheduler/aggregator which routes a query to a speciﬁc host for ranking. Figure4-(c) shows the te mporal locality for the same set of user embeddin g tables, but observed from one host during serving, which shows higher locality. Enforcing a user-to-host sticky policy can he lp increase cache hit rate observed from a host. Figure5 demonstra te s the degree of spatial locality access- Figure 4.Temporal Locality accessing User (a) and Item (b) embeddings. Access to majority of the tables demonstrate power law. For each plot, we track 50 tables at random, for data sampled post hash for 6 days. (c) shows temporal locality for the same set of user tables observed by one host during serving, indicating high er locality. Figure 5.Spatial Locality accessing User and Item embeddings. Value 1.0 in dicate 100% spatial locality. For each plot, we track 50 tables at random, for data sampled for 6 days. The average window is around 25M access per table ing the table. It uses the average ratio of unique index to unique 4KB b lock size, norm a lized to the maximum u nique index per block size per table, as proxy for sp a cial locality. The ratios are captur ed in intervals (average 25M access per table). Value 1.0 indicates the same number of unique index and unique 4KB blocks, i.e. high spacial locality. The heat map an d the cooler temperature overall indicates low spatial locality. The design and organization of the cache also has impact on the overall performance. Uniﬁed Row Ca che: Given th e observation from the locality study (Section4.2), we opted for a uniﬁed row cache. The uniﬁed cache allow for better utilization of the memory space compared to per table cache, and th e lack of considerable spa cial locality motivated the row cache. We use CacheLib (Berg et al., 2020). Without the sm a ll g ranularity access (Section 4.1) we need to copy a block of data into an aligned FM buffer, and then copy the desired portion into the cache. This means more than 2X FM BW needed for every X data pulled in from SM , an d increased latency due to th e m emory copies. With small granu larity access, we ca n directly copy data to the cache storage, and save on FM BW and improve latency. Figure 6.Performance implications of different cache organization choices. We opt for a uniﬁed row cache, which internally implements two caches optimized differently based on embedding size. The cache routes the requests to proper internal cache based on embedd ing dim (Embedding dim <= 255 will be routed to memory optimized cache). The bottom right ﬁgure shows a case where direct placement on DRAM could have considerable impact on QPS. choice to tune for mem ory overhead (less overhead per key-value pair, but requires search in a bucket) or pay for memory overhead and op timize for CPU utilization (hig her overhead per key-value pair). Majority of tables have embedding dim smaller than 256B, but th e re is small but a growing subset which have big ger than 256B embeddin g dim. Given the overhead and perform a nce results shown in Figure6, we opted for a dual cache where tables with embedd ing dim smaller than 256B are rou te d to a Memory Optimized Cache, and to CPU Optimized Cache other w ise. We also evalua te d multi-level cache (row c ache backed by a block cache) but did not o bserve any beneﬁt. Tuning API: Cache sizes an d number o f ca che partitions. The SM cache stores the raw quantized embeddings. For every embedding opera tor, there are pembedd ings read out for tablefrom th e cache or from SM , which then go through dequantization and pooling (Khudia et al. , 2021) to generate the output for Top MLP. If we had the resulting pooled embeddings already cached (o r even partial pooled embedd ings), we could (partially) save lookup, deq uantization and pooling. We pr oﬁle queries to establish whether there is locality in sequence of indices that appear across queries. Table3 shows the proﬁling result. There ischoices for an em- Table 3.Summary of Pooled embedding cache proﬁling f or 100M queries. Hit rate show s the number of queries which got a hit for at least one of the subsequences. G enerated sequences show the normalized number of subsequences generated. While considering 0 < c <= P increases the cache of a hit for a subsequence, in practice the overhead of ﬁnding such subsequence is prohibitive except for c near 1 or near P . Table 4.Average l ength (number of indices) saved for hit in PooledEmb Cache with a 4GB cache size. bedding ope ration with P indices. For a subsequence of indices of size c, 0 < c <= P , the possibility of a repeating subsequence decreases as c increases. However, except for near the edges (e.g. c = 1 or c = P ), the number of possible subsequences is too large. In ou r proﬁling we limit the length of subsequence to 10 , and only proﬁle most frequent indices. Noneth eless, our observation is that in the case of c = P , where we o nly cache the full sequence and only lookup for th e full sequence of indices for each table when a request arrives, provides small enough overhead, and r easonably hig h enough hit rate to have a chan ce at improvin g performance . Algorithm 1 depicts the im plementation. We ob served around 5% hit rate for the pooled embedd ings (Table 4). The average length of requ e sts hit in PooledEmbedding Cache increase as the LenT hreshold is increased. Algorithm1 shows the implementation. We use an orderinvariant hash to create a key from th e sequ ence of indices in a reque st. Tuning API: The min sequence length which could be cached is conﬁgu rable (LenT hreshold). Given the cheaper SM capacity, we evaluated a few approach e s that reverse the schemes commonly used to reduce model size, namely de-pruniung explained here, and de-qua ntization in AppendixA. Input: Table, Indices doPooledEmbCache = len(indices) > LenThreshold if doPooledEmbCache then sequenceKey = hash(indices) if e = look up(t, sequencekey) then return e //pooled emb vector exists in the cache end if end if for i in Indices d o if not E[i] = lookup(t, i) then prepareIO (t, i) end if end for submitIOs(E) // dequantize and pool all the embedding vectors in the sequence for e in E do output += dequant(e) end for if doPooledEmbCache then cache[sequenceKey] = output end if return output 2021)). At high level, th e embeddings rows with valnew tensor is deﬁned to map the indices in the unpruned space to indices in pruned space. The size of a mapping tensor is NumRow(U nprune d) ∗ IdxT ype, IdxT ypeǫ{4, 8}Bytes. To place pruned embedding tables on SM , we can either 1) save bo th the pruned table and mapping tensor to SM , wh ic h means two accesses to SM per embedding lookup; or 2) place the pruned table on SM and keep the mapping tensor in FM . Given th e I O PS boundness with SM , and relative ly smaller size of the m apping te nsor, options 2 is a more desirable choice. However, as the model size increases, the aggregate size of the mapping tensors increases. The space taken by mapper tensors are the memory that is taken away from the SM cache. To free up the memory used by mapping tensors, we can deprune the embeddings at the time of loading. Algorithm2 shows how de-pruning is done. Beside incr e ased model footpr int on SM , de-pruning could lead to extra accesses to SM , and consequently ca che pollution. This is because the pruned emb e ddings now will be accessed and cache d. However, the intuition is that the pruned embeddings ar e also less frequently accessed , he nce the impact would be minimal. Our experiments conﬁrm the intuition b y showing 2.5% in crease in the total requests, while allowing for up to 2x cache size in some conﬁgurations in practice. We see u p to 48 % increase in performance for cases wher e performance is bounded by user embeddings in SM . Input: Tables for t in Tables do if t is pruned-table then nt = new Table(dim=[t.mapper.dim[0], t.dim[1]) for i in t.mapper do if i is pruned-row then else end if end for t = nt end if With a so ftware deﬁned cache in FM , there will be two choice to use the FM space; 1) use all the available space for the cache 2) use portion of FM to m ap tables directly, and portions for the cache. In general, allocating all the tables to SM and relying on the cache to keep the hot rows in FM will perform well a c ross the board. However, given the extra overhead of looking up an embedding row from the cache vs plane memory, there are possibilities to improve the performance further with mor e detailed placem e nt. Ta ble5 lists different placement categories. Figure 6 shows the impact of placement with different budget for direct placement on DRAM on a 150GB model running inferenceEval (whic h is more sensitive to placemen t than inference because the user and item batch sizes are the same). Tuning API: Pre-deﬁned p la c ement policies based on table size and pooling factor can be enabled. We also implemented an option to pr oviding a list of tables which should not be placed in SM (for more elaborate o fﬂine plac ement). All placement policies adhere to a conﬁgurable DRAM budget to place tables on DRAM dir ectly. We consider 3 models with different characteristics which reﬂects mod els in use for different usecase. Size (GB) Emb table dim (B)[90, 172] [32, 288] [32, 512] Avg pooling factor (PF) Emb table dim (B) Avg pooling factor (PF) Table 7.Hw conﬁgurations. All the CPU s are Intel. For SSDs, N stands for Nand Flash, and O for Optane SSDs. See (Lee et al.) for more information on the accelerators. models, as listed in Table 7. The choice of platform is driven by the usecase chara c te ristics and requirements. Hence, some o f the possible evaluation combinations are not feasib le (e.g run ning the exact HW with and without using SSD). In many occasions, a usecase has to select from a very limited set of available host types deployed in DC. Such different host types provide different CPU, DRAM, and Storage capabilities. Usin g SM for M1 allows for lowering the DRAM capacity requirement per host to serve a model. This enables using single socket, 64GB DRAM HW-SS instead of dual socket 256GB DRAM HW-L . While each HW-SS c an sustain lower QPS at the desired latency compared to HW-L , the more favorable compute to DRAM ratio of H W-SS plus having attache d SSDs leads to 20% lower power co nsumption considering the full scale of the serving. Table8 shows the results. The IOPS required by the model at 120 QPS is around 246K (120QP S × 50T ables × 42avgP F ). We observe Table 8.Impact of using SSD on saving power consumption. Reported power is normalized. Deployment with SSD can reach the same latency as deployment on DRAM only, resulting on 20% power saving. cache hit ra te of more than 96% in steady state wh ic h typically is reached within a few minutes after a full model update. This means less than 10K IOPS in steady state. We observe higher p99 latency on HW-SS due to occasional long tail latency o f Nand Flash. Nonetheless p95 is the metric of interest for this usecase, which is matched on HWSS . Using HW-SS saves equivalent of 159.4 TB o f DRAM for this particular mode l in production like settings. M2 uses an accelerator enabled platform (HW-AN ) due to its higher compute intensity (Anderson et al., 2021). The item em beddings as well as th e den se part of the model is mapped to the ac celerator. The user embeddin gs are mapped to the h ost CPU. HW-AN has ade quate accelerator memory to host the item embeddings, however, the 64GB host DRAM is smaller th an the 100GB memory required by the user embeddings. The extra memory requir e d for the user embeddings is achieved throu gh the scale out as presented in (Lui et al., 2021), using HW-S host types. A HW-S on average can serve 5 HW-AN . For this usecase, using SM prevents scale out. However, given the accelerated QPS per host, a hig her degree of IOPS is requir ed from SM ( 450QP S × 450T ables × 25AvgP F = 4.8M IOP S). We observe more than 90% hit rate in the SM cache. So the average sustained IOPS required is aro und 480 kIOPS. As shown in Table9, the two Nand ﬂash on HW-AN provide aggregate minimal IOPS of around 1M. However, due to long latency acce ssing nand ﬂash, we have to considerably und erutilize the devices to keep the latency low. Hence in practice Nand Flash in this setup co nsiderably impacts QPS. However, Optane SSD provide much higher IOPS and lower latency, keeping the user embedding processing out of the critical path. By removing the need to scale out, HW-AO reduces the power consumption by 5%. At the same time, HW- AO sim pliﬁes the serving paradigm, as the scale out paradigm is more complex to operate, and more prone to failures g iven that m any more hosts are involved in serving a single query. While the power saving is modest, it increases as the models grow. Table 9.Impact of using SSD on power consumption for M2 For M3 we p resent the estimated results, as it is a future u se case, with the chance to impact the design of the host type. M3 r epresents a future model which could run on an updated acc e le rator-enabled platform(e.g. see (Smelyanskiy, 2019)). The primary argum ents for SDM in such platform is to limit the amount of DRAM deployed pe r host. This is primarily a cost argument as DRAM power is not a signiﬁcant portion of the total power in such platforms. The power savings come from allowing for increased accelerator utilization witho ut be c oming DRAM memory capacity bound through Multi-tenancy. Multi-tenancy refers to runnin g multiple models on the same ho st. This capability is becoming more important (e.g. see ( Jouppi et al., 2021)) as it allows for co-locating models with different requirements, and balancing the utilization of different resources such as accelerator, CPU, and DRAM. The balanced utilization leads to increase overall host utilization, and power saving. As an example, at any given time , there a re a large number of experimental models running, of which, a subset will eventually be promoted for full scale deployment. Our observation is that given the numb e r of experimental models running per production models, and on aver age it consumes up to quarter of the allocated r esourced. Such experimental models run o n a small volume of trafﬁc, and hence h ave low QPS r equirement pe r model, wh ich could leave the hosts u nderutilized. This becomes more important as more compute capability is packed into a single host with the advent of more powerful accelerators, increasing the cost of a model underutilizing a host. Co-locating more than one mo del on a given host increases utilization. Notably, the memory capacity requirement will scale with the number of models co-located together. Therefore, serving becomes memory capacity bound. Using SM in this case prevents the memory capa c ity boundness due to the multi-tenancy, by increasing memory capacity available to the models per host. To drive the SM capacity and BW requirement per host, we use M3 as the representative model. We estimate the QPS on the target hardware by measuring the QPS on an available similar hardware, and extrapolating the QPS based on the expected increase in co mpute and BW. The BW needed from SM could be calculated accordin g to Equation2. Ta- Table 10.SDM-based HW conﬁguration for future models (M3 ). Number of SSDs are derived based on BW, and hence IOPS needed for the user embedding t ables (IOPS bound). Table 11.Using SDM with SM allow s for placing multiple experimental models on the same host, hence increasing aggregate utilization of the ﬂeet. This results in better perf/watt, and power saving for the given model class. The increase in the power consumption of the host is based on the OptaneSSDs needed. ble 10 shows the need for 36 MIOPS which could be satisﬁed by 9 OptaneSSD, each providing 4 MIOPS. Given the n umber of experimental models and their required QPS, we observe 63% utilization of the hosts at the scale. Table11 shows the rooﬂin e estimation for power saving with multi-tena ncy enabled through leveraging slower memory. T he modeling shows up to 29% power saving. SSDs have bee n u sed to extend memory in different applications. For example (Heo et al., 2021) use SSD to increase 2020) develops a distributed training system using SSD in a hier archical fashion to increase available memory capacity. Inference, however, is more latency sen sitive compared to training, which makes it harder to leverage SSD. (Eisenman et al., 2018) is among the pioneers tac kling the challenges in using SSD for inference. It groups th e embedding vectors of a give n tables to increase the possibility that the grouped embeddings could be accessed together. This helps re duce th e read ampliﬁcation due to large device block size read, which is con sid erably bigger than embedding dimension. Our work does not follow this path due to the implication of grouping on the latency betwe e n m odel updates. In (Wilkening et al., 2021) authors leverage the limited compute a nd DRAM in the SSD c ontroller to collect and pack requested embeddings across different pages, hence making the data transfer over PCIe more efﬁcient. In our work, we pursue techniques to reduce the access gran ularity which addresses read ampliﬁcation and inefﬁcient use of the BW by reading large block size. (Liu et al., 2021) present a recommend ation system whic h c an leverage SSD for embeddings. While they mention the implication of using SSD on latency, they do not further discuss how to re medy such increase in latency, or exact implication of using SSD vs memory. (Lui et al., 2021) use scale out and shard the model across multiple servers to scale the memory c apacity available to the usecase. Rapid increase in Deep Learning Recommendation Model (DLRM) size makes it more expensive in terms of cost and power to serve such models. Power, is particularly among the most important metrics at DC scale. Companies operating DCs are willing to pay for extra compute, but the rate of growth is limited by the rate at which the power cou ld b e provisioned. We leverage the inherit skew in BW among different embedding tables in DLRM to deploy a Software Deﬁned tiere d m emory increasing memory capacity per host by leveraging Storag e Class Memory. We eva luate and addre ss a r a nge of challenges, such as fast IO, capturing locality, trade-off of capacity vs compute and BW. We discuss the value of suc h technology under different deployment scenarios. We observe 20% power saving serving a large model while using a simpler hardware with Na nd Flash, 5% power saving using another comp ute heavy model by avoiding scale-out, and projected 29% improvement in perf/watt by increasing utilization of Acce lerator enabled platforms through multi-tenancy using Optane SSD. Suc h power and perf/watt optimizations are considerable given the power boundness o f serving such models at DC scale. We would like to acknowledge the valuable insight and help from Assaf Eisenman, Jeremy Yang, Jesseh Koh, Mo Cao , Alex Kachurin, Pavan Shetty, Kiran Malwankar, Yinghai Lu, Pallab Bhattacharya, Lu Fang, and Ajay Somani.