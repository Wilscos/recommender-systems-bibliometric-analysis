Recent years have witnessed the successes of deep retrieval models in many large-scale recommendation systems [ improve user experience by enabling personalization and representation learning, they can also raise privacy concerns as user data typically needs to be sent to a centralized server for training. Federated learning (FL) is a decentralized training strategy in which clients collaborate with a coordinating server to train a machine learning model [ distributed client data to learn useful models while preserving user privacy. Bringing deep retrieval models to FL is a promising way to power recommendations and learn representations for users and items while addressing privacy concerns by reducing the centralization of user data. In this work, we observe a challenge with training federated deep retrieval models: the insufﬁciency of negative examples available on a client’s device. A deep retrieval model (shown in Figure 1) learns embeddings representing users’ contexts and items like movies, songs, or websites. For the model to learn meaningful embeddings, it generally uses two types of examples: positive examples and negative examples. The positive examples pull embeddings of the training (context, item) pairs to be close together in an embedding space, while the negative examples push embeddings of unrelated pairs farther apart. Negative examples are typically required to prevent embedding space collapse, where learned representations collapse to a single point and are no longer informative. Typically, negative examples are produced by sampling items from training data. In centralized training, training data is usually assumed to be independent and identically distributed (IID). Therefore, negatives can be sampled reliably from the overall training distribution. However, the IID data 1st NeurIPS Workshop on New Frontiers in Federated Learning (NFFL 2021), Virtual Meeting. Deep retrieval models are widely used for learning entity representations and recommendations. Federated learning provides a privacy-preserving way to train these models without requiring centralization of user data. However, federated deep retrieval models usually perform much worse than their centralized counterparts due to non-IID (independent and identically distributed) training data on clients, an intrinsic property of federated learning that limits negatives available for training. We demonstrate that this issue is distinct from the commonly studied client drift problem. This work proposes batch-insensitive losses as a way to alleviate the nonIID negatives issue for federated movie recommendations. We explore a variety of techniques and identify that batch-insensitive losses can effectively improve the performance of federated deep retrieval models, increasing the relative recall of the federated model by up to 93.15% and reducing the relative gap in recall between it and a centralized model from 27.22% - 43.14% to 0.53% - 2.42%. We also open-source our code framework to accelerate further research and applications of federated deep retrieval models. 2,13,19,17,7,16] and natural language tasks [5,4,18,1]. While these models largely assumption does not hold for federated learning since clients generate data locally based on their circumstances. That means, on each device, negatives may not be present. Even if they are present, they may be relatively few and relatively similar. In practice, we observe that this leads to signiﬁcant performance degradation when FL is applied naively, beyond the typical degradation observed for e.g., classiﬁcation in the FL setting. This work focuses on understanding and alleviating the non-IID issue for deep retrieval models. We make the following key contributions: •Observe that naive federated training of deep retrieval models causes an unusually steep performance drop. Show that performance degradation is primarily caused by sampling negatives from non-IID data, not the typical client drift issue or other aspects of federated training. • Introduce batch-insensitive losses as a class of objectives to alleviate the issue in this setting. •Perform empirical evaluation of different training objectives in a movie recommendation setting, showing that batch-insensitive losses enable performant federated deep retrieval. •Release an open-source framework to accelerate further research and practical applications of federated deep retrieval models. Related Work: works study the general problem of heterogeneity in client data causing client drift when clients perform multiple local computation steps, as in FEDAVG [ sharing some training data across clients [ The developed techniques are not fully relevant to the deep retrieval model, which sees unusually severe performance degradation due to explicit reliance on sampling negatives for training. Though also caused by non-IID data, this issue is orthogonal to the client drift issue: it occurs even when clients perform one local update step. We can combine the techniques explored in this work with previous techniques for addressing client drift. [ spreadout regularizer in federated learning. However, it focuses on classiﬁcation tasks and only considers extreme cases where each user has no access to negative examples. Our work focuses on a more realistic setting, aiming to understand and increase the performance of representation learning and item recommendation with deep retrieval models when each client has access to some but limited and relatively similar negative examples. More broadly, we propose batch-insensitive losses, which can be generally valuable for alleviating non-IID data issues in other federated learning tasks. Deep retrieval models are also referred to as dual encoder, two-tower, or encoder-encoder models depending on the setting [ a variety of real-world applications in embedding and recommendation learning. As illustrated in Figure 1, a deep retrieval model consists of two encoders, each of which can be a fully-connected network, convolutional neural network, Transformer, and so on, depending on the task. The input consists of (context, item) pairs encoded by the left and right encoders, respectively. For example, https://git.io/federated_dual_encoder Most previous research has focused on mitigating the non-IID data issue [6,23, ,21,20] for improving general model convergence in federated learning settings. These in a movie recommendation use-case, a context might be a sequence of previous movies a user has watched, and the item is the next movie they watch. The goals of applying a deep retrieval model in this setting would be to learn a model that can predict the next movie given an unseen sequence of previous movies and learn encoders that produce embeddings for users and movies. More formally, we denote the feature vectors representing contexts and items as encoders are denoted as two parameterized functions andyto a shared embedding space. The model outputs the similarity score between the encoded context and item, e.g., the inner product of context and item embeddings, loss function is applied to enforce that positive examples (i.e., similar context and item pairs) have high similarity, and negative examples have low similarity. Once the parameterized functions andg(·) produced by for other downstream applications, such as classiﬁcation [3, 1]. Loss Function: commonly used one [ − log(e Nis a set of negative labels used to construct negative example pairs incentivized to maximize the similarity between positive example pairs and minimize the similarity between negative example pairs. Note that if it only did the former, then the embedding space produced by good model. A standard method for getting negatives is to use in-batch negatives [ means given a training batch and any ( batch are treated as negatives for X Federated training of a deep retrieval model involves three main steps in each training round. First, a central server sends the current model to several randomly sampled clients. Second, each sampled client trains on its dataset and updates its model locally. Finally, the local model updates are sent back to the server and aggregated to update the server model. The de-facto standard federated optimization method is FEDAVG [ update back to the server to be averaged. We also later refer to FEDSGD, where each client only runs a single local training step at each round, similar to standard distributed training. As discussed in Section 4.2, naively applying federated learning to this setting produces a steep performance drop, worse than typical when comparing centralized and federated performance. We will show that training degrades due to non-IID negatives. Note that this is distinct from the client drift phenomenon discussed in other works [ clients take multiple local steps. In contrast, the problem we observe occurs even in the FEDSGD regime (see Figure 3). This work aims to characterize this problem better and propose methods that enable federated deep retrieval models to perform comparably to centralized counterparts. We propose batch-insensitive losses as a potential solution to address the non-IID data issue. Deﬁnition 1 f(·)and batch-insensitive if it satisﬁes It follows that applying a batch-insensitive loss over several batches of data in parallel (not in sequence) produces the same average loss and gradient update no matter how the examples are batched. We use this to show that we can produce the same gradient update between federated and centralized learning, providing a natural justiﬁcation for batch-insensitive losses. Proposition 1. the aggregated update to the model for that round under FEDSGD as collection of all training examples from clients in are learned, the model can predict relevant items given a new context. The representations f(·)andg(·)are general representations of user contexts and items and can also be used The loss function plays a key role in training a deep retrieval model. The most /e).XandYrepresent all the contexts and items in a batch, and f(·)andg(·)would collapse. Therefore, negative examples are important to learn a (Batch-Insensitive Loss).Given a batch of N examples and two parameterized functions g(·), with all contexts and items in the batch denoted asXandY, a loss function is LetCbe a collection of clients sampled at roundkfor federated learning. Denote using SGD with all examples in as training objective, the same model initialization Θ, and the same learning rate η, we have See Appendix A for proof. Proposition 1 shows that FEDSGD with batch SGD when the number of clients per round is large (so data). This motivates using batch-insensitive losses to mitigate the non-IID data issue. Note that batch softmax loss does not have these properties and is batch-sensitive. We now describe speciﬁc batch-insensitive losses for the deep retrieval setting. This loss is a variation of contrastive loss, composed of a positive term pushing positive examples together (hinge loss) and a negative term preventing embedding collapse (spreadout regularization). This combination was ﬁrst introduced in [21]. Hinge Loss: hinge loss is deﬁned as `(x, y) = max(0, β − f(x) · g(y)) g(y) is the item embedding, and β is a tunable margin set to 0.9 in this work. Spreadout Regularization: embedding space. Given an embedding vocabulary W, spreadout regularizer can be formulated as is a measure of distance, e.g., Euclidean distance or negative dot product. When combined with L2 normalization, the objective pushes embeddings in any loss function in the form of the spreadout regularizer, and α trades off the regularization term and the original loss. Unlike softmax cross-entropy, hinge loss only considers positive examples; the loss can be trivially minimized by collapsing embeddings into a single point. To avoid collapse, we apply spreadout regularizer to the model’s shared embedding table described in Section 4.1, pushing items in the embedding vocabulary to have orthogonal embeddings. The resulting combined loss pushes positive pairs closer while pushing negatives apart, resulting in a full loss for deep retrieval. Both hinge loss and spreadout regularizer are batch-insensitive based on Deﬁnition 1. It is easy to show that a linear combination of them is also batch-insensitive. The global softmax loss is deﬁned as Vis the vocabulary of possible items to predict. Global softmax is an extreme case of negative sampling. For any ( used to calculate the softmax loss. For a larger scale model, only a subset of items can be sampled randomly instead. In either case, the loss is batch-insensitive on expectation according to Deﬁnition 1. We evaluate the efﬁcacy of batch-insensitive losses on the non-IID negatives issue with a movie recommendation task. We train and evaluate a deep retrieval model on the MovieLens 1M dataset The model takes in a user’s movie-watching history and predicts a relevant next movie for this user. Below we describe details on the dataset, model, and tasks. Additional details can be found in our open-source code framework for federated deep retrieval (see Section 5). Dataset: from 6040 users on 3952 movies. Examples are created by taking moving "windows" of the movie https://grouplens.org/datasets/movielens/1m/ Given a positive example pair (x,y), wherexandyare context and item features, the As shown in Table 1, the MovieLens 1M dataset contains approximately 1 million ratings sequence (sorted by timestamps) for each user, resulting in context inputs containing ten movie IDs and item inputs representing one next movie ID. For centralized training, examples are randomly shufﬂed across all users and split to train and test datasets. The train dataset has 894,752 examples, and the test dataset has 99,417 examples. We refer to these as centralized datasets later in this section. For federated training, all examples are grouped by user, forming a natural data partitioning across clients. The train and test examples are split by user ids, resulting in 4832 train, 603 validation, and 605 test users. We refer to these as federated datasets. We sample 100 clients for each training round. Model Architecture: for experiments. It takes a sequence of movie IDs (the movie watching history) as the context and the next movie ID as the item to form the (context, item) pair. The context encoder is a bag-of-word encoder, and the item tower is a simple embedding lookup tower. The two towers share the same bottom embedding layer, which maps from movie ID to dense embedding. The two towers generate context and item embeddings respectively, and similarity is enforced between the positive (context, label) pairs. To make the comparison consistent and fair, we set the batch size to 16 for all experiments. The output dimension of the shared embedding layer is 16. The encoded context embedding and item embedding are also 16-dimensional and L2-normalized. Federated and Centralized Experiments: between federated and centralized deep retrieval model performance. We run experiments for both centralized and federated training and compare performance. Interestingly, we observe that batchinsensitive losses can also improve centralized performance, so we also compare against this. We refer to these models as Improved Centralized later in this section. In both settings, we measure test recall@k for top k nearest item embeddings for an unseen context. To study the effect of non-IID data on federated learning in this setting, we train the ID-based deep retrieval model on federated datasets using FEDAVG [ batch softmax cross-entropy loss. We compare recall across items within a batch (batch recall) with centralized training. As shown in Figure 4, the federated model experiences signiﬁcant performance degradation, especially for recall@1. It is worth noting that performance degradation occurs even when training with FEDSGD, as illustrated in Figure 3. It indicates that client drift, which occurs when clients take multiple local steps with non-IID data, is not the only cause. To test whether non-IID data causes this performance degradation, we train a Federated Shufﬂed model where data is IID across clients. In this experiment, all examples are shufﬂed across users while the number of examples per user remains the same. This ensures the same number of local steps taken on each client as before and isolates the effect of non-IID data on federated learning. As shown in Figure 4, the Federated Shufﬂed result roughly matches the centralized result. We then conclude that non-IIDness causes the performance degradation, not federated training in itself. Figure 2: ID-based deep retrieval model for movie recommendation. k ∈ [1, 5, 10], the fraction of examples for which the correct next movie is within the Table 1: MovieLens 1M Dataset Statistics. ‘E’ is short for ‘Examples’ and ‘U’ is short for ‘Users’. Figure 3: Model performance with batch softmax loss. Federated training uses FEDSGD. However, although shufﬂing examples across users could resolve the non-IID data issue, we cannot do it in practice due to privacy and communication limitations. This section studies the model performance with four different loss functions: batch softmax (BS), batch softmax with spreadout regularizer (BS+S), and two batch-insensitive losses (see Section 3): hinge loss with spreadout (H+S) and global softmax (GS). All the federated models are trained with FEDAVG. We compare recall calculated globally across all items, which has no dependence on examples in a batch, enabling fair comparison. Batch Softmax (BS): It is calculated with in-batch negatives as described in Section 2. Figure 5(a) shows a large gap between centralized and federated global recalls, similar to the batch recall results in Section 4.2. Batch Softmax + Spreadout (BS+S): combined with spreadout regularization. With spreadout regularizer, the recall values of the federated model almost match those of the batch softmax centralized model, which is trained without spreadout regularizer. However, spreadout regularizer also improves centralized training. The federated model still performs signiﬁcantly worse compared to the improved centralized model. The results indicate that spreadout regularization itself is not enough to solve the issue with the non-IID negatives. Although spreadout regularizer pushes embeddings of unrelated pairs farther apart, batch softmax loss still depends on in-batch negatives and can still lead to worse model quality. Therefore, we need a loss function less affected by the training data distribution, motivating batch-insensitive losses. Batch-Insensitive Losses (H+S and GS): with the two types of batch-insensitive losses. With the combination of hinge loss and spreadout regularizer, both the improved centralized model and the federated model perform much better than the baseline model (Figure 5(d)). Also, the gap between improved centralized and federated models is much smaller than with batch softmax. With global softmax (Figure 5(c)), the federated model performs almost the same as the improved centralized model, and both perform signiﬁcantly better than the batch softmax centralized model. Both of the results indicate that batch-insensitive loss alleviates the performance degradation caused by non-IID negatives effectively. We caution that applying global softmax may not be appropriate in all settings. In these experiments, we use all the items in the movie vocabulary as the negatives. In practice, when dealing with items with large or unbounded vocabulary size, we may need other strategies as global softmax is computationally expensive. Overall Comparison: improved centralized, and federated models under different losses. Ratings 1,000,209 Figure 5: Comparison of centralized and federated models when training with different loss functions: batch softmax, batch softmax with spreadout regularizer, hinge loss with spreadout regularizer, and global softmax. The last two are batch-insensitive losses. Table 2: An overall recall comparison between centralized, improved centralized, and federated models. The performance drop is calculated as recall, and R Training with batch-insensitive losses (H+S, GS) achieves the highest recall for both centralized and federated models. In particular, hinge loss with spreadout regularizer appears to perform slightly better than global softmax in terms of absolute recall, but both perform signiﬁcantly better than batch-sensitive losses (BS, BS+S). We also observe that global softmax incurs the smallest performance gap between centralized and federated training. Hinge loss with spreadout regularizer has the next smallest performance gap, and batch-sensitive techniques have a more signiﬁcant performance gap as expected. We expect that the remaining performance drop between centralized and federated models results from client drift (clients are still taking multiple local steps). This suggests that combining batch-insensitive losses with approaches to address client drift may be a promising future direction. We are releasing a general code framework for experimenting with federated deep retrieval models built on the TensorFlow Federated library framework enables reproduction of our experiments and provides a ﬂexible, well-documented interface for researchers to train federated and centralized deep retrieval models with different models and losses. We provide libraries for training and evaluation for MovieLens next movie prediction, which can be easily extended for new tasks. We hope that this framework spurs further research and lowers the barrier to more practical applications. This work investigates the effect of non-IID negatives on federated training of deep retrieval models and proposes batch-insensitive losses to alleviate the issue. We compare model performance using various loss functions and show that batch-insensitive losses produce better federated deep retrieval models that can approximately match centralized models. We also open-source our code framework to accelerate future research and applications. Note that our proposed techniques do not directly address the separate, well-studied issue of client drift when clients do multiple steps of local training–approaches addressing this issue are complementary and can be combined with our work. https://www.tensorﬂow.org/federated is the federated global recall. We thank Warren Morningstar, Chung-Ching Chang, and Zachary Garrett and for their helpful comments and discussions. We also thank Warren Morningstar for his contribution to the federated training pipeline.