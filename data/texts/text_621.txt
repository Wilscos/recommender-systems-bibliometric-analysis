Collaborative Filtering (CF) is a prevalent technique used by recommender systems [21]. The fundamental assumption of CF is that people with similar tastes tend to show similar preferences on items. Therefore, the purpose of CF is to match people with similar interests and make recommendations, by modeling people’s preferences on items based on their past interactions. Generally, CF algorithms can be divided into two following diﬀerent categories. 1) Matrix Factorization (MF) Algorithms: MF works by modeling the user’s interaction on items [17]. Researchers have devoted much eﬀort to enhancing MF and proposed various models such as the Funk MF [1] and the Singular Value Decomposition ++(SVD++) [4], etc. Funk MF factorizes the user-item rating matrix as the product of two lower dimensional matrices. SVD++ has the capability of predicting item ranking with both explicit and implicit feedback. Unfortunately, the cold start problem, causing by insuﬃcient data, constitutes a limitation for MF algorithm [3]. College of Computer Science, Sichuan University, Chengdu, China fangyuhan0719@gmail.com; lyqguitar@gmail.com; ysun@scu.edu.cn Abstract. Collaborative Filtering (CF) is widely used in recommender systems to model user-item interactions. With the great success of Deep Neural Networks (DNNs) in various ﬁelds, advanced works recently have proposed several DNN-based models for CF, which have been proven eﬀective. However, the neural networks are all designed manually. As a consequence, it requires the designers to develop expertise in both CF and DNNs, which limits the application of deep learning methods in CF and the accuracy of recommended results. In this paper, we introduce the genetic algorithm into the process of designing DNNs. By means of genetic operations like crossover, mutation, and environmental selection strategy, the architectures and the connection weights initialization of the DNNs can be designed automatically. We conduct extensive experiments on two benchmark datasets. The results demonstrate the proposed algorithm outperforms several manually designed state-of-the-art neural networks. Keywords: Deep neural networks · Genetic algorithm · Collaborative ﬁltering. 2)Deep Learning Algorithms: The introduction of deep learning and neural methods into CF tasks has been proven eﬀective by various models, for example, the DeepFM [11] and the NeuMF [14]. Combining DNN with Factorization Machines (FM), DeepFM has the ability to model both the high-order and loworder feature interactions. In addition, NeuMF generalized the traditional MF with the neural network methods and constructed a nonlinear network architecture. Both these two models can maximize the feature of the ﬁnite interaction data and improve the recommendation precision. Although deep learning methods have shown promising results, designing the network architecture is a challenging and time-consuming task for most inexperienced developers. Therefore, it is worthwhile to extend the automatic method to the network architecture design, ensuring that people who have no domain knowledge of DNNs can beneﬁt from deep learning methods. To this end, we provide an algorithm based on Genetic Algorithm (GA) [2], to automatically design a competitive neural network with eﬃcient weight initialization approaches. To sum up, the contributions of the proposed Evolve-CF are summarized as follows: 1) Variable-length encoding strategy: To represent DNNs in GA, we design a variable-length encoding strategy, which is more suitable to automatically determine the optimal depth of neural networks than traditional ﬁxed-length encoding strategy. 2) Weights encoding strategy: To automatically choose the appropriate weights intialization methods, we investigate the weights encoding strategy to optimize the weights eﬃciently during evolution. 3) Eﬀective genetic operators: To simulate the evolution process and increase population diversity, we propose eﬀective genetic operators like crossover and mutation which can cope with the proposed gene encoding strategy. 4) Improved slack binary tournament selection: To increase the overall ﬁtness of the whole population during evolution, we improve the slack binary tournament selection to choose promising individuals from the parent population. The skeleton of DNNs for CF consists of the embedding layer, the hidden part, and the output layer. To begin with, the embedding layer is ﬁxed as the ﬁrst layer to take eﬀect in dimensionality reduction and learn the similarity between words [19]. Multiple sequential blocks, which are widely used in DNNs for CF, make up the hidden part. Each block contains a full connection layer, a Rectiﬁed Linear Unit (ReLU) [10] and a dropout layer. A full connection layer is capable of learning an oﬀset and an average rate of correlation between the output and the input. Meanwhile, both the ReLU and the dropout layer can make the model less likely to cause overﬁtting [10]. Besides the arrangement of the blocks, the parameters of each layer aﬀect the performance of DNNs as well, such as the number of neurons in the full connection layer, the dimension of embedding vectors in the embedding layer and the dropout rate in the dropout layer. Both the architecture and the parameters of each layer can be designed automatically. In deep learning tasks, weights initialization plays a signiﬁcant role in model convergence rate and model quality, allowing the loss function to be optimized easily [22]. Generally, there are three common categories of initialization methods that are widely used now: 1) Random Initialization. (R) This method uses the probability distribution as the initializer. Nevertheless, without experience and repeated tries, it’s hard for the designers to select the hyper-parameters in the probability model. 2) Xavier Initialization [9]. (X ) This method presents a range for uniform sampling based on neuron saturation. While the applicable activation function is limited, which should be a linear function. 3) Kaiming Initialization [16]. (K ) This method proposes an initialization works on ReLU, which is a more prevalent activation function in DNNs. However, its eﬀectiveness highly relies on the architectures of DNNs. Taking all the advantages and disadvantages into consideration, the designers may have trouble in deciding which weights initialization approaches to choose when designing the neural network for a speciﬁc CF task. As a consequence, incorporating searching for weights initialization approaches into GA is needed. Thus the weights initialization approaches can be designed automatically. Algorithm 1 displays the framework of the proposed Evolve-CF. Firstly, a population is randomly initialized with the predeﬁned population size, and each individual in the population is randomly generated according to the proposed encoding strategy. Then the evolution begins to work until the generation number exceeds the maximal value deﬁned ahead. In the course of evolution, the ﬁtness of each individual is estimated through a speciﬁc method which is selected based on the dataset. After that, by means of the improved slack binary tournament selection method, parent individuals are selected from the population to further conduct the genetic operations consisting of crossover and mutation. Thereafter the oﬀspring is generated. Next, environmental selection takes eﬀect to choose individuals from both the parent population and the generated oﬀspring population, and thereby create the next generation. Then the next round of evolution begins to work. When the whole evolution process is over, the expected best individual is selected, and then we build the DNN decoded from the individual for the ﬁnal training. Since the genetic operators in GA take eﬀect based on encoding individuals, we present an encoding strategy to encode the DNNs. The architectures of the neural networks, especially the depths [8], play decisive roles in the performance of DNNs and the applicable depths are various. Because the encoding information contains the depth of the DNNs and the hyper-parameters of each layer, and the depth of the neural network is uncertain, the length of encoding should be variational correspondingly. As a result, we propose a variable-length gene encoding Algorithm 1: Framework of the Proposed Algorithm strategy being able to ﬁnd the optimization automatically, freeing designers from constantly adjusting the length. Via our strategy, the embedding layer and multiple sequential blocks make up the whole DNN. Each block consists of a full connection layer, a ReLU, and a dropout layer. Considering that the last layer determines the prediction results, we set the last block as only a full connection layer. In particular, an example of the proposed strategy representing a DNN is illustrated by Fig. 1. Fig. 1. An example of the proposed encoding strategy representing a DNN To eﬃciently initialize the connection weights, we propose a new method, in which the weights initialization information is explicitly represented as the initialization type of each full connection layer. Simply put, the initialization types are randomly chosen from six types in three categories mentioned in Subsection 2.2 (R with normal distribution R normal distribution X Kand uniform distribution K As for the initialization of each individual, we ﬁrst choose an integer randomly within the range and use it as the length of the current individual. Second, the embedding layer with a predeﬁned input size and a random embedding vector dimension is set as the head of the individual. After that, several blocks are added in sequence to the individual until it meets its predeﬁned length. Finally, the last prediction layer is ﬁxed to make sure the prediction results of each DNN are comparable. In GA, the ﬁtness function is usually set based on the task. As for CF, there is no doubt that the ﬁtness function should be able to evaluate the accuracy of recommendations. Because the Normalized Discounted Cumulation Gain (NDCG) [13] can assign higher importance to results at higher ranks on the list, we use it as the ﬁtness evaluation criterion for the task investigated in this work. Speciﬁcally, the NDCG is calculated by Equation (1) where N denotes the total number of involving users, K denotes the length of the top-ranking list, and r(i) refers to the correlation score for the item at position i. Respectively, of the predicted and real ranking list. For NDCG, larger values indicate better performance. Completing all the ﬁtness evaluations, several individuals are selected as parents in the way mentioned in Subsection 3.3. After that, genetic operations like crossover and mutation are performed on the chosen parents. There are two main steps in the mutation operation. The individual is ﬁrstly mutated by the Polynomial Mutation (PM) [7] in a given probability. Next, the length of the DNN is randomly changed. This change happens in a random position, causing either an increase or decrease in the length of the DNN. As for the proposed crossover operation, we use the Simulated Binary Crossover (SBX) [6] for its good performance in local optimization. We ﬁrst align the heads of two selected individuals and then exchange some of their corresponding blocks according to predeﬁned the crossover rate. In the proposed algorithm, we use a developed selection method which is described as Slack Binary Tournament Selection [22]. We add an elite mechanism to the traditional binary tournament selection to maintain the diversity and convergence of a population and avoid the premature phenomenon [18]. In the course of the environmental selection, ﬁrstly, several individuals with the best ﬁtness are directly chosen for the next generation. Secondly, we randomly select two from the rest individuals in the current population using the binary tournament selection, the individuals with larger NDCG are placed into the next population until the population size meets the predeﬁned number. Then, the next iteration starts to work. Consistent with previous CF tasks, we use the MovieLens dataset [12] and Pinterest dataset https://sites.google.com/site/xueatalphabeta/academic-projects The settings of the encoded information of each layer in DNN are listed in Table 1. Besides, all the parameter settings of the evolution are speciﬁed following the conventions of GA community [2], which are listed in Table 2. To evaluate the performance of item recommendation, we adopt the leave-one-out evaluation [14], which has been widely used in CF models. As mentioned in Section 3.2, the NDCG is used as the ﬁtness criterion. To show the eﬀectiveness of the proposed algorithm, the following state-of-the-art algorithms are selected as the peer competitors, including deep learning models: NeuMF, MLP [14], and MF models: GMF [14], eALS [15], BPR [20]. The performance of the ranked recommendation list is evaluated by Hit Ratio (HR), which intuitively measures whether the test item is present on the top-k list, and Normalized Discounted Cumulation Gain (NDCG) [13] mentioned in Subsection 3.2. For both metrics, larger values indicate better performance. Fig. 2 shows the performance of Top-K recommended lists with respect to the number of ranking position K ranges from 1 to 10. As can be seen, Evolve-CF demonstrates consistent improvements over other methods across positions. Table 3. The best performance of HR@10 and NDCG@10 on the two datasets We further focus on the performance of the top 10 recommendation lists. Table 3 shows the best performance of HR@10 and NDCG@10. Obviously, EvolveCF signiﬁcantly outperforms both the traditional MF methods and the previous neural network models on two datasets. These experimental results intuitively indicate the eﬀectiveness and generalization of the proposed Evolve-CF. Table 1. Encoded informationTable 2. Parameters of GA full connection layer16-256Distribution index1 Fig. 2. Evaluation of Top-K item recommendation where K ranges from 1 to 10 on the two datasets In this work, we explored the evolving strategy to automatically design an appropriate DNN for CF. Our proposed algorithm Evolve-CF provided an improved genetic encoding strategy for encoding the DNN architectures and the weights initialization approaches. Moreover, the genetic operators we selected can eﬀectively ﬁnd the optimal DNN architecture during the evolution process and the improved binary tournament selection is capable of selecting promising individuals. The results of several experiments on two benchmark datasets demonstrate the eﬀectiveness of the automated DNN over the other CF algorithms. In the future, further researches will be conducted on ﬁnding out whether there are new components that contribute more to the performance of DNNs for CF tasks.