1.1 Background The use of Content Distribution Networks (CDNs) has b e e n common practice in the Internet [1]. At the same time, large content providers are starting to operate their own CDNs (e.g., Neﬂix Open Connect [2]) by placing and operating smaller data centers inside a network operator, a trend that will continue in the context of mobile edge clouds. Interest in caching research has been revived in the context of Information-Centric Networks (ICNs), and more recently in wireless networks; there, a number of studies suggest to install tiny caches (e.g., hard drives) at every small-cell or femto-node [3], bringing idea s from hierarchical caching [4] into the wireless domain. Caching techniques essentially t ry to predict wha t content users will probably request, and store it closer to the user. Storing content close to the users, can (i) reduce the network cost to serve a request, and (ii) improve user experience (e.g., bette r playout quality). Nevertheless, the rapidly growing catalog sizes (both for professional and user-generated content), smaller sizes per cache (e.g., at femto-nodes) compared to traditional CDNs, and volatility of user demand when considering smaller populations, make the task of caching algorithms increasingly challenging [5], [6]. To overcome such challenges, a radical app roach has been recently proposed [7], [8], [9], [10], [11], [12], [13], based on the observation that user demand is increasingly driven today by recommendation systems (RSs) of popular applications (e.g., Netﬂix, You Tube). Instead of simply recommending interesting content, recommendations could instead be “nudged” towards interesting content with low access cost (e.g., locally cached) [7], [14]: the recommendation quality remains high, and the new content will incur a smaller (network) cost, or even be accessible at bette r quality (e.g., HD), due to the lower latency [15]. This approach is appealing, potentially presenting a win-win situation for all involved parties. It has, nevertheless, attracted some research interest only very recently, mostly in empirical studies [8] or heu ris tic schemes [7], [9]. The main motivation for our p aper, is that a recommendation we do now, not only affects the user’s next choice and the related network cost, but also subsequent choices and costs. However, t he works in [7], [8], [10], [11], [12], [13], base their analysis on independent and identically distributed (I.I.D.) request patt e rns, ignoring the fact that a user’s session often consists of consuming multiple contents in sequence (e.g., YouTube, Spotify). Thus, selecting recommendations towards network cost minimization for this sequential process is the main focus of our work. We brieﬂy present the problem our paper targets: A user starts a session in some m ultimedia (video, m usic, etc.) application, which is equipped with: (a) an RS that suggests a “related list” of N recommended items; this list relates to the item just visited (requested) by the user, and (b) a search bar that can be used for typing, and thus requesting any content of the catalog. The user transits to the next content either from the RS list (potentially exhibiting some preference for the recommendations that are placed higher in the list), or from the search-bar. Our goal is to design a methodology that returns the optimal recommendation policy, which will simultaneously keep the user satisﬁed at every request and minimize the total network cost incurred by her requests in this long session. Having established our goal, here we summarize the technical contributions of this pape r: (i) Sequential request model. We propose an analytical framework based on absorbing Markov chain theory, to model a user accessing a long sequence of contents, driven by a RS (Section 2, and Section 3). The se quential request model better ﬁts real user behavior in a number of popular applications (e.g. YouTube, Vimeo, personalized radio) compared to I R M models used in previous work [7], [11]. (ii) Problem formulation and convex equivalent. We formulate a generic optimization problem for high quality but network-friendly recommendations (we refer to these as “Network-Friendly Recommendations” or NFR). We show that this problem is non-convex, but we prove an equivalent convex one through a sequence of transformations (Section 4). (iii) Position Preference. We extend t he established user model so that it takes into account the e xpressed user preference on some recommendation positions (Section 5). We modify accordingly the optimization problem components, i.e., the v ariables a nd the constraints and show that the new one can also be transformed to a convex equiv alent. (iv) Real-World Data Validation. We validate our algorithms using existing and collected datasets from different content cata logs, and demonstrate performance improvements up to 3× compa red to baseline recommendations, and 80% compared to a greedy cache-friendly recommender, for a scenario with 90% of the original recommendation quality (Section 6). Finally, we discuss related work in Section 7 and present a set of open related problems in Section 8. 2.1 Problem Deﬁnitions We consider a user that consu m e s one or more contents during a session, drawn from a catalogue K of cardinality Deﬁnition 1 (Recommendation-Driven Requests). During the consumption of content i ∈ K, a list of N new contents are recommended to her, and she and picks uniformly among the N contents. a content j (e.g., through a se arch bar) with probability p∈ (0, 1), p= [p, p, . . . , p]. Assumptions on p. For simplicity, we assume a type of time-scale separation is in place, where the probabilities pcapture long-term user behavior (beyond one session). W.l.o.g. we also assu me pgoverns the ﬁrst content a ccessed, when a user starts a session. The above m odeled session captures a number of e veryday scenarios (e.g., watching clips on YouTube , personalized radio), where α captures the average probability of the user following recommendations (e.g., α = 0.5 was measured for YouTub e [16], and 0.8 for Netﬂix [17], or α = 1 in the case of AutoPlay). It is reported that YouTube users spend on average around 40 minutes at the service, viewing several related videos [1 8]. Content Retrieval Cost. We assume that fetching content i is ass ociated with a generic cost c∈ R, c = [c, c, ..., c], which is known to the content provider, and might depend on access latency, congestion overhead, popularity, ﬁle size, or even moneta ry cost. Minimizing cache misse s: Can be captured by setting c= 0 for all cached content and to c= 1, for non-cached content. Hierachical caching: Can be capt ured by letting ctake values out of m pos sible ones, corresponding to m cache layers: higher values correspond to layers farther from the use r [4], [19]. Remark: While we have assumed, for simplicity, that these costs are associated with caching, this is not a requirement for our framework. Any network problem that gives us a s input such cost values ccould be solved by the p roposed approach. What is more, we are assuming that these costs (e.g. the contents cached) a re ﬁxed, at least during some time frame. This is inline with the standard femto-caching approach of “cache today, consume tomorrow” [3], [20], [21], and the recent p aradigm of “popular content prefetching” followed by Netﬂix [2] and Google [22]. However, dynamic caching policies like LRU would require a different treatment (some details in Section 8). Deﬁnition 2 (Matrix U - Content Relations). For every pair of contents i, j ∈ K, a score u∈ [0, 1] is calculated, using a state-of-the-art method.These values populate the square K ×K matrix U, which is assum ed to be known to the RS. Deﬁnition 3 (Control Va ria ble R). Let r∈ [0, 1] denote the probability th at content j is recommended after a user watches content i. These probabilities deﬁne a sq uare K × K recommendation matrix R, over which we optimize . Baseline Recommendations. Recommendation systems (RS) is an active area of research, with state-of-the-art RSs using collaborative ﬁltering [23], matrix factorization [24], deep neural networks [25] and recently Q-Learning [26]. For simplicity, we as sume that the baseline RS works as follows: Deﬁnition 4 (Baseline Recommendations). For every content i ∈ K, the baseline RS at content i will always recommend the N itemswith the highest uvalues [1 6]. In other words, r, the i-th row of R, will be a vector of size K, indexed by N 1’s at the position of the highest u. Thus, for ever y content i, the baseline RS achieves: Our goal is to design a policy R, which is different from R(see Def. 4); Ris based only on U, and satisﬁes the u sers by offering q, whereas we are interested in designing an RS that considers (a) U and (b) access costs cof all contents available in the library, and satisfy also the network needs. As a warm-up, when we formulate our problem in the next section: we will be interested in minimizing a criterion that is based on the access cost, by guaranteeing some level of quality of recommendations (QoR) and treat it as a constraint. Deﬁnition 5 (Network-friendly RS). The q-Network-friendly RS is the one that achieves at le ast q, with q ∈ [0, 1], of the q for every content i. Therefore, the set of q-Network-friendly RS, is the RSs that obey the following set of K inequality constraints. Our q-NFRS with policy r, will guarantee the quality of recommendations (QoR) through the set of constraints Eq. (2) (the achieved QoR is on the left handside of this expression), where q -a tuning p arameter of the RS- decides the percentage of the qquality we offer. I m portantly, when q → 0, QoR is low and the RS recommends based only on the access cost (opportunity for large network gains), whereas if q → 1, the RS becomes R(the optimization problem is “very” constrained) and the RS cannot improve network access cost. In this paper, we will focus on values of q > 70% in order to capture interesting scenarios and see whether low network access cost can be achieved by keeping the users happy at the sa m e time. Remark:In addition to network delivery cost, networkfriendly recommendations might also improve user QoE: for example, a locally cached content could be fetched more efﬁciently (lower latency, higher bandwidth, etc.) and streamed without inte rruptions in High Deﬁnition, an obvious “win-win” situation for the network operator and the users. Recent experimental studies provide evidence and quantify such QoE improvements [15], [27]. In this context, optimization-wise, there are other interesting choices for jointly modeling the QoR and QoE. For instance, a way to capt ure user satisfact ion (in this paper captured only through QoR), would b e to add a second constraint that relates only to QoE-related metrics [27]. Probabilistic Recommendations. T he probabilistic way of deﬁning recommendations enables us t o capture generic scenarios. Consider a library of size K = 5, and an application requiring N = 2 recommended items. Assume that a user currently consumes content 1, and let the ﬁrst row of the matrix R to be r= [0.0, 1.0, 0.5, 0.5, 0.0]. In practice, this means that after consuming content 1, content 2 will always be recommended, and the second recommendation will be for content 3 or 4 with equal probability (r= r= 0.5). Increasing Hit Rate now. To exemplify the q-NFRS concept, assume again K = 5, content 5 is cached, q = 0.8 and u= [0, 1.0, 1.0, 0.2, 0.0], and q= 2.0, see Eq.(1). A q-NFRS with interest in maximizing its cache hit would have the following policy in content 1, t hat is r= [0.0, 0.8, 0.8, 0.0, 0.4]. This way, it would satisfy the constraint but at the same time drive the user also towards item 5 , which is cached. Increasing Hit Rate for the future. The example in Fig. 1 depicts such a s cenario, where the user consumes 5 items in sequence. The RS on the left suggests the most relevant item to the currently viewed all the time; this results in a hit rate of 20%. Interestingly, on the same ﬁgure on the right, we see the RS arranging a non-trivial policy: It offers the most relevant item at all times except when it ﬁnds the user at item 3, where it slightly degrades the quality of recommendations (u= 0.8). This simple move however, dras tically changes the path of requested contents and increases the hit rate in the long run from 20% to 60 %. Table 1 summarizes some important nota tion. Vectors and matrices are denoted with bold symbols. rProb. to recommend j after viewing i qMaximum baseline quality of content i pBaseline popularity of contents uSimilarity scores content pairs {i, j} cAccess cost for content i The goal of this paper is to carefully select recommendations in order to reduce the content access cost for users that have long sessions in multimedia applications. In this section, we init ially cast the use r request process as an Absorbing Markov Chain (AMC) (Section 3.1), which then helps us to derive the expected content access cost for a user session (Section 3.2). Finally we conclude the section by formulating the optimization problem of network-friendly recommendations (Section 3.3). 3.1 Renewal Reward Process As we described earlier, a session for a recommendationdriven us e r consists of a sequence of periods during which she follows recommendations, say S, intermixed with steps at which the user ignores recommendations (see Def. 1). To better visualize such a session see Fig. 2. We will use the following two arguments to model such a session: (i) Each Speriod can be modeled with an absorbing Markov chain with transition ma trix P (show matrix) of size K + 1 × K + 1, where the transient part Q =· R corresponds to the user following recommendations (according to our control K × K variable R); each such period can end at any step with a probability 1−α, modeled as an additional ab sorbing state. (ii) When a recommendation period ends, the process gets “renewed”, that is since the user ”re-enters” the ca talog from the same initial distribution pwhen not following recommendations, each Speriod is I.I.D. Hence, a user session can be modeled as a renewal process, that renews a fter each recommendation period (i.e. every time the user decides to not follow recommendations). In the following, we u se the above AMC to derive the expected cost per recommendation period, and the renewal reward theorem to derive the e xpected cost of the entire session, which will serve as our optimization problem objective. Deﬁnition 6 (Content Sequence). A content access sequence S = {S, S, . . . } deﬁnes a renewal process, with subsequences S, where the user follows recommended content, each ending with a jump outside of the RS. The cumulative cost of contents C(S) that incurred during a cycle Sis the cost of that cycle. 3.2 Long Term Expected Cost The goal of this s ubsection is to derive the long term expected cost of a user session. If we denote a s C(S) t he cost of the i-th cycle, we get the following expression Lemma 1 (Recommendation-Driven Cost). The content access cost C(S) during a (recommendation) renewal cycle Sis given If we further denote as |S| the expected length of such a cycle, then by using the geometric r. v. argument, the expected length is where G =I −· Ris the “fundamental matrix” of the AMC d escribed b y Eq. (3). Proof. Can be found in the Appendix. Finally, the following theorem which gives the long term expected cost, follows immediately from Def. 6, Lemma 1, and the Renewal-Reward theorem [28]. Theorem 1. The LTEC, for a long user session S, given a recommendation matrix R is 3.3 Optimization Problem In OP-Uni we formulate the optimization problem, where the goal is to minimize the expected cost given in Theorem 1 (objective function), by selecting the recommendations R (optimization variab le s). Optimization Problem (OP-Uni). minimizep·I −· R· c subject t or· u≥ q · q As discussed earlier, recommendations need to satis fy the quality constraints of Eq. (2) (captured in Eq. (8b)), be exactly N for ea ch content (captured in Eq. (8 c)), and conform to Def. 3 (captu red in Eq. (8d)). In this section, we deal with OP-Uni, by ﬁrst characterizing its convexity properties and then by applying a series of transformations that lea d to a Linear Programming formulation. Lemma 2. The problem describ ed in OP-Uni is nonconvex. Proof. The problem OP-Uni comprises Kvariables r, and a set of K+2·K linear (equality and inequality) constraints, thus the fea sible solution spa ce is convex. However, assume w.l.o.g that p= c = w; the objective now becomes f(R) = w(I −· R)w. Unless we constrain R to be in the class of symmetric and positive semideﬁnite matrices, the objective is nonconvex [29], [30]. Hence, there is no polynomial time a lgorithm solving problem OP-Uni. While one might be tempted to reduce the feasible solution space of variab le R and force it to be symmetric and positive semideﬁnite, and solve the problem as a convex SDP, this fundamentally leads to suboptimal solutions. 4.1 Road to the Optimal Solution A fundamental difﬁculty of OP-Uni is the inverse matrix in the objective p·I −· R· c. A reasonable ﬁrst action is to introduce K auxiliary variables and set them equal to z= p· (I −· R). Multiplying both sides from the right with (I −· R) yields Intermediate Step (Equivalent formulation). minimizec subject t o z−aN· z· R = (1 − α) · p(10b) Observe that although now the objective is linear in the variable z, the constraint of Eq. (10b) is quadratic in the z, R. There this step does not seem yet like much of a progress. Discussion: The above formulat ion falls under the umbrella of non-convex quadratically constrained quadratic program (QCQP), where it is common to perform a convex relaxation of the quadratic constraints, and then solve an approximate convex problem (e.g., semideﬁnite program (SDP) or Spectral relaxation, see [31] for more details). The problem can also be seen as bi-convex in variables R and z, respectively. Alternating direction method of multipliers (ADMM) can be applied to such problems, iteratively solving convex subproblems [32], [9]. Nevertheless, none of these methods provides any optimality guarant e e s, and even convergence for non-convex ADMM is an open research topic [33], [34]. The above discussion motivates us to pay close r attention to the problem structure and the actual meaning of the variables at hand. For this reason instead, we introduce an additional variable transformation, where we deﬁne variables fdeﬁned as f= z· r. Focusing on the Eq. (10b) in scalar form we have: The new variables are z (vector of size K × 1 v e ctor) and F (matrix of size K × K matrix). Interpretation of f.The vector appearing in the objective Eq. (8a) represents the stationary distribution of a PageRank-like model deﬁned by our stochastic process [9], [35]. Therefore, the scalar quant ity (1 − α) · zexpresses the long-term probability that it e m i is requested. Given that, it is easy to see that z= π·and as a consequence Recall from our deﬁnitions: rdenotes the probability to recommend content j conditioned on the fact that the user is at content i; ftranslates to the percentage of time (in the long run) the user was at i and saw j in her RS list, scaled by the quantity. Optimization Problem (LP (OP-Uni)). minimizec subject t of· u− z· q · q≥ 0, ∀ i ∈ K (13b) For the set of constraints in OP-Uni we simply substituted r= z· fand Eq. (8b) → Eq. (13b), Eq. (8c) → Eq. (13c) and ﬁnally the left handside of Eq. (8d) → Eq. (13e) whereas its right handside becomes Eq. (13d). Finally, r= 0 → f= 0. Lemma 3. The change of variables f= z· r, is a one-to-one mapping between (z, r) and (z, f). Proof. To obtain r, one needs to compute f/zfrom the pair {z, r}. In order to retrieve rfrom the above computation, the value zshould be strictly nonzero, as then rwould be undeﬁned. However, observe from since f≥ 0 and p> 0, ∀i (see Def. 1), this forces z to be strictly positive and thus never zero. Therefore rare always uniquely deﬁned provided that p> 0 ∀ i ∈ K. Note that the only condition we needed to est ablish in order for the Lemma 3 to hold, is tha t all contents must have a nonzero probability to be requested from the user. Combining Lemma 3, along with the deﬁnition of problem in the Intermediate Step, yields the ﬁnal formulation LP (OP-Uni). The LP (OP-Uni) corresponds to a Linear Program and it consists of 2K+ 4K + 1 linear constraints. To combat LPs, there are plently of imp lemented widely used solvers (e.g., ILOG CPLEX, GUROBI, MOSEK). There are two beneﬁts in transforming our problem to an LP compared to the heuristic ADMM we presented in [9]. 1) Opt im ality guarantees for LP (OP-Uni). 2) No need for parameter tuning. Solving OP-Uni via ADMM: (1) returns in principle a suboptimal solution, and (2) its performance heavily depends on carefully selecting t he parameter µ (the penalty on the quadratic term), while the CPLEX has no need of tuning. To validate this, we increase the library size K and solve the exact same instances of OP-Uni, and we report the results (execution t im e , and cache hit rate) in Tables 2 and 3; the details of the problem parameters are provided in Section 6. For the ADMM implementation [9], the inner m inimization loops of the ADMM were implemented us ing cvxpy [36] and more speciﬁcally the solver SCS [37]. In the following simulations we chose the ADMM tuning parameters as µ = 30. These experiments were carried out using a PC with RAM: 8 GB 1600 MHz DDR3 and Processor: 1,6 GHz Dua l-Core Intel Core i5. In both ta bles, the execution times of the LP-based solution is lower and returns a be tter objective value. However note that as we tighten the accuracy of the ADMM loop in Table 3, the suboptimality gap becomes much smaller, but that comes in cost of signiﬁcantly higher execution tim e s. 4.3 Greedy Baseline Scheme Here, we will formalize a probabilistic but myopic RS that aims at minimizing the access cost of only the next immediate request. Importantly, this will serve later as a heuristic baseline RS in the evaluation section. Notably, the Greedy Baseline approach resembles the policies proposed in [7], [8]. More speciﬁcally, the a lgorithm of [7] targets a different context (i.e., caching and single access content recommendation); the Greedy method could be interpreted as applying the recommendation part of [7] for each user, along with a continuous relaxation of the control (recommendation) variables. The approach we are following takes int o account the dependence of actions in consecutiv e steps of the user, and attempts to minimize the long term cost (P + P+ . . . ) · c, which we approximated with the stationary cost, see Eq. (8a). Hence, a simple approximation would be to keep only the ﬁrst term of this expansion. Optimizing this corresponds to a greedy (or “myopic”) algorithm that tries to minimize the cost of the next step only. This gives rise to the following, simpler optimization problem: Optimization Problem (OP-greedy-uni). The problem is an LP, as the objective can be readilyP written asr·p·cand the set of constraints Eq. (17) is the same convex set as OP-Uni without the demanding set of constraints (10b). Remark: The objective can be split in to K different summations, where each summand is independent. Moreover the constraints over each row of R are also independent. Hence, the problem is naturally decomposed into K different LP. In the previous sections, we hav e established that the problem of q-NFRS ca n be optimally solved for a recommendation-driven user deﬁned as in Section 3. A number of possible extensions to this simple model can be considered towards making it more realistic. We consider such an extension in this section. Speciﬁcally, recent studies studies [38], [16], have shown that users have the tendency to click on contents (or products in the case of e-commerce) according to their position in the list of recommendations. Hence, the probability of picking content in the ﬁrst position (v), may be higher than the probability to pick the content in position N (v). In fact, a Zip f-like relation has been observed [16]. Assumption on Position Preference. The user beha ves as in Def. 1 except that when N recommendations are shown to her, if she does follow recommendations (i.e., branch α), she clicks the item at position i with probability v. Note that, in the model considered thus far, it was essentially assumed that v= 1/N for all positions i. Incorporating the position preference presents the following complication: before, we simply needed to decide which contents to recommend, captured by control variables r; now, we need to decide which content to recommend at which position, deﬁning N sets of control variables r. Example. To make the notion of the probabilistic recommendations with positions more concrete, consider a library of K = 4 total ﬁles. A user just watched item 1, and N = 2 items must be recommended. We now focus on the recommendations of content 1, so let the ﬁrst row of the matrix Rbe r= [0.0, 1.0, 0.0, 0.0] and that of Rbe r= [0.0, 0.0, 0.5, 0.5]. In practice, this means that in position 1 the use r will always se e content 2 being recommended (after consum ing content 1), and the recommendation for position 2 will half the time be for content 3 and half for content 4. Objective Change. Similarly to Section 3, the transient matrix is now a convex combination of the N recommenderP matrices as follows Q = α ·v· R. Combining the latter expression and Theorem 1, the goal is to minimize the expected cost of a long session of such a use r ! minimizep·I − α ·v· R Constraint Changes. The budget constraint Eq. (8c) has t o change as we have N distinct stochastic matrices whose rows must s um to one (and not to N anymore). Regarding the quality constraint Eq. (2), we need to redeﬁne the q. To do so, we make use of U(N), which is the set of the N h ighest uvalues in decreasing order. Then, the qbecomesX Thus, a baseline RS (which would achieve q) would place t he most relevant item in t he most p robable position to be clicked and so on. Additionally, one needs to ﬁx the lhs of the quality constraint Eq. (2), and thus the new constraint becomes, vr· u≥ q · q Furthermore, we need to avoid situations where some content appears simultaneously in more than one positions. As an example, suppose a catalog of K = 4, and that we need to suggest the user N = 2 recommendations, and we are interested for the recommendation policy of content #1.For the sake of argument, s ay that we had the following policy for item #1: ﬁrst position in the recommendation list r= [0.0, 1.0, 0.0, 0.0], and the second position r= [0.0, 0.5, 0.5, 0.0]. The RS policy for the ﬁrst pos ition (r) dictat e s to always recommend item #2. The second position policy (r) dictates to recommend item #2 50% of the time (a nd #3 50%), thus leading us to recommend the user item #2 in different slots (50% of the time more speciﬁcally), which is something obviously unwanted. On the contrary, if the sum of frequencies over the N different slots for item #2 was a t most equal to 1, then we would end up with a policy t hat can alwa ys return a s e t of different recommendations. To avoid this situation, we impose K, additional constraints; each of these constraints upperbound the sum of recommendation frequencies of every r (along the N slots-positions), so that it is less than, or equal to 1. This is expressed a s follows, Finally, as in the uniform click-through probability case, we prohibit the RS from suggesting the same content the user is currently i.e., r= 0 ∀ i ∈ K and n = 1, . . . , N which makes up for K · N additional constraints. Wrapping it all up, the optimization problem in hand is the following: Optimization Problem (OP-pref). minimizep· (I − α ·v· R) subject tov· r· u≥ q · q, ∀ i ∈ K (23b) Result. OP-pref is nonconvex: its objective is nonconvex in the variables R, . . . , Rand the constraints are linear similarly to OP-Uni. N oneth eless, it can also be cast as an LP through the same transformation steps described in Section 4.1 (for details see Appendix). Note that for relatively small N (around 2, 3, 4) the scale of the problem remains unchanged, as instead of solving a problem with Kvariables, we will now face a problem with K + N ·Kvariables. This extra computational burden though, gives us the ﬂexibility to ca pitalize on the extra knowledge of the v statistics as we will see later in the simulation section. In t his section we observe the how network-friendly RS can actually increase the performance of the cache hit rate (CHR), and more particularly, we focus on the case where the user session is long. 6.1 The Different Policies Throughout the validation section we will consider three different policies. : The solution of OP-greedy-uni (assumes the user clicks uniformly among the N recommended items). : Corresponds to the optimal the solution of OP-Uni (assumes the user clicks uniformly among t he N recommended items). : The optimal solution of OP-pref (assumes the user clicks with vto the i-th position of the recommendations). We will use the term Gain of policy X over policy Y as the following 6.2 Datasets We use datasets of video and audio content, to obtain realistic similarity matrices U. We also create some synthet ic traces (with similar properties to the real data) for se nsitivity analyses. YouTube FR. We used the crawler of [14] and collected a dataset from YouTube. We considered 11 of the most popular videos on a give n day, and did a breadth-ﬁrstsearch (up to depth 2) on the lists of related videos (max 50 per video) offered by the YouTube API [39].We picked the 11 most popular videos, as this led to tra ce of ≈ 1K contents (i.e., of similar size to our other traces ).We built the m atrix U ∈ {0, 1} from the collected video relations by setting u= 1 if the content j is one or two hops away from i through the related list of i. last.fm. We considered a dataset from the last.fm database [40]. We applied the “getSimilar” method to the content IDs’ to ﬁll the entries of the matrix U with similarity scores in [0,1]. We then keep the largest component of the graph. Finally, as the relation matrix we end up is quite spars e , we saturate the values above 0.1 to u= 1. This is done in order to have a meaningful U matrix with m any entries. MovieLens. We consider the Movielens movie s-rating dataset [41], containing 69162 ratings (0 to 5 stars) of 671 users for 9066 movies. We apply an item-to-item collaborative ﬁltering (using 10 most similar items) to extract the miss ing user ratings , and then use the cosine distance (∈ [−1, 1]) of e ach pair of contents based on their common ratings. We set u= 1 for contents with cosine distance larger than 0.6. Synthetic. We genenerate an Poisson random graph of content relations K = 1000 nodes, where each content/node has on average 8 neighbors. To accompany our results, we present Table 4: a table with metrics of the content relation graphs we gathered and of the synthetic one we created (just one of size K = 1000). We deﬁne here as Neighb(i) the number of neighbors content i has in the relations graph U. Simulation Setup. Here, we consider a simple scenario with c∈ {0, 1}, which corresponds to minimizing the cache miss e s, or e quivalently maximizing the CHR. In all of our presented plots, in the y-axis we dep ict the CHR and on t he x-axis we vary different problem parameters. Importantly our metric for Section 6.3 will be the CHR of a long session of requests as calculated by the objective function Eq. (8a) and for Section 6.4 the one calculated from Eq. (23a). Moreover, we assume α = 0.5 − 0.9 ([17]), a Zipf popularity distribution with exponent s (in the ra nge of 0.4-0.8), and that the C most popular contents of the catalog, according to pare cached. We highlight that we split the simulations section in two subsections; ﬁrst one exploring results related to OP-Uni, and the second one to OP-pref. 6.3 Simulations: Clicking Uniformly over the recommendations In the ﬁgures that follow, we vary key parame ters of the problem by keeping ﬁxed the remaining ones and see how the CHR metric evolves. Impact of Quality of Recommendations (q). The most fundamental para m e ter of the paper is the quality of recommendations a RS provides to its user. To this end, in the ﬁrst simulation result, see Fig. 3, we increase the quality % (xaxis) constraint and present the CHR performance (y-ax is) of the two schemes Pand Palong with the relative gain as described earlier. We keep the ratio cache size/catalogue size (C/K) and number of recommendations (N) ﬁxed throughout. N aturally, we observe that less strict quality constraint a llows higher ﬂexibility in favoring networkfriendly content. Hence, Fig. 3 shows that for lower values of q, the CHR increases both under Pand P. However, when high-quality recommendations are desired, e.g., q ≥ 70%, Pheavily outperforms the baseline P. This can be easily seen through the curves of relative gain, where in all datasets, at q = 95% we observe a gain of at least 40%, in Figs. 3(a), 3(b),and more than 50% in Figs. 3(c), 3(d). Observation 1. The impact of q is the most fundamental result of this work. As q grows and the constraint becomes tighter, the margin for cache gain becomes smaller and smaller. That is when employing a policy equipped with look-ahead capabilities shines the most and when a much less sophisticated method fails to lay-over useful content paths through the recommendation mechanism. Observation 2. Note here that our relation matrices U are binary in the sense that a content is either related or unrelated. T his hints why as q grows, the Gain of Pover Pgrows. As q becomes larger, essentially Pselects at random the related contents it chooses in order to satisfy the constraint whereas Pmakes its decision based on possible future trajectories of the user. Impact of Number of Recommendations (N). The YouTube mobile app usually pop s 2-3, related videos before the us e r ﬁnishes her current streaming session. For such values of N = 2 or 3, in Fig. 4 (a ), Pperforms more than 50% better than the P(MP H = 20.28%), whose performance is not signiﬁcantly affected by N. Observation 3. Comparing the two schemes in Fig. 4(a) reveals an interesting ins ight : it is more efﬁcient to nudge the u ser towards network-friendly content by narrowing down her options N for both network-friendly policies. Note here that the RS’s goal is to ﬁnd N items that are of high uvalue and are also cached. With the increase of N what happens is the following: suppose that the RS found this one item that is useful in all dimensions (cached and related). If N = 1, then we can assign the full budget to this particular item and get a cache hit, whereas if N is larger, then due to the randomness with which the user clicks, it is much harder for the RS to drive her towards the neighborhood it wants. Observation 4. In the previous observation we brieﬂy explained why the CHR drops for larger N for any networkfriendly policy. However, it is evident that Pis more sensitive in this parameter. This can be explained by the fact that the aforementioned situation of the “useful content” is basically just the tip of the iceberg a nd a very favorable scenario. Essentially what happens most of the time , is that the RS does not have such contents, and here is where P does things better: it looks deeper into the session and ﬁnds which contents lead to “useful” contents in future requests. Impact of α. The key message of Fig. 4(b) (MP H = 5%) is that a multi-step vision method such as Ptakes into account the knowledge of user’s behavior (α). This can be mainly seen by the superlinear and linear imp rovement of Pand Pmethods respectively. Impact of zipf paramete r.In the previous plots, we have assumed a zipf parameter from 0.5 to 0.7. In the result we show next, we ﬁx all pa rameters and increase the popularity vector skewdness. As a result, the hit rate of both P and Pwill increase, a s the caching is based solely on popularity. Interestingly, the hit rate increases superlinearly, but similarly to earlier, our focus is on the performance gain when the RS policy has look-ahead capabilities. The result of Fig. 4(c) for the Movielens dataset , validates that the proposed policy outperforms the myopic one in the entire range of the simulated values. 6.4 Simulations: Clicking Non-Uniformly over the recommendations In S e ction 4, we establish theoretically why our method has clear beneﬁts in the regime of long user session by showing optimality of P. In the previous subsection, we presented some results to show in terms of actual numbers, how much of an improvement a method with deep vision such as P can have, over a short-sighted one such as P. In this su bsection, we will slightly change direction and try to understand whether the knowledge of v can deliver even further gains in the regime of long sessions. To this end, we will investigate policies Pand P, which both have look-ahead capabilities , but the ﬁrst is aware of v while the second assumes uniform click over the N recommended items. Moreover, we will employ as a para m e ter, the entropy of the pmf v which is deﬁned as According to H, a user who clicks uniformly between any of the N positions has the maximum entropy H= 1, whereas a user who clicks only in one position (e.g., the ﬁrst up on the screen) has the minimum entropy H= 0 as she clicks deterministically. In Figs. 5(a), 5 (b) (see Table 5 for simulation parameters), we assume behaviors of increasing entropy; starting from users that show preference on the higher positions of the list (low entropy), to users that select uniformly recommendations (maximum entropy). In our simulations, we have used a zipf distribution [16] over the N positions and by decreasing its exponent, the entropy on the x-axis is increased. As an example, in Fig. 5(a), lowest Hcorresponds to a vector of probabilities v = [0.8, 0.2] (recall that N = 2), while the highest one on the same plot to v = [0.58, 0.42]. Thus, we init ially focus on answering the following basic question: Is the non-uniformity of users’ preferences to some positions helpful or harmful for a network friendly RS? From Figs. 5(a), 5(b), it becomes very clear that the lower the entropy, the more the v-awareness helps Pgain over the agnostic policy P. Observation 1. We observe by these plots that a skewed v, is helpful for the NFRS. In the extreme case where v is extremely skewed (H→ 0), where virtually this means N = 1, the user clicks determinist ically, and the op timal hit rate becomes maximum. This can be also validated in Fig. 6(b), where for increasing entropy the the hit rate decreases and its maximum is attained for N = 1. Lastly, we investigate the sensitivity of Pand P, against the number of recommendations (N). In Fig. 6(b), we present the CHR curves of the two schemes for increasing N, where we keep constant the distribution v ∼ zipf (0.9). As expected, for N = 1 (e.g., YouTube autoplay scenario) P and the proposed scheme coincide, as there is no ﬂexibility in having only one recommendation. Observation 2. For large N , Pmay offer the “correct” recommendations (cached or related or both), but it cannot place them in the right positions, as there are now too many available spots. In contrast, the scheme Precommends the “correct” contents, and places the recommendations in the “correct” pos itions. Fig. 6(a ), strengthens even more the Observation 2; its key conclusion is that with high enough enough s (i.e. low H) and more than 2 or 3 recommendations, while Paims to solve the multiple access problem, its position preference unawareness leads to highly suboptimal recommendation placement, and thus a severe drop of its CHR performance compared to the P. Recommendation and Caching Interplay. The relation between recommendation systems and caching has only recently been considered [7], [8], [10], [11], [12], [13], [14], [42]. The problem of optimizing jointly ca ching a nd recommendations in a st atic (IID requests and static placement) setting, for a user accessing the home screen of an application has recently been considered [7], and a heuristic approach was suggested for its s olution. In [8], the authors propose a simple yet effective algorithm according to which, they randomly inject cached content in the “ related lis t” of YouTube, which comes with a small decrease of recommendations quality, and an interesting increase in cache hit ra te. In [11], the focus is on content caching, and the authors introduce the notion of “soft cache hit”. Essentia lly, t he aim is to cache the items that are popular, but also hot in the sense of frequently appearing in the related list of other contents. A more measurement-oriented approach is found in [14], where a true cache-aware RS was implemented, and ba sed on the authors ﬁndings, high gains can be achieved when the RS suggests content from the cache.In [13], a single access us e r is considered. There, caching policy is based on machine learning techniques, where the users’ behavior is estimated through the users’ interaction with the recommendations and this knowledge is then exploited at the next edge cache updates. In [42], the authors introduce a ﬁrst of its kind formulation of a wireless recommendations problem in a contextual bandit framework, which they call contextual broadcast bandit. In doing so, they propose an epoch-based algorithm for its solution and show the regret bound of their algorithm. Interestingly, they conclude that the user preferences/behaviors learning speed is proportional to the square of available bandwidth. The work in [7] considers the joint problem of caching and recommendations in a static setting. Similarly in [13], a single access user is considered. There, caching policy is based on machine learning techniques, where the users’ behavior is estimated through the users’ interaction with the recommendations and this knowledge is then exploited at the next edge cache updates. Recommendations for the Long Term Cost. In [9], the problem of RS design for minimum network cost in the long run ﬁrst appears. We formulated a nonconvex p roblem, and proposed a heuris tic ADMM algorithm on the nonconvex formulation which comes with no theoretical guarantees. In [43], the preliminary version of this work, we convexiﬁed the same problem and offered an optimal solution under an LP formulation. Here, through means of simulation, we show that the LP framework is computationally more efﬁcient than the ADMM previously used in [9], as State of Art LP solvers can be u sed to tackle it. Equally importantly, we extended the framework in order to capture the importance of the recommendations placement in the GUI, again optimally [43]. Note that several of the aforementioned st udies [9], [11], [10] ignore the preference that users exemplify to some the position of recommendations over others. The work in [7], while taking into account the ranking of the recommendations in t he modeling and their proposed algorithm, in the simulation section they assume that the b oosting of the items is equal. Overall, this work serves as a uniﬁcation, as it extends and generalizes our previous works b y including new results from additional datasets, that further strengthen the case of LP-based NFR in long use r sessions. Optimization Methodology. The problem of optimal recommendations for multi-content sessions, bares some similarity with PageRank manip ulation [30], [44], [4 5]. The idea there is to choose the links of a subset of pages (the use r has access to) with the intent ion to increase the PageRank of some targeted web page(s). Although that problem is generally hard, some versions of the problem can also be convexiﬁed [44]. In this section we discuss some open problems that are related to NF-RS design. It is important to highlight t hat most of the following problems are essentially generalizations of what we have presented in this work, and in genera l not addressable by our framework. Joint optimization of caching and recommendation: Long user sessions. A reduction of the general net work-friendly recommendations is to consider it in t he context of caching, i.e., the network cost becomes t he cache miss probability. In our framework, it is not speciﬁed whether we focus on the edge caching problem (single or femto) or a CDN-like architecture. We simp ly need from the operator, who is aware of the network state, the set of content costs and we guarantee to deal with the recommendation side of things. However, a problem that is timely problem is the one of joint cache placement and recommendation under the regime of many content requests. The problem we studied in this paper, could be cast in the single cache framework where the c is the hot vector deciding which contents should be cached and which not. It is quite evident though that this problem is a mixed integer program (c is a binary vector), where the objective is qua dratic (caching decisions are multiplied with the recommendations matrix), and therefore lies in the category of hard problems. Nonetheless, heuristic methods that alternatingly opt imize over the two variables still apply; one could even use the method proposed in this paper for the minimization of the recommendation variables. On the other hand, the joint femtocache content placement and recommendation would require additional modeling work since our optimization objective (if seen under the lens of caching) does not consider different base sta tions. Different user models. In this study, we focused on a simple, yet quite general user behavior. The user model we discussed is fully parameterized and essentially if one has measured data/statistics about the crucial quantit ie s like α (user willingnes s to click on recommended items) or under which q is the user happy, our modeling and optimization approach enjoys optimality guarantees. However, recommendations and how they affect the user’s request pattern is an open problem. A major assumption we made here, was that users have ﬁxed α, which could be considered as unrealistic. In practice, users have a more reactive behavior towards their recommendations; receiving good recommendations might increase their instantaneous α or bad ones might have the opposit e effect. Thus, starting off from the Markov Chain framework, and based on the modeler be lief for the user, one could engineer different models. A ﬁrst nontrivial extension to our model would be to deﬁne a user whose α (i.e., clickthrough on recommendations) is policy dependent; thus instead of modeling the α as constant and incorporating the quality of recommendations as an external hard constraint, we could embed it in the content transition and make α a function of the policy. Moreover, here we have discussed the cases where the item selection is either random or depends in an i.i. d manner from the position the item is placed. An additional modeling twist would be to allow the item selection t o be based on the s im ilarities uof the items. However, these extensions would further complicate things as they would introduce additional nonconvexities to our OP-Uni. Dynamic (caching) conditions. According to our assumptions, the network state, i.e., t he costs of the contents remain the same throughout the course of the day. However, in many practical scenarios the operators already have their infrastructure inside the network where some dynamic caching policy such as LRU, q-LRU or LRU-K [46], [47] is pre-implemented. A big challenge that remains unresolved is the one of designing opt im al recommendation policies over a network of LRU-like cache replacement policies. In our op inion, this framework has two open questions. First of all, as dynamic cache policies are not easy to analyze, approximations are typically employed in order to acquire meaningful metrics. In the dynamic cache setting, the we ll known Che and time-to-live approximations do not capture the effect of a RS over the average lifetime of contents inside the cache. So we consider a nalyzing the effect of any RS on the cache lifetime statistics to be an int e resting topic on its own. Furthermore, a s we know the RS has the power to shape the content popularity and therefore which contents will live longer inside the cache. Nonetheless, now the stage for the recommendation algorithm is much more hostile. Imagine we kept our Markov chain framework and augment its state space to include also the cache conﬁguration. Then plau sibly, one would want to ﬁnd the static (computed ofﬂine) optimal recommendation policy under an LRU caching policy. However, the cache conﬁgurations is the exploding in size unique permutations ofP, resulting to a state space of size K ·P. It becomes obvious that even for a moderate problem size such as K = 200 and C = 3 we have ≈ 1.5 billion state s, and over each one of them we should make decisions. Thus, either the content lifetime approximation should be somehow used as a proxy to the content cost or maybe even some function approximation in order to discover what features of the problem really matter. Unknown, static or dynamic, user behavior and Learning. The solution mindset we employed is the one of modelbased optimization. As such, our solution might need retuning with the change of α during the course of the day. Problems like t his, could be better handled through learning-based (a.k.a. model-free) optimiza tion methods. Although this path sounds very appealing, there are a few pitfalls into it. As an example, if we employ a Q-Learning based algorithm, a question that arises is “Will it converge soon enough?” Maybe by the time it has lea rned the user behavior, the network state has changed and then all the effort on learning the user might have gone to waste.Thus, such approaches cannot be applied straightforwardly in our problem and could be considered as new problems on their own. A promising idea for such a demanding problem would be learn policies through function approximation, which can generalize [48]. This line of problems could also be faced through Online Convex Optimization methods, which are well known to optimize some dynamically changing function, even if it is picked by an adversary [49]. This research is funded by the ANR “5C-for-5G” project under grant ANR-17-CE25-0001, and the IMT F&R, “Joint Optimization of Mobile Content Caching and Recommendation” project. I t is also co-ﬁnanced by Greece and the European Union (European Social Fund- ESF) through the Operational Programme “Human Resources Development, Education and Lifelong Learning” in the context of the project “Reinforcement of Postdoctoral Researchers - 2nd Cycle” (MIS-5033021), implemented by the State Scholarships Foundation (IKY).