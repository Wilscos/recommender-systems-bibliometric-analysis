Application Programming Interfaces (APIs) provided by software libraries or frameworks play an important role in modern software development. Almost all programs, even the basic “hello world!” program, include at least one API. However, there are a huge number of APIs from different modules or libraries. For example, Java standard library [70] provides more than 30,000 APIs. It is infeasible for developers to be familiar with all APIs. To address this problem, many approaches are proposed to recommend APIs based on input queries, which describe the programming task in natural language, or surrounding context, i.e., the code already written by developers. However, a uniform deﬁnition of the current API recommendation task is still absent, leading the task hard to be followed by potential researchers. Some studies [11], [44], [61], [82], [84] regard the task as a code completion problem, and recommend any code tokens including APIs. These studies focus on improving the prediction results of all the tokens instead of only APIs. Some studies [31], [37], [49], [76], [78] recommend relative APIs on different levels given natural language queries. Besides, the evaluation results are difﬁcult to be reproduced by future related work. For example, for query-based API recommendation, manual evaluation is generally adopted, so the performances reported by different studies are difﬁcult to be aligned. Comparing with widely-used IDEs or search engines is another commonly adopted yet inconsistent evaluation strategy in previous research. Therefore, to better facilitate future exploration of the API recommendation task, in this paper, we summarize the recent related approaches and build a general benchmark named APIBENCH. To facilitate the benchmark creation, we group the recent related approaches into two categories according to the task deﬁnition: query-based API recommendation and codebased API recommendation. 1) query-based API recommendation. Approaches for query-based API recommendation aim at providing related APIs to developers given a query that describes programming requirements in natural language. The approaches can inform developers which API to use for a programming task. 2) code-based API recommendation. Approaches for code-based API recommendation aim at predicting the next API given the code surrounding the point of prediction. They can directly improve the efﬁciency of coding. Besides the unreproducible evaluation, the two groups of studies face their own challenges. 1) For query-based approaches, high-quality queries play a critical role in accurate recommendation. However, there may exist a knowledge gap between developers and API designers in choosing terms for describing queries or APIs. For example, developers who do not know the term “heterogeneous list” in API documents would use other words such as “list with different types of elements” in the query. Whether current query reformulation techniques are effective for API recommendation and how effective it is are still remaining unexplored. 2) For code-based approaches, the quality of code before the recommendation point also affects the recommendation performance. Generally, the approaches are evaluated by simulating an actual development, i.e., some parts of a project are removed for imitating a limited context. The APIs to recommend may locate in the front, middle, or back of the code, so exploring the impact of different recommendation points is important for understanding the recommendation capability of existing approaches. Other factors such as whether the APIs are standard or userdeﬁned, lengths of given context, and different domains can also inﬂuence the recommendation performance, which have not yet been fully investigated. To comprehensively understand the above challenges, we evaluate the approaches in APIBENCH from various aspects. APIBENCH is built on Python and Java, and involves two datasets for evaluation, named as APIBENCH-Q and APIBENCH-C for query-based and code-based approaches, respectively. APIBENCH-Q contains 6,563 Java queries and 4,309 Python queries obtained from Stack Overﬂow and API tutorial websites. APIBENCH-C contains 1,477 Java projects with 1,229,698 source ﬁles and 2,223 Python projects with 414,753 source ﬁles obtained from GitHub. Based on APIBENCH, we study the following research questions: code-based API recommendation approaches? techniques on the performance of query-based API recommendation? the performance of query-based API recommendations? mend different kinds of APIs? proaches in handling different contexts? in cross-domain scenarios? APIBENCH involves the implementation of the related approaches proposed in the recent ﬁve years, speciﬁcally including ﬁve query-based approaches and ﬁve code-based approaches. In RQ1, we compare the performances of the approaches in APIBENCH. To answer RQ2 and RQ3, we apply four popular query reformulation techniques to the queries of APIBENCH-Q and observe the performance of the query-based approaches given reformulated queries. To answer RQ4 to RQ6, we analyze the APIs in APIBENCH-C from different aspects and study the performance of codebased approaches under different experimental settings. Key Findings. Through the large-scale empirical study, we achieve some ﬁndings and summarize the key ﬁndings as below. (1) For query-based API recommendation: performance on the class level than on the method level. Recommending the exact API methods is still a challenging task. pansion and query modiﬁcation, are quite effective in improving the performance of query-based approaches. torials that are more similar to real-world queries can signiﬁcantly improve the performance of current approaches. (2) For code-based API recommendation: show superior performance on this task. Meanwhile, current IDEs can achieve competitive performance as recent pattern-based and learning-based approaches. They work far away from just recommending APIs based on alphabet orders. from standard libraries and popular third-party libraries, but their performance drops a lot when recommending user-deﬁned or project-speciﬁc APIs. problem of cross-domain adaptation. Approaches trained on multiple domains achieve satisfying performance when testing on most single domains, and they even outperform those trained on corresponding single domains. Based on the ﬁndings, we conclude some implications and suggestions that would beneﬁt future research. On the one hand, query-based API recommendation approaches should be built along with query reformulation techniques to handle queries with different qualities. We also encourage future work to leverage different data sources and few-shot learning methods to address the low resource challenge in query-based API recommendation. On the other hand, we suggest future code-based API recommendation approaches focus on improving the performance of recommending userdeﬁned APIs as it is the major bottleneck. Contributions. To sum up, our contribution can be concluded as follows. tematically study both query-based and code-based API recommendation techniques on two large-scale datasets including Java and Python. APIBENCH to fairly evaluate query-based and codebased approaches. performance of current approaches, including query quality, cross domain adaptation, etc. would be important for future research in API recommendation. The rest of this paper is organized as follows. We present the background and regular API recommendation process in Section 2. We describe the details of APIBENCH, current baselines and evaluation metrics in Section 3. Then we introduce the experiment results and potential ﬁndings on querybased and code-based API recommendation in Section 4 and Section 5, respectively. Based on the ﬁndings, we conclude some implications and future directions in Section 6. Finally, we discuss threats to validity and related work in Section 7 and Section 8, respectively. In this section, we summarize the query-based approaches and code-based approaches, respectively. 2.1 Query-Based API Recommendation Methods We describe the typical query-based API recommendation process in ﬁgure 1. Given a query “Calculate int value square root”, query reformulation techniques ﬁrst modify the query as “return int value square root” or expand it as “ﬁnally calculate int value square root”. A knowledge base built upon available data sources is also prepared for API candidate selection. Based on the knowledge base, retrieval-based methods or learning-based methods recommend the APIs relevant to the queries. 2.1.1 Query Reformulation Techniques Input queries can be short in length or vague in semantics. Besides, there may exist a knowledge gap between developers and search engines in query description. For rendering search engines better understand the query semantics, query reformulation is a common pre-processing method. In general, there are two major types of query reformulation approaches: 1) query expansion, which adds extra information to the original queries; 2) query modiﬁcation, which modiﬁes, replaces or deletes some words in the original queries. Query expansion. Query expansion aims at identifying important words that are missing in the input queries. The topic is originally stemmed from the ﬁeld of natural language processing (NLP). For example, the work [51] utilizes word embeddings to map words in the vector space and ﬁnds similar words to enrich the queries. For the API recommendation task, since APIs are encapsulated and organized according to classes and modules, class names and module names are important hints for recommendation. Rahman et al. [76], [78] propose to use keyword-API class co-occurrence frequencies and keyword-keyword cooccurrence frequencies to build the relationship between words and API classes, and add the suggested API class for query expansion. Query modiﬁcation. Query modiﬁcation aims at mitigating both the lexical gap and knowledge gap between the user queries and descriptions in knowledge base. The lexical gap ,such as mis-spelling, can be easily addressed by spelling correction and synonym search, etc. Recent work focuses on how to mitigate the knowledge gap by replacing inappropriate words in queries. Mohammad et al. [4] extract important tokens in code, and Sirres et al. [88] leverage discussions and code from Stack Overﬂow posts to build a knowledge base. Cao et al. [12] collect query reformulation history from Stack Overﬂow and propose a Transformer-based approach to learn how developers change their queries when search engines do not return desired results. 2.1.2 Recommendation with Knowledge Base Knowledge base. API recommendation approaches generally require a knowledge base that contains all the existing APIs as the search space. There are three primary sources for the knowledge base creation, including: 1) ofﬁcial documentations which contain comprehensive descriptions about the API functionality and structure. 2) Q&A forums, which provide the purposes of APIs and different API usage patterns. Many studies [37], [75] leverage the Q&A pairs from Stack Overﬂow to select API candidates. 3) Wiki sites, which describe concepts that link different APIs. For example, Liu et al. [49] utilizes API concepts from Wikipedia to help build API knowledge graphs. Retrieval-based methods. Retrieval-based methods retrieve API candidates from the knowledge base and then rank the candidate APIs by calculating the similarities between queries and APIs. For example, Rahman et al. [76], [78] utilize the keyword-API occurrence frequencies and API-API occurrence frequencies to ﬁnd the most relevant APIs. Huang et al. [37] ﬁrst identify the similar posts from Stack Overﬂow by computing query-documentation similarities and choose the APIs mentioned in posts as candidates. Liu et al. [49] build an API knowledge graph to represent relationships between APIs and then calculate the similarities between queries and certain parts of API knowledge graph to rank the APIs. Learning-based methods. Another type of method is to automatically learn the relationships between queries and APIs based on deep learning techniques. The knowledge base provides query-API pairs as the ground truth. For example, Gu et al. [31] formulate the task as a translation problem in which a model is built to translate word sequences into API sequences. They propose an RNN model with a encoder-decoder structure to implement the translation. 2.2 Code-Based API Recommendation Methods We describe the workﬂow of code-based API recommendation in Figure 2. Given a target code, context representation is an essential step. Based on the extracted context, patternbased methods or learning-based methods are adopted by previous studies to recommend the next API. 2.2.1 Context for the Target Code Most code-based API recommendation methods regard the code before the recommendation point as the context. We name such context as internal context since it only considers code in the current source code or current function body. For example, Line 1 ∼ 6 of the target code in Figure 2 belongs to internal context. Xie et al. [98] ﬁnd that replacing external APIs in code (such as Arrays.asList() in Figure 2) with their implementations can help the identiﬁcation of common usage patterns. They propose to build a hierarchical context by integrating the implementation out of the current source ﬁle. We name the implementation of external APIs as external context. 2.2.2 Context Representation We divide the context representation methods into two types, i.e., pattern-based representation and learning-based representation. Pattern-based representations [18], [63], [64], [97], [98] do not consider all the code tokens. Instead, they only identify APIs to build API usage sequences, as shown in ﬁg. 2 (a), API matrix, as shown in ﬁg. 2 (d), or API dependency graphs to represent the current context. Learning-based representations [32], [35], [44], [82], [92] usually represent the context with token ﬂows, as illustrated in Figure 2 (b), or other syntax structures such as Abstract Syntax Trees (ASTs), as illustrated in Figure 2 (c). 2.2.3 Recommendation Based on Context Pattern-based methods. API recommendation is inherently a recommendation task, so some studies [18], [64] follow the collaborative ﬁltering (user-item) methodology of traditional recommendation systems [83]. As shown in Figure. 2 (d), they regard the internal context as the users and APIs as the items. They then calculate the similarities between different users to ﬁnd the most similar API for recommendation. However, the methods do not consider the relationships between APIs. More recent work [97], [98] build API dependency graphs or mines association rules to capture API usage patterns. Learning-based methods. Hindle et al. [35] discover the naturalness of software, rendering it possible to deploy machine learning or deep learning methods on code. Different from pattern-based methods that consider the relationships between API occurrences, learning-based methods regard API as a single code token, and reformulate the code-based API recommendation problem into a next token prediction problem. Many statistical language models [62], [80], [82], [92] are proposed to predict the next code token. Besides using the token sequences, more recent work [32], [44] try to leverage syntax and data ﬂow information for more accurate prediction. In this section, we introduce the scope of the studied APIs, the preparation of benchmark datasets, and implementation details. 3.1 Scope of APIs To fairly compare the current API recommendation approaches, a benchmark dataset should be prepared, during which the scope of studied APIs ﬁrstly needs to be deﬁned. In this work, we focus our evaluation on two popular programming languages, i.e., Python and Java. For facilitating the analysis of the challenges in API recommendation, we divide all APIs into standard APIs, user-deﬁned APIs, and popular third-party APIs. The standard APIs refer to the APIs that are clearly deﬁned and built-in in corresponding programming languages while the userdeﬁned APIs are deﬁned and used in projects along with popular third-party APIs. Note that we evaluate query-based API recommendation methods only on the standard APIs since user-deﬁned APIs are generally not associated with detailed descriptions or extensive discussions for facilitating the recommendation. We evaluate code-based API recommendation methods on all three kinds of APIs. The details of each kind for different programming languages are depicted below. (1) Standard Java APIs. We choose the version Java 8 for our analysis since it is the most widely-used version in current projects according to the 2020 JVM Ecosystem Report [89]. We collect 34,072 APIs from the Java documentation [70] as standard APIs. (2) Popular Java third-party APIs. We choose APIs from the Android library [28] since Android is one of the most popular applications of Java programs. We collect 11,802 APIs from the ofﬁcial documentation of Android in total and regard them as the popular third-party APIs. (3) Standard Python APIs. As Python Software Foundation has stopped the support for Python 2, currently only 6% of developers are still using Python 2, according to the development survey conducted by Jetbrains [39]. Considering that APIs of different versions above 3.0 are similar, we choose the newest version 3.9 to ensure the compatibility, and collect 5,241 APIs from Python standard library [72] as the standard APIs. (4) Popular Python third-party APIs. Python is well extended by a lot of third-party modules. We choose ﬁve widely-used modules with sufﬁcient documentations, including ﬂask [21], django [17], matplotlib [53], pandas [71] and numpy [68]. We collect 215, 700, 4,089, 3,296 and 3,683 APIs from them, respectively. (5) User-deﬁned APIs. For code-based API recommendation, we regard all the functions deﬁned in current projects as user-deﬁned APIs. We do not explicitly collect them as a ﬁxed set because they vary across projects. By inspecting the implementations, we can always identify the user-deﬁned APIs. 3.2 Benchmark Dataset In this section, we describe how we build the benchmark datasets APIBENCH-Q and APIBENCH-C. 3.2.1 Creation of APIBENCH-Q We build the benchmark dataset APIBENCH-Q by mining Stack Overﬂow and tutorial websites. Note that we ﬁnd that currently there is no query-based API recommendation approach specially designed for Python programs, but we still collect the query benchmark for it to facilitate further research. Mining Stack Overﬂow. As one of the most popular Q&A forums for developers, Stack overﬂow contains much discussion about the usage of APIs. Stack Overﬂow is the primary source for building APIBENCH-Q. We ﬁrst download all posts from Aug 2008 to Feb 2021 on Stack Overﬂow (SO) via Stack Exchange Data Dump [20]. Each post is associated with a tag about the related programming language. We ﬁlter out the posts not tagged as Java or Python, resulting in 1,756,183 Java posts and 1,661,383 Python posts. We further ﬁlter out the posts based on the following rules: posts that do not have endorsed answers. such as “error” and “why” that are rarely used by programmers for asking about API recommendation. tag <code>, because we cannot extract any API from them. longer than two lines, since large code snippets in a post usually indicate that the query should be handled by a series of operations, not a single API sequence. that do not contain any APIs involved in the paper, as described in Sec. 3.1. After the rule-based ﬁltering, we obtain 156,493 Python posts and 148,938 Java posts that contain descriptions about APIs. However, some of the posts are not directly related to API recommendation. For example, some posts only ask about comparing two similar APIs. The unrelated posts are hard to be automatically identiﬁed by rules. To ensure the relatedness of the posts in our benchmark dataset, we invite 16 participants with an average of 3-year development experience in Python or Java for manually checking. For each post, two of the participants are involved to check the following aspects: 1) whether the query asks about API recommendation; 2) whether the standard APIs recognized by the previous rules are intact, i.e., including the whole class and method names. 3) whether the APIs in answers exactly address the query. If two participants provide the same answers for one post and also one of the above three aspects is not satisﬁed, we directly remove the post. If the two participants do not reach an agreement, the post will be forwarded to one of the authors to make a ﬁnal decision. We manually check 13,775 posts, in which 1,262 posts do not reach an agreement by the annotators and need further check by one of the authors. We use the commonlyused Fleiss Kappa score [22] to measure the agreement degree between the two annotators and the value is 0.77. The result indicates a high agreement between them. Based on the manual check, 3,245 of the 13,775 labeled posts are remained, including 1,925 Python queries and 1,320 Java queries, comprising the ﬁrst part of our benchmark APIBENCH-Q, as shown in the second column of Table 2. Mining tutorial websites. API tutorial websites are the second major source of query-API pairs. We choose three popular API tutorial websites GeeksforGeeks [1], Java2s [2] and Kode Java [3] to establish APIBENCH-Q. Different from Stack Overﬂow that contains discussion on various topics, API tutorial websites focus on providing examples of how to use APIs. Therefore, manually annotating the relatedness of each query to API recommendation is not necessary. We adopt similar rules as mining Stack Overﬂow to ﬁlter out those without code snippets or associated with large code snippets. We ﬁnally collect 5,243 Java queries and 2,384 Python queries, which comprise the second part of our benchmark APIBENCH-Q, as shown in the ﬁfth column of Table 2. We create the benchmark dataset APIBENCH-C by mining GitHub. GitHub [55] is one of the most popular websites for sharing code and includes large numbers of code repositories on different topics and programming languages. In order to explore the performance of API recommendation under different domains, we ﬁrst determine the domains for analysis. According to the JetBrains’ developer survey [39] and topic labels provided by GitHub, we choose four popular domains for Python and Java, respectively, as shown in Table 1. For Python, we consider the domains “ML”, “Security”, “Web”, and “DL”; while for Java, we involve domains “Android”, “ML”, “Testing”, and “Security”. For each domain, we collect 500 repositories with the most stars and 500 repositories with the most forks on GitHub. Besides the speciﬁc domains, we also build a “General” domain which only considers the popularity of repositories. For the “General” domain, we collect 1,000 repositories with the most stars and 1,000 repositories with the most forks on GitHub regardless of the topics. Not all the collected repositories are applicable for codebased API recommendation. Some popular repositories do not contain enough code, e.g., only including documentations. To remove such repositories, we use cloc [6] to scan the code in each repository and ﬁlter out the repositories that 1) have fewer than 10 ﬁles or 2) have fewer than 1000 lines of code or 3) have code in Python or Java but with the ratio less than 10%. The number of projects, number of ﬁles, and average number of code lines for each domain of APIBENCH-C are shown in Table 1. As most approaches [27], [32], [44], [64], [82] for codebased API recommendation require a training set to learn the API patterns or train the models, we split APIBENCH-C into a training set and a test set with a ratio of 80% and 20%, respectively. Note that we do not split a project both into the training set and test set, but put all the ﬁles of the same project into either the training set or test set, because Alon et al. [8] and LeClair et al. [47] ﬁnd that code in the same project usually share the same variable names and code patterns, and splitting without considering project can cause data leakage. For the approaches requiring a validation set, we prepare it from the training set. In order to study the impact of different recommendation points and different lengths of functions on the performance of current approaches, we analyze the average length of functions in each repository. By using Kernel Density Estimation (KDE), we observe that the length distributions of functions in different domains are similar. The distributions of the “General” domain are depicted in Figure 3. From the ﬁgure we observe that most Python functions contain 5 ∼ 30 lines of code (LOC) and most Java functions contain 5 ∼ 20 lines of code. However, there still exist a few extremely short or long functions. These functions are likely to impact the performance of code-based approaches with extremely long or short contexts. To derive the functions of appropriate lengths for study, we regard the shortest 5% of the functions as extremely short functions, the longest 5% of the functions as extremely long functions, and the middle 90% of the functions as functions with moderate lengths. We then further study the performance of codebased approaches on them in sec. 5.3. The thresholds for distinguishing short and long functions regarding the Line of code are illustrated in the last two columns of Table 1. In order to study the performance of current approaches on different kinds of APIs, we convert source ﬁles of each repository into ASTs and extract all the function calls in them. We label a function call as a standard API or popular third-party API if it matches one of the APIs collected in Sec. 3.1. We label a function call as a user-deﬁned API if its implementation can be found in the current repository via import analysis. The average number of APIs per function, number of standard APIs and number of user-deﬁned APIs are shown in column 6 ∼ 8 of Table 1. In this section, we describe the details of each approach involved in the benchmark and the metrics for evaluation. Query reformulation techniques. We choose four popular query reformulation techniques, including Google Prediction Service [30], NLPAUG [52], SEQUER [12], and NLP2API [76]. The detailed description of each technique is illustrated in Table 3. Google prediction service is included as one of the most effective approaches in practice, while SEQUER [13] is the state-of-the-art approach. NLPAUG [52] is considered since it is widely used for query reformulation in many NLP studies [43], [60], [74], [101]. We also include NLP2API [77] since it differs from major reformulation methods by ﬁrst predicting the API class related to the query and then adding the predicted API class into the query. Query-based API recommendation approaches. We choose ﬁve query-based API recommendation approaches published by recent top conferences, including KGAPISumm [49], BIKER [37], RACK [78], and DeepAPI [31], along with a popular search library Lucene [23]. The detailed description of each baseline is shown in Table 3. We reproduce the ﬁve approaches based on the replication packages released by the authors. Besides, we build a naive baseline that recommends APIs by computing the similarities between queries and API descriptions based on BERTOverﬂow [46]. The native baseline serves as an indicator of the basic performance of similarity-based models. We also notice that different sources are adopted by the approaches for creating the knowledge base. For example, the naive baseline and DeepAPI only consider ofﬁcial documentation, while BIKER and RACK also involve the Q&A forum – Stack Overﬂow. We list the knowledge source of each approach in Table 3. During implementation, we do not align the sources of the approaches, since the sources are claimed as contributions in the original papers. Instead, we design a separate RQ to study the impact of knowledge sources on the performance of API recommendations. During studying the impact of query reformulation on the recommendation performance, we implement all the four query reformulation techniques for each of the six API recommendation baselines because all the baselines do not integrate query reformulation techniques in the original papers. Code-based API recommendation approaches. We choose four IDEs and ﬁve approaches published on recent top conferences as our code-based API recommendation baselines. A detailed description of each baseline is shown in Table 4. For the IDEs and some of the approaches such as TravTrans [44] and Deep3 [80], they can predict any code tokens besides API tokens. In the paper, we focus on evaluating their performance in recommending APIs, following prior research [27], [32], [44], [64], [82] we use the training set of APIBENCH-C to train each of the approaches in academia for a fair comparison. PAM [27] is the only context-intensive approach, primarily designed for intra-project API pattern mining. In the paper, we also extend the approach to cross-project recommendation by selecting the best API from projects in the training set for each test case. The extended version of PAM is named as PAM-MAX, which indicates the theoretical maximum performance the context-insensitive approach can achieve. Evaluation metrics. Since both query-based and codebased API recommendation baselines output a ranked list of candidate APIs, we adopt the commonly-used metrics in recommendation tasks for evaluation. Table 5 shows the details of each metric. The Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), and Normalized Discounted Cumulative Gain (NDCG) metrics are widely adopted by previous API recommendation studies [37]. In this study, we also involve a new metric Success Rate. The Success Rate@k is deﬁned to evaluate the ability of an approach in recommending correct APIs based on the top-k returned results regardless of the orders. To determine the relevance score in NDCG calculation, We use a relevance score of 1 if an approach hits the correct API class, and a relevance score of 2 if the correct API method is hit. Therefore, we can align the performance of class-level and method-level approaches. In this section, we study the RQ 1-3 discussed in Sec. 1 and provide the potential ﬁndings concluded from the empirical experiments. Since currently no query-based API recommendation approach is specially designed for Python APIs, we focus on studying query-based API recommendation approaches for Java. 4.1 Effectiveness of Query-Based API Recommendation Approaches (RQ1-1) To answer RQ1, we evaluate the six query-based API recommendation baselines listed in Table 3 by using the original queries in our benchmark APIBENCH-Q. The evaluation results are illustrated in Table 6. Class-level v.s. Method-level. Regarding the class-level recommendation, as shown in Table 6, we can ﬁnd that BIKER achieves the highest Success Rate, e.g., 0.67 for Success Rate@10, indicating that BIKER is more effective in ﬁnding the correct API class in the top-10 returned results for 60%∼70% of cases. Unsurprisingly, the naive baseline shows the worst performance for all the metrics. Even so, the naive baseline can successfully predict the correct API class for around 20% of cases. However, with respect to the method-level recommendation, all the approaches show obvious declines. For example, the Success Rate@10 of BIKER is only 0.37, decreasing by 44.8% compared to the class-level recommendation. The Success Rates@10 of DeepAPI and Lucene are only around 0.10, which is far from the requirement of practical development. On average, the approaches fail to give the exact methods for 57.8% APIs that they give the correct classes in top-10 returned recommendations. Thus, recommending method-level APIs still remains a great challenge. Finding 1: Existing approaches fail to predict 57.8% methodlevel APIs that could be successfully predicted at the class level. The performance achieved by the approaches is far from the requirement of practical usage. Accurately recommending the method-level APIs still remains a great challenge. Retrieval-based methods v.s. Learning-based methods. By comparing learning-based methods, such as DeepAPI and naive baseline, with the other retrieval-based methods, we can observe that learning-based methods achieve relatively lower performance regarding the Success Rate@10 metric. For example, on average, retrieval-based methods can accurately predict 46.8% class-level and 25.5% methodlevel APIs among all the cases in the top-10 returned results, respectively, while learning-based methods can only successfully recommend 25.5% class-level and 8% methodlevel APIs. A possible reason may be the insufﬁcient training data for the learning-based methods in this task domain. Since there are more than 30,000 APIs from the ofﬁcial documentation, learning-based methods require a large number of query-API pairs for training. However, even the largest Q&A forum, Stack Overﬂow, contains only about 150,000 posts after our pre-processing, which is not enough for model training. Finding 2: Learning-based methods do not necessarily outperform retrieval-based methods in recommending more correct APIs. The insufﬁcient query-API pairs for training limit the performance of learning-based methods. Performance in API ranking. From Table 6, we ﬁnd that there exist obvious gaps between the scores of Success Rate@k and the metrics for evaluating API ranking, such as MAP@k and NDCG@k. For example, RACK achieves Success Rate@10 score at 0.41, but its MAP@10 score is only 0.24. This indicates that although the approaches are able to ﬁnd the correct APIs, they cannot well rank them ahead in the returned results. The low MRR scores, e.g., 0.11 ∼ 0.44 for class-level API recommendation and 0.03 ∼ 0.19 for method-level API recommendation, and NDCG scores also show the poor ranking performance of the approaches. The results manifest that API ranking is still challenging for current approaches. Finding 3: Current approaches cannot well rank the correct APIs, considering the huge gap between the scores of Success Rate and the other ranking metrics. To sum up, accurately recommending method-level APIs and ranking candidate APIs still remain great challenges. Besides, the insufﬁcient data for training hinder the performance of current learning-based approaches. 4.2 Effectiveness of Query Reformulation Techniques (RQ2) Original queries can be short in length or contain vague terms. Query reformulation aims at changing original queries for facilitating downstream tasks. In this RQ, we explore the impact of query reformulation on the performance of query-based API recommendation. We implement the four query reformulation techniques, as listed in Table 3, for the original queries. We name the queries reformulated by query expansion techniques and query modiﬁcation techniques as expanded queries and modiﬁed queries, respectively. For each original query, we conduct the reformulation 10 times, producing 10 expanded or modiﬁed queries, with the statistics shown in Table 2. Note that NLPAUG [52] is a comprehensive data augmentation library for general NLP tasks. We choose the popular word-level insertion and substitution methods designed for manipulating single sentences based on ﬁve models, including BERTOverﬂow [46], Google News Word2vec [29], Stack Overﬂow Word2vec [93], WordNet [57], and Random model, in the library to generate expanded and modiﬁed queries. The queries output by the query reformulation techniques are not ranked in order, and may impact the downstream API recommendation performance variously. To explore the maximum potential effect brought by query reformulation techniques, we evaluate the API recommendation approaches on each reformulated query and choose the best result for analysis. For example, SEQUER [12] generates 10 queries for the original query “Get min value between two double type values”. We choose the one achieving the best performance for analysis, i.e., using the reformulated query “Get min value between two double type values in java” for the recommendation. We study the impact of query reformulation on API recommendation from the following two aspects: 1) whether query reformulation techniques can help predict more correct APIs; 2) whether query reformulation can improve the API ranking performance. 4.2.1 Inﬂuence on predicting more correct APIs With query reformulation v.s. Without query reformulation. The Success Rate metric reﬂects the proportion of the APIs an approach can correctly predict. The results of implementing the reformulation techniques on API recommendation approaches are illustrated in Figure 4 (classlevel) and Figure 5 (method-level). From the ﬁgures, we observe that query reformulation can increase the performance of API recommendation in most cases. Only for a few cases, the performance drops, which can be attributed to the inefﬁciency of some query reformulation techniques. For example, NLPAUG (WordNet) and NLPAUG (Random) tend to poorly modify the original queries for recommendation, as shown in Figure 4 (b) and Figure 5 (b). Overall, on average the process improves the class-level and methodlevel recommendation by 0.11 and 0.08, which is a boost of 27.7% and 49.2% compared with the basic performance on original queries. Finding 4: Query reformulation techniques, including query expansion and query modiﬁcation, are quite effective in helping query-based API recommendation approaches give the correct API by adding an average boost of 27.7% and 49.2% on classlevel and method-level recommendations. Query expansion v.s. Query modiﬁcation. By comparing the class-level and method-level recommendation results of query expansion and query modiﬁcation in Figure 4 and Figure 5, respectively, We observe that all the query expansion techniques improve the API recommendation performance, but not all the query modiﬁcation techniques beneﬁt the recommendation. For example, NLPAUG (WordNet) and NLPAUG (Random) generally decrease the performance of current approaches both in class-level and method-level recommendations. This indicates that query expansion techniques bring more stable improvement than query modiﬁcation techniques. Furthermore, on average, query expansion techniques improve the performance by 0.13 and 0.10 on class-level and method-level recommendation, which is much higher than the improvement of 0.09 and 0.06 achieved by query modiﬁcation techniques. This also suggests that query expansion techniques are more effective than query modiﬁcation techniques. Finding 5: Query expansion is more stable and effective to help current query-based API recommendation approaches give correct APIs than query modiﬁcation. Comparing different query expansion techniques. As shown in Figure 4 (a) and Figure 5 (b), NLP2API and NLPAUG (BERT) present the largest improvement on the performance of query-based API approaches at both class level and method level. For analyzing the improvement, we use two examples to illustrate the query expansion results of NLP2API and NLPAUG (BERT), respectively. In both examples, the most effective approach BIKER fails to predict the API based on the original queries but succeeds given the reformulated queries. In the ﬁrst example, NLP2API expands the query by adding a predicted API class DocumentBuilderFactory that is related to the original query. With such an explicit hint, the recommendation approach can narrow down the search scope and pinpoint the requested API method. In the second example, the query is looking for the API java.lang.StringBuilder.reverse(), whose description in ofﬁcial documentation is “Causes this character sequence to be replaced by the reverse of the sequence”. NLPAUG (BERT) adds a relevant word character to enrich the semantics of the original query. Comparing NLPAUG (W2V) with NLPAUG (BERT) and NLP2API in Figure 4 and Figure 5, we ﬁnd that the NLPAUG (W2V) is much less effective. To obtain a possible reason for such a difference, we give the third example below. As shown in the third example, we ﬁnd that NLPAUG (W2V) adds two irrelevant words into the original query, which negatively impacts the prediction results of BIKER. This also indicates that contextual embeddings such as BERT are more effective than traditional word embeddings. Finding 6: Add predicted API class names or relevant words into original queries are effective query expansion methods for improving the performance of API recommendation. Comparing different query modiﬁcation techniques. Among all query modiﬁcation techniques, NLPAUG (BERT) presents the biggest improvement on all the baselines at both class level and method level. Example 4 illustrates how NLPAUG (BERT) modiﬁes words in the original query. In the example, the original query asks about ways to calculate the time difference between two dates and the correct API is java.time.Period.between(). The description of the API in its ofﬁcial documentation is “obtains a period consisting of the number of years, months, and days between two dates”. However, the word “difference” used in the original query does not clearly describe the functional request. NLPAUG (BERT) modiﬁes the word into “months” which exactly appears in the ofﬁcial description. Based on the modiﬁcations, the correct API is recommended. From the second and fourth examples above, we ﬁnd that BERT-based models show great performance on both query expansion and query modiﬁcation to help improve the performance of current query-based API recommendation approaches. This indicates that even though the current data source limits the performance of them to directly predict the correct APIs, they can be used to improve the query quality as query reformulation techniques. Finding 7: BERT-based data augmentation shows superior performance in query modiﬁcation compared with other query modiﬁcation techniques. 4.2.2 Inﬂuence on the performance of API ranking In this section, we analyze the impact of query reformulation techniques on the performance of API ranking. Since the ideal case is that the correct APIs rank ﬁrst in the returned results, we use the metric NDCG@1 which considers both class-level and method-level recommendation performance. We compute the changes of NDCG@1 scores for the query-based API recommendation approaches before and after query reformulation. Besides, to focus our analysis on the performance of API ranking instead of the overall recommendation accuracy, the computation is performed only on the cases that are correctly predicted with and without query reformulation. The results are illustrated in Figure 6. As can be seen in Figure 6 (a), most query expansion techniques also improve the ranking results of the query-based recommendation approaches. Among all the query expansion techniques, SEQUER, NLPAUG (BERT), RACK and NLP2API can relatively better improve the ordering performance. The biggest improvement 0.14 (32% boost) is achieved by NLP2API on the Lucene approach. We also ﬁnd that on average query expansion also improves MRR by 0.09 (36% boost) and 0.08 (89% boost) on class-level and method-level recommendation, which indicates that the correct APIs are ranked much higher based on reformulated queries. According to Figure 6 (b), compared with query expansion techniques, query modiﬁcation techniques are much less effective in improving the API ranking performance. For example, the average improvement of NDCG@1 brought by query modiﬁcation is 0.01 (4% boost), which is 0.06 (14% boost) for query expansion techniques. Comparing different data augmentation methods, we also ﬁnd that WordNet and random methods tend to negatively impact the ranking results, leading to 24% and 14% drop in terms of NDCG@1, respectively. The results indicate that inappropriate query modiﬁcation will reduce the ranking performance of the query-based recommendation approaches. Finding 8: Expanding queries or modifying queries with appropriate data augmentation methods can improve the ranking performance of the query-based API recommendation techniques. To sum up, query reformulation, especially query expansion, can not only help current approaches recommend more correct APIs, but also improve the ranking performance. However, the reformulation step is generally ignored by current studies. Future work is suggested to involve such a step for more accurate API recommendation. 4.2.3 A special Query Modiﬁcation Method: Word Deletion In previous subsections, we compare and evaluate different query expansion and modiﬁcation techniques. They aim at enriching the original queries by adding, replacing or modifying some words without deleting words. In this section, we focus on studying the impact of word deletion, a special query modiﬁcation method, on the performance of query-based API recommendation approaches. Different from the previous query reformulation techniques which rely on external data sources, the word deletion method we studied does not leverage any extra knowledge. Our goal is to explore whether original queries contain meaningless or noisy words. Speciﬁcally, we randomly delete some words from the original query every time and produce ten different modiﬁed queries for one original query. The maximum and average Success Rate@10 scores based on modiﬁed queries are illustrated in Figure 7. As can be seen, the average performance of the query-based API recommendation approaches, denoted as the orange bar, decreases by 0.05 (13% drop) at class level and 0.03 (18% drop) at method level. The results are not surprising, and indicate that most words in the original queries are helpful for the recommendation. However, the maximum scores, denoted as the green bar, all show that word deletion improves the recommendation performance with an average boost of 38% and 64% for class level and method level, respectively. The improvement demonstrates that the original queries contain noisy words that can bias the recommendation results, although most of the words are useful for recommendation. To understand what kinds of words are noisy for the accurate recommendation, we manually check 545 out of 6,563 queries for which the recommendation approaches perform better after word deletion. We summarize the cases as below: 1) 349 (64%) queries contain unnecessary or meaningless words. In Example 5, the phrases “Standard way to” and “in java” are not beneﬁcial for pinpointing the correct API. Stop word removal also has a limited effect on eliminating these words. 2) 156 (29%) queries contain too detailed words for explanation. In Example 6, the phrase “like 255,0,0” is used to explain the “string”. However, such phrases never appear in the ofﬁcial documentation and the speciﬁc number adversely impacts the recommendation results. 3) 34 (6%) queries contain extreme long descriptions. In Example 7, the words after while actually describe nothing about the task. The long descriptions can decrease the weight of useful words in the queries thus confusing API recommendation approaches. Finding 9: Original queries raised by users usually contain noisy words which can bias the recommendation results, and query reformulation techniques should consider involving noisy word deletion for a more accurate recommendation. 4.3 Data Sources (RQ3) In RQ1-1, we highlight that insufﬁcient data greatly limits the performance of current learning-based methods. In this section, we conduct a deep analysis on the inﬂuence of different data sources on the recommendation results. From Table 3, we can observe that current approaches generally leverage three different data sources: ofﬁcial documentation, Q&A forums, and tutorial websites. For analysis, we choose two methods, Lucene and naive baseline, which are ﬂexible to incorporate different data sources. Speciﬁcally, we evaluate the methods on the part of queries from the tutorial websites collected in APIBENCH-Q, and the method training is conducted based on the following knowledge base: 1) only ofﬁcial documentation, 2) only Stack Overﬂow posts, and 3) both ofﬁcial documentation and Stack Overﬂow posts. The experiment results are shown in Figure 8. As can be seen, training on Stack Overﬂow posts achieves much better performance than on ofﬁcial documentation at both class and method levels. For example, Lucene achieves a 29% boost in class-level and an 169% boost in method-level recommendation when searching based on Stack Overﬂow than on ofﬁcial documentation; and the naive baseline even achieves a 71% boost in class-level and a 602% boost in method-level recommendation. The advantage of leveraging Stack Overﬂow posts may be attributed that the discussion on Stack Overﬂow is more natural and similar to user queries, compared with the descriptions in the ofﬁcial documentation. Besides, the extended usage of some APIs is rarely mentioned in ofﬁcial documentation but is widely discussed in Stack Overﬂow. An example is used to illustrate the inﬂuence of different data sources. In Example 8, the query asks about the API for generating an MD5 hash of a ﬁle. However, there is no standard API specially designed to generate the MD5 hash, so Lucene focuses on two words “hash” and “ﬁle” for recommendation. But the ofﬁcial description of ground truth API java.security.MessageDigest.digest() does not contain the word “ﬁle” since it is a general API that not only handles ﬁles. Under this circumstance, Lucene recommends a more relevant but wrong API java.nio.ﬁle.attribute.FileTime.hashCode(). When involving Stack overﬂow posts, as there already exists discussion on how to generate the MD5 hash, Lucene can easily pinpoint and recommend the API in the posts. The advantage of leveraging Stack Overﬂow for recommendation is also demonstrated by the BIKER approach [37], which is the most effective approach in Section 4.1. Our ﬁnding is consistent with the claim in the work [37] that Stack Overﬂow posts can mitigate the semantics gap between user queries and ofﬁcial descriptions. Finding 10: Apart from ofﬁcial documentation, using other data sources such as Stack Overﬂow can signiﬁcantly improve the performance of query-based API recommendation approaches. In this section, we study the RQ1 and RQ 4 ∼ 6 discussed in Sec 1. To study RQ1, RQ4 and RQ5, we evaluate the performance of all the code-based API recommendation approaches on the “General” domain of our benchmark APIBENCH-C, as shown in Table 1, since the “General” domain includes code with different topics and can reﬂect the overall performance of baselines. For studying the ability of cross-domain adaptation in RQ6, we evaluate the performance of the approaches on all the ﬁve domains of our APIBENCH-C. 5.1 Effectiveness of Existing Approaches (RQ1-2) According to Table 4, three approaches for Python and three approaches for Java are evaluated on the “General” domain of APIBENCH-C. The results are depicted in Table 7. We can observe that the learning-based method TravTrans obtains the best performance on the Python dataset, achieving 0.62 and 0.54 for Success Rate@10 and NDCG@10, respectively. The results mean that TravTrans can successfully recommend 62% of APIs in our benchmark and well predict the API rankings. However, the traditional statistical method Deep3 only achieves 0.43 and 0.32 for Success Rate@10 and NDCG@10, respectively, while the pattern-based method FOCUS and PAM achieve less than 0.10 for both Success Rate@10 and NDCG@10. This suggests that learning-based methods obtain superior performance in code-based API recommendation, which is quite different from query-based API recommendation. The possible reason is that lots of well-organized public code repositories provide sufﬁcient data for training code-based API recommendation models. We also ﬁnd that FOCUS and PAM show low recommendation accuracy, with all the metric values lower than 0.1. The low performance is attributed to the context representation of the approaches. PAM is a context-insensitive approach, which only mines the top-N APIs that are most likely to be used in the training set and directly recommends them for each ﬁle in the test set; while FOCUS takes one step further by extracting the APIs in the test set and building a matrix to match the APIs in the training set. Such coarse-grained context representation or context-insensitive representation do not well capture the relations between APIs. PAM-MAX shows the theoretical best performance context-insensitive methods can achieve. However, the performance of PAM-MAX is still lower than TravTrans and PyART which considers ﬁne-grained code features such as code tokens and data ﬂows. The results indicate the effectiveness of ﬁne-grained approaches for code-based API recommendation. Besides the recent code-based API recommendation approaches, we also compare the widely-used IDEs. Since it is hard to automatically evaluate IDEs’ recommendation performance, we sampled 500 APIs from the original large test set of APIBENCH-C based on the distribution shown in Table 1. We then conduct a manual evaluation by imitating the behaviors of developers on the 500 sampled APIs. We show the results on the sampled test set in Table 7. As can be seen, for Python, Pycharm achieves the Success rate@10 at 0.49 and NDCG@10 at 0.40, which is truly competitive to the performance of TravTrans, with Success Rate@10 and NDCG@10 at 0.50 and 0.44, respectively. For Java, IDEs also show competitive performance compared with the baseline approaches. The results demonstrate that the widely-used IDEs are generally effective in API recommendation and far from relying on alphabet orders for recommendation. Finding 11: DL models such as TravTrans show superior performance on code-based API recommendation by achieving a Success Rate@10 of 0.62, while widely-used IDEs also obtain satisfying performance by achieving a Success Rate@10 of 0.5 ∼ 0.6. 5.2 Capability to Recommend Different Kinds of APIs (RQ4) Exploring which kinds of APIs tend to be wrongly predicted is essential for understanding the bottleneck of current approaches and providing clues for further improvement. In Section 3, we have classiﬁed all APIs into standard APIs, popular third-party APIs and user-deﬁned APIs. In this RQ, we study the performance of current baselines for different kinds of APIs. Speciﬁcally, we evaluate TravTrans, Deep3, FOCUS, PAM and PAM-MAX on the full test set of the “General” domain, with results shown in Figure 9. Note that we do not involve IDEs in this RQ as they are evaluated on the sampled test set. As can be seen in Figure 9, all the approaches achieve a very high Success Rate@10 on standard APIs. For example, TravTrans even successfully recommends more than 90% of standard APIs in the test set. The approaches also present relatively good performance for the popular third-party libraries, e.g., TravTrans achieves a Success Rate@10 of more than 0.8. As standard APIs and popular APIs from thirdparty libraries are widely used in real-world projects, datadriven methods can achieve superior performance. However, the approaches are hard to correctly recommend the user-deﬁned APIs and fail to predict 35.3% ∼ 91.3% more of user-deﬁned APIs comparing to the prediction of standard APIs. Finding 12: Although current approaches achieve good performance on recommending standard and popular third-party libraries, they face the challenges of correctly predicting the userdeﬁned APIs. 5.3 Capability to Handle Different Contexts (RQ5) As context representation is an important part of the current code-based API recommendation shown in Figure 2, it is worthwhile to study the impact of different contexts on the performance of current approaches. In this RQ, we explore the impact of the following two different types of context. of current approaches to handle different lengths of contexts; ommendation points affect how much context an approach can be aware of before recommendation; Capability to handle different lengths of functions. In Section 3 and Table 1 we classiﬁed all functions of APIBENCH-C into extremely short functions, functions of moderate lengths, or extremely long functions by sampling the ﬁrst 5%, middle 90% and last 5% according to the distribution of function lengths. As code-based API recommendation is often based on the context in a function, the length of function can represent the length of context that an approach needs to handle. We study the performance of current baselines on functions of different lengths and show the results of TravTrans, Deep3, FOCUS, PAM, and PAMMAX in Figure 10. From Figure 10, we ﬁnd that most baselines share similar performance distributions on functions of different lengths. They present the best performance on functions with moderate lengths and suffer from performance drops on extremely long or short functions. To be more speciﬁc, the performance drops by 7.1% for extremely long functions and 10.6% for extremely short functions on average. The results indicate that context length can affect the performance of current approaches. Besides, the approaches are more difﬁcult to recommend correct APIs for the functions of extremely short lengths than those of extremely long lengths. Finding 13: Context length can impact the performance of current approaches in API recommendation. The approaches perform poorly for the functions with extremely short or long lengths, and accurate recommendation for the extremely short functions is more challenging. Capability to handle different recommendation points. Similar to the previous work [64], we ﬁrst deﬁne three locations of recommendation points. Suppose that the LOC of a function is n and the total number of APIs used in the function is m. we deﬁne a recommendation point that is on the aline of the function and is the bAPI in the function locates on 1) the front of function if a/n < 1/4 and b/m < 1/4, or 2) the middle of function if 1/4 < a/n < 3/4 and 1/4 < b/m < 3/4, or 3) the back of function if a/n > 3/4 and b/m > 3/4. For all the APIs in the test set of the “General” domain, we replace them with placeholders of the above three types of recommendation points for evaluation. We also remove APIs in extremely long or short functions (according to the thresholds shown in Table 1) to alleviate the inﬂuence of function lengths. We show the results of TravTrans, Deep3, FOCUS, PAM and PAM-MAX in Figure 11. From Figure 11, we observe that most approaches obtain the best performance on the back recommendation points and acquire the worst performance on the front recommendation points. To be more speciﬁc, on average ﬁve approaches achieve 0.316, 0.347 and 0.348 for Success Rate@10 at the front, middle and back recommendation points, respectively. The results demonstrate that the current approaches tend to be less effective on the front recommendation points, which is reasonable since the context information is much limited before the front recommendation points. Finding 14: The location of recommendation points can affect the performance of current approaches. Current approaches are more effective in handling back recommendation points than front recommendation points. To sum up, different contexts can affect the performance of current code-based API recommendation approaches. Among them, the extremely short contexts and front recommendation points bring the most challenges for the accurate recommendation. 5.4 Adaptation to Cross-Domain Projects (RQ6) We have divided APIBENCH-C into ﬁve different domains in Section 3. In this section, we aim at studying the adaption capability of current approaches for cross-domain projects. We train the approaches in one domain and evaluate them in other different domains. We choose the approaches TravTrans, Deep3, and PyART, which are all designed for Python, for analysis. We do not involve the approaches FOCUS, PAM, or PAM-MAX, since they use coarse-grained context representations or context-insensitive feature, and are difﬁcult to incorporate project-speciﬁc information. The ﬁrst four rows of Table 9 list the cross-domain Success Rate@10 of TravTrans, Deep3 and PyART, respectively. According to Table 9, the approaches trained on one domain generally perform best on the test set of the same domain. For example, when trained on data from the “Security” domain, TravTrans, Deep3 and PyART obtain the best scores at 0.54, 0.51 and 0.48 on the test set of the same domain, respectively, in terms of Success Rate@10. However, their performance drops by 2.1% ∼ 43.1% when recommending APIs from different domains. Finding 15: Current approaches using ﬁne-grained context representation are sensitive to the domain of the training data and suffer from performance drop when recommending crossdomain APIs. We also analyze the cross-domain performance of the approaches when training on multiple domains instead of on one single domain. Such analysis is worthwhile to explore whether different domains can complement each other. Then we train the approaches on the projects from the “General” domain of APIBENCH-C and evaluate them on the other four different domains. We show the results in the last row of table 9. From the table, we can see that the approaches trained on the “General” domain generally show the best performance when evaluating on different domains. For example, TravTrans trained on the “General” domain achieves the Success Rate@10 of 0.72, 0.76, 0.78 and 0.74 on ML, Security, Web and DL domains, respectively, which is signiﬁcantly higher than the corresponding best scores obtained by TravTrans trained on single one domain. We observe an average boost of 14% for the performance of the approaches when trained on multiple domains than on a single domain. The results indicate that training approaches on multiple domains greatly improve the recommendation performance. Finding 16: Training on multiple domains helps the current approaches to recommend APIs in different single domains, and the performance is generally better than only training on a single domain. 6.1 Query Reformulation for Query-based API Recommendation In Section 4.2, we ﬁnd that query reformulation techniques can not only help current query-based API recommendation approaches ﬁnd more correct APIs but also improve the ranking performance. Based on query reformulation, BIKER can even achieve a Success Rate@10 of 0.80 in class-level and 0.51 in method-level API recommendation. The results demonstrate that query quality has a great impact on the recommendation results and suggest that query reformulation should become a common pre-processing technique used before query-based API recommendation. We also discover that some query reformulation techniques, such as adding predicted API class names or relevant words, can improve the performance of query-based API recommendation approaches. However, to the best of our knowledge, few studies have considered integrating these techniques, which could be one major reason that current approaches achieve limited performance. By implementing a random deletion strategy, in Section 4.2.3 we ﬁnd that user queries usually contain noisy words, which can bias the recommendation results. We summarize three kinds of cases in which a query contains noisy words. However, there exists very little work that aims to detect and eliminate the irrelevant words for recommendation systems, which poses a great challenge for current approaches to be robust when handling various user queries. Although a random deletion strategy reduces the overall performance on average, the positive improvement of deletion on some speciﬁc words indicates the potential beneﬁts of noisy word deletion. Implication 1: Current query-based API recommendation approaches should be integrated with query reformulation techniques to be more effective. 6.2 Data Sources for Query-based API Recommendation In Section 4.1, we point out that current query-based API recommendation approaches face the problem of building a comprehensive knowledge base due to the lack of enough data such as query-API pairs. In Section 4.3, we further discover that there is a semantic gap between user queries and descriptions from the ofﬁcial documentation. Both the lack of enough data for knowledge base creation and the semantic gap increase the difﬁculty of accurate API recommendation based on only ofﬁcial documentation. Such challenges can not be easily solved by improving learningbased models or pattern-based models. One effective way to mitigate the difﬁculty is to involve Stack Overﬂow posts, as analyzed in Section 4.3. While Stack Overﬂow is only one type of data source, our analysis demonstrates that adding appropriate data sources can improve the performance of query-based API recommendation approaches. Implication 2: Apart from query reformulation, adding appropriate data sources provides another solution to bridge the gap between queries and APIs. 6.3 Low Resource Setting in Query-based API Recommendation In Section 4.1, we ﬁnd that current learning-based methods do not necessarily outperform traditional retrieval-based methods. We attribute the results to the limited data such as query-API pairs in the query-based API recommendation task, which is a low-resource scenario [16], [34]. We also discover that pre-trained models such as BERT show superior performance in query reformulation in Section 4.2. This indicates that current pre-trained models can implicitly mitigate the semantic gap between user queries and ofﬁcial descriptions of APIs. Future work is suggested to explore how to make the best use of pre-trained models for querybased API recommendation based on limited available data. Implication 3: Few-shot learning with powerful pre-trained models can be a solution to further improve the performance of query-based API recommendation. 6.4 User-deﬁned APIs In Section 5.2, we ﬁnd that current code-based API recommendation approaches, no matter pattern-based or learningbased models, all face the challenge of recommending userdeﬁned APIs. User-deﬁned APIs have become the major bottleneck to further improve the performance of current code-based API recommendation approaches. However, as user-deﬁned APIs usually do not appear in the training set, they can hardly be learned by machine learning methods or be mined by pattern-based methods. A possible solution used by current approaches [37], [45] is to regard the API as a code token and predict the token based on previous contexts. However, this solution also fails if the API token never appears in previous context. Thus, accurately predicting user-deﬁned APIs should be one major direction of code-based API recommendation in future work. Implication 4: User-deﬁned API recommendation is one major bottleneck for improving the performance of current code-based API recommendation approaches and remains unsolved. 6.5 Query-based API Recommendation with Usage Patterns In this paper, we only focus on testing whether an approach can recommend the correct APIs, but we believe developers can always beneﬁt more from detailed information about how to use the recommended APIs. A common method is to provide summaries such as the signature and constraints extracted from ofﬁcial documentation along with the recommended APIs. For example, KG-APISumm proposed by Liu et al. [49] provides a detailed summary of the recommended API class. However, ofﬁcial documentation sometimes cannot provide enough usage information about an API, which may cause API misuse. For instance, a fresh developer may search “how to read a ﬁle” in Python and the recommended API should be ﬁleObject.read(), but without sufﬁcient experience to use ﬁle operations, the developers may forget to close the ﬁle after reading it. A possible solution to complement ofﬁcial documentation and avoid possible misuse is to provide usage patterns from other developers. In the above example, a common usage pattern open(), ﬁleObject.read(), ﬁleObject.close() can prevent dangerous ﬁle operations. As there exist some pattern mining approaches on code, we can combine querybased API recommendation with code-based pattern mining methods for better providing the usage pattern. Implication 5: Code-based API recommendation approaches can provide usage patterns to enrich the results returned by query-based API recommendation approaches. In this section, we describe the possible threats we may face in this study and discuss how we mitigate them. 7.1 Internal Validity Our research may face the following internal threats: Baseline Re-implementation. In this paper, we reimplemented several baselines according to the code or replication packages released by their authors. However, as some baselines are not primarily designed for API recommendation, we slightly modiﬁed their code and adapted them into our task and our benchmark. For example, we limit the prediction scope of code completion baselines to only API tokens. Such adaptations may cause the performance of baselines to be slightly different from the original papers. To mitigate this threat and validate the correctness of our re-implementation, we refer to some related work that cites these baselines and conﬁrm our experiment results with them. Data Format. In this paper, we align the performance of all code-based API recommendation baselines with a uniform evaluation process. However, different approaches may require different input formats and different preprocessing methods. For example, TravTrans requires the inputs of ASTs so we need to transform the code in APIBENCH into ASTs. To mitigate the possible impacts brought by such transformations, we make sure that except for the original source code we do not involve extra information and knowledge in this modiﬁcation process. Data Quality. We build APIBENCH-Q by manually selecting and labeling API-related queries from Stack Overﬂow and some tutorial websites. This process involved some human checks so that some subjective factors may inﬂuence the quality of our dataset. To mitigate this threat, we involve at least two persons to label one case and let one of our authors further check if the previous two persons give different opinions to the case. We also implement some rules to automatically ﬁlter out the cases that are explicitly unrelated to API recommendation. 7.2 External Validity Our research may face the following external threats: Data Selection. To the best of our knowledge, APIBENCH is the largest benchmark in API recommendation task. We try to make it more representative by selecting real-world code repositories from the most popular domains at GitHub and real-world queries from the largest Q&A forum StackOverﬂow according to several developer surveys [39], [89]. All ﬁndings in this empirical study are based on this dataset. However, there may still be slight differences when adapting our ﬁndings into other domains and datasets that we do not discuss in this paper. Programming Language. Our study focuses on the API recommendation on Python and Java, the ﬁndings included in this study may not be generalized to other programming languages that have different API call patterns from Python and Java. However, we believe the impacts of programming languages should not be signiﬁcant as Python and Java are the most representative dynamically typed and statically typed languages, respectively, while most programming languages can be classiﬁed into these two categories. In this section, we list the work related to query reformulation, API recommendation and API-related empirical study, respectively. The effectiveness of query-based approaches highly depends on the input natural language queries, which motivates many previous query reformulation works. Query expansion and query modiﬁcation are two major reformulation methods, one of which adds several missing words and phrases which are relevant to the queries, the other one replaces or modiﬁes words and phrases. Existing works show that even some minor textual changes inﬂuence the ﬁnal results a lot. To better evaluate the API recommendation methods, we utilize multiple existing methods to reformulate the original queries. Rahman et al. propose ACER [4]. It takes the initial query as the input and identiﬁes appropriate search terms from source code by using a novel term weight named CodeRank, and then it suggests the best reformulation of the original query by using document structures, query quality analysis and machine learning techniques. Rahman et al. propose NLP2API [76], which automatically identiﬁes the relevant API classes for a given query and then uses these API classes to reformulate the original query. Lu et al. propose to expand the queries by using synonyms. They extract the natural phrases from code identiﬁers, match the expanded queries with the identiﬁers, and then sort the methods in the code base by the matched identiﬁer [51]. Sirres et al. present COCABU [88] to resolve the vocabulary mismatch problem. Their approach uses common developer questions and expert answers to augment the user queries and improves the relevance of returned code examples. Nie et al. propose QECK [66] to solve the term mismatch problem. QECK retrieves relevant question-answer pairs collected from Stack Overﬂow as the Pseudo Relevance Feedback (PRF) documents and identiﬁes the speciﬁc words from the documents to expand the original query. There are also some query reformulation techniques in industry or open-source communities. For example, the Google search engine utilizes its own prediction service [30] to reformulate the natural language queries provided by users. The open-source library NLPAUG [52] provides NLP utilities to conduct data augmentation. 8.2 API Recommendation 8.2.1 Query Based API Recommendation Multiple existing works [15], [31], [37], [48], [49], [54], [58], [73], [78], [90], [91], [99], [100], [102], [103], [104], [109], [109] explore the possibility to provide developers with concrete API recommendation, using the natural language queries as input. Most of these works utilize open-source code bases, and some also use the knowledge in crowd-sourcing forums and wiki websites for augmentation. Retrieval-based methods. Portfolio [54], proposed by McMillan et al. , recommends relevant APIs by utilizing several NLP techniques and indexing approaches with spreading activation network (SAN) algorithms as well as PageRank [10]. Zhang et al. supplement the call graph with control ﬂow analysis and design Flow-Augmented Call Graph (FACG) to utilize it for API recommendation [103]. Chan et al. model API invocations as API graphs and design subgraph search algorithm to recommend APIs [15]. Rahman et al. collect the crowdsourced knowledge on Stack Overﬂow to extract keyword-API correlations and ﬁnd several relevant API classes based on them [78]. Huang et al. propose BIKER [37] to bridge the lexical gap and knowledge gap that previous approaches faced during API recommendation. BIKER obtains API candidates from Stack Overﬂow, and uses the similarity between queries and documentations as well as Stack Overﬂow posts to recommend API methods. Liu et al. propose KG-APISumm [49], which is the ﬁrst knowledge graph designed for API recommendation. KG-APISumm sorts the APIs through similarity calculation between queries and relevant parts of the constructed knowledge graph to recommend API classes. Other than coding problems encountered by developers, there is another source of natural language functionality descriptions for APIs: feature requests from product managers or users. Thung et al. propose a method to recommend APIs based on the feature requests by learning from other modiﬁcations of the projects [91]. Xu et al. propose MULAPI [100], which takes feature locations, project repositories and API libraries into consideration when recommending APIs. Learning-based methods. DeepAPI [31], proposed by Gu et al. , is the ﬁrst approach that combines deep learning with API recommendation. It reformulates the API recommendation task as a query-API translation problem and uses an RNN Encoder-Decoder model to recommend API sequences. Xiong et al. propose to use representation learning to recommend web-based smart service [99]. Ling et al. propose GeAPI [48] based on graph embedding to provide more semantic information about and between APIs. GeAPI utilizes project’s source code to automatically construct API graphs and leverages graph embedding techniques for API representation. Given a query, it searches relevant subgraphs on the original graph and recommends them to developers. Zhou et al. propose BRAID [109] and utilize approaches such as active learning as well as learning-torank based on the feedback of users to further improve the performance. 8.2.2 Code Based API Recommendation Pattern-based methods. Zhong et al. propose MAPO [108] to mine API usage patterns and then recommends the relevant usage patterns to developers. Sch¨afer et al. propose Pythia [87] to utilize static pointer analysis and usage-based property inference to recommend APIs for JavaScript. Wang et al. propose UP-Miner [96] and use source code to extract succinct usage patterns to recommend APIs. Nguyen et al. propose APIREC [61], which uses ﬁne-grained code changes and the corresponding changing contexts to recommend APIs. D’Souza et al. propose PyReco [18]. It ﬁrst extracts API usages from open-source projects and uses such information to rank the API recommendation results by utilizing nearest neighbor classiﬁer techniques. Fowkes et al. propose PAM [25] to tackle the problem that the recommended API lists are large and hard to understand. PAM mines API usage patterns through an almost parameter-free probabilistic algorithm and uses them to recommend APIs. Niu et al. propose another API usage pattern mining approach, which segments the data using the co-existence relationship of object usages to mine API usage patterns [67]. Liu et al. propose RecRank [50] to improve the top-1 accuracy based on API usage paths. Nguyen et al. propose FOCUS [64], which mines open-source repositories and analyzes API usages in similar projects to recommend APIs and API usage patterns based on context-aware collaborative-ﬁltering techniques. Wen et al. propose FeaRS [97], which mines open-source repositories and extracts API sequences that are implemented together in the same tasks frequently to recommend APIs. Learning-based methods. Hindle et al. adopt the n-gram model, a widely-used statistical language model, on the code of software [35], and develop a code suggestion tool based on the n-gram model. Tu et al. propose to enhance the n-gram model by adding a cache component [92]. Raychev et al. propose to extract API sequences from open-source projects and index them into statistical language models to recommend APIs [82]. Nguyen et al. propose a graphbased language model GraLan [62]. Based on GraLan, they design an AST-based language model named ASTLan to recommend APIs. Raychev et al. propose a probabilistic model with decision trees named TGEN [80] to predict code tokens. Several recent works try to utilize syntax and data ﬂow information for more accurate recommendation, besides focusing on token sequences [32], [44]. He et al. propose PyART [32], which utilizes a predictive model along with data-ﬂow, token similarity and token co-occurrence to recommend APIs. Kim et al. leverage Transformer-based techniques to learn the syntactic information from source code [44]. 8.3 Empirical Study on APIs There are several empirical studies focusing on different aspects of APIs. Zhong et al. conduct an empirical study on how to use diversiﬁed kinds of APIs [105]. Hora et al. study API evolution together with its inﬂuences on large software ecosystem [36]. Zhong et al. study API parameter rules [106]. Monperrus et al. conduct an empirical study on rules in API documentation [59]. Ajam et al. study API topic issues on Stack Overﬂow [7], and Linares V´asquez et al. focus on how API changes affect activities on Stack Overﬂow [95]. Moreover, other work with respect to crosslanguage API mapping relations [107], API migration [9], API usability [19], API deprecation [85], API learning obstacles [86], usage of speciﬁc types of APIs [5], [69], the correlation between APIs and software quality [94], as well as order of API usage patterns [14], are also explored. In this paper, we present an empirical study on the API recommendation task. We classify current work into querybased and code-based API recommendation, and build a benchmark named APIBENCH to align the performance of different recommendation approaches. We conclude some ﬁndings based on the empirical results of current approaches. For query-based API recommendation approaches, we ﬁnd that 1) recommending method-level APIs is still challenging; 2) query reformulation techniques have great potential to improve the quality of user queries thus they can help current approaches better recommend APIs. What’s more, user queries also contain some meaningless and verbose words and even a simple word deletion method can improve the performance; 3) approaches built upon different data sources have quite different performances. Q&A forums such as Stack Overﬂow can greatly help mitigate the gap between user queries and API descriptions. For code-based API recommendation, we emphasize the superior performance of current deep learning models such as Transformer. However, they still face the challenge of recommending user-deﬁned APIs. We also ﬁnd different contexts, such as different location of recommendation points and context length, can impact the performance of current approaches. Besides, current approaches suffer from recommending cross-domain APIs. Based on the ﬁndings, we summarize some future directions on improving the performance of API recommendation. For query-based approaches, we encourage to integrate query reformulation techniques with query-based API recommendation approaches to obtain better performance, but how to choose the best query reformulation strategy still remains as future work. We also believe some few-shot learning methods and different data sources can bridge the gap between user queries and knowledge base under low resource scenarios. For code-based approaches, we recommend future work to focus on improving the performance of user-deﬁned API recommendation and train the approach on multiple domains instead of a single domain. We released our benchmark APIBENCH and all experiment results at Github. We hope this empirical study can remove some barriers and motivate future research on API recommendation.