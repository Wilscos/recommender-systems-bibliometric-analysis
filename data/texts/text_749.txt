In order to support a variety of missions and deal with dierent ight environments, drone control programs typically provide congurable control parameters. However, such a exibility introduces vulnerabilities. One such vulnerability, referred to as range specication bugs, has been recently identied. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may aect the drone physical stability. In this paper we develop a novel learning-guided search system to nd such combinations, that we refer to as incorrect congurations. Our system applies metaheuristic search algorithms mutating congurations to detect the conguration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning predictor as the tness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter congurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet ecient detection of incorrect congurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect congurations, of which over 85% lead to actual unstable physical states. Drone security, conguration test, range specication bug, deep learning approximation ACM Reference Format: Ruidong Han, Chao Yang, Siqi Ma, JiangFeng Ma, Cong Sun, Juanru Li, and Elisa Bertino. 2021. Control Parameters Considered Harmful: Detecting Range Specication Bugs in Drone Conguration Modules via LearningGuided Search. In Proceedings of The 44th International Conference on Software Engineering (ICSE 2022). ACM, New York, NY, USA, 12 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn Drones – ying mini robots, are rapidly growing in popularity. They have become essential in supporting the central functions of various business sectors (e.g., motion picture lmmaking) and governmental organizations (e.g., surveillance). Because drones are pilotless and have small physical shape and fast speed, they are often used in missions, such as delivery and surveillance, targeting locations dicult or expensive to reach with other means. However, as drones will be increasingly used also for critical missions, it is important that they be reliable and adaptable to ensure that missions successfully complete. To achieve reliability and adaptability, enhanced ight control systems have been developed. Such systems provide large numbers of control parameters that can be congured to modify the ight states of drones, such as linear and angular positions. Through parameter adjustment, dierent congurations can be set and sent to the ight control program. Based on such a conguration, the ight control program controls the ight states of the drone to complete the ight mission. However, the possibility of adjusting parameters introduces certain vulnerabilities (referred to as range specication bugs) arising from the lack of adequate checking of the control parameter values. Specically, when setting some particular congurations by selecting parameter values within the ranges provided by the manufacturer, unstable ight states might be triggered, such as trajectory deviation or even drone crash. Existing vulnerability detection techniques cannot detect such range specication bugs. If the source of the control program is available, static program analysis can be used to detect data/control dependencies [22,23] and nd such bugs. However, such an approach works well only for small code snippets. If used on large and complex programs, static analysis would have scalability issues. To address scalability, taint analysis can be used by tracking input data ow [10,13,30], which depends more on input construction. However, when dealing with very large numbers of control parameters, each with a wide range of values, analyzing congurations by tainting all the parameter values is time consuming. A recently proposed tool, rvfuzzer [20], tries to address such an issue by generating conguration inputs through fuzzing. Even though rvfuzzer is able to reduce the total amount of congurations to be tested, it is still inecient and unable to provide high-coverage analyses of congurations. As a result, it misses incorrect congurations. Major challenges for detecting range specication bugs include how to validate congurations eectively and how to eciently search for correct parameter ranges. In this work, we address these challenges by developing a learning-guided fuzzing approach specically designed to detect range specication bugs. At a high level, our detection tool, LGDFuzzer, relies on a genetic algorithm (GA) [37] and a ight state predictor to detect the congurations that are potentially incorrect. Specically, LGDFuzzer is equipped with three core components, State Change Predictor, Learning-Guided Mutation Searcher, and Range Guideline Estimator. First, we manually collected a list of ight logs, each of which contains a ight state, sensor data, congurations, and a timestamp. By using such logs, LGDFuzzer trains a state predictor, which is utilized to estimate the state, referred to as reference state, that will be reached by the drone at the next timestamp. Simultaneously, LGDFuzzer runs the learning-guided mutation searcher to generate congurations by leveraging the GA. Unlike the traditional conguration validation schemes that test the conguration on either a ight simulator or a realistic drone, LGDFuzzer leverages the state predictor to estimate the reference state and infers whether a conguration is correct based on the deviation of the reference state with respect to the expected state. Finally, LGDFuzzer validates the congurations that are predicted as “incorrect” and further generates a valid range for each parameter. We used LGDFuzzer to analyze the most popular ight control program, ArduPilot [35]. In total, LGDFuzzer validated 46,500 congurations and labeled 2,319 as “incorrect”, out of which 2,036 incorrect congurations were conrmed. Apart from the identied range specication bugs, we also found 564 zero-day vulnerabilities, referred to as Incorrect Conguration Tackling bugs. These refer to congurations that are detected as incorrect before the drone takes o and, as a result, the ight is aborted. However, the control program will accept these incorrect congurations if they are sent to the control program after the drone has taken o. Our analysis also shows that a signicant number of incorrect congurations are set in order to enhance adaptability. To assist developers and users in building secure ight control programs, LGDFuzzer optimizes the parameter ranges based on a manually set adaptability level. If developers and users prefer higher adaptability, larger parameter ranges will be provided by LGDFuzzer; however the possibility of causing unstable ight would be higher. Otherwise, a smaller range will be set and the ight states of the drone will be more stable. Contributions. (1) We have designed and implemented LGDFuzzer to detect incorrect congurations eectively and eciently. Our system uses a GA to select the “highly possible” incorrect congurations and validates conguration correctness by using a deep learning based state predictor. (2) According to the requirements of reliability and adaptability by developers and users, we designed and implemented a range optimization component in LGDFuzzer to provide the most appropriate parameter value ranges to minimize the possibility of introducing incorrect congurations. (3) We applied LGDFuzzer to a real-world ight control program and identied 2,036 incorrect congurations causing unstable ight states. We also veried 106 incorrect congurations on real-world drones and conrmed that these incorrect congurations cause trajectory deviations or drone crashes. (4) We found a new type of bugs, Incorrect Conguration Tackling bugs, and veried that these bugs also cause unstable ight states. (5) We have open sourced our LGDFuzzer at https://github.com/ BlackJocker1995/uavga/tree/main; the site makes available the tool, dataset, and video recordings of our tests of incorrect congurations. In this section, we rst introduce some information about the ight control programs utilized by drones and the range specication bugs. We then discuss the challenges in the design of an approach to validate parameter congurations, followed by our solutions to address these challenges. During a ight, the ground control station (GCS) communicates with the drone by sending a series of commands to the ight control program. Before the drone takes o, users can congure the ight control program by adjusting the parameters to manipulate the ight states of the drone (e.g., linear position, angular position, angular speed, velocity, and acceleration). To ensure that a drone completes its ight mission successfully, the ight control program periodically observes the current positioned ight state and sensor data (e.g., from GPS, gyroscopes, and accelerometers) to estimate a reference state indicating the next state of the drone. Then the control program generates actuator signals (e.g., motor commands) to move the drone to the reference state. The positioned state and the reference state need to be close enough, i.e., within a standard deviation. If this is not the case, the drone ight may become unstable, leading to trajectory deviations and crashes. Although the value ranges for control parameters are typically hardcoded in the control programs and one would expect that all possible combinations of these values be correct, some of the combinations are actually incorrect. Any congurations triggering unstable ight states are regarded as incorrect. The corresponding control parameter ranges are range specication bugs [20]. Since hundreds of control parameters can be specied by the ight control program, identifying range specication bugs by validating all parameter values is time consuming because some parameters may not aect ight stability. Therefore, we focus on the parameters that might aect angular position and angular speed state, because they directly impact the ight attitude and their incorrect values are more likely to cause unstable states. By analyzing the physical impact on drones, we dene ve unstable ight states: Flight Freeze.A drone is required to keep moving unless the ight control program generates a signal to keep the drone stationary or to instruct the drone to land. However, incorrect congurations may lead the drone to accidentally freeze at a waypoint, when moving forward/backward, or to wander around a position within a minimum range. To determine whether a ight is frozen, we calculate the movement distance between the previous and the current positions within a time interval. If the distance is less than a threshold, the ight state is regarded a frozen, i.e.,“ight freeze”. Deviation.According to the actuator signals generated by the ight control program, the drone will be driven to achieve the reference state as close as possible. In practice, however, the positioned state may not always match the reference state, that is, the drone is deviating from the expected trajectory. When the deviation is small, the control system can generate an actuator signal based on the dierences between the current positioned state and the reference state. However, an incorrect conguration may trigger a signicant deviation. Such a deviation may lead to an erroneous trajectory from which the drone cannot return back to the correct trajectory. If the deviation exceeds a given threshold, we consider the ight state as unstable, i.e., a “deviation” state. Drone Crash.For general deviations, the drone can still land safely even though not at the expected location. A worse case is a deviation leading the drone against an object and eventually crash. Potential Thrust Loss.By driving the drone motor, the ight control program uses motor power to adjust the drone to the reference state. Nonetheless, the adjustment that can be done by the drone motor is limited. If an incorrect conguration is set, the drone may not be able to move close to the reference state even when saturating the motor up to 100% throttle. Remaining in such an incorrect state can cause a decrease in the drone ight altitude and attitude, or even a crash. Incorrect Conguration Tackling.Before taking o, the ight control program validates the conguration and determines whether it will trigger unstable ight states. (i.e., the conguration is incorrect). When one or more incorrect parameters of the congurations are identied, the control program displays a warning message and aborts the taking-o operation. However, if these congurations, agged as incorrect, are set after taking o, they can still be accepted by the ight control program. As a consequence, the drone may end up in some unstable states. In order to validate all congurations and detect range specication bugs, the following challenges must be addressed: Challenge I: How to validate congurations eectively?Approaches proposed for analyzing ight control programs are generally based on static program analysis techniques that explore control and data dependencies [11,19,26]. However, such techniques are not suitable for validating conguration because large numbers of specied parameter values need to be analyzed. Unlike conventional bug detection techniques that statically analyze only small code snippets, the entire ight control program needs to be analyzed in order to achieve high code coverage. The reason is that dierent parameter values often result in execution ows involving very dierent portions of the control program. Unfortunately, ight control programs have huge sizes (e.g., over 700K lines of code) and complex control and data dependencies, Therefore, it is critical that the approach designed for conguration validation be eective, that is, able to provide high coverage of all possible parameter values. Challenge II: How to conduct an ecient conguration validation?Referring to the unstable ight states dened in Section 2, we need to validate each conguration through either a realistic or simulated ight execution. Because of the large number of control parameters, each with its value range, changing the parameter values to generate congurations and validating all these congurations is inecient Completing the entire validation procedure may then end up requiring hours. Therefore, existing approaches [17,32], which analyze all possible congurations, are not suitable. An alternative approach is to use fuzzing [20] combined with a binary search [21] to reduce the search space of the combinations to be analyzed. However, although the search scope is narrowed, the execution time increases because each fuzzing iteration needs to wait until the validation feedback of the previous conguration is obtained. Challenge III: How to balance the requirements of drone adaptability and ight stability?The ight control program supports dierent congurations to adapt to dierent ight missions. When high adaptability is required, each control parameter must have a large value range to adapt to dierent scenarios. As a result, the number of incorrect congurations will be higher, which will then aect ight stability. On the other hand, when parameters have small value ranges, the possible congurations are limited, which reduces adaptability. Hence, complex ight missions cannot be carried out. Identifying proper value ranges is thus challenging. We now introduce our approaches to the above challenges. Solution I: Grey-box based fuzzing.Since dependencies of the ight control program are complex, static program analysis techniques are ineective. Our approach is instead to conduct grey-box based fuzzing by setting various congurations and validating the corresponding ight states. In particular, we apply a GA to carry out fuzzing. The algorithm rst selects some parameter values and validate the resulting parameter conguration by analyzing the impact of the conguration on the ight states. After that, referencing the validation result, the algorithm conducts a mutation to select more incorrect parameter values and set new congurations to validate, We repeat this process to search more incorrect congurations. This approach address challenges I and II. Solution II: Flight state prediction.Although the GA and the fuzzing reduce the search range of parameter values, the number of combinations is still huge. Therefore, validating all the corresponding congurations through realistic/simulation execution is highly inecient. In response, we designed a state generation approach that leverages a machine learning algorithm to train a state predictor. Instead of validating congurations through a realistic/simulation execution, the state predictor takes each conguration as input and predicts the potential ight state to guide the mutation. Such an approach requires much less time than a realistic/simulation execution. It is important to note that data labeling and predictor generation are one-time costs. Hence it is still more ecient to conduct a prediction rather than a realistic/simulation execution. The reason is that conguration validation has to be executed iteratively when each new conguration is generated by the mutation algorithm. This solution addresses Challenge II. Solution III: Multi-objective optimization.To balance adaptability and stability, we utilize a multi-objective optimization approach. Our approach estimates multiple feasible range guidelines according to the detected incorrect congurations. The optimization target is to eliminate the incorrect congurations (improve stability) while providing a wider range for parameters (improve adaptability). Each solution of the multi-objective optimization (i.e., range guideline) is the best solution in specic conditions, i.e., the optimal balance of adaptability and stability. Our approach allows users to choose an appropriate range from multiple range guidelines based on their requirements. This approach addresses Challenge III. In this section, we present the design of LGDFuzzer, our system for nding range specication bugs drone control programs through learning-guided mutation search. We rst present an overview of its architecture and then the detailed design of its three components. LGDFuzzer (see Fig. 1) contains three components: state change predictor, a module to predict the ight state and assess whether a conguration will lead to unstable states; learning-guided mutation searcher, a GA search module to detect incorrect congurations; range guideline estimator, a module to provide secure parameter range guidelines. LGDFuzzer relies on log les that we generate by repeatedly executing drone ight missions (1). The log data is split into two parts, one is used by the state change predictor to generate features (2), and the other by the learning-guided mutation searcher to carry out cluster sampling (5). After that, the features are used to generate a predictor (34). The searcher clusters data and selects representatives samples from each cluster (6). The searcher then uses each representative sample to nd out the corresponding incorrect congurations (7). It iteratively mutates the conguration and uses the predictor to evaluate which conguration is more likely to cause unstable states (8). When the iteration stop condition is satised, the searcher merges the search result of each sample and generates a potentially incorrect conguration set (9). Then, these potentially incorrect congurations are validated by a simulation (1011 12). Finally, by using the validation results, the estimator uses a multi-objective optimization to generate multiple feasible range guidelines that balance availability and stability under specic conditions (1314). Since there is no standard data set for testing drones, in order to extract features for the predictor and generate data for the searcher, we manually y a drone through a simulator to generate a number of ight logs. A ight log contains multiple entries and each entry consists of state information, sensor data, conguration, and a timestamp index. Each ight is set up with the same ight test mission, AVC2013 [34], which is often used to test the drone mission execution capabilities. Such a ight mission is repeatedly executed with dierent congurations. We record all ight logs but discard those causing unstable states. Because unstable state data is uncontrollable and complex (compared to stable state data), dropping them prevents them from aecting the training of the predictor. In our experiments, we recorded 308, 533 system log entries. As mentioned in Sec. 2, the control algorithm estimates the next reference state according to the current positioned state and sensor data. Based on this input-output control process, we leverage a machine learning (ML) predictor to emulate this input-output relationship and assess the impact of congurations. There are several reasons for using such a ML predictor. First, the diversity and exibility of ML predictors make them appropriate to emulate the non-linear input-output control process. Second, a ML predictor can estimate the next reference state while accurately reecting the deviation caused by incorrect congurations. Specically, for ight data, the learning algorithm trains a predictor that takes the positioned state, sensor data, and conguration as input to predicts (estimates) the next reference state. Like the control algorithm, a large deviation between the predicted and actual reference states indicates that the ight state could become unstable. Two steps are executed to train the predictor: feature extraction and predictor generation. Feature Extraction.To train a predictor by using our dataset, the state, sensor data, and conguration must be extracted from the original logs and represented according to a xed feature vector format. Since we focus on the unstable states that inuence the angle (i.e., position and speed), our predictor considers: (i) angular position and angle speed of the state; (ii) data obtained from gyroscopes and accelerometers. A feature consists of a state unit𝑎, a sensor data unit𝑠, and a conguration unit𝑥. For ease of presentation, we refer to the combination of a specic state unit and sensor data unit as contexts𝑐 = {𝑎, 𝑠}. Finally, a feature consists of those three units with a timestamp and is normalized to a vector𝑣{𝑐, 𝑥 }. Predictor Generation.According to previous research [6,7,28] the Long Short-Term Memory (LSTM) [16] technique can eciently t complex input/output of non-linear models. Therefore, we use LSTM as predictor for the next reference state. Specically, for a pre-processed feature vector 𝑣= {𝑐, 𝑥} with timestamp 𝑡, LSTM takes a numberℎof consecutive vectors with timestamp before 𝑡as input (i.e.,𝑉 {𝑣, ..., 𝑣, 𝑣}), and returns the maximum conditional probability prediction of the next reference state units 𝑎. We show in Sec. 4 how we determine the best values forℎ. In the training stage, the weight of the predictor is iteratively updated to ensure that the predicted reference state𝑎is closer to the ground truth state𝑎. We use the Mean Squared Error (MSE) [36] loss for training. To search the incorrect congurations, we use a GA, that is, a metaheuristic search relying on biologically inspired operators such as mutation, crossover, and selection. Initially, as incorrect congurations may only produce unstable states in specic contexts (i.e., state and sensor data), our system conducts searches for multiple specic contexts to nd incorrect congurations. We split the data from logs into multiple segments𝐶= {𝑐, 𝑐, ..., 𝑐}, 𝑖 𝑚𝑜𝑑 (ℎ+1) =0, where𝑖is the number of data. But considering that there are similarities among segments, searching for each one will generate a huge number of duplicate results. To address such issue, we cluster the segments and sample from these clusters in order to reduce redundancy while maintaining diversity. We leverage the meanshift [12], a probability density-based non-parametric adaptive clustering algorithm, to cluster segments and randomly sample𝑚representative examples from each cluster for the subsequent search. Then, for each sample segment, the searcher carries out a GA search to explore incorrect congurations by iterative mutation, crossover, and selection. When the searcher has collected all the incorrect congurations for each sample, it merges and de-duplicates them as a unique set, referred to as set of potentially incorrect congurations. In what follows we provide details on the Fitness Evaluation Function, used to evaluate the tness of a conguration, and the Searching Process. Fitness Evaluation Function.The GA search applies tness to quantify, by using the predictor, how much deviation a given conguration may cause. Intuitively, the tness evaluation function returns the probability of deviation for a conguration. Specically, assume that the current search is carried out for the context segment 𝐶{𝑐, ...𝑐, 𝑐}; the function selects𝐶{𝑐, 𝑐, ...𝑐}to be used as input to the prediction model and𝑎of𝑐to be used as ground truth for calculating the deviation. When evaluating a conguration𝑥{𝑥, ..., 𝑥}(𝐷is the number of parameters), the function merges it with the segment to create features𝑉 {{𝑐, 𝑥}, ..., {𝑐, 𝑥}}. Then, such features are given as input to the predictor, which returns a predicted reference state𝑎. The tness is the L1-distance ∥𝑎− 𝑎∥between the predicted state and the ground truth state. The goal of the search is then to nd incorrect congurations, which are predicted to maximize the tness, that is, the deviation (and thus maximize the probability of causing unstable states). Searching Process.For each context segment sample, the searcher rst initializes a population whose individuals are congurations, and the parameter values of each conguration are set to their default values. We assume that the population size is𝑁 𝑃and the maximum number of iterations is𝐺. The search process iteratively mutates and updates the population as follow: In the𝑔-th generation iteration (𝑔 ∈ [1, 𝐺]), the searcher rst mutates the current population𝑝𝑜𝑝{𝑥..., 𝑥}and generates a variant population𝑝𝑜𝑝{𝑦, ..., 𝑦}. Each congurations of the variant population is obtained as follows: 𝑦= 𝑥+ 𝐹 ∗ (𝑥− 𝑥) + 𝐹 ∗ (𝑥− 𝑥 where𝑥are random congurations,𝑥is the best tness conguration, and 𝐹 is the scaling factor. Then, the𝑝𝑜𝑝is crossed over with the𝑝𝑜𝑝to produce a new experimental population𝑝𝑜𝑝{𝑒, ..., 𝑒}, whose𝑖-th conguration is𝑒{𝑒, 𝑒, ..., 𝑒}, where𝑗 ∈ [1, 𝐷]. The parameter values 𝑒of each conguration are calculated as follows: 𝑒=𝑦, 𝑖 𝑓 𝑟𝑎𝑛𝑑 (0, 1) < 𝐶𝑅 𝑜𝑟 𝑗 = 𝑗𝑥, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒(2) where𝑗is a random integer in[1, 𝐷],𝐶𝑅is the crossover rate, 𝑥is the 𝑗-th parameter value of 𝑖-th conguration in 𝑝𝑜𝑝, and 𝑦is the 𝑗-th parameter value of 𝑖-th conguration in 𝑝𝑜𝑝. Then, the searcher evaluates the tness of each conguration both in𝑝𝑜𝑝and𝑝𝑜𝑝. According to their tness, the searcher selects some congurations from the population to obtain the next generation of population𝑝𝑜𝑝{𝑥, ..., 𝑥}by using the following selection function: where 𝑓 is the tness evaluation function. Finally, if the tness of each conguration in the population does not longer increase or the maximum number𝐺is reached, the search stops the mutation. We select the top 10 highest-tness congurations from the nal generation population as incorrect congurations. The predictor classies a conguration as incorrect if the conguration has a high probability of causing unstable states. However the set of all such congurations need to be validated to conrm that they actually cause unstable states. We thus simulate the ight and use a monitor to observe which potential incorrect congurations actually lead to unstable states during the simulated execution. Specically, the drone is set with potentially incorrect congurations to perform the AVC2013 ight mission. For ight freeze, if drone moves at distances always less than 0.5 meters in 15 seconds, the monitor identies this as a ight freeze. For deviations, if the ight deviation continues to be 15 times higher than 1.5 meters, the ight is considered a deviation. The range guidelines provided to the users should consider stability and adaptability, while at the same time eliminate discrete incorrect congurations and reserving relatively complete space for each parameter. Meanwhile, as we cannot ensure the stability of unveried congurations, the estimation of range guidelines should reference the obtained validation results and refrains from considering incorrect congurations. Therefore, we leverage a multivariate optimization to determine the suitable range guideline. If all parameter values in a conguration are within the range specied by the guideline, we consider the conguration to be covered. Our system attempts to estimate the range guideline (𝑅𝑎𝑛𝑔𝑒) based on the following optimization problem: The optimization problem has two objectives: (a) to maximize the number of validated congurations𝑅covered by the guideline; (b) to minimize the coverage of incorrect congurations 𝑅. If we shrink the allowed ranges to avoid incorrect congurations, the range of each parameter value would also shrink and vice versa. Therefore, instead of dening strict ranges for each parameter, the system solves this optimization problem with multiple constraints to obtain a diverse group of Pareto solutions. These Pareto solutions form a boundary consisting of the best feasible solutions, allowing users to select the best conguration according to their requirements. The system provides this range guideline mainly based on the following considerations: (1) As the number of parameter increases, it would be dicult to completely rule out insecure parameter values as the secure parameter ranges may not be continuous. (2) While stability is paramount, users may be willing to incur some minor risk for better controlling exibility or availability. We assessed LGDFuzzer by considering its eectiveness and eciency in validating congurations. The following research questions (RQs) were answered: • RQ1: Eectiveness.How many incorrect congurations are detected by LGDFuzzer? • RQ2: Adaptability.Can LGDFuzzer provide the most suitable value ranges for parameters with minimum incorrect congurations? • RQ3: Enhancement.How do the mutation and the predictor help improve conguration validation? We applied LGDFuzzer to an open source ight control program, ArduPilot(4.0.3) [35], which is widely used by drone manufacturers such as Parrot, Mamba, and Mateksys [33]. To validate congurations specied in ArduPilot, we utilized three experimental vehicles (shown in Fig. 2) for testing, including two drones with Pixhawk [24] (i.e., CUAV ZD550 and AMOVLab Z410) and a drone simulator (i.e., Airsim [29]). Figure 2: Real and virtual drone vehicles used for experiments. According to the control parameter descriptions provided by the manufacturer, we selected 20 parameters that may aect ight angular position and angular speed. We provide details about these parameters in Appendix A, including parameter name, specied value range, default value, and parameter description. The predictor and the GA searcher are implemented in Python. Concerning the GA, its evolutionary stagnation judgement threshold is set to 0.1, the number of representatives𝑚is set to 3. and the maximum number of evolutionary generations is set to 200. We further set the size of initial population to 1,000 (i.e.,𝑁 𝑃 =1,000) and the scaling factor 𝐹 to 0.4. To evaluate whether LGDFuzzer identies incorrect congurations accurately, we rst assess the prediction accuracy of the predictor in the ight state predictor phase. Then we validate whether the predicted incorrect congurations can impact ight stability. State Prediction.We labeled 1,500 congurations manually by sending the congurations to Airsim and recording the resulting ight states. Then we use previously collected log data about stable states to train and evaluate predictors with dierent input lengthsℎ. Specically, our experiment rst extracts the context (i.e., state and sensor data) segments from log data. Then, each labeled conguration is randomly merged with a segment. We use the same method in Fitness Evaluation Function to calculate the deviation for each conguration. If the deviation is greater than a threshold, the conguration is considered unstable; it is considered stable, otherwise. In particular, we use the maximum deviation in the predictor training process, that is, the maximum deviation from the stable ight data, as the threshold. This experiment tests the accuracy, precision, and recall of predictors. The results are reported in Table 1. In general, the results show that our predictor is robust for different input lengths. The deviation values, observed from the experiments, of incorrect congurations are larger than the values of the correct congurations. That is the reason why we can use the Table 1: Predictor accuracy, precision and recall for dierent input lengths. predictor in the GA search to drive congurations to a higher deviation. In addition, the experiment results show that the recall of the prediction does not linearly increase whenℎexceeds 4. Therefore, we choose the predictor with the best recall i.e.,ℎ =4 as the input length to carry out the subsequent experiments. To assess the prediction accuracy for state changes, we send the congurations to Airsim and record the corresponding ight data. Then, we randomly select 150 consecutive features from the data and give them as input to the predictor to predict the state change and then compare it with the ground truth state. The results in Fig. 3 show that the predictions closely match the real states (an example of angle roll). In the gure, the dotted curve denotes the actual states, the solid curve denotes the predicted states, and the histogram at the bottom (the blue bar) indicates the prediction errors as dierences between the two curves. The small error shows that the trained predictor is able to accurately predict ight state changes. Figure 3: Match between real behavior and prediction. Conguration Validation. Given the predicted incorrect congurations, we validate them through realistic/simulation execution. Since the incorrect congurations may cause drone crash, we examine all of them on Airsim. For the 465 samples obtained by clustering and sampling, LGDFuzzer searches out 2,319 unique incorrect congurations. Finally, after a validation, 2,036 conguration of 2,319 are marked as truly incorrect resulting in 500 Deviations, 2 Flight Freeze, 2 Crashes, 564 Incorrect Conguration Tackling, and 968 Potential Thrust Losses. In this experiment, we use the previous validation results about incorrect congurations to estimate the range guidelines, and discuss how they balance the stability and adaptability. With reference to the validation results, the estimator nds out 143 Pareto solution results (see Fig. 4). In the graph, the horizontal axis represents the number of validated congurations covered by the range guideline, and the vertical axis represents the ratio of incorrect conguration in the range guideline. Each Pareto solution represents a feasible solution (i.e., range guideline) satisfying specic constraints (i.e., adaptability and stability constraints). For instance, we select some range guideline examples in Table 2 and further analyze their stability and adaptability. To illustrate the modied guidelines generated by LGDFuzzer, we select several parameters out of 20 ones to show their details. The table shows three examples in which stability decreases and the congurable space (i.e., adaptability) increases. Guideline i avoids the majority of incorrect congurations. It covers 28 validated congurations, only one of which causes an unstable state; this means high stability. However, compared with the original parameter ranges, this guideline reduces too much the available space, which results in low adaptability. In contrast, Guideline iii reserves relatively complete ranges for the parameters. It covers 236 validated congurations but more than half of them (133) are incorrect, which results in low stability. Guideline ii is an intermediate choice, covering 91 validated congurations where only 29 are incorrect. If users have more stringent stability requirements, they can use the lower error ratio range guideline, at the cost of limiting the conguration space and thus being unable to satisfy other ight requirements. On the contrary, if users have to satisfy special mission requirements (e.g., the mission is a time-limited task, or it needs a large ight angle to reach the target speed), they may consider sacricing a bit the stability in order to improve adaptability. In fact, they can choose an appropriate range guideline from the Pareto solutions according to their stability and adaptability requirements. To show the advantages of our system, we experimentally compare it with rvfuzzer [20], a recent approach for searching range specication bug. rvfuzzer relies on the One-dimensional Mutation and Multi-dimensional Mutation search to detect incorrect congurations. By using One-dimensional Mutation, centered on the default parameter values, rvfuzzer separately conducts binary searches to narrow the upper and lower bounds until a midpoint value is obtained that does not any longer cause unstable states. In the Multi-dimensional Mutation multiple parameters are considered, each of which determines a novel binary mutation search congured with dierent extreme values (i.e., maximum and minimum) of the other parameters. All these experiments are based on the six parameters utilized in rvfuzzer (see Appendix A). In the experiments we consider the unstable states listed in Section 2.1. Comparison with Respect to Missed Incorrect Congurations. In the One-dimensional Mutation, the optimal solution may not be consistent with the right optimum. For example, using such a mutation strategy, the search indicates that the correct range for INS_POS1_Zshould remain within[−4.7,0.0]when it searches for the lower bound. However, there are still incorrect congurations, specically between−1.0 and 0.0, inside this range. The reason is that, because of the binary search, as the rst midpoint (i.e.,−2.5) does not cause an unstable state, the search directly skips values greater than−2.5. It thus does not cover the[−1.0,0.0]space, which results in missing potentially incorrect congurations. In the case of multiple parameters mutations, we rst apply the Multi-dimensional Mutation to determine the correct range. After that, based on this correct range, we leverage LGDFuzzer to start another search to evaluate whether there are incorrect congurations. LGDFuzzer still detects 727 potentially incorrect congurations, of which only 185 are conrmed. Such result indicates that the ranges provided by the Multi-dimensional Mutation are not correct. In our opinion, the Multi-dimensional Mutation is still a one-dimension mutation, because it uses binary search to mutate parameters separately but only imports the extremes of the value ranges of other parameters. It can be regarded as multiple one-dimensional mutations with a limited correlation between control parameters; as such, it does not consider the inuence of values dierent from the extremes of the ranges. Comparison with Respect to Correct Range Guidelines.The six parameters utilized in this experiment are listed in Appendix A. We leverage LGDFuzzer to search incorrect congurations for these six parameters. The search detects 1,199 unique potentially incorrect congurations. Then the validation determines that 714 out of those congurations actually lead to unstable states. After that, we use them to generate the feasible range guidelines and choose the lowest incorrect ratio result. Table 3 lists the ranges obtained by three methods, where 1 is One-dimensional mutation, M is Multi-dimensional mutation, and GA (our Genetic Algorithm mutation). rvfuzzer method 1 obtains little reduction for each parameter range; thus it is not able to rule out incorrect congurations. M avoids some of the incorrect congurations but still misses others. Because our GA greatly reduces the ranges, it provides high stability. A special case is related toINS_POS3_Z; the range given by GA is larger than the range given by M. But this does not mean that our range forINS_POS3_Zis incorrect. The reason is that, as other parameters have a smaller range,INS_POS3_Zcan have a larger range, since the validity of congurations is decided based on multiple parameters instead of a single one. As for other parameters, ANGLE_MAX inuences the ight inclination angle. Under the inuence of other parameters, a too large value forANGLE_MAXis more likely to cause ight problems. A too small value for the waypoint speedWPNAV_SPEEDmakes the drone more likely to freeze; both M and GA reduce the lower part of the range for this parameter. ForINS_POS*_Z, the ranges returned by GA are smaller than the ones returned by M, and closer to default values. In fact, changingINS_POS*_Zinuences the position judgment of the inertial measurement unit. Therefore, this parameter should be changed carefully and should not deviate too much from the default value. PSC_VELXY_Paects the output gain of the system for acceleration; a too large gain can easily cause drone deviations or thrust losses. Comparison with Respect to Time Requirements.We analyze the time taken by LGDFuzzer and Multi-dimensional Mutation. To determine the range guideline for the six parameters, we started multiple simulations and took 696 seconds to collect log data. The predictor takes 700 seconds to train until reaching convergence. The GA search takes another 156 seconds to iterate and update its population (1,000 congurations). Finally, from generating data to searching out 1,000 incorrect congurations, our system totally takes 1,552 seconds. Also, over 85% congurations are validated and detected to actually cause unstable states. On the contrary, depending on the conguration values, rvfuzzer usually takes between 20 and 120 seconds per-round to complete a AVC2013. Even we allocate 1,552 seconds to Multi-dimensional Mutation, it can only carry out 78 rounds of mission test. In fact, the Multi-dimensional Mutation search takes more than 2 hours to estimate the guidelines for the six parameters. In addition, for 20 parameters, the time taken by toolMulti-dimensional Mutation increases exponentially. Instead, the time consumption of LGDFuzzer is still closer to the consumption of six parameters because the time required by predictor is almost unchanged. After obtaining the fuzzing results, we select several representative examples of unstable states to analyze their characteristics. Deviation.There are two main deviation situations in our experiment: overshoot and y away. When ying from one waypoint to another, the drone rst accelerates to reach the target velocity and decelerates while approaching the next waypoint. If the conguration is set improperly, the drone is unable to reduce its speed when close to the waypoint, which causes an overshoot deviation (see the video of a simulation in which the drone is not able to brake at [4]). In comparison, y away is much more dangerous, in that the drone keeps moving away from its mission-planned path. We leverage a real drone experiment to demonstrate the damage resulting from those unstable states. The experiment sets up a simple surround ight mission and a drone conguration is set that, in the simulation test, results in a y away deviation. As shown in the video [3], the drone deviates from the mission-planned path. Unfortunately, even though we used the RC controller (remote manual controller) to manually switch its mode to land in order to stabilize the drone and make sure it would land slowly, the drone was still unable to stabilize and kept deviating. In fact, after checking the oine ight log, when we manually switched to the land mode, the system reported a switching error indicating that it could not stabilize and land. In other words, if the incorrect conguration is not dealt with in time, the ight stability cannot be even corrected manually. Flight Freeze.There are two main situations of ight freeze. On the one hand, an incorrect control gain parameter makes the drone fail to reach the desired position in time, or prolong the time to reach a particular waypoint or the desired position. On the other hand, an invalid conguration aects the position and causes the drone to around-ight within a small area. We tested the around-ight situation with a real drone experience. The video at [5] shows that the drone keeps circling and is unable to complete the given mission. We analyzed the oine ight log, and from the log, we could see that the drone kept ight switching between althold (hovering y) and land but could not land successfully. Drone Crash.Drone crash may be caused by a rollover during takeo or by hitting the ground due to improper descent speed. The video at [2] shows an actual example of a drone crashing when taking o. Potential Thrust Loss.This situation is mainly caused by an excessive change in angle or speed; the ight controller attempts to recover its position to normal, but the power of the motor cannot satisfy the requirement. Failure in recovering from the thrust loss or stabilizing the altitude would lead to a drone crash. In the postmortem analysis, we found that changing the control parameters related to the inertial measurement unit position (e.g.,INS_POS*_Z and the PID angle controller (e.g.,ATC_ANG_*_P/I/D) can force the drone into a gradually amplied oscillation. Incorrect Conguration Tackling.Incorrect conguration tackling is the new type of error we dened. A control program usually contains a checking mechanism to prevent obvious errors in congurations. If the parameters are related to the position controller (e.g., ATC_*_*_P/I/D), the incorrect conguration will raise a warning when the GCS tries to arm the drone. But in fact, if we update the conguration after the drone take o, the problem is that the the control system still accepts it, which ultimately drives the drone to become unstable and out of control. The video at [1] shows an example in which uploading incorrect conguration during the mission causes the drone to crash. A large number of drone fault detection methods/systems have been proposed to prevent errors during ights. Among them, Choi et al. [14] use the control invariant to identify physical attacks against robotic vehicles. But their approach is unable to prevent attacks that exploit the range specication bug. To detect faulty components in a drone system, G. Heredia et al. [18] construct an observer model to estimate the output in fault-free conditions from the history inputs and outputs. Such a system utilizes the dierence between actual outputs and the predicted values to detect faulty sensor and actuator components, but it still does not consider the instability caused by range specication bug. Ihab et al. [27] leverage the analytical redundancy between ight parameters to detect sensor faults. Like the other approaches, it only considers external factors. In comparison, LGDFuzzer implements a search system to avoid incorrect congurations that are instability factors within the system. From the perspective of learning-based testing, there are three relevant systems or methods. NEUZZ [31] is a gradient-guided search strategy. It uses a feed-forward neural network to approximate program branching behaviors and predicts the control ow edges to cover more test space, but the predictive model is not used for guiding testing. ExploitMeter [38] uses dynamic fuzzing tests with machine learning-based prediction to updates the belief in software exploitability. Yuqi et al. [9] use an LSTM network to model the input-output relation of the target system and a metaheuristic method to search for specic actuator commands that could drive the system into unsafe states. Konstantin et al. [8] apply a deep Q-learning [25] algorithm to generate an optimized policy for the fuzzing-based testing of PDF processing programs. DeepSmith [15] trains a generative model with a large corpus of open source code and uses this model to produce testing inputs to examine the OpenCL compiler automatically. These approaches make use of prior knowledge to drive the mutation input. Similarly, we introduced a machine learning model to guide the search test process. However, our LGDFuzzer combines the model with the genetic algorithm to carry out a large-scale multi-parameter search. Incorrect congurations of drone control parameters, set by legitimate users or worst sent by attackers, can result in ight instabilities disrupting drone missions. In this paper, we propose a fuzzing-based system that eciently and eectively detects the incorrect parameter congurations. LGDFuzzer uses a machine learning-guided fuzzing approach that uses a predictor, a genetic search algorithm, and multi-objective optimization to detect incorrect congurations and provide correct feasible ranges. We have experimentally compared LGDFuzzer with a state-of-the art tool. The experimental results show that LGDFuzzer is superior to such a tool in all respects. Even though our methodology has been designed for aerial drones, we believe that it can be used for other mobile devices with complex controls, such as underwater drones.