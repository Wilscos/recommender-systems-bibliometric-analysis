We propose a novel approach to explainable AI (XAI) based on the concept of “instruction” from neural networks. In this case study, we demonstrate how a superhuman neural network might instruct human trainees as an alternative to traditional approaches to XAI. Speciﬁcally, an AI examines human actions and calculates variations on the human strategy that lead to better performance. Experiments with a JHU/APL-developed AI player for the cooperative card game Hanabi suggest this technique makes unique contributions to explainability while improving human performance. One area of focus for Instructive AI is in the signiﬁcant discrepancies that can arise between a human’s actual strategy and the strategy they profess to use. This inaccurate self-assessment presents a barrier for XAI, since explanations of an AI’s strategy may not be properly understood or implemented by human recipients. We have developed and are testing a novel, Instructive AI approach that estimates human strategy by observing human actions. With neural networks, this allows a direct calculation of the changes in weights needed to improve the human strategy to better emulate a more successful AI. Subjected to constraints (e.g. sparsity) these weight changes can be interpreted as recommended changes to human strategy (e.g. “value A more, and value B less”). Instruction from AI such as this functions both to help humans perform better at tasks, but also to better understand, anticipate, and correct the actions of an AI. Results will be presented on AI instruction’s ability to improve human decision-making and human-AI teaming in Hanabi. Keywords: XAI, explainability, interpretability, hanabi, human-machine, teaming, instructive, instruction AI systems have demonstrated the ability to perform tasks remarkably well from games Many of these AI are comprised of deep neural networks from which it is very hard to extract insight or explanations of the decision the network makes. Therefore, in many cases a neural network can discover novel insights into a domain but cannot communicate these insights to the humans that developed the network. This fundamental problem has sparked the active ﬁeld of research into explainable AI (XAI), some measures in place to facilitate human understanding of the AI’s decision. agents who must collaborate with the AI (which typically requires some level of common understanding) and those in which humans are signiﬁcant stakeholders (e.g. when the AI is recommending medical treatment). A research eﬀort at JHU/APL entitled “Learning to Read Minds” studied this challenge in the context of humanmachine teaming in the collaborative card game Hanabi. information that requires players (human or machine) to be able to infer the knowledge, intentions, and future actions from the behavior of their teammates. optimization (i.e. training an AI through millions of games played between copies of the same AI) does not lead to successful human-machine performance, (e.g. repurposing an in-game clue to mean something entirely diﬀerent from its semantic meaning) that will be automatically understood by their mirror image during self-play, but completely incomprehensible to a human. This is why agents such as the Simpliﬁed Action Decoder, scores in self-play, yet achieve low scores when playing with human teammates. In some cases, the unexplainability of AI is a barrier to its use. Such cases are those in which humans are eﬀective XAI techniques, there appear no practical means for these complex self-play AIs to explain these obscure conventions to humans (setting aside whether humans are even capable of implementing these conventions once understood). developing AI agents that would excel when playing Hanabi with human strangers. The winning JHU/APL agent not only achieved human-play scores higher than any found in literature to date, a way that was constrained to allow human-readable descriptions of strategy (Figure 1). In particular, the JHU/APL agent demonstrated the ability to develop deep insights into human strategy through observation of human play, to understand how the human strategy interacted with the agent’s strategy, and to adapt to discover a play style which complements the human strategy. This study summarizes the agent’s structure which enabled it to successfully collaborate with human teammates, and introduces a novel type of explanation (we call “instruction”) to share AI insights with human observers. The JHU/APL agent (henceforth referred to as “agent”) was developed under the philosophy that if it could play like humans, it would play well with humans. The agent was designed to convert the input space of the game state to a latent space of a small number of human-preferred factors (HPFs) which are aspects of the game that humans are known to attend to when making decisions. The agent utilizes twelve HPFs (Table 1) which were suggested by intermediate Hanabi players. Constraining the attention of an AI in this fashion in order to guarantee some level of interpretability after training is a known practice. Hanabi agent, an expected reward for each possible action is computed based on the expected eﬀect the action will have on the HPFs. In particular, the expected value of an action is the inner product of a factor vector with a weights vector ~w. Therefore, where y expected changes that an action will induce on each of the HPFs (e.g. for the HPF of playing a playable card, the corresponding element in The elements of ~w are the relative values of each HPF with respect to one another. Thus, while H represents information about the game state, ~w represents the agent’s strategy. Altering the elements of ~w can dramatically alter the play style of the agent. always chooses the action with the highest expected reward among the legal actions available. Of note, this technique does not involve and consideration of moves beyond the ply under consideration. Rather, the agent is pursuing an immediate improvement of the game state with respect to the chosen HPFs. While human-like play was the preliminary goal during the agent’s development, the ﬁrst training eﬀorts were aimed at generating decent self-play scores. For this phase, the training of the agent was separated into epochs. Each epoch consists of a four-dimensional, full factorial design experiment on a subset of four elements from ~w. Each element under test was given three test values (a low, medium, and high) around the neighborhood of where the optimal value was expected to be. Therefore, each epoch tested 3 vector tested, 200 games were played between identical copies of the agent. The elements of ~w under test were not altered until a an epoch occurred for which the highest score was achieved by assigning the medium value for each element under test (i.e. increasing or decreasing any element led to poorer performance). The progression of self-play scores during this development phase are shown in Figure 2. lead to play that was as human-like as possible. To facilitate this exploration, a dataset of 376 decisions was collected by examining the play of one of the authors. With a dataset of decision made by a single human, we aimed to determine if a strategy vector ~w could be ﬁtted to a particular human’s play style rather than a ~w The “Learning to Read Minds” research project included a JHU/APL-internal challenge tasking staﬀ with is the expected reward for action i, and vectors~hform the columns of H. The elements of~h are the On each move, the agent calculates ~y which stores the expected reward for each possible action. The agent Once the agent was optimized for self-play, the next objective was to ﬁnd a strategy vector ~w that would Table 1. The twelve factors, along with their values for three diﬀerent JHU/APL agent play styles (human-like, humancomplementary, and self-play). To help interpret some of the factors, consider the following deﬁnitions. endangered card - a card for which no copy has been played, yet there is only one copy of this card remaining in play. unneeded card - A card which cannot be played in the future, for any reason. Figure 1. The score distributions are shown for Simpliﬁed Action Decoder (a special oﬀ-belief version made for the competition), Rainbow, Fireﬂower, and the JHU/APL agent. These scores were obtained by pairing an agent with a human teammate (drawn from a pool of 21). Figure 2. Self play scores are shown for diﬀerent versions of the agent during the initial development process. Each transition from one version to the next was accompanied by changes to the weights vector ~w or the addition of new elements to the weights vector. Notably, the human-like agent had signiﬁcantly poorer self-play scores, consistent with the fact that the human-like agent was optimized to agree with a database of human decisions. which represented some ambiguous (perhaps bad) play style that was averaged across humans with potentially dissimilar play styles. The progression of increasing humanness is displayed in Figure 3. The highest humanness fraction of any agent was 64.2%, achieved by the human-like agent (that is, the agent was able to independently agree with the human decision in 64.2% of the game states examined). ﬁnal training eﬀort was made by pairing a training version of the agent with the human-like version. In this fashion, the training process was intended to approximate playing with a human teammate. As before, full factorial design experiments were run altering four elements of ~w per epoch, each across three levels. At then end of the training process, the “human-complementary” version of the agent was created. Cross play results (Figure 4) illustrate the performance of diﬀerent combinations of agents developed. It is worth noting that the JHU/APL agent needed to make signiﬁcant changes to its initial ~w vector in order to accurately predict human decision making in the game, despite the fact that the initial ~w given to the agent was intended to accurately describe human decision making. For this reason, it became clear that human players could not accurately depict the weights they attributed to HPFs. This has profound implications for XAI. A common XAI approach would have involved taking the values from the self-play strategy vector in Table 1 and describing these to a human player (e.g. “You should value discarding a non-endangered card at 0.8”). However, if a human already egregiously misunderstands what value they actually attribute to these HPFs, it is unlikely that the human will be able to act on this insight. Rather, it would perhaps be more suitable for us to look at the diﬀerence in weights between the human-like agent and the self-play agent, since doing so would allow us to specify corrections a human should make to their strategy (e.g. “you should value discarding a non-endangered card more”). These corrections are human interpretable regardless of whether the human accurately understands their current strategy. This is the principal idea behind AI instruction. Once the human-like version of the agent had been optimized for ﬁtting the dataset of human decisions, a Figure 3. The humanness of major versions of the agent are shown. Humanness is deﬁned as the fraction of human decisions with which the agent agrees when analyzing a database of 376 game decisions made by one of the authors. Of the agents shown, only the “human-like” agent was explicitly optimized for maximal humanness. The considerable humanness of the other models indicates how optimizing an HPF focused agent for self play can lead to considerably human-like performance. Figure 4. Average scores are shown for diﬀerent pairings of the agents (and humans). The human-human score is included for reference, but is an average across a small number of games played within the development team. All other scores are averages across at least 10 games. Error margins for each bin are less than 1 point. It is clear that the humancomplementary agent achieves the highest score of any agent paired with the human-like agent (since these are the exact conditions under which the human-complementary agent was optimized). It is notable that the average human + self-play and human + human-compl. scores are identical in this plot (where a diﬀerence is expected), but it is also worth noting that a very small number (two) of very experienced humans were represented in these data. Therefore, while these scores are useful for comparing how non-human agents perform in diﬀerent pairings, generalizations about human play from this ﬁgure should be made with great caution. AI instruction is deﬁned in the context of explaining diﬀerences in strategy in the form of changes on weights. Therefore, it is relevant to consider a diﬀerence in outputs (say, ~y 3.1 A note on Strategy vs. Perception It is possible to imagine a diﬀerence in performance, δ~y, arising not from a diﬀerence in strategy (δ ~w), but rather a diﬀerence in perception of the game state δH. This is particularly practical if the elements of H concern complex changes in the game state such as the probabilities of certain outcomes (as it does in Hanabi). In this case, In fact, there is ambiguity between this and (3), since HH and thus the relation indicates that any strategic diﬀerence δ ~w could be interpreted instead as an observation error δH. This illustrates yet another reason for providing instruction in the form of δ ~w rather than an explanation in the form of ~w By the very nature of this equivalence relation, a recommended change in strategy, δ ~w can compensate for both misperception and strategic deﬁciency (and mixtures thereof). 3.2 Non-uniqueness of and Constraints on δ ~w Since H is satisﬁed for any ~n in the null space of H needed to elicit the decision diﬀerence δ~y. This non-uniqueness of δ ~w is advantageous, because it allows multiple possible δ ~w to be compared for ﬁtness according to human friendly constraints (e.g. norm minimality, sparsity, etc.). 3.3 Generating AI Instruction Suppose that a human subject is presented with g game states H are stacked into a 12 × 20 × g tensor H. As before, a strategy ~w can be combined with a game state to yield a vector of outputs, ~y. In the following formalism, a subscript h corresponds to a human, while a subscript i corresponds to an ideal (typically a successful AI). Let’s assume that the index of the maximum element of ~y will be taken (per strategy ~w the same action if their maximal elements occupy the same index. If not, then it is worth describing the nearest (min |~y than the maximal element of ~y let m be the average of all the terms of ~y is a tall matrix (20 × 12), Hhas a non-empty null space. Therefore, the condition − ~z|) vector ~z such that ~yand ~z have maximal elements in the same index position (a position diﬀerent where ε is some small, positive tie-breaking factor. If the vectors ~z (of which there are g) are made the columns of a 20 × g matrix Z, and the output vectors y we can relate Z and the game state tensor H as follows. Where δ ~w is a strategy change needed so that ~w in H. To calculate for δ ~w, we can utilize the following matrix unfolding. This is an overspeciﬁed linear system, so a least squared error solution can be taken for δ ~w. Then, δ ~w is the norm-minimal change to apply to ~w strategy of a human, and ~w inverse (−δ ~w) has elements which comprise the instructions that should be given to the human. In essence, the instructed changes are the opposite of those needed for the ideal to be altered to make the same decisions the human made. 3.4 Properties of the Generated δ ~w δ ~w is not guaranteed to produce consensus between the starting strategy and the ideal when adopted. Formally, it does not always hold that However, it is possible (and desired) for this relation to circumstantially hold for many k values. Increasing ε in the above formulation will tend to increase the number of game states in which consensus is built but at the expense of a larger norm δw (i.e. bigger recommended changes to ~w ideal and modiﬁed strategies is rarely achieved because the model for decisions generated from game states given by (1) may not accurately describe all decisions (~y) made by a human (e.g. due to momentary misperception, distraction, and attention to factors not captured in H). The quantity λ deﬁned as may be introduced as a ﬁgure of merit for the list of factors which deﬁne the strategy vector ~w. Furthermore, λ can be used to measure the utility of elements of ~w by examining the change in λ induced by the removal or inclusion of factors. Ideally, the only factors kept would be those whose inclusion result in a signiﬁcant decrease in λ. If n quality q(δ ~w) of instructions as q(δ ~w) falls in [0, 1] and can be interpreted as the fraction of human decisions that can be understood as a variation (δ ~w) on an ideal ( ~w Similarly, one can deﬁne a ﬁgure of merit for generated instruction. If we deﬁne f(~a,~b) as is the index of the decision the human instructee made for game state k, then it is possible to evaluate the Instructive AI Algorithm α ← Chose accuracy threshold in [0,1] H, ~n ← Observe g human decisions for k ∈ [1, g] do end for δ ~w ← Solve q ← while q > α do end while Give −δ ~w to human We recommend the algorithm in Figure 5 for generating AI instruction. The algorithm has two preparation steps. The ﬁrst is to train up an ideal strategy (~w ~n paired with the game states in which they were made (slabs of the H tensor). While it is possible to terminate the algorithm after the step that assigns δ ~w, this algorithm includes a post-processing component which seeks to zero out as many elements of δ ~w as possible while maintaining some preset explanatory ﬁdelity α to the human decision set. The purpose of this post-processing is to generate instruction which concerns changes in as few of values as possible. This is motivated by the assumption that low dimensional instructions are easier for humans to understand (i.e. require focusing on fewer aspects of the game in subsequent play). During the “Learning to Read Minds” challenge, a database of 376 human decisions in Hanabi games was generated. Preliminary results are shown based on analysis of this dataset. To illustrate the instruction generation process, a trial agent was created by copying the self-play agent. Because the self-play agent already agrees with human decision at a high rate, the weight for the non-endangered discard was inﬂated (to a value of 10). Then, in an iterative process, instructions were generated (on how the trial agent could better emulate human decision making based on the dataset), the trial agent applied the instructed changes to its weights, and a new set of instructions were generated. This process is shown to lead to asymptotic improvement in the agreement between the trial agent and the human dataset (Figure 6). discard weight was shown to be brought into closer agreement with the target strategy. Importantly, this (and other initially matching weights) were shown to drift to new equilibrium values. This serves as an empirical demonstration of the non-uniqueness of strategies as described in the previous section. However, it is important to note that the generation of a norm minimally diﬀerent Z matrix may not provide a linear system in (10) that admits a solution that produces high prediction accuracy when observing the target strategy. This is because the matrix Z may be a poor estimation for the target strategy’s output vectors, a circumstance that is increasingly likely when the instructee strategy diﬀers signiﬁcantly from the target strategy. which, taken iteratively, can lead to signiﬁcant improvement in the agreement between the instructee strategy and the ideal. In this way, instructions serve as something of a proxy gradient of a cost function, namely, agreement with the ideal. Utilizing the instructions as a gradient for agent training was shown in this experiment to lead to better humanness scores (68% vs. 64%) in a much shorter computation time (minutes vs. hours) compared to the full factorial approach. Additionally, these instructions provide a novel approach to portraying AI insight in Y(:, k) ← H(:, :, k)~w Z(:, k) ← (Y(k), n(k)) per (8) δ ~w ← zero out element with smallest impact on qP q ←f(H(:, :, k)(δ ~w+ δ ~w), n) High agreement (68%) was achievable after 40 instruction based weight updates. Furthermore, the spurious These results (Figure 6) indicate that AI instruction can indeed provide stepwise improvements to strategy Figure 6. The eﬀect of following successive batches of AI instructions are shown. For this experiment, a trial version of the self-play agent was created that had a non-endangered discard weight of 10 rather than the normal 0.8. Instructions are generated for how this agent can better emulate the self-play agent. (a) The agreement between the agents is shown as a function of how many batches of instructions were generated. After each batch, the trial agent applies the correction to its weights and reexamines agreement. (b) The trial agent’s inﬂated discard weight decreases with time. Notably, it does not appear to stabilize at the same value as the ideal (0.8). (c) The weight for discarding unneeded cards is shown over the experiment. Note that while the two agents initially had the same value (0), this weight drifts to a new value as a result of the instructions, indicating that a new set of weights is being discovered that agrees with the self-play agent over the decisions studied. (d) Another weight is shown (which does not pertain to discards) to further illustrate how instruction may not encourage the same, unique weights as the ideal. a way that is understandable to human observers. Speciﬁcally, these instructions can be phrased as corrections to the weights attributed to human-preferred factors, allowing for AI systems to develop an understanding of human decision making and to share those insights through tailored instructions. Leveraging insights obtained from the development of a highly successful, artiﬁcially intelligent human teammate for Hanabi, we propose a technique of instructive AI to better enable humans to obtain insight from complicated AI systems. There are assumptions in this approach that may not hold true for certain contexts. For instance, this technique hopes that the requisite δ ~w for consensus building is small. If not, then implementing a δ ~w may be just as confusing for humans as being told ~w by (1) may not accurately capture a majority of a human’s decisions, and ~w is always at risk of missing elements that are crucial to a human’s decision making. In general, it is challenging to produce a complete set of values relevant to human decision making. For the purposes of this experiment, the list of values is produced from human introspection and trial and error. Techniques to organically learn the needed values may be possible and highly valuable to the task of generating AI instruction, but are beyond the scope of the experiments described above. AI shows promise to circumvent some of the greatest challenges of XAI and provide a novel framework in which further research might push the frontier on extracting human-useful insight from complex AI systems. Many of the challenges described above apply in similar form to other methods of XAI. However, instructive