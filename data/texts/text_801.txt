Articial intelligence systems increasingly involve continual learning to enable exibility in general situations that are not encountered during system training. Human interaction with autonomous systems is broadly studied, but research has hitherto under-explored interactions that occur while the system is actively learning, and can noticeably change its behaviour in minutes. In this pilot study, we investigate how the interaction between a human and a continually learning prediction agent develops as the agent develops competency. Additionally, we compare two dierent agent architectures to assess how representational choices in agent design aect the human-agent interaction. We develop a virtual reality environment and a time-based prediction task wherein learned predictions from a reinforcement learning (RL) algorithm augment human predictions. We assess how a participant’s performance and behaviour in this task diers across agent types, using both quantitative and qualitative analyses. Our ndings suggest that human trust of the system may be inuenced by early interactions with the agent, and that trust in turn aects strategic behaviour, but limitations of the pilot study rule out any conclusive statement. We identify trust as a key feature of interaction to focus on when considering RL-based technologies, and make several recommendations for modication to this study in preparation for a larger-scale investigation. A video summary of this paper can be found at https://youtu.be/oVYJdnBqTwQ. CCS Concepts: interaction design. Additional Key Words and Phrases: reinforcement learning, predictions, representations, virtual reality, communication • Human-centered computing → Empirical studies in HCI; Virtual reality; User studies; Empirical studies in 1 INTRODUCTION Technology increasingly relies on learning to improve performance. Autonomous systems that continually support human users are expected to soon need to learn continually even during use in order to perform well in their general and changing settings of interest (e.g., assistive technologies; [3,4,18,24]). Humans that use these systems will interact with technology that has a constantly changing level of competency and reliability, but the ramications of a system’s continual learning on human behaviour and human-machine interaction are not well understood. Here, we begin to investigate this interaction by considering a human involved in a timekeeping task, partnered with a machine agent that learns from a blank slate to help the human. In general terms, an intelligent machine of this sort is able to make predictions about the dynamics of the world that a human partner either cannot or does not want to compute on their own (possibly due to the diculty or time-consuming nature of the computation, [20], or the human’s inability to sense relevant information). In order to convey the benet of these predictions, it is natural that machine agents must be able to communicate information to the human [2,9]; learned communication is built upon relationships, and relationships can be built up through interaction [22,23]. Take for example your interactions with a wristwatch: if up to now it conveyed accurate time-information to you, you would have every reason to continue trusting its information the next time you consulted it. If its degree of competency degraded for some reason, and the information communicated were incorrect, you would quickly lose trust in the device and look to other sources for the information you need. Now suppose instead that your wristwatch was not designed to convey regular time intervals, but instead predict the onset of stochastically reoccurring events. How would your interactions with your wristwatch be aected by the fact that the device must continually learn, update, and change its behaviour while you are using it? In this paper we describe a pilot human-agent interaction study, investigating how time-based prediction agents can augment human predictions, and how the relationship between the human and agent develops as the agent develops competency. Specically, we describe and compare two simple agents that learn to predict future stimuli using general value functions ([26]; from the eld of reinforcement learning), and communicate those predictions to a human participant using Pavlovian control (which maps predictions to a small set of actions, [12]). We introduce a virtual reality (VR) task designed to assess human-agent interaction in a time-interval prediction task. VR is a compelling tool for human-computer interaction (HCI) research because it is immersive, allows exibility and control for experiment parameters, and enables measurement of human movements which provide a window into decision-making [6]. VR also requires physical participation—due to COVID-19, we were unable to recruit external participants. We took this as an opportunity to engage in the present work: a thorough preliminary investigation in search of interesting trends and themes that might deserve careful investigation with respect to continual learning during human-machine interaction. 2 BACKGROUND 2.1 Prior Work on Human Interaction with Learning Systems Human interaction research regarding autonomous systems spans from early software interfaces for email and calendar applications [11] to more complex and personal domains such as the control of prosthetic limbs [18,21], and has included a wide variety of automation techniques. Automation has traditionally been hand engineered to provide reliable performance, and therefore reliable human interaction. More recent machine learning systems are typically pre-trained before deployment, after which their parameters remain xed. Research specically involving interaction with continually learning algorithms has hitherto mainly focused on investigating agent learning dynamics using human interaction as part of the learning signal [10]. Autonomous systems that learn from human signals are important technologies, but system learning dynamics are inherently intertwined with interaction dynamics. Amershi et al. [ convincingly argue the case for separating human interaction from agent learning in order to study “how people actually interact—and want to interact—with learning systems”. They describe case studies involving people interacting with machine learning systems, and by specically focusing on the human component of the interaction, they are able to discover novel modes of interaction, unforeseen obstacles, and unspoken assumptions about machine learners. A meta-review of factors that aect trust in human-robot interaction [ behaviour, predictability, and failure rates greatly aect human trust in autonomous systems, justifying a system-specic investigation of human interaction with RL-based systems as distinct from machine learning systems. The particular feature of the RL algorithm that we study that distinguishes it from other autonomous systems and warrants direct investigation is continual learning during the course of a task, and the eect that will have on human interaction. 2.2 General Value Functions Reinforcement learning [ through a process of trial-and-error. The value of a state (a prediction of how much reward can be expected in the future from that state) is learned by incremental updates to a value function corresponds to the horizon of the prediction, and is typically between 0 (for next-step predictions) and 1 (for an innite horizon). By substituting any signal of interest (called a cumulant, a general value function (GVF) GVF represents a prediction question: what will be the total accumulated value of some signal of interest over the next specied time window? Equation 1 gives the formal GVF denition, for a simple xed- In practice, an agent learns to approximate the above value by interacting with a stream of states and corresponding cumulants. Let where 𝑤 where 𝛼 is a scalar learning rate and 𝑒 ∈ R 2.3 Pavlovian Control Inspired by prediction learning for reexive control in animals [ use of a GVF to predict an external stimulus, coupled with a xed reexive control policy dependent on that prediction (c.f., [3,12 stimulus is below a certain threshold a communication signal [ of the prediction, as well as the amount of advance notice needed before the external stimulus in order to take action. 𝑥 (𝑠) ∈ Rbe a feature vector summarizing the state𝑠. We approximate the value by𝑣(𝑠) ≈ 𝑤𝑥 (𝑠), ∈ Ris the weight vector at time 𝑡. We use the TD(𝜆) algorithm to update 𝑤on each time step: ]). A simple Pavlovian control policy emits a discrete action𝑎when the GVF prediction of the external 3 EXPERIMENTAL METHODS Our experiment situates a human participant in a virtual reality (VR) environment we call the Frost Hollow, wherein they must keep track of an external event that occurs on a roughly periodic schedule (c.f., [19]). They are paired with a machine agent that uses a GVF to predict the onset of this event, and cues the human when its prediction exceeds a threshold. We look at task performance, behavioural dierences, and qualitative notes to compare two agent architectures against the control condition where the participant completes the task with no agent assistance. 3.1 Virtual Reality Environment The premise of the Frost Hollow task is that the participant stands in a “warm” center region of the environment (radius 0.165 m, participant position reported by the headset) to slowly gain heat, and must periodically dodge out of a hazard region (radius 1 m) when the wind blows to avoid losing heat. When standing in the center region, a heat gauge visible to the participant lls from 0.0 to 5.0 at a rate of 0.1875 heat/second (26.67 seconds to ll the gauge); when the gauge is full, the participant can raise one of their VR controllers above their headset to cache the heat gained as a point (one unit of game reward). When hit by the hazard, the participant loses 25 heat/second, so any hit longer than 200 ms will deplete the gauge. Cached points are not lost. Our VR environment (depicted in Figure 1) was implemented in Unity 2019.2.17f1 (Unity Technologies, USA) with Steam VR (Valve Corporation, USA) and presented to the participant via a Valve Index headset and two handheld controllers (Valve Corporation, USA; headset max render rate of 144 Hz) at a base Unity time step length of approximately 8ms (VR protocol follows from prior work [17]). Detailed descriptions of the audiovisual presentation of the environment are available from Pilarski et al. [16]. We studied three inter-stimulus-interval (ISI) conditions between the hazard pulses: xed, drifting, and random. The base ISI was set to 20s (as measured from the falling edge of the pulse to the next pulse’s falling edge); the hazard pulse duration was 4s in all conditions. For the random condition, the inactive portion of each ISI was varied uniformly by [-4s, 6s] between 12s and 22s in length. For the drift condition, the inactive portion of each ISI was shifted by a uniform random amount between [-2s, 2s] from the previous duration, with all ISI durations outside [12s, 22s] cropped to the extremes of the range. When the hazard pulse was active, the participant’s left hand-held controller vibrated, and a visual bloom stimulus was presented on hazard contact; communication from the agent was presented as vibration in the right hand-held controller (c.f., [5, 14]). 3.2 Agent Architectures Two agent architectures are compared which dier only in the way that they represent the passage of time between stimuli: a bit cascade (BC) representation, and a tile-coded trace (TCT) representation (depicted visually in Figure 2). These representations were motivated by and modeled after biological models of time-keeping in animal brains [ representation is modelled after population clocks (sequentially ring chains of cells), while the TCT representation is informed by ramping models (changes in the tonic ring rate of cells or cell populations). The bit cascade representation involves a one-hot vector of 40 features which activate sequentially, with each feature being active for 0.5s. The tile-coded trace representation also involves a one-hot vector of 40 features which activate sequentially, but the activation timing for each feature is prescribed by an exponential decay trace with a per-step decay rate of 0.998. Both the BC and TCT representations restart their sequence (i.e. the rst feature is active) immediately after the hazard pulse deactivates. The timing parameters for both representations were set so that both used roughly the same number of feature bins when presented with an ISI of 20s. Learning parameters were empirically determined for an acceptable learning speed over a 5 minute trial time, resulting in determined to give adequate lead-time for a human participant in advance of a pulse after learning had converged, resulting in handheld controller when its prediction rose above discarded and re-initialized to zero between trials so the agent learned from a blank slate for each trial. 3.3 Experiment and Analysis Protocol We engaged a single participant (male, age 40, no history of sensorimotor impairments) who was also a member of the study team due to COVID-19 constraints (see Section 8), and followed our approved human research ethics protocol. This participant had a deep understanding of the task and dynamics, but was not practiced with the particular conditions. The study followed a within-participant 3 (ISI type) x 3 (agent type) design; experimentation took place over the course of ten sessions, each consisting of nine trials that were ve minutes long (one for each pairwise combination of [xed ISI, drifting ISI, random ISI] and [no agent, TCT agent, BC agent]). Trial order was randomized and blinded to the participant, and the initial ISI duration for the xed and drifting conditions was randomized to further obfuscate the conditions. Each individual session was conducted in roughly one hour, with small breaks between each of the trials for the participant to remove the headset and drink water or write qualitative notes. Sessions were spread over a one month collection period, with one or two sessions per day on data collection days. This protocol was found to be slightly physically fatiguing and moderately cognitively fatiguing, depending on the trial. To avoid injecting biases into the Fig. 2. Representations of time used in this experiment. Time (state) is represented as a one-hot vector of features which activate according to a trace function which resets at the falling edge of each stimulus pulse. 𝜏 =10 for both agents. The xed control policy was set such that the agent vibrated the participant’s Table 1. Statistical analysis results. Comparisons are made across assistant pairings (N = no agent; BC = bit-cascade agent; TCT = tile-coded trace agent) for each ISI condition. Significance (𝛼 =0.05) is indicated in bold text. For Friedman’s tests,𝜒(2) =6.20. analysis, the team member who acted as participant for the study did not re-engage with the study until both qualitative and quantitative analyses were completed by other team members. Statistical analyses were conducted to determine whether for this participant there were any dierences in performance across agent types. Data violated assumptions of normality in nearly every comparison, so non-parametric methods were used. Data were grouped pair-wise by session, so Friedman’s tests were conducted followed by Wilcoxon Signed-Rank tests with a Holm-Šidák correction for multiple comparisons. Signicance is reported in Figure 3, in all cases at the family-wise𝛼 =0.05 level. Specic results of the statistical analyses are reported in Table 1. Fig. 3. Performance metrics. Bars represent the mean over trials for each metric, normalized by the maximum possible value of that metric. Error bars represent the 95% confidence interval. N = no agent; BC = bit-cascade agent; TCT = tile-coded trace agent. (a) Mean interval between signal from agent and hazard onset over trial length. The minimum useful lead time (dashed line) before the hazard pulse (doed line) is 0.89 seconds, and corresponds to the participant’s mean exit speed. It does not include reaction time. (b) Time interval between signal from agent and goal region exit, shown as a trajectory over the length of trials. Negative data indicate the participant leaving the goal region in advance of the agent’s signal. Fig. 4. Data are shown as the mean (solid line) and 95% confidence interval (shaded region) of the data for each pulse. Due to the randomization of the starting ISI and fixed trial duration, some trials with shorter ISIs presented more pulses than others. This led to the occurrence of one or two trials with high pulse count (>14), resulting in the large confidence intervals at the ends of these plots. 4 QUANTITATIVE ANALYSIS Looking rst at overall task performance (Figure 3a), we see a small (and not statistically signicant) increase in performance in the xed ISI condition when the participant was paired with either agent. For the more dicult conditions where the ISI changes over the course of the trial, there is no clear dierence in overall task performance depending on agent pairing. In general, these results suggest that overall task performance is not a clear indicator of any dierences between human-agent pairings in this setting. Figure 3b shows dierences in the proportion of time-steps where the participant was hit by the hazard. In the xed ISI condition, the participant spends less time being hit by the pulse when paired with either agent as compared to none. In the random ISI condition, the participant is hit by the pulse less when paired with the TCT agent than when paired with the BC agent, or no agent. Figure 3c displays the participant’s heat gain in each condition, which corresponds to the proportion of time spent in the goal region. Dierences here appear in the more challenging conditions, where the participant spends less time in the goal region when paired with the TCT agent than when paired with the BC agent. Considering the charts of Figure 3 together, it appears that the participant engages in more cautious behaviour when paired with the TCT agent as compared to the BC agent (they gain less heat, and are hit by the hazard less often), while attaining comparable task performance. This result suggests possible dierences in participant behaviour across agent pairings. We are able to examine agent learning directly because the agent’s learning of the task does not depend on the participant’s actions. Figure 4a shows the mean interval between the agent signal and the onset of the hazard pulse, for each pulse over the length of the trials. This interval can be interpreted as “how long before the onset of the pulse did the agent’s prediction of the pulse rise above the threshold for signalling?”. When the agent’s cue is less than 0.89s before the hazard (above the dashed line), the signal doesn’t give the participant enough time to react given how long it takes to leave the hazard region. In the simplest prediction task with xed ISI, the BC agent is unable to reliably give a useful signal (below the dashed line) until after about the sixth or seventh pulse while the TCT agent is able to give a reliably useful signal after only the second pulse. More challenging conditions introduce more variance in these intervals, but the trend remains that the TCT agent provides useful signals earlier, and more reliably. This is because the BC representation has ner feature bins in the region near the pulse compared to the TCT representation, leading to more accurate but slower learning. The length of time between the agent’s signal and the participant’s exit from the goal region is plotted in Figure 4b. A negative value on this chart indicates that the participant left the goal region before being cued by the agent. Here, we see that the participant exhibits clear behavioural dierences when interacting with each agent. When paired with an agent with a TCT representation, the participant nearly always waits for the agent signal before leaving the goal region (data above the dotted line). When paired with an agent with a BC representation, the participant is much more likely to exit the goal region before the agent gives a signal. In the xed ISI condition, when paired with the TCT agent, the participant seems to move after the agent’s cue as early as the second or third pulse of a trial. Under the same conditions, when paired with a BC agent, the participant relies entirely on their own internal timing. For the more dicult conditions, the participant eventually moves after the cue of either agent, but aligns their movements with the TCT agent’s cue more readily than with the BC agent’s cue. While it is tempting to interpret this feature of the data as the participant relying on the TCT agent’s cue more than the BC agent’s cue, there is insucient evidence from these charts alone to conclude how the participant is using either signal, as we will see in Section 5. 5 QUALITATIVE ANALYSIS Qualitative data was gathered by the participant after each session in free-form text, prompted but not restricted by the following questions. Experimenter-developed questions were posed by the member of the study team conducting the qualitative analysis at the outset of the trials. Participant-developed questions were generated independently by the member of the study team acting as the participant, and evolved as the study progressed. Experimenter-developed questions: • Are you trying to gure out how the agent (and environment) work? • For the whole trial? • If not, did you gure it out or just start to trust it? • After time, or successes? • How much do you notice or think about the other agent at the beginning? The middle? The end? Participant-developed questions: •Changes in when and how I counted: did I count from the start of the trial? Did I shift to just counting from the agent cue and not counting from the beginning? When did I shift between these and under what conditions or observations on timing? • What agent behaviours did I like and not like? • Adaptation rates: what were my expectations on response or learning times for agents? • Thinking of agents as adaptive systems / predictors or not? • What conditions did I lose condence in the machine; when did I gain condence? • When did trust in the agent occur quickly? Discourse analysis seeking recurring sentiment and themes indicated that trust was a major component of the participant’s interaction with the system, which aected other factors including cognitive load and use of the agent’s cue in unexpected ways. The participant noted that they trusted the agent more when it was demonstrably correct earlier in the trial. Once trust was built, they noticed a decrease in cognitive load: “With trust in my agent, I can let [my] mind wander”. Notes such as “[The] agent helps me feel like I have a lower b ound of safety once it is trained, and then can cho ose my risk based on its feedback” suggest that the participant engaged with the task actively and strategically, and used the agent’s cues as part of that strategy in more complex ways than rote cue-to-movement. In fact, with sucient experience with the agent, the participant would sometimes engage in risky behaviour: “I was at times racing the pulse; the agent would cue me but I would see the heat bar almost full and then gamble that it would ll fast enough before [the hazard] came, given what I knew about the relationship between cue and future pulse.” Even in cases where the agent inadequately predicted the hazard, the participant still used the agent signal as information to inform their strategy, but relied on their mental timekeeping to inform their movements. Regarding these situations the participant notes: “it was not fast enough to be useful in advance of [the hazard], so I mainly used it as a checksum”, indicating that they veried their mental timekeeping by comparing it against their acquired knowledge of how the agent keeps time. While this particular behaviour is likely unique to participants familiar with GVF agents of this nature, the anecdote provides a clear example of how a user’s mental model of an agent will aect their interactions with it. It also indicates that evaluating future participants’ understanding of how the agent learns will be key to understanding their interactions. 6 COMPARING QUANTITATIVE AND QUALITATIVE RESULTS In both the quantitative and qualitative analyses we see human trust of the agent emerging as an important theme. The participant’s notes suggest that using the sign of the signal-to-exit interval (Figure 4b) as an indicator of human trust might miss parts of the picture, since the participant makes use of the agent signal in other ways than as simply a cue to move. Other quantitative measures of trust should be sought, to corroborate this interpretation. One particular notion of intense trust called out in the participant notes (when the participant is “racing” the pulse, caching points after the agent signal but before the hazard) is also visible in the quantitative data. Of the 14 instances where a point caching event is recorded after an agent signal and before a hazard, 13 of these instances occurred when the participant was paired with the TCT agent. This risky behaviour with the TCT agent contrasts with the indications from Figure 3 that the participant behaved more cautiously with the TCT agent. Pairing this contrast with the qualitative discussion, we see that with high levels of trust in the agent, the participant is able to more exibly choose a strategy, behaving boldly or cautiously as the situation warrants. It should however be noted that (as shown in Figure 4a) the TCT agent reliably gives more lead-time than necessary before the pulse, leaving time for pulse racing that the BC agent does not, meaning that pulse racing may not be a fair indicator of trust. 7 DISCUSSION Specic quantitative and qualitative measures to assess human trust in the agent would be particularly informative for future studies, especially if such measures could assess changes in levels of trust over the course of a trial or across sessions. One such task modication might involve the introduction of a secondary, voluntary and cognitively demanding task that could be performed simultaneously while gathering heat. While engaged with the secondary task, the participant would need to place trust in the agent to keep track of the timing in the primary task (i.e., eect a form of cognitive ooading [20]), making engagement in the secondary task a good measure of trust. Supposing that future studies with a direct measure of trust conrm that participants trust the TCT agent more than the BC agent, two points of discussion emerge. First, the apparent dierences in levels of trust between the types of agent can only be attributed to the dierent representations of the agents, as all else is equal. The threshold and representation bin-widths in this experiment were chosen considering late-trial performance, so that once the predictions stabilized both representations would give roughly equal notice before a pulse. A lower threshold or wider feature bins would likely have allowed the BC agent to provide reliably useful signals earlier in the trials. Understanding the relationship between feature representation and threshold levels in both early and late-learning contexts will be important for any future studies or applications making use of Pavlovian signalling for communication. Second, the participant in this experiment displayed more richly varied strategies with the TCT agent than the BC agent, presumably because of a greater degree of trust. Specic assessments regarding how participant strategies are aected by trust in the agent may be illuminating, and should involve specic metrics to assess changing strategies over time. Finally, using a pre-trained agent as a baseline comparison will be necessary to assess the eect of active learning on these measurements. 8 LIMITATIONS The generality of this pilot study is limited by our use of a single participant who was also a member of our study team. While blinded from the particular conditions they were interacting with, they were deeply familiar with the agent architectures, task dynamics, and learning machines in general. We expect that the introduction of naïve participants will also involve a co-learning phase at the beginning of sessions where the participant and agent are both learning the task simultaneously. Since we found early interactions to be of great inuence in trust-building with our expert participant, we expect that a co-learning phase will aect trust, but make no hypothesis about what that eect might be. 9 CONCLUSIONS AND FUTURE WORK This pilot study examined an approach to agent-human support characterized by real-time machine learning and straightforward ongoing interactions; our results suggest that trust in the system’s capabilities is a major component of a human’s interaction with a continually learning system. There are also indications that this trust may be dependent mainly on early interactions with the system, while the agent is still developing competency. Future studies should include metrics that specically measure trust, and should include analyses to determine possible correlations between levels of trust and agent competence. There may also be correlations between levels of trust and strategic behaviour. Finally, a future study should include a greater number of participants, with a diversity of experience in interacting with learning machines. For other future time-based prediction experiments or applications involving human actors with machine agents, we make no particular recommendations about representation or threshold choices, as we understand these to be task-specic. We do however stress the importance of these choices, and recommend that they be made with both early and late learning stages in mind, and considering the interaction between the human and machine’s actions.