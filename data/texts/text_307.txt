Cloud-based software has many advantages. When services are divided into many independent components, they are easier to update. Also, during peak demand, it is easier to scale cloud services (just hire more CPUs). Hence, many organizations are partitioning their monolithic enterprise applications into cloud-based microservices. Recently there has been much work using machine learning to simplify this partitioning task. Despite much research, no single partitioning method can be recommended as generally useful. More specically, those prior solutions are “brittle”; i.e. if they work well for one kind of goal in one dataset, then they can be sub-optimal if applied to many datasets and multiple goals. In order to nd a generally useful partitioning method, we propose. This new algorithm extends the CO-GCN deep learning partition generator with (a) a novel loss function and (b) some hyper-parameter optimization. As shown by our experiments,generally outperforms prior work (including COGCN, and others) across multiple datasets and goals. To the best of our knowledge, this is the rst report in SE of such stable hyperparameter optimization. To enable the reuse of this research,is available on-line at https://bit.ly/2WhfFlB. Data mining is a powerful tool but, like any other software system [35], analysts are often puzzled by all the options for the control settings. For example, consider the task of converting monolithic enterprise software into cloud microservices. For the task, it is a common practice to apply some clustering algorithm to decide how to break up the code into𝑘smaller microservices. A common question asked by programmers is “what is a good value for𝑘"? More generally, across all the learners used for microservice partitioning, currently there is little support for selecting appropriate control settings [11, 36]. Tools that can automatically learn settings for data miners are called hyper-parameter optimizers (HPO). These tools can learn (e.g.) good𝑘values while also optimizing for other goals including cluster coherence (which should be maximized) and coupling (which should be minimized). But HPO suers from hyper-parameter brittleness. For example, Tu and Nair[34]reported that if an optimizer works well for one kind of goal in one data set, they can be suboptimal if applied to multiple datasets and goals. In the case of redesigning software monoliths as cloud microservices, Yedida et al. [36]recently reported that dierent HPO tools perform best for dierent sets of goals being explored on dierent datasets. This eect can be seen in Figure 1a. In the gure, useful methods are shown ingreen. Yet, visually, Figure 1a exhibits a “patchwork quilt” appearance where the rankings vary widely. From Figure 1a we observe that no specic prior partitioning method can be recommended as generally useful. Thus, we consider this as a signicant problem. As designs get more complex, partitioning methods become very slow [36]. For example, at the time of this writing, we are running our algorithms for an industrial client. That process has taken 282 CPU hours for 1 application. The premise of this paper is that analysts should not be asked to run multiple partitioning algorithms, especially when each of those algorithms runs very slowly. Instead, we should be able to oer them one partitioning method that is generally most useful across a wide range of problems. To nd a generally useful partitioning methods, this paper seeks HPO tools that perform best across multiple datasets and goals. Figure 1: Ranking partitioning methods (shown on the xaxis) across the data sets of Table 4.1 using the statistical methods of §4.5. Colors denote the comparative ranks of different methods using the y-axis goal metrics (dened in Table 1). Best metho ds are ranked as #1 and shown in green . Table 1: Yedida et al. [36] report ve widely used metrics to assess the quality of microservice partitions. In this table, [-] denotes metrics where less is better and [+] denotes metrics where more is better. For details on how these metrics are dened, see §4.4. Note that not all the metrics of this table appear in Figure 1 since, as discussed later in this paper, some of these are just synonyms (since they exhibit very tight correlations). Thus, we propose, which is a novel combination of optimization (using Bergstra’s hyperopt tool [6]) and a novel weighted loss function. As shown in Figure 1b,generally works well across multiple goals and data sets. To understand the benets of, we investigate ve research questions. RQ1:How prevalent is hyper-parameter brittleness? We verify Yedida et al. [36]’s results whether hyper-parameter optimizers are brittle and not useful across multiple goals and data sets. The verication is crucial to set the motivation for our contribution. RQ2:Is hyper-parameter optimization enough to curb optimizer brittleness? We will verify if the standard hyper-parameter optimization methods are enough to solve brittleness. RQ3:If hyper-parameter optimization methods are not enough then we investigate How else might we x the brittleness problem? Here we check the results of augmenting hyper-parameter optimizers with our new weighted loss function. RQ4:Doesgenerate “dust" or “boulders"? Here, we show thatavoids two anti-patterns. Specically,does not create “boulders” (where all functionality is loaded into one partition). Nor does it build “dust” (where all partitions contain only one class each). RQ5:How useful is the loss function? Here we runwith without the loss function (just a nal sanity check). The novel, signicance, soundness and veriableness of this work is as follows.Novelty:has not been published previously. Using, we are the rst to oer stable advice on hyperparameter optimization for microservice partitioning. Signicant:To use cloud-based software, organizations need to transition their current services from monolithic designs to much smaller sets of cloud services. With our approach, microservices can be created relatively cheaply and built incrementally by porting existing monolithic code to the cloud. Sound:Our paper emphasizes the issues raised by RQ1 and RQ2, and then, based on motivation from literature, proposes a detailed but general approach for automated microservice partitioning. Also our empirical results were checked via state-of-the-art nonparametric eect size and signicance tests. Further, this study is far more detailed than prior work since it explores multiple partitioning methods and multiple case studies (and prior work [11,15,16,22,23] tended to explore just one or two partitioning methods). Veriable and transparent:To support reproduction and replication, all our code and data is open-source and on-line. The rest of this paper is structured as follows. We provide a detailed background on the problem and the various attempts at solving it in §2. We then formalize the problem and discuss our method in §3. In §4, we discuss our experimental setup and evaluation system. We present our results in §5. Next, we show how the crux of our approach extends beyond this problem in §6. We discuss threats to the validity of our study in §7. Finally, we conclude in §8. Before that, we digress to make the following point. The message of this paper is that (a) for microservice partitioning, prior work had not produced a generally useful hyper-parameter optimization system; and (b) such a generally useful method can be achieved by combining deep learning and hyper-parameter optimization with a novel loss function. To be precise, by “generally useful”, we mean “generally useful across the data sets and metricsexplored thus far”. It is an open question, worthy of future research, to test if our methods apply to other datasets and goals. Daya et al. [9]report on the long queue of applications waiting to be ported to the cloud. Once ported to cloud-based microservices, code under each microservice can be independently enhanced and scaled, providing agility and improved speed of delivery. But converting traditional enterprise software to microservices is problematic. Such traditional code is monolithic, i.e., all the components are tightly coupled, and the system is designed as one large entity. Thus, some application refactoring process is required to convert the monolithic code into to a set of small code clusters. When done manually, such refactoring is expensive, time-consuming, and error-prone. Hence, there is much current interest in automatically refactoring traditional systems into cloud services. There are many algorithms and partitioning goals that can guide the partitioning of traditional enterprise software into cloud applications: •Table 1 lists the various coupling and cohesion goals used by prior work on partition generation. Note that in column one, any goal marked with “[-]” should be minimized and all other goals should be maximized. •See also Table 2 which lists some of the partitioning tools, as well as their control parameters. In the table, the core idea is to understand what parts of the code call each other (e.g., by reecting on test cases or uses cases), then cluster those parts Table 2: Hyper-parameter choices studied in this paper. All the methods take runtime traces (or uses cases) as their input. These methods return suggestions on how to build partitions from all the classes seen in the traces. For further details on these partitioning algorithms, see §3. of the code into separate microservices. These algorithms assess the value of dierent partitions using scores generated from the Table 1 metrics. In such a rich space or options, dierent state-of-the-art approaches may generate dierent recommendations. Yedida et al. [36]concluded that looking at prior work, we seem to have a situation where analysts might run several partitioning algorithms to nd an algorithm that performs the best based on their business requirements. For example, analysts might prefer Mono2Micro [16,17] over others considering the clean separation of business use cases where as they might prefer CoGCN [11] and FoSCI [15] for low coupling. As stated in the introduction, this is hardly ideal since these partitioning algorithms can be slow to execute. Accordingly, this paper seeks optimization methods that support partitioning for a wide range of data sets and goals. Informally, partitioning algorithms take domain knowledge and propose partitions containing classes that often connect to each other. For example, given a set of uses cases or traces of test case execution: •Identity the entities (such as classes) used in parts of the uses cases/test cases; • Aggregate the frequently connecting entities; • Separate the entities that rarely connect. This can be formalized as follows. Consider classes in an application 𝐴asCsuch thatC= {𝑐, 𝑐, . . . , 𝑐}, where𝑐represents an individual class. With this, we dene a partition as follows: Table 3: Deep learning: a tutorial. In this work, deep learning is used by both CO-GCN and DEEPLY . A deep learner is a directed acyclic graph. Nodes are arranged into “layers", which are proper subsets of the nodes. Each node computes a weighted sum of its inputs, and then applies an “activation function", yielding its output. Deep learning (DL) [13] is an extension of prior work on multi-layer perceptrons, where the adjective “deep” refers to the use of multiple layers in the network. The weights form the parameters of the model, which are learned via backpropagation [25] using the rule𝜃 = 𝜃 − 𝜂∇Lwhere𝜃represents the parameters of the model,𝜂is the learning rate, andLis the loss function (described below). A deep learner with𝐿layers produces a predictionˆ𝑦, and the network learns parameters by using gradient descent to minimize the loss function L (𝑦,ˆ𝑦). This loss function can be arbitrarily designed, as done by the authors of CO-GCN [11] to suit the specic needs of the application. For static learning rates, there has been an increased interest in learning rate schedules, where𝜂is replaced by a function𝜂 (𝑡), where𝑡is the iteration. The proposed schedules have both theoretical [29,39] and empirical [30,31] backing. Generally, however, all the papers agree on the use of non-monotonic learning rate schedules. Specically, Smith[30]argues for cyclical learning rates, where the learning rate oscillates between(𝜂, 𝜂), a preset range for a specied number of cycles. More recently, Smith and Topin[32]proposed a “1cycle" learning rate policy, where a single cycle is used, and showed this to be eective, but simple. Dropout [33] is a technique which involves removing some nodes with a probability𝑝during training, and adjusting the weights of the model during testing. Srivastava et al. [33]argues that this enforces sparsity in the model weights, which improves the model performance and makes it more robust to noise. Definition 1. A partition Pon Cis a set of subsets {𝑃, 𝑃, . . . , 𝑃} such that •Ð𝑃= C, i.e., all classes are assigned to a partition. • 𝑃≠ 𝜙 ∀𝑖 = 1, . . . , 𝑛, i.e., there are no empty partitions. • 𝑃∩ 𝑃= 𝜙 ∀𝑖 ≠ 𝑗 , i.e., each partition is unique. A partitioning algorithm𝑓is a function that induces a partition Pon an application with classesC. The goal of a microservice candidate identication algorithm is to identify a function𝑓that jointly maximizes a set of metrics𝑚,𝑚, . . . ,𝑚that quantify the quality of the partitions, i.e., given an application characterized by its class set C, we aim to nd: where𝐵denotes the set of all partitions of𝐶and𝑝is the number of partitions. FoSCI[15] uses runtime traces as a data source. They prune traces that are subsets of others, and use hierarchical clustering with the Jaccard distance to create “functional atoms". These are merged using NSGA-II [10], a multi-objective optimization algorithm to optimize for the various goals. Since, FoSCI relies on a somewhat older optimization algorithm (from 2002 [10]), and all the goals are given equal priority; this can lead to suboptimal results if the dierent goals have, for example, dierent scales. Bunch[23] is based on search techniques such as hill-climbing to nd a partition that maximizes two specic metrics. It is well established that such greedy search algorithms can get stuck in local optima easily [26]. Mono2Micro[16,17] collects runtime traces for dierent business use cases. Then, they use hierarchical clustering with the Jaccard distance to partition the monolith into a set of microservices. However, as noted by Yedida et al. [36], this approach takes as input, the number of partitions, which dierent architects may disagree on, or may not know the value of. CO-GCNusing the deep learning technology discussed in Table 3. More specically,CO-GCN[11] uses the call graph (built from the code) as input to a graph convolutional neural network [18]. They develop a custom loss function (dened in Table 3) that relates to the metrics being optimized for, and thus, the neural network can be seen as a black-box system that optimizes for(1). However, while their use of a custom loss function tailored to the goals is novel, their approach has several hyper-parameters that a non-expert may not know how to set, and their study did not explore hyper-parameter optimization . CO-GCN uses an exponential learning rate scheduling policy (see Table 3 for details), where the learning decays exponentially. In our approach, we instead use the 1cycle [32] policy, which has more experimental backing. CO-GCN also uses dropout [33], which involves removing some nodes with a probability𝑝during training, and adjusting the weights of the model during testing. Srivastava et al. [33]argues that this enforces sparsity in the model weights, which improves the model performance and makes it more robust to noise. CO-GCN is controlled by the settings of Table 2. Later in this paper, we show that there are several benets in using hyperparameter optimization to automatically select good subsets of these hyper-parameters. Note that (a) all our partitioning methods (CO-GCN and all the others shown below) utilize hyper-parameter optimization; (b) even better results can be obtained via augmenting HPO with a novel loss function (and that combination of HPO+loss function is what we call DEEPLY ). Summarizing the approaches in prior work, we note that they suer from the following limitations: (a)They have hyper-parameters that practitioners may not understand or know how to set. (b)They rely on techniques that can easily get stuck in suboptimal, local minima. (c)They treat all the metrics as having equal priority, when it may not be the case (e.g., some feature scaling may be needed, or the business priorities are dierent). To address the limitations, we propose an extension to the COGCN [11] deep learner.uses an hyper-parameter optimization technique that is known to deal with local optima better, and (b) a novel reweighting of the metrics based on data. When algorithms produce a highly variable output, then hyperparameter optimization , described in §3.5.2, can be used to automatically learn the control settings that minimize the variance. Specically, we use the fact that a hyper-parameter optimization algorithm, in searching for optimal parameters, must test dierent hyper-parameter congurations, each of which produces a dierent result. The results are aggregated together to form a frontier, from which we can pick the “best" sample. To do so, we notice that simply picking a model based on one metric sacrices the performance in others. Moreover, using a sum of all metrics has the disadvantages of (a) dierent scales for dierent metrics (b) learners doing well in the “easy" metrics but not in the others (c) ignoring correlations among the metrics. Therefore, we design a custom weighted loss function to choose the best sample. For further details on all the concepts in the last two paragraphs, see the rest of this section. 3.5.1 Feature Engineering.uses the deep learning technology discussed in Table 3. We format our datasets into the form required by a graph convolutional network Kipf and Welling[18]as suggested in CO-GCN Desai et al. [11]. A graph convolutional network models a graph as a pair of matrices(𝐴, 𝑋 ), where𝐴is the (binary) adjacency matrix and𝑋is the feature matrix. Our graph is characterized by nodes corresponding to classes, and edges corresponding to function calls between two classes. If a class is not used by any of the APIs published by the software, then it is removed from the graph. The adjacency matrix is trivially dened as𝑀=1 if an edge exists between the vertices 𝑖, 𝑗, and 0 otherwise. For the feature matrix, we follow the approach of suggested in CO-GCN Desai et al. [11]. Specically, we rst dene the entry point matrix𝐸 ∈ {0,1}(where𝑉is the set of classes, and 𝑃as the set of entry points. Entry points refer to APIs published by a software, each potentially for dierent functions. Then, for each such entry point (i.e., API), we consider the set of classes invoked in its execution. We consider the entry𝐸=1 if class 𝑖is part of the execution trace of entry point𝑗, and 0 otherwise.. Additionally, we consider the co-occurrence matrix𝐶 ∈ {0,1} such that𝐶=1 if both classes𝑖, 𝑗occur in the same execution trace, and 0 otherwise. Finally, we dene the dependence matrix 𝐷 ∈ {0,1}as𝐷= 𝐷=1 if class𝑖inherits from class𝑗or vice versa, and 0 otherwise. The feature matrix𝑋 ∈ R is the concatenation of𝐸, 𝐶, 𝐷(in that order), and is then rownormalized. CO-GCN also uses dropout [33], which involves removing some nodes with a probability𝑝during training, and adjusting the weights of the model during testing. Srivastava et al. [33]argues that this enforces sparsity in the model weights, which improves the model performance and makes it more robust to noise. 3.5.2 Hyper-parameter Optimization. Hyper-parameter optimization is the systematic process of nding an optimal set of hyperparameters for a model. In the specic case of optimizing CO-GCN, those parameters are shown in Table 2. Several approaches for hyper-parameter optimization now exist. Of note is the work by Bergstra et al. [5]which discusses three dierent approaches. In this paper, we use a newer, widely used hyper-parameter optimization algorithm called Tree of Parzen Estimators (TPE) [5]. TPE divides data points seen so far into best and rest, each of which are modeled by a Gaussian distribution. New candidates are chosen so that they are highly likely to be in the best distribution. Evaluating these candidates adds to the data points, and the algorithm continues. This algorithm was open-sourced by its authors in a package called hyperopt Bergstra et al. [6]. Therefore, in this paper, whenever we say “hyperopt", we mean TPE as implemented by this package. 3.5.3 Loss Function. Table 3 discussed loss functionsLthat oers weights to the feedback seen by the learner during the inner-loop of the learner process. Numerous researchers report that augmented loss functions can enhance reasoning: •Ryu and Baik[27]used reweighted Naive Bayes for crossproject defect prediction; •In a similar approach, a reweighted Naive Bayes was used by Arar and Ayan[4]for defect prediction and Yedida and Menzies[37]explored weighted loss functions in deep learners for defect prediction. •In the AI literature, Lin et al. [21]propose a “focal loss" function that uses an exponential weighting scheme. To the best of our knowledge, augmenting the loss function has not been previously attempted for improving microservice partitions. One challenge with designing an loss function is how to obtain the required weights. In, we use a sampling approach to obtain an empirical distribution of our metrics over 1,000 runs. The correlations between metrics (objectives) is shown in Figure 2. Specically, we rst run the algorithm 1,000 times to generate a set of metrics shown in Table 1. Then, we check the correlation among the metrics using the Spearman correlation coecients. If any two metrics have strong correlations we prefer to keep one to remove the redundant metrics for the optimization. Figure 2 indicates the correlations among the metrics. Clearly, some of the metrics are highly correlated. Therefore, we use a reduced set of metrics for evaluating our approaches. We set a threshold of 0.6 to prune the set of metrics. We observe that MQ has higher correlations with other metrics than SM. Since IFN and ICP are highly correlated across all datasets, we arbitrarily choose ICP. This leads to the nal set of metrics: BCS, ICP, and SM. Note that to ensure that we do not generate “dust" (individual classes in a partition) or “boulders" (monolithic partitions), we also include NED in the nal metric set. Based on these three metrics, we choose a loss function to pick one model from the frontier of best solutions. This loss function is where𝑤are weights assigned to each of the metrics𝑚, and𝑃 is the partition. We notice that the most eective yet simple way to set the weights was to set the weights for BCS and ICP to 1, and calculate the SM weights based on the correlations. Additionally, we divide the BCS weight by 10 (for an overall weight of 0.1) to normalize its value to the same order of magnitude as the other metrics. For SM, we set the weight to−1/corr(𝑆𝑀, 𝑀𝑄)(the negative sign is required since we wish to minimize this loss function). Finally, we add in a term for the NED metric, so that we do not generate “boulders" or “dust" or “boulders" (e.g., a monolith with 20 classes is not partitioned into 19 and 1). For this, we found that a weight of 0.2 was a good balance between all the metrics. Next, we change the clustering method used in CO-GCN Desai et al. [11]. Rather than using the k-means clustering, which assumes globular clusters, we use the spectral clustering, which can produce more exible cluster shapes [38]. We also change the cluster loss function to where 𝑥and 𝑥are data points. We also update the learning rate scheduling policy to the “1cycle" policy [30,32], which has been shown to lead to faster convergence (see Table 3 for details). We update the learning rate every 200 steps. Finally, we change all the activation functions in the network to ReLU (𝑓 (𝑥) = max(0, 𝑥)) [24], except the last layer (which we leave unchanged to𝑓 (𝑥) = 𝑥). Specically, like Desai et al. [11], we use an encoder-decoder architecture with 2 layers per each; each layer being a graph convolution as dened by Kipf and Welling [18]. Therefore, for an input feature matrix 𝑋 , we consider Algorithm 1 shows the overall approach. In the algorithm the lines in red (18-21,24) are from the CO-GCN algorithm (and all else is our extension). We rst collect correlation statistics using a Monte Carlo-style data collection (lines 1-3). Then we set weights (lines 4-5) and run hyperopt for 100 iterations (lines 8-13), while collecting the results of each run. The weighted loss function then picks the “best" candidate in line 14, and returns the optimal hyperparameters in line 15. The actual model training itself is delegated to CO-GCN-modified. We form the graph Laplacian in lines 1-3, and pretrain (for 1 epoch) the encoder(4)and decoder(5). We initialize the clusters in line 5, but use spectral clustering instead. Then, we run the actual training for 300 epochs In summary, our new methoddiers from CO-GCN in the following ways: •We use a better learning rate scheduling policy from deep learning literature (discussed in Table 3). •We update the activation functions used in the neural network (also see Table 3). •We run hyper-parameter optimization using a custom weighted loss function over a reduced space of metrics (discussed in §3.5.2). •We update the clustering method employed, and the corresponding term in the deep learner loss function (see §3.5.3). In this section, we detail our experimental setup for the various parts of our research. Broadly, we follow the same experimental setup as Yedida et al. [36]. However, that paper concluded that dierent optimizers performed dierently across datasets and metrics, and that there was no one winning algorithm. In this paper, using the method described in §3.5, we show that our approach wins most of the time across all our datasets and metrics. We use four open source projects to evaluate our approach against prior work. These are acmeair(an airline booking application), daytrader(an online stock trading application, jpetstore(a pet store website), and plants(an online web store for plants and pets). These applications are built using Java Enterprise Edition (J2EE), and common frameworks such as Spring and Spring Boot. In Table 4, we show statistics about the datasets (number of classes and methods) and the runtime traces that we used (number of traces, and their class and method coverage). We note that in future work, we could expand this to include (a) more datasets, and (b) bigger datasets. Algorithm 1: DEEPLY. Note CO-GCN-modied is a subroutine containing our changes to the original CO-GCN system. The the lines in red (18-21,24) are from the original CO-GCN algorithm (and all else is our extension). Input : graph dataset 𝐷 = (𝐴, 𝑋 ) Output : partitioned microservices ˜𝐴 ← 𝐴 + 𝐼Í pretrain the encoder and decoder using(4)and(5)respectively from [11] and 1cycle [32] Table 4: Statistics about the datasets used in this study. We use the Tree of Parzen Estimators (TPE) [5] algorithm from the hyperopt [19] package for hyper-parameter optimization . As discussed in §3.5, we useL(𝑃 )from(2)to guide hyperopt towards an optimal set of hyper-parameters. Table 2 lists the hyper-parameters that we tune, along with their ranges. We run the hyper-parameter optimizer for 100 iterations. We train for 300 epochs using an initial learning rate of 0.01. Following our literature review, we use the baselines shown in Table 2. All these approaches, except Mono2Micro and CO-GCN, were either open-source, or re-implemented by us. For fairness, we tune each algorithm with hyperopt for 100 iterations (which is the same as our approach). We use the same loss functionL(𝑃 ) shown in(2), but set all the weights to either 1 (for metrics we wish to minimize), or -1 (for metrics we wish to maximize), since the correlation-based weights are a novel idea for our approach. For a fair comparison with prior work, we must compare using the same set of metrics. We choose a total of ve metrics to evaluate our core hypothesis that hyper-parameter tuning improves microservice extraction algorithms. These are detailed below. These metrics have been used in prior studies, although dierent papers used dierent set of metrics in their evaluations. For fairness, we use metrics from all prior papers. These metrics evaluate dierent aspects of the utility of an algorithm that might be more useful to dierent sets of users (detailed in the RQ1), e.g., BCS evaluates how well dierent business use cases are separated across the microservices. Inter-partition call percentage (ICP) [17] is the percentage of runtime calls across dierent partitions. For lower coupling, lower ICP is better. Business context sensitivity (BCS) [17] measures the mean entropy of business use cases per partition. Specically, where𝐾is the number of partitions and𝐵𝐶is the number of business use cases in partition𝑖. Because BCS is fundamentally based on entropy, lower values are better. Structural modularity (SM), as dened by Jin et al. [15], combines cohesion and coupling, is given by where𝑁is the number of classes in partition𝑖,𝐾is the number of partitions,𝑐𝑜ℎis the cohesion of partition𝑖, and𝑐𝑜𝑢𝑝is the coupling between partitions𝑖and𝑗,𝑐refers to a class in the subset, 𝐶, 𝐶are two partitions,𝜎is a general similarity function bounded in[0,1], and the𝑖 < 𝑗condition imposes a general ordering in the classes. Higher values of SM are better. Modular quality (MQ) [23], coined by Mitchell and Mancoridis [23], is dened on a graph 𝐺 = (𝑉, 𝐸) as where is the indicator function, and𝐶, 𝐶are clusters in the partition. Higher values of MQ are better. The interface number (IFN) [23] of a partition is the number of interfaces needed in the nal microservice architecture. Here, an interface is said to be required if, for an edge between two classes in the runtime call graph, the two classes are in dierent clusters. Finally, the non-extreme distribution (NED) is the percentage of partitions whose distributions are not “extreme" (in our case, these bounds were set to min=5, max=20). However, as shown in Figure 2, some of these metrics are highly correlated with others. Therefore, to avoid bias in the evaluation and the loss function (i.e., if metrics𝑀and𝑀are correlated, and optimizer𝑂performs best on𝑀, it likely performs best on𝑀as well), we use a subset of these metrics. Specically, we prune the metric set as discussed in §3.5.3. Note that across all our gures and tables, a “[-]" following a metric means lower values are better, and a “[+]" following a metric means that higher values are better. For comparing dierent approaches, we use statistical tests due to the stochastic nature of the algorithms. According to standard practice [12], we run our algorithms 30 times to get a distribution of results, and run a Scott-Knott test as used in recent work [1,37] on them. The Scott-Knott test is a recursive bi-clustering algorithm that terminates when the dierence between the two split groups is insignicant. The signicance is tested by the Cli’s delta using an eect size of 0.147 as recommended by Hess and Kromrey[14]. Scott-Knott searches for split points that maximize the expected value of the dierence between the means of the two resulting groups. Specically, if a group𝑙is split into groups𝑚and𝑛, ScottKnott searches for the split point that maximizes E[Δ] =|𝑚||𝑙 |(E[𝑚] − E[𝑙])+|𝑛||𝑙 |(E[𝑛] − E[𝑙]) where |𝑚| represents the size of the group 𝑚. The result of the Scott-Knott test is ranks assigned to each result set; higher the rank, better the result. Scott-Knott ranks two results the same if the dierence between the distributions is insignicant. RQ1: How prevalent is hyper-parameter brittleness? Here, we ran the approaches from Table 2, 30 times each, then compared results from those treatments with the statistical methods of §4.5. The median results are shown in Table 5a and the statistical analysis is shown in Table 5b. Table 5: Results on all datasets. The top row shows our median performance scores over 30 runs, while the bottom row shows the Scott-Knott ranks (lower is better). Gray cells indicate the best result according to the Scott-Knott test. All algorithms were tuned by hyperopt. In those results, we see that across dierent datasets and metrics, those ranks vary widely, with little consistency across the space of datasets and metrics. For example: •For jpetstore, we see that the best algorithms for the three metrics are Mono2Micro, CO-GCN, and MEM respectively; •For acmeair, the best algorithms are Mono2Micro, and Bunch; •For plants, the best algorithms are FoSCI, CO-GCN, and MEM; •For daytrader, the best are Mono2Micro, CO-GCN, and Bunch. Hence we say: For all the data sets and goals studied here, there is widespread performance brittleness. As to why this issue has not been reported before in the literature, we note that much of the prior work was focused on one algorithm exploring one case case study. To the best of our knowledge, this is rst to perform a comparison across the same datasets and metrics and also propose a novel approach. Note that theseRQ1results generated the data summarized in Figure 1. RQ2:Is hyper-parameter optimization enough to curb optimizer brittleness? Here we check of brittleness can be solved by tuning the partitioning methods. Since all the algorithms in our study were tuned by hyperopt, the results of Table 5 show that even post-tuning, there is a large brittleness problem. Accordingly, we say: Hyper-parameter optimization does not suce to x performance brittleness. Since hyper-parameter optimization is not enough to tame brittleness, we ask: RQ3: How else might we x the performance brittleness problem? When we added the novel loss function (discussed above in §3.5.3), we obtained the results shown in the last line of the two tables of Table 5b. When measured in terms of ranks seen amongst the dierent populations, the last line of Table 5b is most insightful. Here we see thatwither performs either best or second best compared to all other optimizers across all our case studies. No other algorithm explored in this study comes even close to that performance. For a visual overview of these results, see Figure 1. Hence we say: Weighted losses together with hyper-parameter optimization x the brittleness problem. RQ4: Do we generate “dust" or “boulders"? Two anti-patterns of partitioning are “dust” (generating too many partitions) and “boulders” (generating too few partitions). To analyze this, Figure 3 shows the distribution of partition sizes seen in 30 random samples. From that gure we say that our methods usually generate small number of partitions. Also, since we are usually dealing with classes ranging from 33 in acmeair to 109 in daytrader, Figure 3 tells us that we are not generating partitions with only one item per partition (for example, on plants, the median size is 5.4% × 111 = 6 classes). Hence we say: Figure 3: Analysis of partitions generated by our approach. Distribution of class size percentages across datasets (median results). Our partitions are neither “boulders” nor “dust” Finally, we ask if we can simplify our approach. Therefore, our nal research question is: RQ5: How useful is the loss function? The standard way of testing if an implementation is too complex via an “ablation study” where some portion of the code is removed. Here, we run30 times without the weighted loss-based selection, and then report the results. The results of this ablation study can be seen in the second last line of the tables in Table 5. Note that the row “w/o loss" in Table 5b shows several ranks 3 and 4, Hence we say: The loss function is an essential component of DEEPLY . Yedida et al. [36]listed four lessons learned from their study. Our results suggest that some of those lessons now need to be revised. (1)We fully concur with Yedida et al. when they said “do not use partitioning methods o-the-shelf since these these tend to have subpar results.. That said, currently we are planning to package (in Python) thesystem. Once that is available then Algorithm 1 should accomplish much of the adjustments required to commission DEEPLY for a new domain. (2)Yedida et al. said “the choice of tuning algorithm is not as important as the decision to tune. The random and hyperopt optimizers performed dierently on dierent metrics and datasets, but in a broader view, shared the same number of highest statistical rankings. Therefore, the decision to tune is important, but the choice of algorithm may not be as important.”. We disagree since, as shown by Table 5b, our methods demonstrate an overall better results (as indicated by the statistical ranks). (3)Yedida et al. said “More research is required on tuning. While our results shows that tuning is better than non-tuning, we also show that no single approach is a clear winner across multiple datasets or multiple performance goals.”. We concur but add that state of the art in hyper-parameter optimization. (4)Yedida et al. said “There is no best partitioning algorithm... there is no one best algorithm across all metrics.” We agree, but only partially. The last row of Table 5b shows that even our preferred methoddoes not always obtain top rank. That said, At a higher level, hyper-parameter tuning is akin to searching for various options and guessing which one is the best; weighted loss functions directly encode business goals in this process, making it a more focused eort. For example, changing business goals can be trivially implemented in our framework, since the weighted loss function used in the hyper-parameter tuning is based on the metrics (i.e., no additional mathematical derivations need to be done). Importantly, business users can (a) make a choice among many from the frontier of best solutions of a size that they can choose, with the understanding that higher sizes mean more options but takes slightly longer (b) see the trade-os between dierent architectural design choices directly in terms of metrics (c) easily choose new metrics if they see t. That is, our approach provides businesses with exibility and transparency into the working of the model. While the deep learner itself is a black box, end users typically do not care about the internal workings of the model; only in interpretable ways to do better in their goals (this was studied by Chen et al. [8]). For businesses, we oer the following advantages: • Stability:Business can be assured that our approach will perform well across any dataset, no matter the metric used. This stability is important for time-constrained businesses who need some guarantees of good results before using a new approach. • Performance: Our approach achieved state-of-the-art performance across dierent and uncorrelated metrics on four open-source datasets. This high performance inspires condence in our approach for businesses looking to adopt an automated system for the architectural change. • Openness:Our code is fully open-source, but also modular. That is, businesses are not limited to our specic approach (which builds upon CO-GCN and uses hyperopt); rather, they are free to use our techniques to build upon any existing infrastructure that they may have (e.g., IBM Mono2Micro). The ideas of this paper extend to other tasks and domains. In this section, we elaborate on the specic ideas and their broader utility. Our approach is a general method that can be applied to any dataset, since the components themselves can be changed. For example, a dierent hyper-parameter optimizer such as Optuna [2] can be used instead of hyperopt, and a dierent weighting mechanism can be chosen instead of our correlation-based weights. Finally, a dierent set of preprocessing can be used. Therefore, a generalized version of our approach is: (1) Feature extraction.Dierent features sets can be extracted from code. For example, one might use code2vec Alon et al. [3], which transforms code snippets to xed-length vectors. (2) Hyper-parameter optimization.Any hyper-parameter optimization approach can be plugged in here, such as random sampling, TPE, DODGE, etc. (3) Loss-based selection.Any loss function can be applied to the frontier of best solutions to select a conguration. Moreover, the loss function of the learner itself can be modied, as done in CO-GCN. From a research standpoint, we oer the following: • Advancing the state-of-the-art:Our approach consistently outperforms all the prior state-of-the-art approaches we tested against across three dierent metrics, on four datasets. • Modular approach:Our approach can be adapted by changing the base algorithm (CO-GCN) and hyper-parameter optimization algorithm (hyperopt) and used on any dataset as discussed above. • Documenting the success of weighted losses:Our paper adds to the body of literature that documents the success of using weighted losses for dierent problems, possibly motivating future work to also use them. More generally, our idea can be applied to any multi-objective optimization problem, using the general method from our paper: produce a frontier of best solutions (through hyper-parameter optimization, swarm optimization, etc.) and use a weighted loss to choose the best candidate. This idea has been used implicitly in the deep learning eld. In particular, Li et al. [20], Santurkar et al. [28]show that a smoother loss function surface is benecial for optimization. In the case of Santurkar et al. [28], the adding of “skip-connections" in the deep learner modify its loss function, making it smoother and therefore easier to optimize in. This idea has also been used by Chaudhari et al. [7], who change the loss function to shape it into a more amenable form. Sampling bias:With any data mining paper, it is important to discuss sampling bias. Our claim is that by evaluating on four dierent open-source projects across dierent metrics, we mitigate this. Nevertheless, it would be important future work to explore this line of research over more data. Evaluation bias:While we initially considered comparing across all the metrics we found in the literature, we noticed large correlations between those metrics. Therefore, to reduce the eect of correlations, we chose a subset of the metrics and evaluated all the approaches across those. Moreover, in comparing the approaches, we tuned all of them using the same hyper-parameter optimization algorithm, for the same number of iterations. External validity:We tune the hyper-parameters of the algorithm, removing external biases from the approach. Our baselines are also tuned using hyperopt for the same number of iterations. Internal validity:All algorithms compared were tuned using hyperopt. Because our approach involves using weighted losses and other tweaks to CO-GCN, these w ere not applied to the other algorithms. In this paper, we presented a systematic approach for achieving state-of-the-art results in microservice partitioning. Our approach consists of hyper-parameter optimization and the use of weighted losses to choose a conguration from the frontier of best solutions. Broadly, the lesson from this work is: At least for microservice partitioning, weighted loss functions can work together with tuning to achieve superior results. We rst analyzed the existing state-of-the-art. Through this review, we noticed (a) highly correlated, and therefore redundant, metrics used in the literature (b) inconsistent comparisons being made in prior work (c) prior work showing performance brittleness across dierent goals and datasets. We argued that this creates an issue for businesses looking to use a microservice partitioning tool for internal software. Through Monte Carlo-style sampling, we chose a reduced, less correlated set of metrics, and used those as data for choosing weights for each goal, accounting for their dierent scales. We built upon an existing tool, CO-GCN [11], to buildin this paper, which xes the issues listed above. To the best of our knowledge, ours is the rst structured attempt at (a) reviewing the literature for a list of state-of-the-art microservice partitioning approaches (b) comparing all of them on the same datasets and the same metrics (c) xing the performance brittleness issue using weighted losses and tuning. We investigated the performance of the model on several datasets and checked whether the loss function was signicant. Finally, we discussed the broader impacts of the approach specied in this paper, generalizing the concept beyond the specic case of microservice partitioning. Our approach is extensible and modular, consistently outperforms other approaches across datasets and metrics, and can easily be adapted to any metric that an enterprise is interested in. Moreover, the two loss functions (for the deep learner and the hyperparameter optimization algorithm) can be tweaked to suit business goals. Our approach being modular leads to several avenues of future work, which we discuss in this section. Because we apply weighted losses at the hyper-parameter optimization level, we can apply the same approach using a dierent base algorithm than CO-GCN. Specically, we could build a Pareto frontier using a dierent state-of-the-art algorithm and then use our weighted loss function to choose a “best" candidate. Further, it would be useful to explore our methods on more datasets and metrics. In particular, it would be benecial to test our methods on large enterprise systems. In addition, businesses might be interested in how much faster our system is compared to human eort. Finally, our method (using weighted losses to guide exploration of the Pareto frontiers) is a general method that is not specic to microservice partitioning. Specically, since our losses choose from a Pareto frontier generated by a hyper-parameter optimizer, (a) the choice of optimizer is left to the user (b) the application that the optimizer is applied to can be freely changed. Therefore our methods might oer much added value to other areas where hyper-parameter optimization has been applied. This research was funded by [blinded for review]