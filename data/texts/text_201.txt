Domain adaptation of neural networks commonly relies on three training phases: pretraining, selected data training and then ﬁne tuning. Data selection improves target domain generalization by training further on pretraining data identiﬁed by relying on a small sample of target domain data. This work examines the beneﬁt of data selection for language modeling and machine translation. Our experiments assess the complementarity of selection with ﬁne tuning and result in practical recommendations: (i) selected data must be similar to the ﬁne-tuning domain but not so much as to erode the complementary effect of ﬁne-tuning; (ii) there is a trade-off between selecting little data for fast but limited progress or much data for slow but long lasting progress; (iii) data selection can be applied early during pretraining, with performance gains comparable to long pretraining session; (iv) data selection from domain classiﬁers is often more effective than the popular contrastive data selection method. Machine learning models, and neural networks in particular, beneﬁt from large training sets. However, for many application domains, the amount of training data representative of the inference conditions is limited. It is therefore common to train a model over a large amount of generic, out-ofdomain data while relying on a small amount of target domain data to adapt such a model. In the recent years, a large body of work has focused on leveraging large amount of web data to train neural networks for language modeling (Peters et al., 2018; Devlin et al., 2019) or translation systems (Bañón et al., 2020; Koehn et al., 2020). Such systems are then adapted to the target distribution, typically via ﬁne tuning (Liu et al., 2019; Raffel et al., 2020). This work studies data selection, an intermediate training phase that visits a subset of the out-ofdomain data that is deemed closer to the target domain. Previous work has proposed conducting a data selection step after pretraining (van der Wees et al., 2017a; Wang et al., 2018; Gururangan et al., 2020; Aharoni and Goldberg, 2020), either as a ﬁnal training stage or before regular ﬁne tuning. Data selection is meant to identify a subset of the out-ofdomain pretraining set which might be the most helpful to improve generalization on the target distribution. This selection is typically conducted by estimating the probability that each data point belongs to the target domain (Moore and Lewis, 2010; Axelrod et al., 2011). Recently, (Aharoni and Goldberg, 2020) introduced the use of domain classiﬁers for data selection. This work examines the beneﬁt of data selection for language modeling and machine translation. We compare different selection methods and examine their effect for short and long pretraining sessions. We also examine the beneﬁt of selecting varying amount of training data and the impact of selection on the subsequent beneﬁt of ﬁne-tuning. In addition to this novel analysis, our machine translation experiments compare the beneﬁt of selecting data with a classiﬁer based on source language, target language or both. The effectiveness of data selection is dependent on (i) the similarity of the pretraining data to the target domain data, (ii) the precision of the selection method to identify in-domain examples from the pretraining set, (iii) the extent to which training on the selected data is complementary to ﬁnetuning. This work focuses on selecting data from the pretraining set so (i) is ﬁxed. We show that (ii) beneﬁts from the use of domain classiﬁers, in particular, ﬁne-tuned pretrained language models, outperforming the more popular constrastive methods (eg. Wang et al. (2018)) in all settings that we tested. We present the ﬁrst analysis of (iii), which we refer to as the complimentarity of selected data to ﬁnetuning data. We show that some data selection methods can actually erode the effectiveness of subsequent ﬁne-tuning. In some settings, we even report that a poor complementarity of selection and ﬁne tuning can result in their combination reaching worse results than ﬁne tuning alone. Effective application of data selection requires careful selection of when to switch from pretraining to selection, how much selected data to train on and how long to train on selected data before switching to ﬁnetuning. Much of the previous work on data selection either evaluates small models that converge quickly (Moore and Lewis, 2010; Axelrod et al., 2011) or does not describe the extent of grid search over selection size, number of steps of pretraining and number of steps of training on selected data. We are the ﬁrst to analyze the hyperparameter selection tradeoffs for data selection on large neural models, where models may be undertrained (Liu et al., 2019) and evaluating many selection sizes may be prohibitively expensive. We evaluate data selection on checkpoints with variable numbers of pretraining steps and show that data selection provides consistent results between minimally and extensively pretrained models. We also show the challenges of searching over selection sizes because smaller selection sizes always converge more quickly but are outperformed by larger selection sizes trained for more steps. Our ﬁndings are the following: (i) the data selection mechanism must select data that is similar, but complementary to the ﬁne tuning dataset (ii) the amount of selected data introduces a trade-off between quick but limited improvements when limiting selection to the best data, and long lasting but slow progress when selecting more data with an overall worse quality, (iii) data selection techniques are not created equal and domain classiﬁers often outperform contrastive scoring, the most common data selection method, (iv) we propose three simple variants of domain classiﬁers for machine translation that can conditions the classiﬁer on either source, target or both. We demonstrate these ﬁndings on language modeling and two language pairs for neural machine translation. In Natural Language Processing (NLP), adaptation methods have been applied to language modeling (Moore and Lewis, 2010), machine translation (Axelrod et al., 2011; Daumé III and Jagarlamudi, 2011), dependency parsing (Finkel and Manning, 2009) or sentiment analysis (Tan et al., 2009; Glorot et al., 2011). With the growing popularity of neural methods (Collobert et al., 2011; Bahdanau et al., 2015; Goldberg, 2017), the adaptation of neural models via ﬁne tuning has become wide-spread for various NLP applications (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020). Data selection is another popular technique (van der Wees et al., 2017b; Wang et al., 2018) which can be used on its own or in combination to ﬁne tuning. Data selection is a common domain adaptation method. It has been been introduced before neural methods were popular (Moore and Lewis, 2010; Axelrod et al., 2011) and has later been adapted to neural networks (Duh et al., 2013; van der Wees et al., 2017b; Wang et al., 2018). Data selection relies on an intermediate classiﬁer which discriminate between in-domain and out-of-domain data. This classiﬁer is trained relying on the small indomain dataset and the large out-of-domain dataset and is then applied to the out-of-domain set to identify the examples closest to the targeted domain. Choosing a selection model and the amount of outof-domain data to select have a strong impact on the effectiveness of the selection methods (Aharoni and Goldberg, 2020; Gururangan et al., 2020). Our experiments explore these aspects, in addition to the complementarity of selection with ﬁne tuning. Data selection can be performed in multiple rounds, either to gradually restrict the out-ofdomain dataset to less and less data (van der Wees et al., 2017b) or to re-evaluate out-of-domain data as pretraining progresses (Wang et al., 2018). Data selection can also be performed as a continuous online process (Wang et al., 2018, 2021; Dou et al., 2020). Our work focus on single round data selection, the most common setting. The beneﬁt of dynamic selection effectiveness has shown to be variable (Wang et al., 2018) and its use involves deﬁning a complex schedule which is a research topic in itself (Kumar et al., 2019). Data selection for domain adaptation is also related to data selection for multitask learning. In that case, the out-of-domain dataset is composed of heterogeneous data from different tasks/domains and the training algorithm favor data from some tasks at the expense of others (Graves et al., 2017; Wu et al., 2020; Standley et al., 2020). Contrary to our setting, selection operates only at the task level and the association of training examples to tasks is already known. Multitask learning is an active area of research. This area has explored dynamic selection with reinforcement learning (Graves et al., 2017; Guo et al., 2019) as well as update projections to align out-of-domain gradients to in-domain gradients (Yu et al., 2020; Dery et al., 2021). Some of these ideas have later been investigated in the context of data selection for domain adaptation (Wu et al., 2018; Kumar et al., 2019; Wang et al., 2021). This section presents the selection method our experiments will focus on and introduce the trade-offs involved in choosing data selection hyperparameters. Domain adaptation has been introduced for application domains where data reﬂecting the inference conditions is only available in limited quantity. This setting considers that two training sets are available, a large generic out-of-domain dataset and a small specialized in-domain dataset from the targeted domain (Søgaard, 2013). Classical machine learning assumes that training and test data originate from the same distribution. At the same time, statistical modeling reaches better generalization performance with large training sets (Vapnik, 1998). Domain adaptation therefore faces a tension between using a large data set with a distribution possibly far from the test conditions and using a small training set matching the test condition. Data selection tries to address this dilemma. It examines the out-of-domain data and identiﬁes training examples likely to be most effective at improving the in-domain training loss. For neural methods, data selection is often used in conjunction with ﬁne tuning in a three phases process, as shown in Algorithm 1. In a ﬁrst phase, the model is pretrained on all the out-of-domain data. In a second phase, an intermediate classiﬁer is trained to distinguish in-domain from out-of-domain data, using both training sets. The classiﬁer is applied to the out-of-domain set to identify examples considered close to in-domain data. The intermediate classiﬁer is then no longer required and the main model is trained on the selected data starting from the pretrained parameters. Finally, the main model is ﬁne tuned, i.e. it is trained on the small in-domain training dataset starting from the parameters after the selection phase. Input: D, T out and in domain train sets. Output: θ trained model parameters. Function Select(D, T , n): w ← trainClassiﬁer(D ∪ T ) Y ← classify(w, D) return argtop(Y ) Function Main(D, T ): θ← initParam() θ← train(θ, D) #pretraining D← select(D, T, n) θ← train(θ, D) θ← train(θ, T ) #fine-tuning return θ Contrastive Data Selection:Commonly, classiﬁcation is done by estimating the probability that a given out-of-domain examplexbelongs to the target domain,P (T |x). Such an estimation can be done by contrasting the likelihood estimated by in-domain LM,P (·|T )and an out-of-domain LM, P (·|D), i.e. whereCis a constant (log prior ratio). This method was introduced as intelligent selection (Moore and Lewis, 2010) and was later renamed contrastive data selection (CDS) (Wang et al., 2018). Initially, it relied on independent n-gram LMs for estimatingP (·|T )andP (·|D), trained respectively on the (small) in-domain training setTand the (large) outof-domain training setD(Moore and Lewis, 2010; Axelrod et al., 2011). With neural LMs,P (·|T )can be estimated by ﬁne-tuningP (·|D)as suggested by (van der Wees et al., 2017b; Wang et al., 2018). The ﬁne tuning strategy is particularly efﬁcient when one performs data selection to adapt a language model. In that case, there is no need for an intermediate model. The pretrained language model to adapt is itself ﬁne-tuned in a few steps on Tand is itself used to score the out-of-domain set. Classiﬁer Selection:Discriminative classiﬁcation (DC), introduced by Aharoni and Goldberg (2020); Jacovi et al. (2021), trains a binary classiﬁer to distinguishTandDexamples. This classiﬁer is either trained from scratch or ﬁne tuned from a pretrained model (Devlin et al., 2019; Liu et al., 2019). Aharoni and Goldberg (2020) train the domain classiﬁer, which they refer to as “Domain-Finetune”, only on the source (English) side of the parallel corpus. We propose two alternative domain classiﬁers, that instead condition the classiﬁer on either the target language or both source and target concatenated. To ﬁnetune language models on the target language data, we use BERT models that are pretrained on German (deepset.ai), Russian (Kuratov and Arkhipov, 2019) and multilingual BERT (Devlin et al., 2018). The motivation for these alternative classiﬁers are two fold: (1) noisy web crawled translation datasets often have incorrect translations (or even languages) which could be missed by the domain classiﬁer if only conditioning on the English source data, (2) the multilingual domain classiﬁer is able to model the interaction between the source and target and is more analogous to the bilingual cross-entropy difference proposed by Axelrod et al. (2011) Compared to CDS, DC trains a different model which adds training overhead. On the other hand, a distinct intermediate model offers more ﬂexibility. The classiﬁer might be pretrained on a different task (e.g. masked LM to select translation data) and its capacity can be selected independently from the hyperparameter of the model to be adapted. Both aspects are important since intermediate models can easily overﬁt given the small size of the target domain set T . Nearest Neighbor Selection:A lesser used methods is sentence embedding nearest neighbors (Gururangan et al., 2020; Aharoni and Goldberg, 2020). Embedding nearest neighbors relies on a pretrained model (Devlin et al., 2019; Liu et al., 2019) to represent sentences as vectors and then measure a domain-score by comparing the distance between a candidate sentence vectorv(x)and the averageP in-domain sentence vectorx. In our experiments, we evaluate both constrastive data selection, the most common method by far, and selection with discriminitative classiﬁers as it has been shown more effective in subsequent work (Aharoni and Goldberg, 2020). Previous work and our preliminary experiments indicated that nearest neighbor selection was not competitive with other baselines so we do not include it in our analysis. Data selection for domain adaptation requires selecting several hyperparameters: the number of pretraining steps, i.e. when to transition from training on the full out-of-domain set to the selected subset; the number of selection steps, i.e. how long to train the model on the selected data; the fraction of selected data, i.e. the size of the selected subset. These parameters are important as they impact the computational cost of training and the target domain generalization performance. To examine these trade-offs, the difference between pretraining and ﬁne-tuning is important. Pretraining on a large dataset starts with an initial strong generalization improvement, followed by a long session where the rate of generalization improvement is still positive but ever diminishing. Fine tuning gives a strong generalization improvement in a few steps before overﬁtting quickly. The fraction of selected data allows trading off between these two extremes: a large fraction of selected data results in a large training set with a distribution close to the out-ofdomain distribution while a small fraction results in small training set with a distribution close to the in-domain distribution. This means that settings with large fractions can perform more steps with generalization improvement albeit at a slower pace compared to lower fraction settings. Thus the number of selection steps and the selected fraction parameter interact. Our experiments investigate this interaction. We characterize the effects of overﬁtting of the intermediate selection classiﬁer, which uniquely affects data selection in conjunction with ﬁnetuning. The intermediate classiﬁer is trained on the small target domain setT. As any machine learning model, it is biased toward its training set and the data it selects can reﬂect this bias. The selected out-of-domain examples might resemble the examples ofTmore than other in-domain examples unseen during training. This bias transferred to the selected data is itself inherited by the model trained on the selected data. This indirect overﬁtting is crucial for later ﬁne tuning: we report that, in some cases, the selected data is too similar to T. There, the complementary value of selection and ﬁne tuning vanishes as data selection fails to identify data providing updates complementary to those provided later by ﬁne tuning on T . We evaluate domain adaptation with data selection on two tasks, language modeling (LM) and machine translation (MT). For both tasks, we have a large out-of-domain dataset and a small number of examples from the target domain. Both sets of data fulﬁl two functions each. The out-of-domain data is used to pretrain the model and all the selected data come from the out-of-domain set. The small target domain set is used to train the intermediate model that scores examples for data selection and, critically, this same set is used for ﬁnetuning the ﬁnal model. For evaluation, we also have a validation set and test set from the target domain. The validation set is used to select hyperparameters and early stopping points and the test set is only used for the ﬁnal model evaluation. For language modeling, we use the 4.5 million sentences from the One Billion Word corpus (Chelba et al., 2013) as the out-of-domain set and 5k sentences from the Yelp corpus as the target domain. This dataset was used for domain adaptation by (Oren et al., 2019) and we use their ﬁltered and preprocessed version of the data, including the 1k Yelp validation set and 10k Yelp test set. We train 2 language models; a 2-layer LSTM recurrent network (Zaremba et al., 2014) and a base-size transformer (Vaswani et al., 2017). Our machine translation experiments focus on English-to-German and English-to-Russian. For the out-of-domain set, we use 4.5 million Englishto-German pairs and and 5.2 million English-toRussian pairs taken from ﬁltered Paracrawl (Esplà et al., 2019). Paracrawl is composed of translations crawled from the web. Even though we use the ﬁltered version of the dataset, Paracrawl is still noisy including examples of entirely mismatched sentences and occasionally incorrect languages. As in domain data, we rely on news data from the News Commentary Dataset (Tiedemann, 2012), which are high quality translations from the news domain. Our in-domain set is limited to 6k sentence pairs. We use an additional 3k for validation and 10k as the test set. As a neural MT model, we train a base transformer (Vaswani et al., 2017). Code to reproduce our experiments is available. Models are implemented with Flax (Heek et al., 2020). We ﬁnetune on the small in-domain set by grid searching for a learning rate and using the validation set for early stopping. Contrastive Data SelectionThe base pretrained (PT) model is ﬁne-tuned (FT) on the small target domain dataset. This model acts as the “intermediate” model in this setting. Each example in the out-of-domain dataset is scored by the difference between the log likelihoods of the ﬁne-tuned model and the pretrained model. The full dataset can be ranked by this score and a threshold is selected to train on a uniform distribution of only the top examples. Discriminative ClassiﬁerThe target domain dataset is used as positive examples and random samples from the out-of-domain dataset are used as negative examples to train a discriminative domain classiﬁer. The classiﬁer can be a new model trained from random weights, the base model with a binary classiﬁcation head or a pretrained model from another task (such as a generic masked language model). Unlike CDS, the base model is not necessarily reused. The input features to the classiﬁer may either be representations learned from the pretrained base model, other embeddings or the raw text data. In the case of machine translation, the classiﬁer can be trained on the source, target or both. In our transformer experiments, we evaluate CDS and two classiﬁers, (i) a logistic regression model on bytepair encodings (Sennrich et al., 2016) and (ii) a ﬁne-tuned BERT classiﬁer (deepset.ai; Kuratov and Arkhipov, 2019; Devlin et al., 2018). We use four settings for the BERT classiﬁer, training on the source, target, mean of the former two, and concatenated language pairs, using the respective language speciﬁc pretrained BERT. For the concatenated case, we use a multilingual BERT. Table 1: Data selection for machine translation of English to German and English to Russian. BLEU in italics next to log-perplexity (log PPL). For both datasets, models were trained to 200K steps of pretraining and 15k steps of data selection. Table 2: Paired bootstrap comparison: each value reports the fraction of samples with worse mean performance than PT + DC-BERT + FT for 1k samples of 10k sentences sampled from a 10k sample test set. Machine TranslationTable 1 reports the logperplexity and BLEU scores on two language pairs for each of the selection methods described above. Data selection always outperforms the baseline without selection, with the BERT domain classiﬁer producing the best log-probability and BLEU on both datasets. The effectiveness of DC compared to CDS is a surprising result given the popularity of CDS. We ﬁx the number of training steps on the selected data to 15K and pretrain the baseline model for an additional 15k steps so there is the same number of pretraining + ﬁnetuning steps for all settings. We search the optimal selection size for this cutoff of training steps, which we found to be 1 million for En-Ru and 500k for En-De. We report results before and after ﬁnetuning to highlight the variation in effectiveness of ﬁnetuning after the alternative selection methods. This is particularly noticeable for En-Ru where CDS outperforms the logistic regression classiﬁer before ﬁnetuning but is worse after ﬁnetuning. In all settings, ﬁnetuning is more effective after data selection with a discriminative classiﬁer rather than with CDS. Section 4.3 provides insight as to why this is the case. Table 2 reports the paired bootstrap resampling (Koehn, 2004) where the PT + DC (BERT) + FT model is compared to the baseline models, in terms of loss and BLEU, corresponding to Table 1. Each value is computed from the 10,000 example test set. We draw 1,000 bootstrap samples of 10,000 points each, with replacement. This test shows that the classiﬁer method of data selection outperforms CDS with over 99% statistical signiﬁcance on logperplexity. Figure 1 shows the log-probabilities at different checkpoints ranging from 50k to 1 million steps of training. The relative beneﬁt of FT and DC+FT over PT is diminishing as training progresses. However, there are consistent beneﬁts from data selection, so longer pretraining on large models is not sufﬁcient to replace data selection. Even pretraining up to 1m steps and ﬁnetuning (log ppl = 1.530) does not reach the loss from DC + FT at 400k (log ppl = 1.519). The relative improvement between methods is surprisingly constant across pretraining steps with a slight decline in the complementary beneﬁt of combining ﬁne tuning with selection. This means that comparing the adaptation methods early in the pretraining process is indicative of their relative loss at a later stage. Further evaluation of performance at different checkpoints throughout pretraining can be found in the Appendix. Figure 1: The validation loss curves for pretraining, data selection and ﬁnetuning (MT En-De). The pretraining loss (PT) is a single training run, whereas all the other points are checkpoints from the base run that were trained on selected data and/or ﬁnetuned. Domain Classiﬁer VariantsTable 3 reports the log-perplexities and BLEU scores for the four variants of the BERT domain classiﬁer. Aharoni and Goldberg (2020) propose the Source DC method. We propose also exploring targetlanguage-conditioned domain classiﬁers, and in fact, ﬁnd that the Target DC selection method outperforms Source DC on En-DE. Concatenation DC does not yield the best results despite having access to the most data (ie. both source and target). This may be because of the pretraining mismatch, in that Multilingual BERT was not trained on pairs of segments from different languages. We also take evaluate using the mean score of the source and target models as a simple alternative to the multilingual BERT approach. Future work may explore alternative methods for fusing source and target language representations for training a domain classiﬁer. Table 3: Different types of BERT classiﬁers, target uses the target language (De/Ru), the source is English and Concat concatenates source and target and trains classiﬁer on multilingual BERT. Mean takes the mean scores from source and target classiﬁers. All models are evaluated at 200k pretraining steps, similar to Table 1. Table 4: Language modeling results (log-perplexity) across selection methods for an LSTM and a basetransformer. The LSTM was pretrained for 115k steps and the transformer was trained for 20k steps. Language ModelingFor language modeling we evaluate on both a modestly sized LSTM and a base-size transformer. For the LSTM domain classiﬁer, we reuse the pretrained language model as the feature representation for a simple linear domain classiﬁer (LM Hidden), as a smaller domain classiﬁer seems appropriate given the smaller language model. We see similar results for the two models despite the large differences in number of parameters, training steps and proximity to convergence. The LM results in Table 4 show that ﬁne tuning (PT+FT) and data selection (CDS, DC) are improving the pretrained model on target domain validation data. The beneﬁt of FT alone is generally greater than selection alone but both approaches are complementary with the best result obtained with combined approaches (CDS+FT, DC+FT). When comparing methods we observe that DC is worse than CDS on its own but it is equivalent or better in combination with ﬁne tuning (DC+FT vs CDS+FT). This indicates that the methods differ in their complementarity with FT and evaluating selection approaches before ﬁne tuning is not sufﬁcient. 4.3 Overﬁtting and Complementarity Our work compares two common data selection techniques, contrastive data selection (CDS) and a discriminative domain classiﬁer (DC). As discussed in the previous section, we found the combination of DC+FT to be the most effective combination both for our LM and MT settings. One reason of this success is the complementarity of DC with FT. CDS did not beneﬁt as much from subsequent ﬁne tuning as DC selection. In Figure 2 (left), we show the learning curves for both CDS and DC (BERT) with the same selection size of 1m for MT with 200k steps of pretraining. The red dotted curve show that the CDS model reaches excellent performance on the targetdomain training set, but fail to perform as well on the target-domain validation set. This means that the MT model trained on CDS selected data suffers more from overﬁtting than the MT model trained on DC selected data. This is particularly surprising given the large selection size of nearly 1/4th of pretraining data. The data selected by CDS is too speciﬁc to the target-domain training set. This bias also certainly explains the worse complementary of CDS and FT, i.e. if CDS selects a training set whose effect is similar to the target-domain training setT, the updates fromTat ﬁne-tuning are less beneﬁcial. Lastly, we examine important pitfalls to avoid when comparing selection methods and validating their parameters. Figure 2 (middle) shows that when considering selection sets of different sizes, training curves converges at different rates. Small selected subsets progress at the fastest rate but reaches their best generalization quickly, and subsequently overﬁt, while large subsets progress at a slower rate but their best generalization later. This means that short diagnostics to pick the subset size will under estimate the value of large subsets. This is problematic for efﬁciently deﬁning curriculum with data selection (Kumar et al., 2019). Similarly, the generalization loss of model which went through a data selection phase but prior to ﬁne tuning is also misleading to predict its loss after ﬁne tuning as illustrated in Figure 2 (right). 4.4 Effectiveness of Data Selection The purpose of the intermediate data selection model is to rank all the out-of-domain data from Figure 2: Effects of overﬁtting and complementarity: Left: Validation and training loss on the target domain during training on selected data (MT En-De). The dotted line falling below the solid line indicates the model is overﬁtting to the small target domain dataset despite never seeing this data in training. Middle: Loss curves for 6 different data selection sizes for DC (BERT) at the 100k checkpoint (MT En-De). Larger sizes improve loss more slowly but can be trained for longer to eventually outperform the smaller sets. For readability, we display the best checkpoint up to each step. Right: Validation loss on MT En-De during ﬁnetuning. Both data selection methods start at a loss that is better than pretraining but CDS does not beneﬁt much from ﬁnetuning, reaching a loss similar to ﬁnetuning without data selection. Classiﬁer selection has large a improvement from ﬁnetuning. most to least similar with respect to the in-domain data. We evaluate and report the performance of CDS and DC for both LM and MT tasks. The data selection model is never used explicitly as a binary classiﬁer but rather as a scorer. However, as a proxy for the quality of scoring, we evaluate the binary classiﬁcation accuracy on an unseen set of in-domain and out-of-domain data. We also report the average quantile of the in-domain validation data which simulates where in the ranking true in-domain examples would appear. We split the out-of-domain data into 100 equal bins and take the average of the bin index that each in-domain example would fall into by its data selection score. Table 5 shows good performance of CDS and DC for language modeling but clear underperformance of CDS as a binary classiﬁer in the MT setting. Also, it is noteworthy that logistic regression on byte-pair unigrams outperforms CDS and approaches the performance of BERT while having many fewer parameters and a much lower training cost. (En-De) Table 5: Binary classiﬁcation accuracy of domain classiﬁer and average quantile of in-domain data when binned with ranked out-of-domain data. This work explores data selection, a popular method for domain adaption for neural language modeling and neural machine translation. Data selection typically divides a training run into three phases: pretraining on out-of-domain data, training on out-of-domain data selected to resemble target domain data and ﬁne tuning on target domain data. We compare the most common selection methods, contrastive data selection and discriminative model classiﬁer and measure their complementarity with ﬁne tuning. Our experiments motivate several practical recommendations for the practitioner: (i) pretraining followed by data selection and ﬁne tuning can reach a given generalization loss several time faster in terms of total training steps than pretraining with ﬁne tuning; (ii) a data selection method should not be evaluated before ﬁne tuning since not all methods/parameters bring the same complementary value compared to ﬁne tuning; (iii) data selection should care about overﬁtting to the in-domain training set, since this type of overﬁtting results in selected data very similar to the ﬁne tuning set and impacts the complementarity of data selection and ﬁne tuning; (iv) longer pretraining runs are always beneﬁcial to later adaptation stages for ﬁne-tuning, data selection and their combination but pretraining has diminishing return; (v) despite the popularity of contrastive data selection, discriminative domain classiﬁers consistently outperformed this method in our experiments.