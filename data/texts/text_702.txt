Recommendation engines enable access to a range of products by helping consumers efﬁciently explore options for everything from restaurants to movies. But these systems typically rely on ﬁne-grained user data—which both necessitates users’ sharing such data, and makes it difﬁcult for platforms without existing data sources to enter the space. In this paper, we propose a new approach to recommendation system design that partially mitigates both of these challenges: basing recommendations on existing recommendations developed by other services. We call the new service’s recommendation task the target, and call the prior recommendations we rely on the source. Source recommendations are always available to the user, and are sometimes even public. We show that as long as the source and target recommendation tasks are sufﬁciently related, users can enable services Preprint. Under review. Berkeley, CA 94704Cambridge, MA 02138 ndurvasula@berkeley.edufranklyn_wang@college.harvard.edu Recommendation systems are a key modern application of machine learning, but they have the downside that they often draw upon sensitive user information in making their predictions. We show how to address this deﬁciency by basing a service’s recommendation engine upon recommendations from other existing services, which contain no sensitive information by nature. Speciﬁcally, we introduce a contextual multi-armed bandit recommendation framework where the agent has access to recommendations for other services. In our setting, the user’s (potentially sensitive) information belongs to a high-dimensional latent space, and the ideal recommendations for the source and target tasks (which are non-sensitive) are given by unknown linear transformations of the user information. So long as the tasks rely on similar segments of the user information, we can decompose the target recommendation problem into systematic components that can be derived from the source recommendations, and idiosyncratic components that are user-speciﬁc and cannot be derived from the source, but have signiﬁcantly lower dimensionality. We propose an explore-then-reﬁne approach to learning and utilizing this decomposition; then using ideas from perturbation theory and statistical concentration of measure, we prove our algorithm achieves regret comparable to a strong skyline that has full knowledge of the source and target transformations. We also consider a generalization of our algorithm to a model with many simultaneous targets and no source. Our methods obtain superior empirical results on synthetic benchmarks. to make high-quality recommendations for them without giving away any new personal data. Thus our method offers a middle ground between consumers’ current options of either giving platforms personal data directly or receiving low-quality recommendations: under our system, consumers can receive high-quality recommendations while only sharing existing recommendations based on data that other services already possess. For intuition, consider the following example which we use throughout the paper for illustration: There is an existing grocery delivery service, which has become effective at recommending grocery items to its users. A new takeout delivery service is seeking to recommend restaurants to the same set of users. Users’ preferences over groceries and restaurant food rely on many of the same underlying user characteristics: for example, a user who loves pasta will buy it at grocery stores and patronize Italian restaurants; meanwhile, a user who is lactose intolerant will prefer not to buy milk products and prefer to avoid ice cream parlors. Thus, high-quality grocery recommendations provide valuable data upon which we could base restaurant recommendations. From the user perspective, a restaurant recommendation system based on existing grocery recommendations would be able to provide high-quality restaurant suggestions immediately, rather than needing to learn the user’s preferences over time. And yet the user would not need to share any data with the restaurant recommendation system other than the grocery recommendations themselves—which by nature contain (weakly) less user private information than they were derived from. Crucially, the user’s food preferences might theoretically be driven by a health condition like lactose intolerance, but they could also just be a matter of taste (not liking milk). In this case, the grocery delivery platform’s recommendations reﬂect that preference information in a way that is useful for extrapolating restaurant recommendations, without directly revealing the potentially sensitive underlying source of the user’s preferences. And while these food recommendation instances are a bit niche, there are many settings in which recommendations built on personal data may be available, even if we do not have access to the personal data itself. Search engines, for example, utilize user data to serve advertisements, and at least two notable projects—the Ad Observatory project [ automatically logged social media advertisements that were served to users. Such advertisements could be used as contextual input for a variety of recommendation problems. In our framework, we consider an agent that aims to make recommendations for a collection of users over a collection of target tasks. Each task takes the form of a standard stochastic linear bandit problem, where the reward parameter for user ofu’s high-dimensional personal data vector. We think of these transformations as converting the personal data into some lower-dimensional features that are relevant for the task at hand. These transformations and the personal data vectors are unknown to the agent. However, the agent is given access to the optimal arms (and its ﬁnal reward) for each of the users on a source task. No other “white-box” information about the source task (e.g., the reward history for the agent that solved the task) is known. That is, only the recommendations themselves—which are served to (and thus may be captured by) the user—in addition to the ﬁnal reward (which may also be captured by asking the user to rate the recommendation) are accessible to the agent. We describe algorithms that allow the agent to effectively make use of this information to reduce regret, so long as the source and target task transformations make use of segments of the high-dimensional personal data similar to those used in the source task. Notably, such a recommendation system functions without the user providing any personal information to the agent directly—and moreover, to the extent that the source task abstracts from sensitive data, all of the target task recommendations do as well. Formally, we set up two related contextual bandit problems for the recommendation problem. When we have one target and many users, we can decompose each user’s reward parameter into idiosyncratic components, which are unique to each user, and systematic components, which are shared among all users. If the source and target tasks utilize similar segments of the users’ personal data, the idiosyncratic component is low-dimensional. As a result, if between the subspaces, our regret bounds improve upon the LinUCB baseline [ In our second iteration, we consider the general problem of having many targets and many users, but no sources. We show that if we perform what is essentially an alternating minimization algorithm to solve for a few tasks at ﬁrst, use the solutions to solve for all the users, and then use those to solve for Figure 1: Visualizing source and target tasks. In our model, users’ personal data θ spaceR parameter for the source task, and Row(A) ⊆ R personal data that are used by source and target tasks respectively. If the source and target tasks make use of similar segments of the users’ personal data—that is, if dimension—then, we may construct a low rank model (boxed) to represent both the source and target tasks. In this case, having knowledge of Aθ all the tasks, we obtain strong experimental performance. We call our collective methods recommending with recommendations. In both analyses, we make a crucial user growth assumption, which states that in several initial phases, we have beta groups where only some subset of the tasks and some subset of the users can appear. This assumption enables us to estimate systematic parameters accurately, as there are fewer idiosyncratic parameters. It also reﬂects how a web platform grows. Section 2 introduces our formal model and compares it to prior work. In Section 3, we describe our approach for inferring the relationship between the source and target tasks when we have source recommendations for multiple users. We then develop and analyze an algorithm that uses this approach to reduce regret. In Section 4, we generalize our algorithm from Section 3 to our second setting where we have multiple target tasks. In Section 5, we demonstrate our methods in synthetic experiments. Finally, Section 6 concludes. In our setup, we aim to solve corresponding expected reward) for each user on a source task. Formally, we associate to each user of which is potentially sensitive. We think of this space as being extremely high-dimensional—i.e., n  1—and encapsulating all of the salient attributes about the user. We associate to the source task a matrix parameters that the task matrices rank. These task matrices transform a user’s personal data into the reward parameter for the task. For example, if a takeout delivery service (given by target task some user such a way that the vector B We now consider a multi-armed bandit problem with ﬁnite horizon agent observes a context consisting of a user-task pair an arm x from a ﬁnite and time-dependent decision set A The assumption that all target tasks have the same dimensionality is made for expositional simplicity and can be relaxed. . For a given source matrixAand target matrixB, the imageAθ∈ Rgives the reward andRow(B) ⊆ Rcan be thought of as the corresponding partitions of the full A ∈ Rand similarly associate to each target taskt ∈ [T ]a matrixB∈ R. The a, b ≤ ndenote the dimensionality of the source target tasks, respectively.We assume u, the task matrixBtransformsu’s personal data into the space of possible restaurants in where{η decision sets are well-conditioned, such that for any arm assume max Crucially, in our setup, the agent is not given access to any of the personal data vectors the source/task matrices αand corresponding reward used to approximate the source reward parameter agent’s arm space. Namely, we assume that the source agent’s arm space is a net of some ball of radiusS is then approximately given by remainder of this paper we assume, for simplicity, that this approximation is exact. As usual, the objective of the agent is to maximize the expected cumulative reward equivalently minimize the expected regret Our central contribution in this paper is demonstrating how knowledge of the optimal arms for the each of the source tasks may be used to signiﬁcantly reduce regret relative to the naive approach solving each of the the following baseline regret bound. (All proofs are in Appendix A.) Theorem 2.1 has regret bounded by An interpretation of Theorem 2.1 is that on average, each user receives an arm with reward at most R/H = b more tasks, more users, and higher dimension, but becomes easier with time. Recommendation Systems data on users, items, and/or their pairwise interactions—have been one of the most impactful applications of modern machine learning [ led to a number of foundational methodological breakthroughs [6, 7]. Contextual Bandits in which bandits are allowed to draw upon information beyond the arms themselves—the context—to help make their decisions. Moreover, our framework follows that of a linear bandit. However, our framework differs from the standard model of contextual linear bandits [ represents the rewards as reward parameter. While our question can be ﬁt into that standard framework by using such a setup does not allow for us to easily incorporate our information about as it merely views standard contextual bandits formulation (without further assumptions) are weak relative to ours, as θhas dimension bT U. Explore and Reﬁne Approaches algorithm of [ }are i.i.d., mean-0, independent1-subgaussian random variables. We assume that the , whence we may writeα≈ S. The corresponding expected reward, as per (1), (LinUCB Bound).A LinUCB agent that naively solves each of theU ·Ttasks separately logworse than the best, meaning that the problem is harder when there are θas an unrolling of a vector. In particular, the direct bounds obtained through the 10], which also explores to obtain a subspace containing the solution, and imposes a soft penalty to make the estimated reward parameter close to that subspace. Indeed, our approach draws upon their LowOFUL algorithm. However, the problem we seek to solve is quite different from that of [10]—we have either multiple tasks or side information in each of our models. Cross-Domain Recommendations an area where data from other recommendation domains is used to inform recommendations in a new domain. Here we highlight works in cross-domain recommendations that we ﬁnd particularly relevant. [12] makes a low-rank assumption on the tensor of item-user-domain, which is similar to but differs from our assumption, as our assumption does not make any assumptions regarding the quantity of items, and hence about rank constraints along that axis. Furthermore, those works recommend items, whereas our work chooses from a set of arms—the featurizations in our model are provided ahead of time, whereas in [12] they are learnt. [13] uses a similar cross-domain recommendation idea; but they use explicit relationships between items as hinted by the title (i.e. aﬁcionados of the book "The Devil Wears Prada" should also enjoy the movie "The Devil Wears Prada"), which our work does not depend on. Our work makes purely low-rank based assumptions, making it more general than [13]. Meta-Learning have several interrelated tasks (namely, each (service, user) pair can be viewed as a separate task). In [16], the authors address a meta-learning task by assuming that the task matrices come from a simple distribution, which are then learnt. One way to think of this work is that it corresponds to ours in the case where distributional assumptions on the tasks to get nontrivial guarantees. Transfer Learning and Domain Adaptation as we use parameters from one learned task directly in another task, as well as domain adaptation, because we use similar models across two different settings. Transfer learning has commonly used in the bandit setting [17, 18, 19, 20]. Most relevant to our work, the model we present in Section 3 is similar to the setting of [ the target features are a linear transformation away from the source features, and the bandit has access to historical trajectories of the source task. One way to interpret this in our model is that the work assumes knowledge of the full set of features to solve for a nontrivial regret guarantee. However, the result is not directly comparable, because the model contains substantially more information than ours, namely access to the full view Aθ as well as past trajectories on a bandit problem. The paper [ can achieve superior performance to certain baselines. However, it is not clear how to implement this in a bandit-based framework. User-similarity for recommendation mendation problems by using information about similar users, like users in a social network. They assume that people who are connected on social media have similar parameter vectors. However, this assumption may potentially violate privacy by assuming access to a user’s contacts, and by using many tasks (as these works effectively have without using social data. In this section, we consider the case where we have recommendations can be used to generate target recommendations in two phases. We ﬁrst show how an agent with full knowledge of the source and target matrices can reduce regret using the source recommendations by reducing the dimensionality of target recommendation problem. We then show that agent without knowledge of the source and target matrices can still reduce regret under a realistic assumption on the ordering of the contexts by learning the underlying relationship between the source B. It then solves forBby applying linear regression to align the contexts, obtaining 22] uses domain adaptation to approach cross-domain recommendation and shows that it and target matrices from observed data. One quantity that relates to the success of our approach is the common information, which we deﬁne as This quantity can be thought of as the fraction of information about the underlying reward parameter Bθthat can be possibly be explained given the source information we can see that if all contained by the segments used by the source task – a situation that arises often in practice. Stated differently, this implies that the source and target matrices this, observe that the rank of the block matrix r := rank In our running example, the matrix represents a restaurant recommendation task. Our algorithm ﬁnds the minimal set of information needed to predict both tasks, and then predicts the user’s restaurant preferences by looking at all possibilities that are consistent with the user’s grocery preferences. Skyline Regret Bound value decomposition of the stacked matrix as Projecting onto the principal components, we can create a low-dimensional generative model whereφ We call reward parameter from the source domain into the target domain. From the above, we have that the residualBθ orthogonal basis for this space, it follows that we may write whereD gives the corresponding weights. We call the above expression from a lower dimensional vector decomposition of the target reward parameter into a systematic component that can be inferred from the source reward parameter, and an idiosyncratic component that must be learned independently for each user. An agent with full knowledge of the source and task matrices can compute Das given above, and thus needs only solve for each of the subspace. As a result, it attains a lower skyline regret bound. Theorem 3.1 and B may attain regret Comparing to Theorem 2.1, we see that we obtain an identical bound, but with a factor of front. Thus, as alluded to earlier, the improvement depends on how large then we obtain a signiﬁcant improvement, corresponding to being able to use a substantial amount of information from the source recommendations. This result only applies when the agent is given the source and target matrices. κ ≈ 1, then the target task uses segments of the user’s information that are almost may use this structure to reduce regret. By the above, we may write the compact singular ∈ R. Lettingπ∈ Rdenote the orthogonal projector onto the ﬁrstacomponents ∈ Rdenote the orthogonal projector onto the lastbcomponents, we have that = Aθ, whence we may writeφ∈ (πU)Aθ+ Null(A). Here,(πU)denotes the D:= πUφ∈ πU (πU)∈ Rthe transformer, as it transforms the known − DAθlies inπU Null(A), which is a space of dimension(1 −κ)b. Choosing an ∈ Rhas orthogonal columns given by the basis elements, andψ∈ R (Skyline Regret).An agent with full knowledge of the source and target matricesA Learning from Observed Data matrices are unknown. We show how the transformer observed data under a realistic assumption on the ordering of the contexts. We assume that some subset of the users the ﬁrstH for the target task. This assumption makes learning the transformer and generator easier as it reduces the number of idiosyncratic parameters we must fully learn in order to understand the relationship between the two tasks. Our approach is brieﬂy summarized as follows. In the ﬁrst policy that independently pulls arms for each user. In practice, this policy could be adaptive, like the baseline LinUCB algorithm. To keep our analysis tractable, we instead assume that the exploration policy is oblivious. That is, we assume that arms pulled in the ﬁrst any observed rewards, and that the arms are randomly drawn from that the same oblivious policy is used across users. This is essentially equivalent to using different oblivious strategies for different users, as we have no prior information on the reward parameters. In our experiments, we show that this obliviousness assumption does not appear necessary to achieve good performance. We ﬁnd that adaptive policies (namely LinUCB) are just as, if not more effective at learning the model relating the two tasks, while achieving performance equivalent to the baseline during the exploration phase. Immediately after iteration have observed thus far to obtain estimates transformer also be computed given U . We compute the singular value decomposition we then use Although if our approximation is close) then in the coordinates of some orthogonal basis forR, where outside of works in precisely this setting to select arms for all iterations description of our algorithm in Algorithm 1. More concretely, the regret for our approach may be decomposed as whereH comparable to the skyline agent that has knowledge of the source and target matrices. Each term iterations. After the ﬁrstHiterations, all users are allowed to request recommendations Dis determined by the left singular vectorsU. AsNull(A) = Null(πU),Dmay bU, which corresponds to the ﬁrst r columns of the decomposition, as a proxy for U . Uφ = Aθ, φ ∈ R}. bUforU, and subsequently computingbDandbDas in Equation (4), we obtain the bSis an afﬁne space, it is a subset of the linear spaceE. IfBθis close to the spaceE(i.e. Wis a basis forE,Bθis approximately sparse (i.e. components ofBθthat lie Ehave small magnitude). We invoke an algorithm known as LowOFUL [10], which is the number of times userurequests a recommendation. Our approach achieves regret LowOFUL Lemma 3.2. With probability 1 − δ, we have that for all u, whereq = We now apply the original LowOFUL [10, Corollary 1] regret bound to show: Theorem 3.3. Observe that the ﬁrst sum term in this bound essentially matches the skyline bound (Theorem 3.1), and has regret scaled by that comes from the approximation error in predicts that we obtain more regret when the number of users in the beta group have to learn more idiosyncratic components, justifying the user growth assumption, and less regret as the time some constant c and U → ∞ while holding U The previous section showed that we can obtain learning guarantees from a source. In this section, we address that case where we have no sources but many targets – the reward at time byX= hx, B compare to a LinUCB baseline (Theorem 2.1) that uses one bandit for each of the In the previous section, we found that source tasks with low rank information in this setting that there exist user-speciﬁc vectors U, . . . , U In our running example, the matrices on different food sources. Our low-rank assumption posits that there is substantial redundancy between the different tasks (in the example: user-speciﬁc food preferences apps). One beneﬁt is that after many users has been solved for, solving new tasks is very easy because we have data on a large set of users. Skyline Regret Bound this case, we obtain a reduction to U separate dimension-r bandit problems, one for each user. Lemma 4.1 (Skyline Bound). With access to all task matrices, we obtain a regret of is bounded in terms of the magnitude of the “almost sparse” componentsL:= ). With methods from perturbation theory, we show: With an oblivious exploration policy that draws identical arms for each user from ), assuming that each user appears equally many times, with probability at least1 − Hwe have to learn the idiosyncratic components increases, Finally, whenH= cHfor θi + η, wheretanduindex the task and user respectively. As before, we κ) were effective in producing recommendations for the target task. We similarly assume such that Bθ= Uφfor all u and t. An Algorithm based on User and Task Growth the existence of a beta group of users to learn the relationship between the source and target. In this multitask setting, we additionally require a a beta group of tasks. We ﬁrst apply an exploration strategy as in Section 3 to make estimates subsequently compute a singular value decomposition, as in Equation (6), to produce estimates fort ∈ [T make estimates for the low rank latent parameters least-squares on the arm-reward history, noting that for any arm x, In the last phase, we let all approximations, in conjunction with the reward history, to learn all Observe that the sample complexity of learning the estimates t, we are able to collect some sample that helps in approximating estimate of the low rank latent parameter analysis of this algorithm (for barriers described in Appendix D) we show in synthetic experiments that our approach offers a clear advantage over the baseline approach of solving each of the contexts separately. We give a full description of the algorithm in Algorithm 2. In this section, we present several simulation experiments to benchmark our approach. Rec2’s regret, using a LinUCB implementation following that of [ mental setup in Appendix C, only highlighting the most important details here. Experiment with One Source, One Target, Many Users a = 20, randoma × d greatly improve upon the baselines. Notably, while we only have bounds on the excess regret after exploration for the oblivious exploration policy, it appears that similar bounds hold for both the oblivious exploration policy and the LinUCB exploration policy. Experiment with No Sources, Many Targets and Many Users b = 3,T = 30 Algorithm 2 for phase, and outperform the baseline in the third phase. Ideally, we would also perform an experimental evaluation on real-world data. To accomplish this, however, we would need to obtain data on user behavior for the same collection of users over multiple services. The closest matches to this requirement are datasets such as Douban [ a real cross-domain recommendation service. Unfortunately, although the Douban dataset does record user behavior over multiple services, the only actions that we know rewards for are those that were executed by the service. That is, each user only gave feedback for items recommended to them by the service, which in practice is a very small fraction of the total action space. Thus, this dataset cannot be used to assess our algorithms, since the recommendations our algorithms would make would not coincide with those that received feedback in Douban. Each run of our algorithms takes at most ﬁve seconds on a 8-core 2.3GHz Intel Core i9 processor. ]. Next, we let allUusers request recommendations on theTtasks, and use thebUto cφto estimate eachbU. That is, no matter which useruappears for a given task context b = 20,U= 25,U = 500, and|A| = 40,H= 2, 000andH = 8, 000. We generate by lettingφbe randomd-dimensional vectors, whered = a + b(1 − κ), andA,Bare andb × d-dimensional matrices. As we can see, the results of ourRec2approaches ,U = 50,r = 6,|A| = 40, andT= 3,U= 5. We choose random vectors , and each task matrixBis ab × rmatrix. Actions are random vectors of lengthb. We run H= 1, 000timesteps in the ﬁrst phase,H− H= 3, 000timesteps in the second H − H= 9, 000timesteps in the third phase and see that the results of Rec2 strongly Figure 2: exploration phase (after downwards, as predicted by our theorem, yet somehow the LinUCB exploration policy appears to incur equal or even less regret after the exploration phase without sacriﬁcing any regret in the exploration phase. In (b) it appears that while the ﬁrst and second phases, the regret becomes sharply lower in the third phase. We introduced a contextual multi-armed bandit recommendation framework under which existing recommendations serve as input for new recommendation tasks. We show how to decompose the target problem into systematic and idiosyncratic components. The systematic component can be extrapolated from the source recommendations; the idiosyncratic component must be learned directly but has signiﬁcantly lower dimensionality than the original task. The decomposition itself can be learned and utilized by an algorithm that that executes a variant of LinUCB in a low-dimensional subspace, achieving regret comparable to a strong skyline bound. Our approach extends to a setting with simultaneous targets but no source, and the methods perform well in simulation. We believe that recommending with recommendations should make it possible to reduce recommendation systems’ reliance on sensitive user data. Indeed, there are already large corpora of recommendations (in the form of advertisements) produced by search engines and social media platforms that have access to tremendous amounts of user information; these span a wide range of domains and thus could be used as source input in many different target tasks. platforms such as grocery delivery services are developing expertise in recommending products within speciﬁc domains; these could naturally be extrapolated to target tasks with some degree of overlap, such as the restaurant recommendation service in our running example. Operationalizing the approach we provide here just requires a way for users to share the recommendations from one service with another. There are already public data sets for this in advertising domains, but in the future it could be accomplished more robustly either through a third-party tool that users can use to scrape and locally store their recommendations, or through direct API integrations with existing recommendation engines. But in any event, once a given user’s existing recommendations are available, they can be loaded into a new service (with the user’s consent) and then can instantaneously learn the systematic component and provide valuable recommendations using the approach we described here. In addition to the overall sources rather than the underlying user data, our method a way for new services to bootstrap off of other platforms’ recommendations. Moreover, this approach to some extent recommendations that platforms have already generated from their data. Ironically, in this sense, advertisements effectively act as a privacy ﬁlter – they can actually serve as a fairly complete proﬁle of a person, while abstracting from much of the underlying sensitive data. Average Regret, with 95% conﬁdence intervals shown.Note that in (a) the post-