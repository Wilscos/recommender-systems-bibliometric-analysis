A long-standing challenge in Recommender Systems (RCs) is the data sparsity problem that often arises when users rate very few items. Multi-Target Multi-Domain Recommender Systems (MTMDR) aim to improve the recommendation performance in multiple domains simultaneously. The existing works assume that the data of dierent domains can be fully shared, and the computation can be performed in a centralized manner. However, in many realistic scenarios, separate recommender systems are operated by dierent organizations, which do not allow the sharing of private data, models, and recommendation tasks. This work proposes an MTMDR based on Assisted AutoEncoders (AAE) and Multi-Target Assisted Learning (MTAL) to help organizational learners improve their recommendation performance simultaneously without sharing sensitive assets. Moreover, AAE has a broad application scope since it allows explicit or implicit feedback, user- or item-based alignment, and with or without side information. Extensive experiments demonstrate that our method signicantly outperforms the case where each domain is locally trained, and it performs competitively with the centralized training where all data are shared. As a result, AAE can eectively integrate organizations from dierent domains to form a community of shared interest. Recommender Systems, Multi-Organization Learning, Distributed Machine Learning ACM Reference Format: Enmao Diao, Vahid Tarokh, and Jie Ding. 2021. Privacy-Preserving MultiTarget Multi-Domain Recommender Systems with Assisted AutoEncoders. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn In recent years, Recommender Systems (RSs) have become one of the most popular techniques in web applications that involve big data. This is because they can eectively extract helpful information for relevant users, e.g., in recommending restaurants, videos, and e-commerce products [1,18]. However, a long-standing challenge in recommender systems is the data sparsity problem because the users usually rate very few items. To address this issue, the research direction of Cross-Domain Recommendation (CDR) has been developed to leverage ratings from a source domain where users may have relatively more information to improve the performance of a target domain [2]. A system that simultaneously improves the performance of both the source and target domains is referred to as Dual-Target CDR [33]. Since then, Multi-Target Multi-Domain Recommendation (MTMDR) has become a popular solution to improve the recommendation performance of multiple domains simultaneously. However, most existing works on this topic require that the data of dierent domains are fully shared, and the computation must be performed in a centralized manner [3,28,31,34]. Since most recommender systems are built upon users’ sensitive data, e.g., user proles and usage history, and the model and task information are also proprietary to organizational learners [25], collaborations among organizations from dierent domains are often restricted by ethical, regulatory, and commercial constraints [27,33]. Therefore, a privacy-preserving MTMDR, as shown in Figure 1, is the keystone for leveraging isolated data held by various domains. This work proposes an MTMDR based on Assisted AutoEncoders (AAE) and Multi-Target Assisted Learning (MTAL), which help different organizations improve their recommendation performance simultaneously while preserving their local data, models, and task labels. In particular, each organization will calculate a set of ‘residuals’ and broadcast these to other organizations. These residuals approximate the fastest direction of reducing the training loss in hindsight. Subsequently, other organizations will t the residuals using their local data, models, and objective functions and broadcast the tted values back to each other. Each learner will then assign weights to its peers to approximate the fastest direction of learning. The prediction will be aggregated from the tted values. The above procedure is repeated until all organizations accomplish a sucient level of learning. Moreover, our approach can handle explicit or implicit feedback [9], user- or item-based alignment [33], and with or without side information [24]. We perform extensive experiments to demonstrate that our method signicantly outperforms the case where each domain is locally trained. It performs competitively with centralized training where all data are shared. As a result, our method can eectively integrate organizations from dierent domains to form a community of shared interest, as shown in Figure 1. Our main contributions are summarized below. •We present a new paradigm of privacy-preserving Multi- Target Multi-Domain Recommendation (MTMDR), which can eectively improve the recommendation performance of dierent domains simultaneously, without sharing their local data, models, or task labels. Figure 1: (a) User aligned (b) Item aligned Muti-Target Multi-Domain Recommender Systems. Organizations from dierent domains form a community of shared interest. Assisted Learning as a systematic solution to the proposed MTMDR scenario. It exchanges domain-specic information through pseudo-residuals tted with local data and models. Our method covers broad application scenarios, including explicit or implicit feedback, user- or item-based alignment, and with or without side information. •We conduct extensive experiments to demonstrate that our method can signicantly outperform the case where each domain is locally trained and perform competitively with the centralized training where all data are shared. As a result, AAE can eectively integrate organizations from dierent domains to form a community of shared interest. Recommender Systems (RCs) predict users’ preference on items and provide personalized recommendations for users [1,18]. Recommendation approaches are mainly classied into three categories [1,10], namely collaborative ltering, content-based recommendation, and hybrid systems. Specically, collaborative ltering learns from user-item interactions, while the content-based recommendation is primarily based on side information. Hybrid systems leverage both user-item interactions and side information. Our proposed method is a hybrid recommender system that can integrate 1) user-item interactions, 2) either explicit feedback (e.g., user’s previous ratings) or implicit feedback (e.g., browsing history), and 3) side information. Cross-Domain Recommender SystemsIn most realistic scenarios, users usually provide very few ratings among many items [19]. It results in a highly sparse interaction matrix which hinders the recommendation performance. To address the data sparsity problem, cross-domain recommendation (CDR) [2] has been proposed to utilize relatively richer information from the source domain to improve the recommendation performance in the target domain. Recent CDR methods use the latent factors obtained from a source domain for a target domain [6,16,29,30,32]. Furthermore, DualTarget CDR was proposed to improve the performance of both source and target domains [33]. A natural extension of Dual-Target CDR is Multi-Target Multi-Domain Recommendation (MTMDR), which aims to improve the recommendation performance of multiple domains simultaneously [28,34]. In this research direction, the existing works assume that dierent data domains can be trained in a centralized manner. However, in many practical scenarios, separate recommender systems are operated by various organizations, prohibiting the sharing of private data and model parameters. To address this challenge, we will propose a method to enhance the performance of multiple domains simultaneously without transmitting local data and models. Autoencoder-based Recommender SystemsAutoRec [20] applies AutoEncoders (AE) to RCs and has found many successful applications. It takes user vectors or item vectors as input and reconstructs them in the output layer. Several recent studies improve the performance of AutoRec by using denoising AE [13,22], variational AE [14,15], and Dropout [5,12,21]. Side information can also be leveraged with autoencoders to tackle the cold start problem [23]. AutoEncoder-based recommender systems have two variants, namely user-based and item-based. Our method can leverage user-based and item-based AutoEncoders to handle user-based and item-based alignment in MTMDR, respectively. Assisted Learning (AL) [26] is a collaborative learning framework where organizations being assisted or assisting others do not share private local data and models. The recently proposed Gradient Assisted Learning (GAL) [4] generalizes AL from a sequential protocol to parallel aggregation across multiple organizations. This work develops and generalizes the GAL algorithm for privacy-preserving MTMDR based on a novel design of autoencoder-based recommender systems. Recommender SystemsLetU = {𝑢, . . . , 𝑢}andV = {𝑣, . . . , 𝑣} denote respectively the set of users and items, where𝑚is the number of users and𝑛is the number of items. We have a user-item interaction or rating matrixR ∈ R, where𝑟∈ Rdenotes the rating that user𝑢gives to item𝑣. A recommender system 𝐹 (·)predicts the ratingˆ𝑟given a pair of user and item(𝑢, 𝑣), i.e.ˆ𝑟= 𝐹 (𝑢, 𝑣). The recommender system can also incorporate side information such as user prole𝑠∈ Sand item attributes 𝑠∈ Sthat are associated with the user𝑢and item𝑣, so that ˆ𝑟= 𝐹 (𝑢, 𝑣, 𝑠, 𝑠). We train a recommender system by minimizing an average of loss values in the form of 𝐿(ˆ𝑟, 𝑟). Multi-Target Multi-Domain Recommender Systems (MTMDR)Suppose that there are𝐾observed domains including the user setsU, . . . , Uand the item setsV, . . . , V. We have a set of rating matricesR, . . . , R, where domain𝑘has𝑚users and𝑛items. Domain𝑘can train a separate recommender system ˜𝐹(·). However, in order to resolve the data sparsity problem, a multi-domain recommender system𝐹(·)aims to improve the performance of the locally trained recommender system by leveraging the common usersUor the common itemsVthat appear in other sets of users or items. The common users or items are those shared between a pair of domains, described by As shown in Figure 1, multi-domain recommender systems can be categorized based on their alignment. Depending on the application scenario, a user-aligned multi-domain recommender system leverages the common users, while an item-aligned one leverages the common items. The general goal is to develop a multi-domain recommender system that signicantly outperforms each locally trained recommender system, and that performs competitively with the recommender system𝐹 (·)jointly trained from all user and item sets. In other words, where the expectationEis over test data. To achieve the above objective for all data domains whose data are distributed in practice, we develop a multi-target multi-domain recommender system that does not share the rating matrixR, user proleS, item attribute S, model 𝐹(·), and objective function 𝐿(·). AutoEncodersAn autoencoder is a network consisting of an encoder𝑧 = 𝐸 (𝑥):𝑅→ 𝑅and a decoderˆ𝑥 = 𝐷 (𝑥):𝑅→ 𝑅, where𝑑and𝑑are dimensions of the input vector𝑥and output vectorˆ𝑥, respectively. Unlike Collaborative Filtering, the input of an autoencoder-based recommender system is not a pair of user 𝑢and item𝑣. Instead, the input of a user-based autoencoder is a partially observed vectorr= (𝑟, . . . , 𝑟) ∈ Rwhich represents the ratings of user𝑢giving to all items𝑣. . . 𝑣. We represent the partially observed vectorras a sparse vector, where the unknown ratings are zeros. Similarly, the input of an itembased autoencoder isr= (𝑟, . . . , 𝑟) ∈ R. The encoder transforms the observed vector into a dense lower-dimensional code, namelyz= 𝐸 (r)orz= 𝐸 (r). The decoder produces the output vectorˆr= 𝐷 (𝐸 (r)) ∈ Randˆr= 𝐷 (𝐸 (r)) ∈ R, where𝑛is the number of items, and𝑚is the number of users. The dimension of the input vector is the same as that of the output vector for an autoencoder-based reocommender system. We train an autoencoder-based recommender system by minimizing the average loss between the input and output vectors. When computing the loss function, we mask out the unobserved output ratings. Assisted AutoEncodersOur model is inspired by a series of autoencoder-based recommender systems [5,12,20], but it has an unusual architecture. Suppose that there are𝐾observed domains. Each domain can train its own Assisted AutoEncoder (AAE). The input of a user-based AAE is a partially observed vectorr= (𝑟, . . . , 𝑟) ∈ Rthat represents the ratings of user𝑢giving to all items𝑣. . . 𝑣in domain𝑘, where𝑛is the number of items in domain𝑘. We represent the partially observed vectorras a sparse vector, where the unknown ratings are zeros. Similarly, the input of an item-based AAE isr= (𝑟, . . . , 𝑟) ∈ R, where 𝑚is the number of users in domain𝑘. The key dierence between previous works and ours is that the output vector of a user-based and item-based AAE has the dimension of total number of items or users, i.e.ˆr= 𝐷 (𝐸(r)) ∈ Randˆr= 𝐷 (𝐸(r)) ∈ R, where 𝑛is the total number of items, and𝑚is the total number of users, across all domains. Figure 2: Assisted AutoEncoders (AAE) incorporate side information by summing up the codes encoded with two additional encoders corresponding to the user prole and item attribute. For each domain 𝑘, the input dimension is the number of users or items of that domain, while the output dimension is the total number of users or items of all domains. It uses tanh(·) for nonlinear activation and Dropout for regularization. Figure 2 demonstrates an example of the AAE network. Both encoder and decoder consist of fully connected (FC) layers. We usetanh(·)as our nonlinear activation function because it is important for hidden layers to contain negative parts [5,12]. We adopt Dropout [21] at the encoded space as suggested by [12]. We also consider side information of domain𝑘such as user prole S∈ Rand item attributesS∈ R, where𝑑and 𝑑denote the feature dimension of user prole and item attribute at domain𝑘, respectively. We can transform non-structural side information such as text, image, and video into dense feature representations [24]. AAE can train two additional encoders for the user prole and item attribute. For a user-based AAE, the input of the encoder for user prole iss∈ R, and the input of the Figure 3: Learning and Prediction stages for Multi-Target Assisted Learning (MTAL). Organizations from dierent domains construct the set of common users or items. The domain 𝑘 learns local models 𝑓 predicted outputsˆ𝑟from all the domains 1 to 𝐾. encoder for item attribute is the summation of the observed itemÍ attributes, namelys∈ R, whereVrepresents the set of observed items for user𝑢. When considering side information, we sum up the codes encoded from ratings and side information. We cannot train AAE directly because, in our privacy-preserving setting, an isolated domain does not have access to the ratings from other domains. To address this issue, we apply the assisted learning framework and develop a multi-target algorithm for MTMDR. We propose Multi-Target Assisted Learning (MTAL) as demonstrated in Algorithm 1, so that each domain can operate on its own local data, model, and objective function. Our algorithm considers both user- and item-aligned multi-domain recommender systems. The item-aligned case can be regarded as a transposed version of the user-aligned case. Figure 3 gives a ow chart of our algorithm. Next, we describe the learning and prediction procedures in detail. At the beginning, each organization, or domain𝑘, coordinates with others to construct the set of common usersUor itemsV, depending on whether the system is user-aligned or item-aligned. During the Learning stage, each domain initializes with an unbiased base model𝐹(R) = E(R)= 𝐵Ír∈ R. For the explicit feedback, the base model is the average ratings, where𝐵 is the number of ratings of each item at domain𝑘. For the implicit feedback, the base model is the popularity estimates, where𝐵is the number of users at domain𝑘. At every assistance round𝑡, each domain computes its own ‘pseudo residuals’𝑟and broadcast its common residuals 𝑟to another domain 𝑙, where 𝑟= −𝜕𝐿𝐹(R), R, 𝑟= {𝑟, 𝑖 ∈ U∩ U}, (5) 𝐿(·)is the overarching loss function used by domain𝑘, and𝐹(R) is the output from the previous assistance round. Here, the superscript of𝑟means the domain𝑖transmits the residuals to domain 𝑗, or the domain𝑗receives from domain𝑖. In Figure 3, we let𝑟 denote all the received residuals of domain𝑘from domains 1 to𝐾. Then, each domain aggregates its own residuals together with the received common residuals from other domains into ‘pseudo targets’ˆR= {𝑟, . . . , 𝑟} ∈ R. Note thatˆRis a sparse matrix of size𝑚× 𝑛. The dimension of items increases from𝑛 to𝑛, because the received common residuals from other domains introduce unobserved targets of items. Next, each domain will t a local AAE𝑓with the pseudo targetsˆRand the local loss functionℓ(·). Each domain will then broadcast the common predicted outputsˆ𝑟to domains 1 to 𝐾. Subsequently, each domain can train suitable gradient assistance weights𝑤to aggregate received outputs, and gradient assisted learning rate 𝜂to minimize the overarching loss. where𝑃= {𝑤 ∈ R:Í𝑤=1, 𝑤≥0}denotes the probability simplex. We note that optimizing𝑤and𝜂is optional. We have conducted ablation studies of𝑤and𝜂in Section 5. Finally, the output at round 𝑡 is The above procedure can be iterated for𝑇assistance rounds until we obtain a satisfactory performance (e.g., on validation data). In the Prediction stage, each domain will predict outputs from their local models𝑓for all assistance rounds from 1 to𝑇. The Algorithm 1: MTAL: Multi-Target Assisted Learning for Recommender Systems. Input: 𝐾 decentralized data domains, domain 𝑘 holding rating matrix R ∈ R Learning Stage: Alignment: Prediction Stage: Gather predictionsˆ𝑟= 𝑓(R), 𝑡 = 1, . . . , 𝑇 from each domain 𝑗, 𝑗 = 1, . . . , 𝐾 Return 𝐹(R) predicted results will be broadcast to other domains, which will aggregate them with gradient assistance weights𝑤and gradient assisted learning rate𝜂, to eventually generate a nal prediction 𝐹(𝑥)that is implicitly operated onR. Compared with the Learning stage, the Prediction stage does not need synchronization of each assistance round because we can operate all local AAEs across𝑇 rounds before broadcasting the outputs. Our framework can help participants to form a shared community of interest. In particular, every data domain can provide its own task and seek assistance from others. The end-to-end assistance provided for another organization does not require the sharing of anyone’s proprietary data, models, and objective functions. In practice, the participating organizations may receive nancial rewards from the assisting ones. Consequently, all participants can become mutually benecial to each other. Why AutoEncoders?We choose autoencoders as our local backbone model because AE is naturally compatible with our proposed MTAL algorithm. In particular, autoencoders can treat the rating matrix as tabular data. The rows and columns of the rating matrix are considered as data sample and feature space, respectively. A multi-domain system can be viewed as each domain holding a subset of the feature space. We can adopt user-based autoencoders for ˆR= {𝑟, . . . , 𝑟} ∈ R ˆ𝑟to other domains , 𝑖 ∈ U∩ U} user-aligned multi-domain recommender systems, and item-based for item-aligned systems. On the contrary, collaborative ltering takes user-item pairs as the input, which do not contain a feature space. Thus, it is not suitable to integrate collaborative ltering with our MTAL algorithm. It is worth mentioning that the proposed MTAL algorithm is not restricted to autoencoders. A possible future work is to discover better local models which can treat the rating matrix as tabular data for MTAL. Loss functionOur algorithm involves two kinds of loss functions, namely a local loss functionℓ(·)for tting the pseudo-targets ˆR, and an overarching loss function𝐿(·)for tting the ratings Rin hindsight. Since tting the pseudo-targets is a regression problem, it is standard to letℓ(·)beℓ- orℓ-norm. Depending on the ratings are explicit or implicit feedback, we choose dierent overarching loss functions. We use regression loss functions such as theℓ-norm for explicit feedback and classication loss functions such as binary cross-entropy for implicit feedback. Technical noveltiesOur proposed AAE absorbs many merits of previous autoencoder-based recommender systems, such astanh(·)activation function [5,12], encoders for side information [23], and Dropout [5,12]. AAE extends the scope of standard autoencoders for multi-domain recommender systems by increasing the output dimension. Moreover, the proposed MTAL algorithm develops AL in two ways. First, we generalize AL from a singletarget to a multi-target learning framework. In particular, GAL [4] assumes multiple participants assist one organization. Our MTAL method ts multiple targets of all domains with a single local model. Therefore, MTAL can simultaneously improve the performance of all participants. Meanwhile, we also perform ablation studies of gradient assistance weights and gradient assisted learning rate. Our results show that 1) a constant gradient assisted learning rate suits the explicit feedback, while an optimized gradient assisted learning rate suits the implicit feedback, and 2) optimized gradient assistance weights tend to improve the result if the domain partition is Non-IID. PrivacyOur proposed algorithm allows dierent domains to improve their recommendation performance without sharing their local data, models, and objective functions. We consider this requirement as a bottom line for privacy-preserving MTMDR. Nevertheless, we are aware that it is possible to apply further privacy enhancement techniques such as Dierential Privacy (DP) by adding noises to the transmitted residuals [4]. DataWe conduct experiments with MovieLens100K (ML100K), MovieLens1M (ML1M), and MovieLens10M (ML10M) datasets [7] under various circumstances. We have following control settings. 1) Explicit vs. implicit feedback. The explicit feedback is the default rating(1−5), while the implicit feedback is the binarization of default ratings (positive if greater than 3.5). 2) User- vs. item-alignment. We introduce two types of data partition for multi-domain recommender systems. The ‘Uniform’ data partition splits items (respectively users) for user-aligned (respectively item-aligned) multi-domain recommender systems into𝐾 =8 domains. Each domain has roughly the same number of items or users. The ‘Genre’ data partition splits items according to𝐾 =18 of movies for user-aligned multi-domain recommender systems. 3) With vs. without side information. The user proles like gender and occupation are available for ML100K and ML1M datasets. The item attributes are one-hot indicators of genres. 4) Ablation studies. We conduct ablation studies for gradient assisted learning rate, gradient assistance weights, and partial alignment. In addition, the summary statistics of each dataset are elaborated in Table 7 in the Appendix. For all datasets, we train on 90% of the available data and test on the remaining. We conduct four random experiments for all datasets with dierent seeds. The standard errors of all our results are smaller than 1𝐸 − 3. ModelWe compare the results of AAE with the unbiased Base model described in Section 4.2, and collaborative ltering methods such as Matrix Factorization (MF) [17], Multi-Layer Perceptron (MLP) [8], and Neural Collaborative Filtering (NCF) [8]. We also use a subscript𝑠to represent models incorporating side information, namely MFand AAE. Our proposed method is a novel integration of Assisted Autoencoders (AAE) and Multi-Target Assisted Learning (MTAL) algorithm. Although we have used the same model architecture for every domain in the experiments, our algorithm does not require every domain to use the same local models. An organization can choose the size of autoencoders based on its local computational capabilities. We use theℓ-norm as the local loss functionℓthroughout experiments. We use the Adam optimizer with a learning rate of 10[11]. We set the number of assistance rounds𝑇 =10 throughout our experiments. The number of local training epochs at each round is 20. To optimize the gradient assistance weights and gradient assisted learning rate, we use the Limited-Memory BFGS (L-BFGS) optimizer with a learning rate of 1. Details of model architecture and learning hyper-parameters are included in Tables 5 and 6 in the Appendix. BaselineWe compare the proposed method with two baselines, ‘Joint’ and ‘Alone’. The ‘Joint’ denotes the centralized case where all the data are held by one organization. The ‘Alone’ denotes the case where every domain trains and tests on its local data. As mentioned in Equations 3 and 4, the goal of our method is to signicantly outperform the ‘Alone’ case and perform competitively with the ‘Joint’ case. We note that in ‘Alone’ and ‘Joint’ cases when AAE is not equipped with MTAL, the input and output dimensions of AAE are the same. In that case, the AAE reduces to a conventional autoencoder-based recommender system. We tabulate experimental results in Tables 1, 2, and 4. We illustrate evaluations across all assistance rounds in Figures 4, 5, and 6. We also provide ablation studies of gradient assisted learning rate, gradient assistance weights, and partial alignment in Tables 3, 8, and 9, respectively. We provide detailed discussions below. Explicit vs. Implicit feedbackAs shown in Tables 1, 2, and 4, AAE is able to eectively handle explicit and implicit feedback by using dierent objective functions. We consider explicit feedback as a regression task. For explicit feedback, the objective function is ℓ-norm, and the metric is Root Mean Squared Error (RMSE). For implicit feedback, it was shown that we could treat the problem as a binary classication task [17] and use a ranking criterion Mean Average Precision (MAP) to evaluate the performance. Therefore, we use binary cross-entropy as our objective function for implicit feedback. As shown in Tables 1, 2, and 4, AAE performs competitively with the baseline recommender systems in ‘Joint’ scenario with explicit feedback. However, compared with baseline models, AAE does not perform well in the ‘Alone’ scenario with explicit feedback. Moreover, as shown in the results of ML10M, AAE sometimes performs worse in ‘Joint’ than it does in ‘Alone’. It shows that the performance of autoencoder-based recommender systems may be sensitive to the input dimension, namely the number of local items 𝑛or the number of local users𝑚in a user- or item-based recommender. Specically, when an organization has explicit feedback for a small number of items or users, it should use collaborative lter methods instead of autoencoder-based methods. Moreover, when the organization has explicit feedback for a very large number of items or users, the width and depth of hidden layers of AAE may also increase to alleviate the curse of dimensionality. On the other hand, AAE outperforms baseline models in both ‘Joint’ and ‘Alone’ cases with implicit feedback. Our proposed method AAE equipped with MTAL signicantly outperforms all ‘Alone’ cases with dierent backbone models for both explicit and implicit feedback. Our proposed method also performs competitively with the ‘Joint’ cases with explicit feedback. Furthermore, our method can moderately outperform the ‘Joint’ case with implicit feedback and ‘Genre’ data partition. It was demonstrated in [4] that local models could specialize on the Non-IID partitioned data and outperforms the ‘Joint’ case. User- vs. Item-alignmentIt needs to point out that that userand item-alignment have no impact on baseline models because the input of the base model and collaboratively lter models can be a pair of user and item. However, we use dierent training procedures for user- and item-aligned recommender systems to compare with autoencoder-based systems. Recall that user- and item-based autoencoder-based recommender systems handle userand item-alignment in a tabular fashion, respectively. Therefore, each data sample of a user- or item-aligned task corresponds to the ratings of one user giving to associated items or one item received from associated users. The results of baseline models for explicit feedback are similar for both user- and item-alignment. As for implicit feedback, the results of baseline models are dierent because of the ranking criterion MAP. We point out that the item-based AAE outperforms baseline models in the ‘Joint’ case with both types of feedback, and in the ‘Alone’ case with implicit feedback. AAE equipped with MATL signicantly outperforms all ‘Alone’ cases for both types of alignment, and also performs close to the ‘Joint case’ for useralignment. Furthermore, our method also moderately outperforms the ‘Joint’ case for item-alignment. With vs. Without side informationRecall that we use a subscript𝑠to denote models incorporating side information. The side information contains user proles such as occupation and gender, and item attributes such as genre. The results show that all models can benet from the side information. However, autoencoder-based recommender systems do not leverage the side information as much as collaborative lter methods. Some potential future works include incorporating raw side information such as text, image, and video, better modeling techniques of side information, and using side information from domains without ratings. Gradient assisted learning rateAs shown in Tables 3, 8, and 9, we perform an ablation study of the gradient assisted learning rate 𝜂. We have three control groups of𝜂, including ‘𝜂=0.1’, ‘𝜂= 0.1’, and ‘Optimize𝜂’. Here, ‘𝜂=0.1’ and ‘𝜂=0.3’ represent a constant gradient assisted learning rate for all domains. In all three cases, the optimization of gradient assistance weights is disabled. In particular, the weighted average becomes a direct average of outputs. As demonstrated in Figures 4, 5, and 6, the optimization𝜂 for explicit feedback may result in gradient explosion due to a very large𝜂. However, compared with𝜂=0.1, a moderately large 𝜂=0.3 can improve the convergence rate and result. On the other hand, the optimization of𝜂for implicit feedback greatly improves the performance, because the binary cross-entropy function may require a large𝜂at the early assistance rounds. The results show that𝜂has a large impact on the success of our method. A more reliable solution to choose 𝜂is to use a validation set. Gradient assistance weightsWe set𝜂=0.1 for the ablation study of gradient assistance weights𝑤. As shown in (b,d) of Figures 4, 5, and 6, the optimization of𝑤labeled as ‘AAE (Optimize 𝑤)’ moderately improves the performance for ‘Genre’ data partition. Although the impact of𝑤is not obvious in other scenarios, previous works show that using𝑤is robust against noisy residuals when we apply privacy enhancement techniques. A potential future work is to develop better techniques to aggregate outputs from each domain. Partial AlignmentTo compare with baseline models, we assume all users or items are common for user-aligned or item-aligned recommender systems. As mentioned in Equations 1 and 2, the common users or items are shared between a pair of domains. Specically, two domains can share a subset of their users or items. Therefore, we conduct an ablation study of partial alignment. We assume 50% of users or items are shared among all domains, labeled as ‘50% Alignment’, while the other half is unique for each domain. We set𝜂=0.1 and disable the optimization of𝑤. As demonstrated in Figures 4, 5, and 6, 50% alignment performs worse than full alignment with explicit feedback and close to the full alignment with implicit feedback. It indicates that our proposed solution is robust enough to improve the performance of partially aligned domains. Table 1: Results of the ML1M dataset for explicit and implicit feedback measured with RMSE and MAP, user- and itemalignment, with and without side information, and Uniform (𝐾 = 8) and Genre (𝐾 = 18) data partition. In this work, we present a new framework of privacy-preserving Multi-Target Multi-Modal Recommender system (MTMDR) based on Assisted AutoEncoders (AAE) and Multi-Target Assisted Learning (MTAL). Our proposed solution allows dierent organizations to improve their recommendation performance simultaneously while preserving their local data, models, and task labels. Our method covers broad application scenarios, including user- or item-based Figure 4: Results of the ML1M dataset across all assistance rounds. MTAL signicantly outperforms ‘Alone’ and p erforms close to ‘Joint.’ (a-d) Explicit feedback and User-Alignment. (e-h) Implicit feedback and User-Alignment. (i-q) Item Alignment. Table 2: Results of the ML10M dataset for explicit and implicit feedback measured with RMSE and MAP, user- and item alignment, with and without side information, and Uniform (𝐾 = 8) and Genre (𝐾 = 18) data partition. alignment, explicit or implicit feedback, and with or without side information. Extensive experiments demonstrate that our method Table 3: Ablation study of the ML1M dataset for gradient assistance weights, gradient assisted learning, and partial alignment. signicantly outperforms the case where each domain is locally trained, and it performs competitively with the centralized training where all data are shared. Consequently, our approach can eectively integrate organizations from dierent domains to form a community of shared interest. TheAppendixincludes further experimental details and results.