Under the “internal-ratings-based” (IRB) approach advocated in the Basel II and III regulation, banks use their internal risk rating systems to estimate the risk exposures, credit rating migration probabilities and the probability of default (PD) in order to evaluate their regulatory capital requirements [See e.g. Basel Committee on Banking Supervision (2004, 2009), Hull (2012)]. Under Pillar II, ﬁnancial institutions must also conduct stress tests to determine the proper level of capital needed to absorb losses in worsening economic conditions. For these reasons, banks perform their own credit rating migration analysis in order to monitor the changes in borrowers’ credit quality and to predict borrowers’ potential default in a volatile economic environment. This analysis concerns the “internal” or “in-house” established credit rating histories of borrowers that can be classiﬁed into various numbers of credit quality categories determined independently of the ratings publicly provided by the rating agencies such as the Moody’s. ical probabilities of default and migration probabilities. It diﬀers from the analysis of their risk-neutral counterparts, which underlies the pricing of credit derivatives, such as credit default swaps (CDS), or derivatives written on iTraxx [see, e.g. Duﬃe, Eckner, Horel, Saita (2009), Azizpour, Giesecke, Schwenkler (2018) in continuous time, Gouri´eroux, Monfort, Polimenis (2006) in discrete time, Gouri´eroux, Monfort, Mouabbi, Renne (2021) for joint historical and risk-neutral analysis]. The internal ratings are used for pricing the portfolios of credits to a large number of small and medium-size ﬁrms whose assets are not traded on the markets. Even for large ﬁrms, the historical and risk-neutral probabilities of default can diﬀer signiﬁcantly. Therefore, the analysis of internal ratings is consistent with prudential banking supervision and aims at avoiding a pure mark-to-market pricing of risk. of borrowers’ credit quality over time with respect to their previous ratings [Altman, Saunders (2004)]. These are recorded in the form of monthly or quarterly time series of credit migration matrices comprising qualitative ratings of ﬁrm, ranked from the low risk category A to the most risky rating D, of default. The ordered Probit model for credit ratings arises as a natural speciﬁcation, which has been extended to the Asymptotic Stochastic Factor Model (ASFR), by Vasicek (1991) [see also Vasicek (2015), Nickell, Perraudin, Varotto (2001)] and recommended under the Basel III regulatory measures. The ASFR is a stochastic factor probit model of default with an independent and identically distributed common random unobserved factor, capturing the systematic risk eﬀect. The factor is assumed to drive the parameters of a latent quantitative score function in the model, which determines the observed qualitative ratings. Due to the presence of the unobserved common factor, the observed rating histories are cross-sectionally dependent, which can explain default correla- The credit rating migration analysis concerns the changes [i.e. upgrades or downgrades] tion. Koopman, Lucas, Monteiro (2008), Feng, Gouri´eroux, Jasiak (2008), Creal, Koopman, Lucas (2012), and Creal, Schwaab, Koopman, Lucas (2014) extended this setup to multiple credit rating categories and a serially correlated factors, for predicting the future credit ratings of ﬁrms. to derive the joint density of observed ratings, the history of the latent factor has to be integrated out. Therefore, the exact likelihood function based on the joint density of rating histories involves an integral of high dimension that increases with the number of observations. Due to the presence of these integrals, the exact maximum likelihood is commonly replaced by an approximation. There exist various approximation methods, most of which involve a set of arbitrary control parameters, which have a signiﬁcant impact on the associated required capital. These are, for example, the discretization step [Farmer (2021)], smoothing parameters, penalization rate in neural networks, etc. The eﬀect of the statistical approximation and optimization method can go as far as to partly circumvent the need for keeping an internal capital reserve. Therefore, it is called a “statistical regulatory arbitrage”. This explains why these approximations are generally not validated by the supervisory authorities who are regularly auditing the internal databases and estimation techniques. banking supervisory authority has validated standardized approximation methods, such as the granularity adjusted approximation, corresponding to large cross-sectional asymptotics [see, Gagliardini, Gouri´eroux (2015) for general discussion] and the Simulated Maximum Likelihood method with a large number of simulations. stochastic factor ordered Probit model, as simple alternative methods providing consistent and normally distributed estimators. The MCL estimation method has been widely used in the statistical literature to handle complex likelihood functions [for example, see, e.g. Lindsay (1988), Varian (2008), Varian, Reid, Firth (2011) and Gouri´eroux, Monfort (2018)]. The composite likelihood functions are obtained by multiplying a collection of component likelihoods and are known to provide consistent parameter estimators (Varian, Reid, Firth , 2011). In this paper, the MCL estimator maximizes an objective function based on the exact likelihood function of the factor ordered Probit model. We propose three MCL estimators of diﬀerent complexity and prove their consistency and asymptotic normality. The performance of the estimators is examined in a simulation study. rating transitions. Section 3 describes the stochastic factor ordered Probit model. Section 4 introduces the composite maximum likelihood estimators and derives their consistency and asymptotic normality. The simulation results are presented in Section 5. Section 6 concludes the paper. Proofs, simulation details and additional simulation results are gathered in Appendices. The estimation of the ordered Probit model with a latent factor is challenging. In order This paper introduces a set of Maximum Composite Likelihood (MCL) estimators for the This paper is organized as follows. Section 2 describes the ordered probit model for credit In this section, we introduce the stochastic factor ordered probit model and its state space representation. Next, we derive the expression of the complete likelihood function to highlight the presence of multiple integrals of large dimension. Let y The latent continuous quantitative score (y y. More precisely, variables y with K rating categories : k = 1, . . . , K. The score is discretized in order to obtain the individual qualitative ratings. Therefore, a rating is determined by: where c credit rating (y c= −∞ and c the state space representation of the model. common latent factor f The multivariate, continuous, latent processes y (δ), volatility eﬀects (σ sensitivities. When coeﬃcient β is large (small, resp.), the eﬀect of systematic risk carried through the factor is strong (weak, resp.). While the idiosyncratic risks (u versiﬁed, the systematic risk (f risk generates risk dependence in the model. The following autoregressive model of order 1 (AR(1)) represents the common factor dynamics: where η the state equations of the state-space model. Let us introduce the following assumptions: Assumption A.1: The errors u normal variables. and ydenote the (credit) score and rating of ﬁrm i, i = 1, ..., N at time t, t = 1, ..., T . < ··· < care thresholds. Relation (2.1) shows how the observable endogenous The conditional distribution of the quantitative scores given the past depends on the deﬁnes the shock to the common factor. The system of equations (2.2)-(2.3) deﬁnes ing separately the idiosyncratic and systematic innovations. The assumption of identical distribution and the fact that coeﬃcients in (3.2) are independent of the ﬁrm implies that we consider a homogeneous set of ﬁrms, obtained by crossing the country, the industrial sector and the size, in conformity with the current regulation. Assumption A.2: The factor process (f sive equation (2.3). that the joint n−dimensional process y its state discretized version y i = 1, . . . , n are not independent due to the eﬀect of the common factor f the error variance in equation (2.3) has been set equal to 1 − ρ fis marginally distributed with mean 0 and variance 1. This constraint is imposed on the marginal distribution of the factor for identiﬁcation of parameters δ Assumption A.3: The variables (y network framework is also available, if we distinguish three layers: the deep layer generates the factors values and the idiosyncratic errors; the intermediate layer computes the latent scores; the output layer provides the observed rating. By analogy with the graphical neural network representation, we obtain the following scheme displayed in Figure 1 (given with n = 2 and K = 3). each individual (i.e. ﬁrm) can request the records of its own score history. However, the complete score database is, in general, proprietary and the information on the quantitative scores is not available to an outsider econometrician/data scientist. This is the rationale for introducing Assumption A3. studied in Tuzcuoglu (2019), who considers idiosyncratic correlations, but does not include the systematic factor component required in the Basel regulation for stress tests. cross-sectional correlation between individual risks. Second, to get a complete dynamic model that can be used to predict the defaults in the future. A bias would result from directly replacing factor f VIX, a consumer sentiment index, consumption growth, a business cycle indicator [see e.g. Berndt, Douglas, Duﬃe, Fergusson (2018), Azizpour, Giesecke, Schwenkler (2018)], or the slope of the yield curve. Moreover, with observed factors, the predictions could not be performed without introducing an additional model for all components of The independence assumption allows for performing impulse response analysis by shock- As the processes (f), (u), i = 1, . . . , n are independent and strictly stationary, it follows This assumption explains the state space interpretation of the model. Moreover, a neural In practice, the underlying quantitative scores are computed by a credit institution and The above state space speciﬁcation diﬀers from the autoregressive probit panel model The factor fis assumed unobserved for the following two reasons: First to create the In order to derive the joint density of observations y observed factor history has to be integrated out. As a consequence, observations y cross-sectionally dependent and serially dependent with a non-Markovian serial dependence. More precisely, the stochastic migration probabilities between dates t −1 and t, conditional on f where Φ denotes the cumulative distribution function (c.d.f.) of the standard normal. Thus each row of the transition matrix conditional of (f with a common explanatory factor f correlated, as in (2.3), the transition matrices are stochastic and serially dependent. vector θ includes the parameters of the state space model, which are parameters δ 1, . . . , K in the quantitative score, and parameters c Figure 1: Graphical Representation in Neural Network when n = 2 and K = 3 , are given by: p= p(f; θ) = P [y= k|y= l, f] = P [c≤ y< c|y= l] Let us now deﬁne the log-likelihood function of the stochastic migration model. The the conditional migration matrices are functions of parameter vector θ as well as of the common factor values (f where n Y = (y to be integrated out and the log-likelihood function, given the initial value y where ψ refers to the joint probability distribution function (p.d.f.) of factor values. The above log-likelihood function contains a multivariate integral. The dimension of this integral is of order T , as there is a common factor value for each transition at time t. Therefore the exact computation of this likelihood is impossible and its approximation is often not suﬃciently robust (see, Section 1). The MCL estimators are convenient alternatives for complicated nonlinear dynamic state-space models allowing for circumventing the high-dimensional integral. The process of transition matrices {P which provide the probabilities of transitions from state l to state k between times t −1 and t given f ) for i = 1, ..., n and t = 2, ..., T , F = (f), t = 2, . . . , T and y= (y, . . . , y). Since the factor history is not observed, the distribution of factor values (f, ..., f) has . From (2.4), it follows that the elements of matrix Pare: = p(f; θ) = P[y= k|y= l, f] = Φ−Φ, k, l = 1, ..., K. Let us now compute the product of two successive transition matrices P= PPto obtain the probabilities of transition at horizon 2 from state l to k between times t − 2 and t given (f The elements of matrix P : are obtained by integrating out the unobserved factor factor value f Proof. See Appendix A.1. The elements of matrix P (2) are: Lemma 2 Under Assumptions A1, A2 and A3, we have where φ is the p.d.f. of the standard normal. ). The elements of matrix Pdepend on f, fand are given by: p= p(f, f; θ) = P[y= k|y= l, f, f] =[p(f, θ)p(f or, p(2; θ, ρ) = P[c< y< c|y= l]. Proof. See Appendix A.2. compute numerically. This section presents the conditional composite likelihood function for the migration model with an unobserved AR(1) factor. The composite likelihoods [see Cox, Reid (2004), Varian, Reid, Firth (2011)], are often based on misspeciﬁed likelihoods, which are easier to calculate. In our framework, the composite likelihoods are constructed from the exact expected migration probabilities at horizons 1 and 2 to reduce the dimension of the integrals. L(θ) , is deﬁned as: where n to k in one step over the period. The log-likelihood L by the common factor. Moreover, L these were components of a Markov chain with transition matrix P , while (y are not Markov due to the factor integration, which increases the memory of the process. Therefore, the CL(1) is a quasi (pseudo) log-likelihood. The composite log-likelihood CL(1) depends on parameter vector θ only, and cannot be used to estimate the factor dynamic, i.e, the autoregressive coeﬃcient ρ. For that purpose, it is necessary to increase the lag. The transitions at horizon 2 involve one-dimensional integrals only, which are easy to i) The Conditional Composite Log-Likelihood at Lag 1 The conditional composite log-likelihood function at lag 1, called CL(1) and denoted by ), i = 1, ..., n, were independent across the individuals, while in reality they are linked ii) The Conditional Composite Log-Likelihood at Lag (2) The conditional composite log-likelihood at lag (2), called CL(2) and denoted by L(θ, ρ), is: where n The composite log-likelihood function L ditional on (y another and (y a quasi (pseudo) log-likelihood too. mentioned above, we can expect to identify θ from L characterizing the cross-sectional dependence. L suﬃcient to identify ρ. the previous composite log-likelihoods at lags 1 and 2: This artiﬁcial objective function cannot be interpreted as a quasi likelihood. As shown in Section 2.2, the complete log-likelihood has a complicated expression including a high-dimensional integral, with the dimension increasing with T . From the granularity theory [Gagliardini, Gouri´eroux (2014, 2015)], it follows that when n tends to inﬁnity, an estimator of θ, asymptotically equivalent to the ML estimator can be obtained from the log-likelihood conditional on the factor path by maximizing this conditional log-likelihood with respect to both parameter θ and the factor path. This log-likelihood conditional on An important diﬀerence between Land Lis the set of identiﬁable parameters. As iii) The Conditional Composite Likelihood up to Lag 2 The conditional composite log-likelihood up to lag 2, CL(1,2), is deﬁned by summing up iv) The Conditional Log-likelihood (f) is: It resembles the composite log-likelihood L p(θ) has been made independent of f to θ, f the factor values. In the second step, an estimator of ρ is obtained by regressing lag 1, as it does not take into account the serial dependence due to the factor, a loss of information results and the estimator is not asymptotically eﬃcient. The estimator of θ obtained by maximizing the conditional log-likelihood (3.6) captures that serial dependence through the “nuisance” parameters f granularity theory. serial dependence parameter ρ, we expect to partly reduce the lack of eﬃciency for θ under the CL(1) approach. for each of the conditional composite log-likelihoods are discussed. The parameters to be identiﬁed and their respective numbers are as follows: The total number of independent parameters to identify is 4K −2. The negative two is due to the score y , . . . , f, it provides not only an estimator of θ, but also an approximationˆfof , t = 2, . . . , T . v) Loss of information in the CL approaches Intuitively, under an identiﬁcation restriction, the composite log-likelihood estimator at ˜θ= ArgmaxL(θ), is a consistent estimator of the true parameter value. However, By considering the conditional composite log-likelihood up to lag 2 that involves the In the next subsection, the identiﬁcation constraints, and the order and rank conditions a linear function of factor f The intercept and slope of that linear function are not identiﬁable. In this subsection, the order conditions for each conditional composite log-likelihood are discussed. ments p due to the unit mass restriction on each column. Hence, the order condition is: by taking into account the absence of parameter ρ in the objective function. This order condition is satisﬁed for K > 5. the integral expression of p respective numbers are as follows: where  = ±1 is an unknown sign, since the distribution of f is symmetric. This implies that the integral expression in Lemma 2 is also valid with f replaced by −f. There is only one such invariance property and therefore the sign  is equal for all l. The total number of identifying functions of parameters is 2K(K − 1) + 2K = 2K is: The order condition holds for any K. i) Identiﬁcation under CL(1): The identifying functions are the reduced form parameters involved in CL(2), i.e. the ele(θ) of matrix P . There are K(K−1) of these elements that are linearly independent ii) Identiﬁcation under CL(2): The identifying functions are determined by observing that the factor f varies within component of the total composite log-likelihood. Therefore, the order condition is: The order condition is satisﬁed for any K. Proposition 1 Under the CL(1) log-likelihood function and the identifying constraints c 0, γ the γ Proof. See Appendix B.1. Proposition 2 Under the CL(2) composite log-likelihood function and the identifying constraints c sign  for β Proof. See Appendix B.2. such as: The unknown sign  is a problem of global identiﬁcation and not of local identiﬁcation. Hence, when the asymptotic properties of the estimators are derived (see Section 4), this inequality constraint has to be taken into account to obtain the consistency of the estimator. It has no eﬀect on asymptotic normality. The asymptotic properties of the composite log-likelihood estimators are discussed in the next Section. iii) Identiﬁcation under CL(1,2): The total number of functions available is equal to the sum of functions available for each = 1, we can identify the thresholds c, k = 2, ..., K, the intercepts δ, l = 1, ..., K, andp =β+ σ, l = 2, ..., K. In order to identify the unknown sign , an additional constraint needs to be introduced In a panel data framework, the asymptotic analysis can be performed with respect to the cross-sectional dimension n and time T that can tend to inﬁnity as follows: The double asymptotics in case (iii) has been recently developed for applications to big data [Gagliardini, Gouri´eroux (2014, 2015), Bonhomme, Jochmans, Robin (2017)]. It corresponds to a long panel of high dimensional time series. literature concerns the granularity adjusted version of the (complete) maximum likelihood method, i.e. the estimation of θ, f Gouri´eroux (2014, 2015)]. Let us denote the maximizers of equation (3.4) by and 1, ..., T by ˆρ (i) If n → ∞, T → ∞, 1, ..., T (but the convergence is not necessarily uniform in t). c. ˆρ In the migration model with an unobserved factor, the asymptotic analysis existing in the ˆθ, and the autoregressive coeﬃcient estimator obtained by regressingˆfonˆf, t = is consistent of θ, asymptotically normal and converges at speed. ˆfis consistent of f, asymptotically normal and converges at speed, for any t = is consistent of ρ, asymptotically normal and converges at speed. Depending on the setup, other asymptotic results can be considered. For example: converges to a stochastic limit, is consistent of θ, asymptotically normal and converges ˆis consistent of f, asymptotically normal and converges at speed, for any t. is inconsistent. negligible measurement errors of consistency. are much easier to derive than the asymptotic properties of the complete ML estimator. Indeed, the composite log-likelihood functions are ﬁnite sums of products of summary statistics and functions of parameters. This simpliﬁes the proof of uniform convergence with respect to the parameters. The next section examines the asymptotics (i) -(iii) and describes the properties of the conditional composite maximum likelihood estimators. This section examines the consistency of the maximum conditional composite likelihood estimators of the identiﬁable parameters. To prove the consistency, we need the following additional assumption: Assumption A4 set. are inﬁnitely continuously diﬀerentiable with respect to f (i) Short panel asymptotics: n → ∞, T ﬁxed. posite ML estimators to be consistent, for T ﬁxed. This is a consequence of the cross-sectional dependence due to the common systematic factor f T = 2 and consider the maximum conditional composite likelihood estimator based on CL(1). For T = 2, the conditional composite log-likelihood is: with n are the vectors with elements c restriction in Proposition 1, that are c that the rating indicators are nonnegative and bounded, we can apply the Strong Law of In this latter case, it is easy to see that the regression used to estimate ρ involves non The asymptotic properties of the maximum conditional composite likelihood estimators i) The parameter set of (θ, ρ) is compact, and strictly included in the set σ> 0, ∀l, |ρ| < 1. ii) The model is well-speciﬁed, i.e. the true value (θ, ρ) is in the interior of the parameter The condition σ> 0, ∀l, ensures that the transition probabilities p(f; θ) [resp. p(θ), p(2; θ, ρ)] As for the granularity-approximated complete ML estimators, we cannot expect the com- Large Numbers. The normalized log-likelihood tends a.s. to: where where p parameter value θ eters. Then, they admit at least a maximum on the parameter set by Assumption A4 ii). Let us denote by θ problem, we have: By applying the Jennrich inequality [Jennrich (1969)], if the pseudo true value is unique, there exists a sequence of maximum conditional composite likelihood estimators Argmax ues are functions of θ not depend on f limit whose distribution depends on the distribution of f (iii) Double asymptotics: n → ∞, T → ∞ is the frequency of individuals in category l. We have: (θ) is the stationary probability of being in rating category l evaluated at the true lima.s.L(c, δ, γ) 'p(θ)( lima.s.ˆp) log p(c, δ, γ) (c, δ, γ) and its limit L(c, δ, γ) are continuous functions of the identiﬁable paramθ= (c, δ, γ) = argmaxp(θ)p(θ, f) log p(c, δ, γ). This is the multivariate time series framework. The standard results for the consistency Let us now consider the double asymptotics with CL(1) approach. We have: 1, ..., K, and (n vergence. A suﬃcient condition for this uniform convergence is Assumption A.5: n, T → ∞ with T/n → 0. than the time dimension. Under this Assumption, the uniformity in t is easily checked [see Appendix C]. composite log-likelihood is maximized iﬀ Then, by the identiﬁability of θ = (c, δ, γ) (see Proposition 1), we get θ consistency follows. above, the composite log-likelihood is continuously diﬀerentiable. Since the estimator (ˆc rameter set, the estimator will also be asymptotically in the interior of the parameter set and will satisfy the necessary ﬁrst-order conditions for large T . Therefore, we have: We can perform a Taylor-McLaurin expansion with respect to θ. Let us assume: Assumption A.6: The parameter set Θ for θ is convex. Since T is now varying, we need uniform a.s. convergence of the ratios (n/n), l = This corresponds to the panel estimation with the cross-sectional dimension much larger By the property of the Kullback-Leibler divergence measure, we know that this limiting ,ˆδ, ˆγ) tends to the true value θ= (c, δ, γ), which is in the interior of the pa- Assumption A7: The matrix J where vec denotes the vectorialization with respect to the indexes k, l. erty holds for the K dimensional process [vec(p ˜θis an intermediate value betweenˆθand θ. By applying the same argument as for the uniform a.s. convergence of the log-likelihood (1) is a negligible term in probability. Let us assume: Then, by normalizing the expansion (4.1) by 1/(nT ), we get: The common factor fis strictly stationary, geometrically mixing. Thus, the same propsince we can commute the derivative bounded on Θ) and since θ we get the asymptotic normality of: by the geometric ergodicity of the p follows. Proposition 3 Under Assumptions A.1-A.7, the maximum conditional composite likelihood estimator at speed where As expected, we obtained the following: (i) The speed of convergence of complete log-likelihood. This is a consequence of the crude cross-sectional aggregation of the data in the composite approach as if the observations y (ii) The asymptotic variance is obtained from the ”sandwich” formula, as it is common in a mis-speciﬁed (pseudo) maximum likelihood approach [see Hubert (1967), White (1982)]. (iii) The terms (p They are correlated due to the factor dynamics (except when ρ = 0, that is the case of an i.i.d. factor). Therefore, the covariances have to be taken into account even if we consider ˆθobtained by maximizing L(θ) is consistent, converges to the true value θ × vec, h = 1, 2, ... only a small number of values of lag h. Note that the sum the geometric ergodicity of the factor process. lihood that usually considers either i.i.d. individuals, or ﬁnite dimensional time series [see e.g. Cox, Reid (2004),Varian, Reid, Firth (2011)]. tor is consistently estimated by considering appropriate sample counterparts of components In this section, we undertake a Monte Carlo experiment to assess the ﬁnite sample properties of estimators based on the conditional composite likelihood function. The designs include K = 8 ratings, which satisfy the order conditions in Section 3.3.1. These eight states are distinguished by aggregating 14 states of credit quality used by the bank. These credit rating categories have been aggregated into 8 categories resembling the the Standard & Poor’s credit ratings scale for long-term bonds, which consists of the following eight categories: AAA, AA, A, BBB, BB, B, CCC/CC, and D. The best rating AAA means an “extremely strong” capacity of the borrower to repay its debt, while the worst rating D means that the issuer of the bond is “in default”. The intermediate ratings between the two extreme cases indicate a decreasing capacity to repay which corresponds to “very strong”, “strong”, “adequate”, “faces major future uncertainties”, “faces major uncertainties”, and “currently vulnerable and/or has ﬁled for a bankruptcy protection as Chapter 11”, respectively. In the following, each rating is denoted by k = 1, . . . , 8, where a higher k indicates a lower capacity to repay debt. Given the rating at time t −1, i.e. y continuous quantitative score y The above asymptotic analysis is diﬀerent from the main literature on composite like- The asymptotic variance-covariance matrix of the composite maximum likelihood estima- . The sample counterparts of Jand Iare described in Appendix D.1. where the rating is determined by: with the thresholds (c The thresholds and intercepts are ranked in an increasing order, and their values are chosen to get higher transition probabilities on the main diagonal and decreasing probabilities when a ﬁrm transits to other states. The treatment of “absorbing state D” corresponding to l = 8 is discussed later on. The uncertainty on migrations is driven by speciﬁc shocks u f. To see the eﬀects of the distribution of risks between the systematic and the speciﬁc components, we consider diﬀerent designs for σ identiﬁcation properties of the composite likelihood methods CL(1), CL(2) (see Propositions 1 and 2, in Section 3.3.2) and the diﬀerent deﬁnitions of the transformed parameters γ consider three designs. In the ﬁrst design, appropriate for CL(1) (see Proposition 1), we set β for CL(2) (see Proposition 2), we take into account the autocorrelation parameter ρ and ﬁx Design 1: The idiosyncratic and systematic components have, for each l, the same impact, that is: σ downgraded. Design 2: The two risky components have, for each l, the same impact, that is: σ Design 3: The impact of the systematic component relative to the idiosyncratic one decreases with l. This means that the idiosyncratic errors largely explain the junk bonds in non crisis = σ=, such that γ=σ+ β= 1. In the two other designs, appropriate =√, such that γ=σ+ (1 − ρ)β= 1. Next, we have also to ﬁx values of , l ≥ 2, compatible with the ﬁxed values of σ, β. , with r = 0.05, taking the autocorrelation parameter into account. environment. To capture this feature, we consider the ratios where β where the autocorrelation parameter ρ measures the persistence and f tion of the Value of the Firm model introduced in Vasicek (2015). Options Exchange volatility index (VIX) over 60 months from January 2013 to December 2018. Indeed, the VIX index is often considered as a good proxy for systematic risk factor, since it represents the investors forecasts of future market uncertainty, as reﬂected in the prices of derivatives written on the S&P 500. of monthly values of VIX over the period from January 2008 to December 2008 during the ﬁnancial crisis. practice, this time unit is one month. Therefore, a value ρ = 0.1 corresponds to a daily autocorrelation ρ returns. For ρ = 0.7, the associated autocorrelation in daily data is about 0.99. The last state, i.e. default, is an absorbing state. Therefore, if we follow a given population of corporates, all of the corporates will default at some date, and the number of still alive corporates (the so-called Population-at-Risk (PaR)) will diminish. Theoretically, the process of observed ratings is asymptotically stationary with a stationary distribution equal to a point mass on default. Hence, the regulatory conditions for the convergence and asymptotic normality of the maximum composite likelihood estimators are not satisﬁed. This diﬃculty can be (partly) solved in two alternative ways. i) There is currently a change of regulation known as the resolution step. The idea is There is also a persistence of the systematic factor f, that satisﬁes: We consider three values for the autocorrelation parameter ρ, that pertains are: i) ρ = 0, that corresponds to independent migration matrices. This is the basic assumpii) ρ = 0.4, that corresponds to the autocorrelation at lag 1 of the monthly Chicago Board iii) We also consider a value of ρ, ρ = 0.7. This value corresponds to the autocorrelation The autocorrelation value depends on the time unit in our discrete time model. In to assist the very risky ﬁrms before they enter into default. Loosely speaking, in the risky grade 7, the supervisory authority can partly take control of the ﬁrm and monitor its restructuring and its debt renegotiation to avoid default, when default is due to transitory diﬃculties. This reduces the probability of default, while avoiding ii) The second approach assumes that newly created corporates oﬀset the corporates entering gration that can be or not adjusted by taking into account the newly created ﬁrms. When the newly created ﬁrms are taken into account, the migration matrix will be indexed as P Let us for instance consider the design 3 with ρ = 0.4. For each individual i, we can compare diﬀerent time series, such as the series of the underlying scores, of the ratings as well as the series providing the expected stability in the current score. The series are y where We can also consider the similar series, with f factor, in order to provide insights on the eﬀect of f Figure 2 with an initial factor value set at f corresponding to AA. The trajectories correspond to three diﬀerent corporate bonds. At time 0, a bond is issued with a rating 2 (AA). It has some downgrading after month 10 up to default at month 21. At this time a new bond is issued to balance the defaulted bond at a rating 1 (AAA), with a systematic downgrading up to default at month 44. Then a new bond is issued at time 45 and so on. tic number of ﬁrms, not to a single ﬁrm. This stochastic number is equal to the number of times default is reached plus one. The advantage of this practice is in ensuring the stationarity of the process, and also getting the rating histories of equal length T. this probability being equal to zero. We do not incorporate the resolution step in our modelling, as that would require a modelling of the supervisory authority behaviour. into default, thus ensuring a PaR of constant size. This corresponds to the model with equal birth and death rates used in epidemiological studies (see e.g. Harko, Lobo, Mak (2016)). As at the time of new ﬁrms arrival their rating are high, we replace the last row of the migration matrix at the individual level, corresponding to a standard absorbing state, by the row of assignment of new entries at the population level, Thus we have to distinguish individual migration matrices P, from the population mi- In such an equal birth-death rates environment, each trajectory corresponds to a stochas- In this section we present the matrices for design 3, ρ = 0.4. Recall that the time unit is one month and the horizon one and two refer to the periods of one and two months, respectively. Matrix P large values and two adjacent diagonals with larger values for downgrade than the upgrade. Moreover, there are signiﬁcant probabilities of default from grades 6 and 7, corresponding to “junk bonds”. Note also the high probability of default for rating 1 (AAA). Then we can look for the nondegenerate stationary distribution π corporate bond will default and the asymptotic stationary distribution at the individual level would be a point mass at 8 (D). The interpretation of stationary distribution π and at the population level. It provides the long run rating structure of the population of corporate bonds under rebalancing. This long run structure is given in Table 4. In practice, when analyzing the ratings, the stationary distribution provides information on how the ratings agencies are discretizing their scores to deﬁne the ratings. In our experimental design, they are discretized to get rather similar proportions of bonds rated 1, . . . , 7. Let us now consider the migration matrix at horizon 2. Table 5 shows the matrices P We observe a common feature of a migration matrix, which is the main diagonal with As noted earlier, due to the absorbing state, and without equal birth-death rates, each ). (P)is computed under closed form and P(2) computed by Monte-Carlo integration with S = 50, 000 replications (see Lemma 3). The changes in P to P matrices have now non zero elements on the diagonals distant by 2 from the main diagonal by time aggregation. The matrices P the eﬀect of the systematic uncertainty. Next we repeat the same exercise, when there is no systematic eﬀect, that is if f presented in Table 6. As expected P are due to variation in the aggregate eﬀect of both speciﬁc and systematic shocks. Both P(2) = (P) To give some insights into the accuracy of the MCL estimation, in terms of the number of months T and the factor autocorrelation parameter ρ, we conduct some Monte-Carlo experiments. The estimation is performed with N = 1, 000 ﬁrms, including the adjustment for the newly created ﬁrms, and for the diﬀerent designs described above. The number of observation periods is ﬁxed to T = 60 (5 years), T = 120 (10 years), T = 240 (20 years). For each experiment, we perform S = 1, 000 simulations of individual trajectories, with initial ratings y ratings. The stochastic migration model depends on a large number of parameters that are 19 identiﬁable parameters (c (c. . . , c identiﬁcation conditions c use their estimates to illustrate the prediction of the following downgrade probabilities and probabilities of default. i) the downgrade probabilities at horizon 1 and 2 of a ﬁrm currently rated A (l = 3): ii) the term structure of the probability of default at diﬀerent horizons h for a ﬁrm currently In the next sections, we discuss the CL(1) estimation results based on design 1, and the CL(2) estimation results based on designs 2 and 3. Table 7 in Appendix D.2 shows the accuracy of the CL(1) estimates for design 1 when the autocorrelation parameters ρ = 0. In the table, we report for each identiﬁable parameter, its true value, the mean absolute bias of the CL(1) estimates, and the associated standard errors in terms of the number of months T . The absolute mean bias is computed by averaging the absolute value of the bias. To ﬁnd the standard errors, we compute the estimator in Proposition 3, which is the heteroskedasticity and autocorrelation consistent (HAC) estimator. The HAC estimator is obtained using a quadratic spectral kernel and a bandwidth, , δ. . . , δ, β. . . , β, σ. . . , σand ρ) for the CL(2), taking into account the two DP (1|A), DP (2|A) rated A. The horizons are ﬁxed to 1 month, 1 year, 2 years, 3 years, and denoted P D(1|A), P D(12|A), P D(24|A), P D(36|A) dV ar√Tˆθ− θof the asymptotic covariance matrix Σ= JPIJ which we set to be 4 (T /100) obtained from the diagonal elements of the average of the estimated variance over all simulations after dividing by T . relative to their true value. Both decrease as the number of months increases. For instance, when ρ = 0, the largest absolute bias is 1.3, when estimating the threshold parameter c and T = 60, which remains small relative to its true value c when T = 240. Meanwhile, the standard error decreases from 1.20 to 0.60. These results are consistent with the asymptotic results on the Proposition 3. Also, we notice that, in general, there is a small eﬀect of ρ on the consistent estimation of the diﬀerent parameters. Tables 8 and 9 show that when the autocorrelation parameter increases to ρ = 0.4 and ρ = 0.7, the bias and standard errors remain small. When ρ increases, there is more persistence in f f. This impacts the accuracy that diminishes when ρ increases, while being still compatible with the consistency of the estimation. asymptotically valid t-statistics by plotting the empirical probability distribution function (PDF) of each computed t statistic when testing that each of the parameters equals their true value. See Figures 3 − 11 in Appendix D.2. In Figures 3 − 5, we plot the empirical probability distribution by depicting the histogram of the computed test statistics over the S simulations for the threshold parameters (c the diﬀerent values of ρ. Figures 6 − 8 show the empirical probability distributions for the intercepts (δ variances (γ simulations, while the y-axis contains their frequencies. A common feature in the ﬁgures is that, when T varies, the distribution of t statistics is centered on zero. In most cases, the simulated t-statistics belong to the intervals [−1.96, 1.96], as would be expected for asymptotically normally distributed t statistics, when the level of the test is 5%. However, we observe on Figures 6-8, that when the autocorrelation parameter becomes higher (ρ = 0.4 and ρ = 0.7), for some of the δ mean due to the smaller variance, suggesting an under-estimation of the variance of δ shift is negative for the negative values of parameters δ of the parameters. We provide in Table 10 (see Appendix D.3) the mean absolute bias and the standard errors of the estimates of c for design 2 when ρ = 0. The estimate for β condition γ Table 7 shows that the CL(1) estimates have a small estimated bias and standard errors Next, we illustrate the ﬁnite sample performance of the inferences based on the standard for each parameter its true value, the mean absolute bias and the standard error of the CL(2) estimation for each value of T . The main conclusion is that for each value of the time period T , the estimates have, in general, small estimated biases and standard errors for the thresholds, the intercepts, and the autocorrelation parameter. The biases and the standard errors decrease as the number of periods increases, conﬁrming the asymptotic results in Proposition 3. For instance, when estimating c to 0.091 for 240, and are small relative to the true value of the parameter, which is 6. The standard error decreases from 0.72 to 0.36. We observe similar results for the autocorrelation parameter, the estimated factor sensitivities, and the volatilities. cially for the junk bonds rating. This is related with the nonidentiﬁcation of β Even with CL(2) the two parameters are still weakly identiﬁable. This could be improved by applying a two-step procedure, in which CL(1) is applied to the parameters identiﬁable by CL(1). Next, these estimates are plugged in the CL(2) criterion, that is next optimized w.r.t. to the β avoid a large number of nested simulations. ρ = 0 . Nevertheless, the biases and standard errors are relatively slightly higher for these parameters when the persistence in the systematic factor increases, but reduce as T rises (see Tables 11 and 12). Tables 14, 15, and 16 are about the mean absolute biases and the standard errors of the CL(2) estimators when the impact of the systematic component relative to the idiosyncratic one decreases with l in design 3. The ﬁndings remain the same as for design 2. typically the case of the threshold parameters c have such direct practical interpretation. In fact, the real parameters of interest are generally nonlinear function of β previous tables.For instance, we might be interested in the relative weight of the idiosyncratic and systematic shocks, that is the ratio β tables which provides no information on the dependence between of interest are the downgrade probabilities and the probability of default. They will be analyzed in the next paragraph. diﬀerent horizons for design 2 and design 3, respectively. The downgrade probabilities and probabilities of default are computed from the average over S Monte-Carlo approximations of the probabilities of transition at horizon h, P (h) = E(P in the estimated parameters. Let us ﬁrst discuss the results for design 2 when ρ = 0 in the ﬁrst panel of Table 13. We noted that the downgrade probability and the probability of default increase as the horizon increases, and this feature is reproduced after the estimation. At horizon 1, when T = 60, the downgrade probability is 33.29%, while its true value is Note however that the accuracy improves slowly with T for the parameters β, σ, espe- We also observe that the estimated autocorrelation parameter is very accurate when Some of the parameters introduced in the model have important interpretations. This is Tables 13 and 17 present the downgrade probabilities and the probabilities of default at 32.52%. At horizon 2, when T = 60, the estimated probability increases to 44.67%, which is also close to the true value, 43.35. At horizon 1, all the probabilities of default are very close to zero for all values of T . This is consistent with the fact that the probability for a ﬁrm with a strong capacity to repay its debt (a ﬁrm rated A) defaults is negligible within one month. The estimated probabilities are closer to their true values when the autocorrelation parameter is small. See also the second panel and the third panel of Table 13, illustrating design 2 and design 3 when ρ = 0.4 and ρ = 0.7, respectively. The ﬁndings for design 3, presented in Table 17, are very similar. This paper proposes a MCL estimation method for the stochastic factor ordered Probit model, as an alternative to other approximation-based estimation methods validated by the banking supervisory authority. The advantage of this method is that it can be easily adjusted to a varying numbers of credit quality categories for internal credit rating analysis. In addition, it allows for introducing one or possibly multiple latent factors to capture the common dynamics of credit quality of the borrowers or of a leading indicator on their market. Because, the conditional migration matrices are functions of the parameters θ as well as of the common factor values (f integral of order T . We propose three MCL estimators of diﬀerent complexity: conditional composite log-likelihood function at lag 1, the conditional composite log-likelihood at lag 2, and the conditional composite likelihood up to lag 2. The paper discusses the identiﬁability of the model parameters and establishes the asymptotic properties of the maximum conditional composite likelihood estimators. It derives the conditions for the uniform convergence with respect to the identiﬁable parameters, and for the asymptotic normality of the proposed estimators. We illustrate the ﬁnite sample properties of the conditional composite loglikelihood at lag 1 and the conditional composite log-likelihood at lag 2 through Monte-Carlo experiments. ∼ N(0, 1) and f∼ N(0, 1) are independent. Then, if y= l, y∼ N(δ, σ+ β). It follows that: and We have: Since f where the components of matrix A are given by: by the independence between (η are: P (2) = E[P (f; θ) P (f; θ)] = E[P (ρf+1 − ρη; θ)] P (f; θ)]. and ηare independent, η∼ N(0, 1) and f∼ N(0, 1), we get: a(f; θ, ρ) = Pc< y< c| y= l, f p(f; θ) = Φ− Φ, k, l = 1, ..., K. From Lemma 1, and the deﬁnitions c functions of parameters are: Therefore parameter ρ is not identiﬁable. Moreover, parameters σ from β There are K(K − 1) identifying functions (a.1), that we would like to use to identify the (K − 1) values of c follow Gagliardini, Gouri´eroux (2015) and add the identifying constraints: Next, we proceed as follows: (b) For l = 1, we have γ c, k = 2, ..., K are identiﬁed. p(2; θ, ρ) =[a(f; θ, ρ) p(f; θ)]φ(f)df (c) Then the identifying functions can also be written as: Then from the identiﬁcation of the c identiﬁed, and from (c), we identify δ We have the following identifying functions of parameters: Let us deﬁne: and use the identifying constraints: Then we proceed as follows: (d) Then, it follows from (b) that parameter σ and equation (1), we identify. and equation (3), we identify, l = 1, . . . , K. Hence, from (c), it follows that c (f) From equation (1), the quantities are identiﬁed since γ Then, by (a), the ratios (g) From (f) and (e), parameters γ (h) From (a) and (g), parameters δ (i) From (b) and (h), parameters σ (j) From equation (4) and result (i), parameters β (k) From (2), we get the ratios (l) Finally, from (j) and (k), we identify parameter ρ. Let us introduce a more precise notation: ˆp are introduced to explicit the dependence in the number of individuals n and the number of dates T . Indeed, by the Hajek, Renyi inequality [ Hajek, Renyi (1955), Csorgo (1967, ineq (2.8))], we get: where c is a constant. Then, it follows that: P [Max|ˆp(m, T ) − p(f, θ)| > ] <p(f, θ), ∀ > 0, ⇐⇒ P [Max|ˆp(m, T ) − p(f, θ)| > ] <p(f, θ), ∀ > 0, For n, T large, the upper bound: the geometric ergodicity of factor (f and the required uniformity . Therefore, after the normalization, the a.s. limit of the normalized composite log-likelihood is: (by the uniform a.s. convergence) More precisely, L lima.s.nlog p(θ) a.s.nˆplog p(θ) a.s.limˆp(n, T )log p(θ) a.s.P[c< y< c, c< y< c|f, f] log p(θ) (c, δ, γ). This section provides more details for the implementation of the simulation experiments and additional results. To compute the estimated asymptotic variances, we use the estimator of the asymptotic covariance matrix Σ the heteroskedasticity and autocorrelation consistent (HAC) estimator where k (·) is a kernel function, and B using a quadratic spectral kernel with the bandwidth, which we set to 4 (T /100) suggested by Newey, West (1994). The kernel k(·) is a decreasing function, which accounts for the the decaying dependence between the observations at t and t + h when h increases. second-order partial derivative in the deﬁnition of the variance estimator for the CL(2) is obtained using an outer-product argument. In particular, we note that: All derivatives were obtained using the numerical gradient function in Matlab. The which is equivalent to For the CL(2) estimators of variances, p respectively. In the diﬀerent estimations, we replace β tiﬁcation condition γ to impose the sign restriction on one of the β log-likelihood are taken into account using the estimated constant transition probabilities to states k = 1, k = 2 and k = 3. Furthermore, the composite log-likelihood at lag 2 and its derivative depend on an integral which cannot be computed analytically. This integral is the expected value of where the source of the randomness is f, which follows a standard normal distribution. Given the treatment of the default state, the migrations from this state in the composite 1 − ρ is simulated S = 1, 000 times from a normal distribution using f= ρf+ η, f∼ N(0, 1) and η∼ N(0, 1), to incorporate the correlation among the factors. Table 7: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 1 when ρ = 0 Unconditional Volatities T = 60 T = 120 T = 240 T = 60 T = 120 T = 240 Table 8: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 1 when ρ = 0.4 Table 9: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 1 when ρ = 0.7 Unconditional Volatities T = 60 T = 120 T = 240 T = 60 T = 120 T = 240 Table 10: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 2 when ρ = 0 Table 11: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 2 when ρ = 0.4 Factor Sensitivities T = 60 T = 120 T = 240 T = 60 T = 120 T = 240 Table 12: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 2 when ρ = 0.7 Factor Sensitivities T = 60 T = 120 T = 240 T = 60 T = 120 T = 240 Table 13: Average of Estimated Downgrade Probabilities and Probabilities of Default (in %) for Design 2 Table 14: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 3 when ρ = 0 Table 15: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 3 when ρ = 0.4 Factor Sensitivities T = 60 T = 120 T = 240 T = 60 T = 120 T = 240 Table 16: Average of Absolute Bias and Standard Errors for Estimated Parameters for Design 3 when ρ = 0.7 Factor Sensitivities T = 60 T = 120 T = 240 T = 60 T = 120 T = 240 Table 17: Average of Estimated Downgrade Probabilities and Probabilities of Default (in %) for Design 3