Abstract—In this paper, we develop a recommender system for a game that suggests potential items to players based on their interactive behaviors to maximize revenue for the game provider. Most of today’s recommender systems in e-commerce and retail businesses are built based on supervised learning models and collaborative ﬁltering, while our approach is built on a reinforcement-learning-based technique and is trained on an ofﬂine data set that is publicly available on an IEEE Big Data Cup challenge. The limitation of the ofﬂine data set and the curse of high dimensionality pose signiﬁcant obstacles to solving this problem. Our proposed method focuses on improving the total rewards and performance by tackling these main difﬁculties. More speciﬁcally, we utilized sparse PCA to extract important features of user behaviors. Our Q-learning-based system is then trained from the processed ofﬂine data set. To exploit all possible information from the provided data set, we cluster user features to different groups and build an independent Q-table for each group. Furthermore, to tackle the challenge of unknown formula for evaluation metrics, we design a metric to self-evaluate our system’s performance based on the potential value the game provider might achieve and a small collection of actual evaluation metrics that we obtain from the live scoring environment. Our experiments show that our proposed metric is consistent with the results published by the challenge organizers. We have implemented the proposed training pipeline, and the results show that our method outperforms current state-of-the-art methods in terms of both total rewards and training speed. By addressing the main challenges and leveraging the state-of-the-art techniques, we have achieved the best public leaderboard result in the challenge. Furthermore, our proposed method achieved an estimated score of approximately 20% better and can be trained faster by 30 times than the best of the current state-of-the-art methods. Index Terms—Recommender System, Reinforcement Learning, Big Data In the last decades, recommender systems have been deployed in many industrial applications in many different ﬁelds. They aim to suggest items that would maximize user satisfaction and the provider’s revenue. They can be seen in various online services such as music and video services, gaming, online retail, restaurants, and online dating. They become more and more critical, especially when retail e-commerce is growing rapidly. In 2020, global retail e-commerce sales were 5.23 trillion USD, and they are expected to reach 6.54 trillion USD in 2022. Such growth poses many challenges for recommender systems, such as the volume and velocity of data, item-wise interactions, sparse data, dynamic user preference, and scalability [1]. The majority of today’s ecommerce and retail businesses build their suggesting systems by conducting supervised learning-based models and aim at optimizing clients’ satisfaction in a greedy manner. Nevertheless, the item-by-item greedy recommendation approach is not well-suited to many real-world applications. In sequential recommendation situations, for example, traditional supervised learning methods frequently assume different stages in a session to be independent and therefore miss the oportunity to ﬁnd the optimal strategy. Furthermore, the conversion rate of an item does not just depend on the item itself among the recommended items. For example, when similar but more expensive items surround an item, the likelihood of purchasing it increases, which is known as the decoy effect [2]. However, the number of possible combinations of all items can be in the billions, which is an NP-hard problem that is understudied in traditional supervised learning [3]. Recent research has been adopting reinforcement learning to tackle this challenge in suggesting systems. As recent research displays, the recommendation can be described as a series of interactions between the user (environment) and the recommender system. Since reinforcement learning algorithms are naturally designed to maximize long-term rewards, explore the combinatorial space, and solve multi-step decision-making challenges, utilizing reinforcement learning to design recommender systems is an interesting contribution.978-1-6654-3902-2/21$31.00 ©2021 IEEE In this paper, we tackle the challenge of designing a recommender system for the trading system in an online game. The recommender system would suggest items to players that are suitable for their playstyle and stage in the game and maximize the revenues for the game provider. This problem was proposed by the FUXI AI Lab, Netease, and was organized in the frame of the IEEE Big Data 2021 Cup. In this challenge, participants are given an ofﬂine data set with more than 250,000 playing sessions, 381 items, and approximately 40,000 users [4]. There are three stages in each episode of the game, and in each stage, a user is recommended three items. The goal of the recommender system in this game is to maximize revenues earned from the nine recommended items. The features in the data set include user portraits, clicking history, their stages in the game, item features, and which items were purchased by each user. To tackle this problem, we proposed a design of a recommender system where roughly 400 features are examined and extracted by an unsupervised algorithm. This information is then used to cluster the state of the environment, and parallel groups of reinforcement models are trained on the clustered information. By implementing this strategy, we helped the reinforcement learning models learn the most important features from the environment, thus providing better-suited recommendations based on user portraits and their stages in the game. After that, by utilizing the Bellman equation as a simple value iteration update [5], our parallel reinforcement learning models can ﬁnd sets of items to generate maximum revenues. Our contributions in this work can be described as follows. ment learning system based on feature extraction, clustering, and parallel training of Q-learning models performance by using parallelism on CPU and GPU. state-of-the-art Reinforcement Learning approaches and showed that our method could generate higher revenues than other methods. Furthermore, our system can be trained faster than the other methods by up to 24 times. The remainder of this paper is organized as follows. In Section II, we describe the problem in detail. Section III brieﬂy summarizes existing state-of-the-art solutions. Our solution is introduced in SectionIV. Section V describes the implementation details of our proposed solution. Section VI presents our results and the comparisons to other techniques. In each user session, the goal is to recommend nine items such that the total revenue from purchased items is maximized. As shown in Figure 1, the recommender system needs to respond to each client request with three-item lists (3 items per list), and the next item lists cannot be purchased until all three items on the current list are purchased. The users’ purchasing behavior is inﬂuenced by the items on the next lists as well as the current item list. This challenge can be framed as a multi-stage decision-making problem in two ways. In the ﬁrst way, there are 9 stages with one item recommended in each stage. In the second way, there are 3 stages with three items recommended in each stage. A live interactive training environment was not provided to the participants. Instead, an ofﬂine training data set was provided. The training data set is gathered over a threemonth period and includes a few sales initiatives. Essentially, the environment is a test system on clients’ purchase data and is built using data 10 times the size of the data being generated. The test environment is the same as it was when the recommendation was evaluated, and it is not available to participants of the competition. The training data set includes information about user sessions and items. In each user session, the provided information includes a timestamp, the user’s click history on all items, ten user portraits features, the nine items recommended by a recommender system, and a list of nine binary labels indicating whether the items were purchased. For each item, information about the item’s content features, price, and location are provided. An item can only be recommended on a list whose location matches the item’s location. For example, an item with location 1 can only be recommended on the ﬁrst item list, i.e., only position 1, 2, or 3 of the nine recommended items. In other words, an item with location 1 can only be placed on the ﬁrst row of Figure Lacking a live environment is a common situation in reallife applications of recommender systems due to various reasons such as security, user privacy, high cost for maintaining a live environment [6]. However, the lack of a live environment for training and testing a recommender system creates many difﬁcult challenges for designing a reinforcement learningbased approach, such as issues with bootstrapping from out-ofdistribution actions and overﬁtting. These problems are caused by erroneously optimistic value estimates for sample states that fall outside of the scope of the ofﬂine training data set [7]. In Section IV, we will describe our approach for overcoming the difﬁculties of training from an ofﬂine data set. In this section, we describe the current state-of-the-art development in the literature. We use the notations from Hasselt et al. [8] and Fujimoto et al. [9].In reinforcement learning, an agent interacts with its environment, typically assumed to be a Markov decision process (MDP). In [9], the MDP process is formally deﬁned by a tuple (S, A, p, r, γ), where each component of the tuple is listed below: (s|s, a) is a transition dynamic, when performing action a in state s and ending at the state s, At each discrete time step, the objective of the agent is to maximize the expected sum of discounted rewards when taking action ain state swhich is deﬁned as follows. The agent selects actions with respect to a policy π : S → A which exhibits a distribution µ(s) over the states s ∈ S visited by the policy. Under the given policy π, the true value of an action a in a state s is Q(s, a) = E[R|s, a]. The corresponding action value can be computed through the Bellman operator T: TQ(s, a) = E[r + γQ(s, π(s)] where Q(s, a) = maxQ(s, a). The Bellman operator states that the estimated long-term reward for a given action can be determined by the immediate reward from the given action plus the expected reward from the best future action taken at the next state. By picking the highest valued action in each state, an optimal policy is easily found from the optimal values. Since most interesting problems such as playing modern video games have nearly inﬁnitely large number of possible states, it is extremly difﬁcult to compute all action values in all states separately using the Bellman operator. Q-Learning proposed by Watkins [10] attempts to learn estimates of the optimal action (reward) values called Q-values when taking an action a in a particular state s via a parameterized value function. Watkins et al. [10] proposed that we can learn the Qvalues of all actions in any number of possible states using the parameterized value function Q(s, a; θ), in which we would like to ﬁnd the weights θ that updates the value of the function Q(s, a; θ) towards a target value Y. The weights can be found using stochastic gradient descent algorithm. Therefore, the standard Q-learning update for the parameters after taking action Ain state Sand observing the immediate reward R and resulting state Sis then: θ= θ+ α(Y− Q(S, A; θ))OQ(S, A where α is a scalar step size and the target Yis determined as follows. B. Deep Q-Network (DQN) To make Reinforcement Learning more applicable to realworld problems, Mnih et al. [11] proposed using a deep convolutional neural network (CNN) in combination with Qlearning. A DQN is a multi-layered convolutional neural network that maps a given state s to a vector of action values Q(s, ., θ), where θ are the parameters of the network. The Q-network can be written as a function f : {0, 1}→ Rthat maps an input state s ∈ {0, 1}to an output y ∈ Rwhere n is the number of states, m is number of actions, and R is a set of real numbers. Two important ingredients of the DQN algorithm are the use of a second target network and the use of experience replay [8]. We utilize the target network’s parameter θto calculate the target action values Y. The target network with parameters θhas the same architecture as the Q-network but with parameters being copied every τ steps from the Qnetwork, so that θ= θ, and kept ﬁxed on all other steps. The target used by DQN is deﬁned as follows. The second addition to considerably increase the DQN algorithm’s performance is experience replay [12]. The main idea of the experience replay is that we can store agents’ observed transitions and uniformly sample batches of them to train the CNN. C. Double Deep Q-Network (Double DQN) The main motivation behind Double DQN proposed by Hasselt et al. [8] is that the Q-network often overestimates the action values. In order to reduce overestimations, instead of including a maximization step when computing the target Q-values in the target network, the authors propose using the Q-network in the DQN architecture to choose an action and utilize the target network to generate the target Q-values for that action. By decomposing the max operation in the target into the action selection and action evaluation, the overestimation is substantially reduced. Below is the new Double DQN equation for updating the target value: Y≈ R+ γQ(S, arg maxQ(S, a; θ), θ) D. Batch Constrained deep Q-learning (BCQ) In 2019, Fujimoto et al. [9] proposed an ofﬂine (or ”batch”) RL method which aims to train an agent to learn from large ofﬂine data sets, but without any interaction with the environment. In real-world problems, an environment interaction may be expensive, unsafe and time consuming [13]. In this work, the main idea is to run normal Q-learning. Instead of evaluating the max action value over all potential actions shown in Equation (3), we only want to examine actions a that (s, a) really existed in the batch of data by removing actions that are unlikely to be chosen by the behavior policy in each batch π[14]. Firstly, BCQ utilizes a state-conditioned generative model G(s) which given the state as input, produces actions that are likely to be selected from the batch. Secondly, BCQ contains a perturbation model ξ(s, a), which further modiﬁes the actions within a set range [−Φ, Φ] and trains the perturbation model using the deterministic policy gradient [15]. Finally, the authors use a weighted version of Clipped Double Q-learning At test time, the authors sample N actions via the generator, perturb each, and pick the action with the highest estimated Q-value. Therefore, the policy is deﬁned as below: where [a∼ G(s)]. E. Conservative Q-learning (CQL) Existing ofﬂine RL methods suffer from a major challenge related to the distribution shift between the behaviour policy dataset and the learned policy [16]. This limitation results in an overestimating Q-values for out-of-distribution states and actions. Therefore, poor actions are selected [17]. The CQL [16] algorithm addresses this overestimation by ensuring conservative approximation of the Q-values via penalizing Qvalues for out-of-distribution states and actions [17]. As a result, the learned policy will be trained to keep closer to the known behaviors in the behaviour data rather than incorrectly favoring inﬂated values. To achieve such a lower-bound on the true Q-values, CQL uses an objective function J(θ) which depends on the choice of action distribution and is deﬁned as follows. J(θ) = minElogexp Q(s, a) − EQ(s, a) where (s, a, s) represents state, action, next state tuples; r(s, a) is the reward; ˆπ(a|s) deﬁnes the behaviour policy which generates the dataset D, andˆQ(s, a) is the Q-function approximator [17]. We propose a training pipeline as presented in Figure 2. In this pipeline, we ﬁrst perform feature extraction on the high-dimensional state space to reduce the problem of high dimensionality and retain features that are important to our reinforcement learning models. Next, we use the extracted features to cluster users. Finally, samples from grouped users are given to the parallel Q-Learning models according to their group membership. The Q-Learning models are updated by following the Bellman equation [18]. Preprocessing data involved parsing two text ﬁles: one that contains the user sessions and one that contains item information, as subsequently described. In the text ﬁle that contains the user sessions, each line contains a user ID, the user’s click history (i.e., the items which have been previously clicked by the user), ten user portrait features, the nine items proposed to the user by a recommender system, nine binary labels indicating whether the user purchased each item, and a timestamp for the session. In the text ﬁle that contains item information, each line contains item ID (which can be matched to the ﬁrst ﬁle), ﬁve-item content features, the item’s price, and the item’s location. An item can only be recommended on a list whose location matches the item’s location. For example, an item with location 1 can only be recommended on the ﬁrst item list, i.e., only position 1, 2, or 3 of the nine recommended items. In other words, an item with location 1 can only be placed on the ﬁrst row of Figure 1. As mentioned in Section III, each training record for all reinforcement learning methods is a tuple of (S, A, p, r, γ), where S deﬁnes state spaces, A denotes action spaces, p(s|s, a) denotes the next state given the current state and the taken action, r(s, a) represents the reward, and γ ∈ (0, 1) represents the discount factor. In this problem, we use a combination of user portrait features and user click history to represent states. There are 10 user portrait features and 381 one-hot-encoded user click history features, but we use a feature extraction method to reduce the number of features representing states (details in sub-section IV-B). There are three ways to construct the action space: a nine-step decisionmaking problem with each action being an item ID, a threestep decision-making problem with each action being a set of three-item IDs, and a one-step decision-making problem with each action being a set of nine-item IDs. The nine-step setup has a signiﬁcant disadvantage of not being able to hold multi-item interaction information. The one-step set up has a huge action space with= 4.24 ∗ 10possible actions. Therefore, we chose the three-item setup because it allows us to maintain multi-item interaction while having a reasonable action space with= 4.24 ∗ 10≈ 9 million possible actions. The next stage p(s|s, a) can be determined as being terminated if the user does not purchase all three recommended items or being the next incremented step with the same user features. Reward r is deﬁned as the total price of the purchased items among the three recommended items. As described in the previous section, the state space contains 10 user portrait features and 381 one-hot-encoded user click history features. Such a sparse state space may pose signiﬁcant problems for deep learning algorithms [19]. For example, our training data set may not have enough coverage on the entire feature space and is, therefore, more likely to have missed many maxima and minima of the loss function. Therefore, it is ideal to extract only the important features to reduce the dimensionality of the state space. Principal Component Analysis (PCA) [20] and AutoEncoder (AE) [21] are two common and standard methods for feature extraction and didimensionsssreduction. However, one drawback of these two techniques in dealing with sparse data is that they use all of the original features to compute the output features, which may introduce unnecessary noise from unimportant and rare features [22]. To tackle this problem, Sparse PCA [23] and Sparse AutoEncoder [24] were introduced. It has been shown that the classical PCA can retain consistency when the number of features is much larger than the number of data points, a situation where the classical PCA generates large variance [22]. We utilize these two methodologies to generate a state space with smaller dimensions from the training data set. The number of dimensions is selected through a cross-validation process by using our proposed scoring system as described in Section IV-E. One of the major drawbacks of the classical Q-Learning approach is that its memory requirement grows linearly with a product of the size of the state space and the size of the action space. Therefore, when the state space or the action space gets large, it is preferable to use Neural-Network-based approaches to approximate the quality value given a tuple of (state, action). In this problem, we have a large state space with high dimensionality. Even after dimension reduction, the state space is still inﬁnitely large. To tackle this problem, we use clustering techniques to divide the state space into regions. Within each region of the state space, we construct sub-state space that only depends on the step of the item list. This means that all users who fall into a region of the state space would share the same Q-table and would be given the same action policy. The intuition behind this design is that we can divide users into groups based on their user portraits and click history and that users within a group are likely to react similarly to the same action policy. This kind of approach has been popular in human resources management and investment [25], [26]. We utilized K-Means clustering and DBSCAN, the two standard approaches that have decent performance. The parameters on each approach are tuned, and the best approach is chosen based on our proposed scoring system as described in Section IV-E. Q-learning is a model-free reinforcement learning algorithm that may be used to memorize the value of an action in a certain state [18]. It does not require a model of the environment and can handle stochastic interactions and rewards without modiﬁcations. Starting from any state, Q-Learning discovers an optimal solution for any ﬁnite Markov Decision Process in the sense of maximizing the expected total reward throughout any and all progressive stages. Given an inﬁnite training time and a partly-random policy, Q-Learning may identify an optimal action-selection policy for every given Markov Decision Process. Q-Learning relies on storing in memory a Q-Table with size (Number of states) × (Number of actions). At initialization, all values in the Q-Table are initialized to 0. For each tuple of (S, A, p, r, γ) in the training data set, the Q-Table is updated according to the Bellman equation [18]: Q(S, A) =Q(S, A) + α(r + γmax(Q(p, a) − Q(S, A)) where α is the learning rate. For all ﬁnal states S, Q(S, a) is never updated but is set to the reward value r observed for the ﬁnal state S. One key challenge in training ofﬂine reinforcement learning models is that the models often overestimate the reward values when calculating samples that are under-represented in the training data set [6]. There are two approaches to tackle this problem. The ﬁrst approach is to restrict the recommended actions to the ones that are close to the training data set, such as the BCQ model proposed by Fujimoto et al. [9]. The second approach is to collect the training data set by running an online reinforcement learning model that has good coverage for all scenarios [27]. Our proposed method inherently follows the ﬁrst approach. By ensuring that each user cluster has a sufﬁcient amount of samples, we can guarantee to restrict the recommended actions in each cluster to the ones that are well represented because they are the ones with the highest Q values in the Q-Tables. Since the scoring metric is not disclosed to participants in this competition, we propose the following evaluation metric to evaluate the quality of a model quickly. By splitting the training data set into a train set and a validation set, we can train any model on the train set and use the model to make recommendations on the validation set. To evaluate our method performance, we design a metric that reﬂects potential rewards based on the price of recommended items and the user’s actual purchased items. Our method is then evaluated on the testing set, which consists of 20% of the entire data set. The output of our metric is a weighted score that evaluates the total expense of a user for buying items in the game at different steps. Each 3-items list is a step, and the score in the higher steps should be put higher weights. This is an appropriate strategy since the game provider will beneﬁt the most if the player survives through the end of the game. As the game rules restrict certain items on each step, if the items are not allowed to suggest on a particular step, their values will be zero. All recommended items in each step that are eventually purchased by the players will be counted in our ﬁnal score. Our metric can be described in the algorithm as follows. Algorithm 1 Algorithm to Calculate metric Input: Recommended Items R, Purchased Items P, Items’ Prices , Restricted Items. Parameter: Wis step weights N : number of testing samples Output: Score for st in steps do Items ← i in R ∪ P for it in purchased Items and item in restricted items do V alue+ = it ∗ P rice end for end forP Score =w∗ value return The proposed evaluation metric can be used to compare the models and to tune parameters. As shown in Section VI, our evaluation metric are a fair representation of the real metric used by the competition organizer. We have implemented our proposed method in Python. For the feature extraction part, we utilized the library ScikitLearn’s SparsePCA method and the Sparse AutoEncoder implementation by Makhzani and Frey [24]. For the clustering part, we utilized Scikit-Learn’s KMeans and DBSCAN methods. Parameters for these methods are selected through cross-validation by using the proposed metric in Section IV-E. We have implemented the Q-Learning models for highperformance scaling on GPU. First, we initialized all the Q-Tables on the GPU’s global memory. For each cell on the tables, we created an exclusive lock to prevent multiple threads from updating a cell at the same time. Since all tuples (S, A, p, r, γ) from the train set can be processed and used to update the Q-Tables independently, processing an entire batch is an embarrassingly parallel problem where we let one thread process the computation for a tuple and then atomically update the cells with the use of the exclusive locks. A parallel implementation on CPUs was also programmed in a similar manner. First, we show that our proposed metric is a good representation of the private metric used by the competition organizers. First, we split the training data set into an 80% set for training and a 20% set for validation. Then, the metric proposed in Section IV-E is calculated on the validation set. Finally, we use the trained model to make recommendations on the testing data set provided by the organizers and obtain the score from the organizers’ scoring system. We repeat this process with the four deep reinforcement learning described in Section III and our proposed method with different sets of parameters (number of extracted features, clustering parameters, and learning rate for Q-learning). The resulted scores on the testing set and our calculated metric are plotted in Figure 3. Since the organizers allow only one query per day to the scoring system for calculating scores on the testing set, we could only obtain a few data points. Figure 3 shows that our proposed metric is consistent in representing the organizer’s calculated metric on the testing set. Next, we show that our proposed method outperforms the four deep reinforcement learning methods described in Section III by using both our proposed metric and the organizers’ scoring system. The four deep reinforcement learning methods are Deep Q-Network (DQN), Double Deep Q-Network (Double DQN), Batch Constrainted Deep Q-Learning (BCQ), and Conservative Q-Learning (CQL). We used the Python package d3rlpy for the implementation of these methods. In addition, we reproduced the classical Q-Learning method and applied it to this problem. Parameters are tuned by using cross-validation with our proposed metric, and only the best parameters for each method are reported. The results of DQN, Double DQN, BCQ, CQL, Classical Q-Learning, and ComFOR are presented in Figure 4. In addition, the organizers released the ofﬁcial baseline scores as follows. By using logged ofﬂine actions, the optimal score achievable is 770,378,225. By using a Long-Short-TermMemory environment simulator in combination with Deep Deterministic Policy Gradient, the optimal score achievable is 1,033,481,948. Finally, to compare performance and scalability, we present the training time for each method in Figure 5. All methods are executed on an AMD Ryzen Threadripper with 32 threads. Results from Figure 4 show that our method ComFor outperforms other methods in both our proposed metric and the organizers’ metric. Speciﬁcally, our method achieved a score that is 16% higher than that of the state-of-the-art BCQ and 110% higher than that of the baseline method provided by the organizers. Figure 5 shows that ComFOR can train signiﬁcantly faster than other methods, roughly 10 times faster than the classic Q-Learning method and 30 times faster than the state-of-the-art BCQ. In this study, we tackle the challenge of building a recommender system for a game that can be viewed as a multistep decision-making problem. This problem also presents a difﬁculty for building recommender systems in the real world, which is the unavailability of a live training environment. The majority of today’s recommender systems in e-commerce and retail businesses are based on supervised learning are built based on supervised learning models and collaborative ﬁltering. In recent years, more researchers have proposed that deep reinforcement learning should be utilized for building recommender systems because they are well suited for addressing multi-item interactions, maximizing long-term rewards, and solving multi-step decision-making challenges. However, they suffer from the aforementioned problem of training on an ofﬂine data set. To tackle these challenges, we propose a novel reinforcement learning pipeline based on compressive features and clustering for assisting parallel Qlearning models. The results from our experiments show that our proposed method can train signiﬁcantly faster than the other deep reinforcement learning methods and produce action policies that generate higher rewards.