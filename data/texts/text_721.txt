{ykchen,yfzhang,king}@cse.cuhk.edu.hk,{yingxue.zhang,h uifeng .guo,lijingjie1,tangruiming,hexiuqiang1}@huawei.com Due to the promising advantages in space compression and inference acceleration, quantized representation learning for recommender systems has become an emerging research directio n recently. As the target is to embed latent features in the discrete embedd ing space, developing quantization fo r user-item representations with a few low-precision integers confronts the challenge of high information loss, thus leading to unsatisfactory performance in Top-K recommendation. In this work, we stud y the problem of representation learning for recommendation with 1-bit quantization. We propose a model named Low-loss Quantized Graph Convolutional Network (LQ-GCN). Diﬀerent from previous work that plugs quantization as the ﬁnal encoder of user-item embeddings, LQ-GCN learns the quantized representations whilst capturing the structural information of user-item interaction graphs at diﬀerent semantic levels. This achieves the substantial retention of intermediate interactive information, alleviating the feature smoothing issue for ranking caused by numerical quantization. To further improve the model performance, we also present an advanced solution named LQ-GCNwith quantization approximation and annealing training strategy. We conduct extensive experiments on four benchmarks over Top-K recommendation task. The experimental results show that, with nearly 9×representation storage compression, LQ-GCNattains about 90∼99% performance recovery compared to the state-of-the-art model. Recommender systems (RSs), as a useful tool to perform personalized information ﬁltering [12, 17] , nowadays play a cr itical role throughout various Web appl icat io ns, e.g., social networks, Ecommerce platforms, and st reaming media websites. Learning vectorized user-item representations (a.k.a. embeddings) for prediction has become the core of modern recommender systems [10, 18, 19]. Among existing techniques, graph-based methods, i.e., Graph Convolutional Networks (GCNs), due to the ability of capturing highorder relations in user-item interaction topology, well simulate the collaborative ﬁltering process and thus produ ce a remarkable semantic enrichment to the user-item representations [17, 18, 38, 43]. Apart from the representation informativeness, space overhead is another important criterion for realistic recommender systems. With the explosive growth of interactive data encoded in the graph form, quantize d representation learning recently provides an alternative option t o GCN-based recommender method s fo r optimizing the model scalability. Generally, quantization is the process of converting a continuous range of vectorized values into a ﬁnite discrete set, e.g., integers. Instead of using continuous embeddings, Figure 1: Illustration of conventional quantized C NN and semantic propagation within the user-item interaction gra ph. e.g., 32-bit ﬂoating points, 1-bit quantization however emb eds useritem l atent features into the binary embedding space, e.g., {−1, 1}. By enabling the usage of low-precision integer arithmetics, 1-bit quantized representations have the promising po tential in sp ace compression and inference acceleration for recommendation [39]. Despite the previous attempts to quantize tradit io nal model-based RS methods [24, 48, 4 9], quantizing graph-based RS models [40] however receive less attention so far. Due t o their unique message passing mechanism [17, 26] in encoding high-order interactive information, it is emerging as a good research topic t o study representation quantization for GCN-based recommender models. Currently, it is still challenging to approach this target as previous work falls short of satisfaction in terms of recommendation accuracy. The crux of this phenomenon is mainly twofold: • Intuitively, the performance degradation for quantizing representations is mainly caused by the limited expressivity of discrete embeddings. Diﬀerent from applications in other domains, e.g., Natural Language Processing, Computer Vision [4, 6, 15], the top principle of quantization for recommendation is ranking preserving. However, compared to full-precision embeddings, the vectorized latent features of both users and items tend to be smoothed by the discreteness of quantization naturally. For instance, after the quantization into binary embedding space {−1, 1}, only the digit signs are kept, no matter what speciﬁc values of continuous embeddings originally are. Consequently, this leads to the information loss when estimating users’ preferences towards diﬀerent items, thus drawing a conspicuous performance decay in ranking tasks, e.g., Top-K recommendation. • Technically, previous work mainly takes inspiration from the methodology of Quantized Convolutional Neural Net works [31, 34, 35] (illustrated in Figure 1(a)) to pl ug quantization as a separate encoder that is posterior to the GCN architecture. Being the ﬁnal encoder for object embeddings (i.e., users and items), this however ignores the intermediate representations when graph convolution performs on the sub-stru ctures of interaction graphs. As pointed out by [43], the intermediate information at diﬀerent layers of graph convolution is important to reveal diﬀerent semantics of user-item interactions. For example, as shown in Figure 1(b), when lower-layers propagate information between users and items that have historical interactions, higher-layers capture higher-order proximity of users (or items). Hence, ignoring intermediate semantics fails to the feature enrichment for embedding quantization. Furthermore, due to the “doubleedged” eﬀect of GCN, ﬁnal embeddings at the last graph convolution layer may be over-smoothed [28, 29] to become uninformative accordingly. This implies that simply using the outpu t embeddings may be risky and problematic [18], which leads to the suboptimal quantized representations for recommendation. In this paper, we investigate the p roblem of 1-bit quantized representation learning for recommendation wit h the GCN framework. We propose a model named Low-loss 1-bit Quantized Graph Convolutional Network (LQ-GCN). LQ-GCN interprets the user-item representations in the discrete space {−1, 1 }. We design the quantizationbased graph convolution such that LQ-GCN achieves the embedding quantizatio n whilst capturing diﬀerent levels of interactive semantics in exploring the user-item interaction graphs. Intuitively, such top ology-aware quantization makes the user-item representations more comprehensive, and thus signiﬁcantly alleviates the information loss of numerical quantization that causes the performance de cay. Speciﬁcally, we propose two solutions, namely LQGCNand LQ-GCN, to provide ﬂexibility towards diﬀerent deployment scenarios. We conduct extensive experiments on four benchmarks over Top-K recommendation task. To summarize, our main contribu tions are as follows: (1) We implement our proposed network d esign in LQ-GCN that is trained in an end-to-end manner. Our exp erimental results demonstrate that, LQ-GCNachieves nearly 11× representation compression and about 40% inference acceleration, while retaining over 80% recommendation capacity, compared to the state-of-the-art full-precision model. (2) To further improve the recommendation performance, we propose an advanced solution LQ-GCNwith approximation that is trained by a two-step annealing t raining strategy. With slightly add itional space cost, LQ-GCNcan attain 90∼99% performance recovery. (3) We release codes and datasets to researchers via the link [1] for reproducing and validating. Organization. We ﬁrst review the related work in Section 2 and present LQ-GCNand LQ-GCNin Sections 3 and 4. We report the experimental results on four benchmarks in Section 5 and conclude the paper in Section 6. Recently, GCN-based recommender systems have become new state-of-the-art methods for Top-K recommendation [18, 43], thanks to their capability of captu ring semantic relations and topological structures for user-item interactions [17, 26, 50]. Motivate d by the advantage of graph convolution, prior work such as GC-MC [7], PinSage [47] and recent state-of-the-art models NGCF [43] and LightGCN [18] are proposed. Generally, they adapt the GCN framework to simulate the collaborative ﬁ ltering process in high-order graph neighbors for recommendation. For example, NGCF [43] follows the GCN-based information p ropagation rule to learn embedd ings: feature transformation, neighborhood aggregation, and nonlinear activation. LightGCN [ 18] further simpliﬁes the graph convolution b y retaining the most essential GCN components to achieve further improved recommendation performance. In our experiments, we settle these two state-of-the-art full-precision methods as the b enchmarking reference for LQ-GCN in Top-K recommendation. Network binarization [22] aims to binarize all parameters and activations in neural networks so that they can even be trained by logical units of CPUs. Binarized models will dramatically reduce the memory usage. Despite t he progress of binarization CNNs [34, 35] fo r mul timedia retrieval, this technique is not adequ ately studied in geo metric deep learning [3, 41, 45]. Bi-GCN [41] and BGCN [3] are two recent trials. However, they are mainly designed for geometric classiﬁcation tasks, but their capability of link prediction (a geometric form of recommendation) is unclear. Fu rthermore, compared to focusing on the quantization for user-item representations only, a complete network binarization will further abate the numerical expressivity of modeling ranking information for recommendation. This implies that a direct adaptation of binar y GCN models may draw large performance decay in Top-K recommendation. Quantization-based recommender models are attracting growing attention recently [24, 37, 40, 46]. Compared to network binarization, they do not pursue extreme model compression, but focus on quantization fo r user-item representations with a few integers. These models can be generally categorized into model-based [24, 46, 48, 49] and graph-based [40]. HashGNN [40], as the state-ofthe-art graph-based solution, takes both advantages of graph convolution and embedding quantization. Speciﬁcally, HashGNN [40] designs a two-step quantization framework by combining GraphSage [17] and learn to hash methodology [ 13, 16, 42, 52]. Generally, Learn to h ash aims to learn hash functions for generating discriminative codes. HashGNN ﬁrst invokes the two-layer GraphSage as the encoder to get the embeddings fo r users and items; then it stacks a hash layer to get the corresponding binary encodings afterwards. However, the main inadequacy of HashGNN is that, the quantization process only proceeds at the end of multi-layer graph convolution, i.e., using the aggregated output of t wo-layer GraphSage fo r representation binarization. While multi-layer convolution helps to aggregate local information that lives in the consecutive graph hops, HashGNN may thus not be able to capture intermediate semantics from nodes’ diﬀerent layers of receptive ﬁelds, producing a suboptimal quantization of node emb eddings. Compared to HashGNN, our proposed LQ-GCN model conducts the quantization-based graph convolution when exploring the useritem interaction graph in a layer-wise manner. We just ify the effectiveness in Section 5. User-item interactions can be represented by a bipartite graph, i.e., G = {(𝑢, 𝑖)|𝑢 ∈ U, 𝑖 ∈ I}. U and I denote the sets of users and items. We denote 𝑦= 1 to indicate there is an observed interaction between 𝑢 and 𝑖, e,g., browse, click, or purchase, otherwise 𝑦 = 0. Notations. We use bold lowercase, bold uppercase, and call igraphy characters to denote vectors, matrices, and sets, respectively. Non-bold characters are used to denote graph nodes or scalar s. Due to the page limit, we summarize all key notations in Appendix A. Task Description. Given an interaction graph, the problem studied in this paper is to learn qu antized representations Qand Qfor user 𝑢 and item 𝑖, such that the online recommender system model can predict the probability ˆ𝑦that user 𝑢 may adopt item The general idea of LQ-GCN is to learn node representations by propagating latent features via the graph topology [18, 26, 44]. It performs iterative g raph convolution, i.e., propagating and aggregating information of neighbors to update representations (embeddings) of target nodes, which can be formulated as follows: where 𝒗denotes node 𝑥’s embedding after 𝑙 layers of information propagation. N(𝑥) represents 𝑥’s neighbor set and 𝐴𝐺𝐺 is the aggregation function aiming to transform the center node feature and the neighbor features. We illustrate the framework in Figure 2. 3.2.1 antization-based Graph Convolution. We adopt the graph convolution paradigm working on the continuous space from [18] that recently shows good performance under recommendation scenarios. Let 𝒗∈ Rand 𝒗∈ Rdenote the continuous feature embeddings of user 𝑢 and item 𝑖 computed in the 𝑙-th layer. They can be respectively updated by utilizing information from the (𝑙 − 1)-th layer in iteration as follows: After getting the intermediate embeddings, e.g., 𝒗, we conduct 1-bit quantization as: where 𝑾 ∈ 𝑅is a matrix that transforms 𝒗to 𝑑-dimensional latent space for 1-bit quantization. Function sign(·) maps ﬂoatingpoint inputs into the discrete binary sp ace, e.g., {−1, 1}. By doing so, we can obtain the quantized embedding 𝒒whilst retaining the latent user features directly from 𝒗. For ease of LQGCN’s binarization storage, we can further conduct a numerical translation to these quantized embeddings from {−1, 1}to {0, 1}. After 𝐿 layers of feature propagation and quantization, we have built the targeted quantized representations Qand Qas: Q= {𝒒, 𝒒, ··· , 𝒒}, Q= {𝒒, 𝒒, ··· , 𝒒}. (4) Both Qand Qtrack the intermediate information binarized from full-precision embeddings at diﬀerent layers. Intuitively, they represent the interactive information that is propagated back and forth between users and items within these layers, simulating the collaborative ﬁltering eﬀect to exhibit in the quantized encodings for recommendation. 3.2.2 Model Prediction. Based on the quantized representations of users and items, i.e., Qand Q, we predict the matching scores by naturally adopting the inner product as: where function 𝑓 abstracts the way to utilize Qand Q. In this paper, we implement 𝑓 by taking an element-wise summation: Clariﬁcation.In this work, we interp ret quantized representations as integers and apply i nteger arithm e t ics in computation. As we will show later in experiments, this introduces about 40% of computation acceleration. Cer tai nly, it can be extended to binary arithmetic logics for further optimization, and we leave it for future work. 3.2.3 Model Optimization. We then introduce our objective and backward-propagation strategy for optimization. Objective Function. Our objective function consists of two components, i.e., graph reconstruction loss Land BPR loss L. The motivation of such design is basically twofold: • Lreconstructs the observed to pology of interaction graphs; • Llearns the relative rankings of user p references towards diﬀerent items. Concretely, we implement Lwith the cross-entropy loss: where 𝜎 is the activation function, e.g., Sigmoid. Lbases on the full-precision embeddings at the last layer, e.g., 𝒗, providing the latest intermediate information for topology reconstruction as much as possible. As for L, we employ Bayesian Personalized Ranking (BPR) loss [36] as follows:ÕÕÕ Lrelies on the quantized node representations, e.g., Q, to encourage the prediction of an observed interaction to be higher than its unobserved counterparts [18]. Finally, our ﬁnal objective function is deﬁned as: where Θ is the set of trainable parameters and embeddings, and ||Θ||is the 𝐿2-regularizer par ameterized by 𝜆 to avoid over-ﬁtting. Clariﬁcation.Please notice that at th e training stage, we usually take the mean value of 𝑓 (Q) by shrinking it as 𝑓 (Q) = 𝑓 (Q)/(𝐿 +1) (likewise for 𝑓 (Q)). Essentially, this useful training strategy [18, 21] reduces the absolute value of predicted scores to a smaller scale, whi ch signiﬁcantly stabilizes the training process to avoid the undesirable divergence in embedding opti mization; but most importantl y, it has no eﬀect on the relative r anki ngs of all scores. Backward-propagation Strategy. Unfortunately, the sign function is not diﬀerentiable. This means that the original derivative of the sign function is 0 almost everywhere, making the intermed iate gradients accumulated before quantization zeroed here. To avoid this and approximate the gradients for backward propagation, we adopt the Straight-Through Estimator [5] with gradient clipping as: Derivative 1can be viewed as passing gradients e.g., Δ, through hard-tanh function [11], i.e., max(−1, min(1, Δ)). This passes gradients backwards unchanged when the input of sign function, e.g., 𝜙, is within range of {-1, 1}, and cancels the gradient ﬂow otherwise [ 2]. We illustrate the process of gradient propagation in Figure 2. So far, we have already introduced the skeleton of our proposed network that learns t he quantized representations Qand Qvia exploring the interaction graph topology. Since it can be trained in an end-to-end manner, we directly name it as LQ-GCN. Although LQ-GCNcan achieve the quantization for useritem representations, we argue that there may still exist avenues for fur ther improvement. In this section, we ﬁrst explain the d iﬃculty of quantization in LQ-GCN, and then give an advanced solution with the annealing training strategy, namely LQ-GCN, to enhance the recommendation performance. To show it may be challenging to directly quantize LQ-GCN for binary representation, we simulate the optimization trajectories of learnable embedd ings and visually compare the loss landscapes of it with its full-precision version (excluding the qu antization compo nent) in Figure 3. Concretely, following [4, 33], we manually assign perturbations to the learnable user-item embeddings as foll ows: 𝒗= 𝒗±𝑝 ·|𝒗| · 1, 𝒗= 𝒗±𝑝 · |𝒗| · 1, (11) where|𝒗| represents the absolute mean value of embedding 𝒗 and perturbation magnitudes 𝑝 are from {0.01, 0.02, ·· · , 0.50}. 1 is an all-one vector. For each pair of perturbed user-item representations, we plot t he loss distribution accordingly. As we can observe, the full-precision version with no quantization produces a ﬂat and smooth loss surface, showing the local convexity and thus easy to optimize. On the contrary, LQ-GCN has a bumping and complex loss landscape. The steep loss curvature reﬂects LQ-GCNis more sensitive to perturbation, showing the diﬃculty in quantization optimization. To alleviate the perturbation sensitivity and further improve the model perfor mance, we propose LQ-GCN. 4.2.1 antization with Rescaling Approximation. Our proposed LQ-GCNadditionally includes layer-wise positive rescaling factors for each node, e.g., 𝛼∈ R, such that 𝒗≈ 𝛼𝒒. In this work, we introduce a simple but eﬀective approach to directly calcul ate these rescaling factors as: Instead of setting 𝛼as learnable, such deterministic computation substantially prunes the search space of parameters whilst attaining the approximation functionality. We demonstrate this in Section 5.6. Based on the quantized user-item representations and the corresponding rescaling factors, we have: Consequently, LQ-GCNapproximates Qand Qby Aand A, and updates Equations 5 and 6 for model prediction accordingly. Space Cost Analysis. The total space cost of LQ-GCNfor storing the quantized user-item representations is 𝑂 ((𝐿 + 1)𝑁 𝑑) (or bits), where 𝑁 is the number of users and items and 𝑑 is the dimension of binary embeddings, e.g., 𝒗. Furthermore, since LQGCNdevelops quantization with approximation, supposing that we use 32-bit ﬂoating-points for those rescaling factors, the space cost is 𝑂 ((𝐿 +1)𝑁 (𝑑 +32)) in total. Compared to the full-precision embedding table at each single one layer, e.g., 32-bit ﬂoating-point 𝒗, Qand Qhave the following theoretical compression ratios: Normally, stacking too many layers will cause the over-smoothing problem [28, 29], incurring performance detr iment. Hence, A common setting for 𝐿 is 𝐿 < 5 [17, 18, 26, 43], which can still achieve considerable embedding compression. 4.2.2 Annealing Training Strategy. Another constructive design of LQ-GCNis the two-step annealing training strategy: (1) we ﬁrst mask the quantization function and train LQ-GCN with the full-precision embedd ings; (2) when it converges to optimum, we tr igger the quantization afterwards to ﬁnd the targeted quantized representations. Intuitively, LQ-GCNmoves from a tractable training space to the targeted one for quantization. This can avoid unnecessary exploration towards diﬀerent optimization directions at the beginning of quantization, guaranteeing t he numerical stability in the whole model training. Then at the middle period of training when triggering quantization, as presented in Figure 4, LQ-GCNﬁrstly meets a performance retracement, bu t shortly afterwards, it recovers and continues to converge. As we will demonstrate later in experiments, this straightfor ward but eﬀective strategy can furt her produce better quantization representations with approximation for Top-K recommendation. Clariﬁcation.we point out that the time cost of t raining LQ-GCN shares the same order of magnitud e with LQ-GCN. In our implementation, we simply assign half of the total trainin g epochs for the ﬁrst step and leave the second half for q u ant ization. One may also opt for more ﬂexible strategy to trigger the quantization, e.g., earlystopping, or full-precision version pre-training. Figure 4: Annealing performance on MovieLens dataset. So far, we have introduced all technical details of the proposed LQ-GCNand LQ-GCN. For the cor responding pseudocodes, please refer to Appendix B. In the following section, we present the experimental results and analysis on ou r models. We evaluate our model on Top-K recommendation task with the aim of answering the following research questions: • RQ1. How does LQ-GCN perform compared to state-of-the-art full-precision and quantization-based recommender models? • RQ2. H ow is resource consumption of LQ-GCN? • RQ3. How do proposed components of LQ-GCNand LQ- GCNaﬀect t he performance? • MovieLensis a widely adopted benchmark for movie recommendation. Similar t o the setting in [9, 20, 40], 𝑦= 1 if user 𝑢 has an explicit rating score towards item 𝑖, otherwise 𝑦= 0. In this paper, we use the MovieLens-1M data split. • Gowallais the check-in dataset [30] collected from Gowalla, where users share their locations by check-in. To guarantee the quality of the dataset, we extract users and items with no less than 10 interactions similar to [18, 40, 43]. • Pinterestis an implicit feedback dataset for image recommendation [14]. Users and images are modeled in a graph. Edges represent the p ins over images initiated by users. In this dataset, each user has at l east 20 edges. We delete repeated edges between users and images to avoid data leakage in model evaluation. • Yelp2018is collected from Yelp Challenge 2 018 Edition. In this dataset, local businesses such as restaurants are treated as items. We retain users and items with over 10 interactions similar to [43]. Table 2: Performa nce comparison (underline represents the best performing model; R and D refer to Recall and NDCG). ModelR@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100 HashGNN-soft 18.54 41.83 36.56 55.16 11.49 25.88 17.84 26.53 10.87 34.14 12.35 24.99 4.29 14.03 8.30 1 7.73 Quant-gumbel 17.48 42.08 34.57 56.50 10.78 26.58 15.62 25.08 9.78 30.63 11.15 22.92 3.91 13.00 8.07 17.08 LQ-GCN20.52 49.27 38.41 60.03 14.62 32.24 21.24 30.97 12.52 35.67 13.92 26.37 5.10 16.31 9.62 1 9.88 % Capacity 82.05% 88.44% 85.87% 92.23% 82.13% 87.10% 85.78% 88.71% 84.71% 89.22% 87.49% 91.15% 83.47% 90.36% 87.85% 92.08% LQ-GCN22.81 51.96 42.44 62.96 16.12 34.39 23.62 33.52 13.87 38.25 15.31 28.13 5.74 17.63 10.67 21.32 % Capacity 91.20% 93.27% 94.88% 96.73% 90.56% 92.90% 95.40% 96.02% 93.84% 95.67% 96.23% 97.23% 93.94% 97.67% 97.44% 98.75% We compare our model with two main streams of methods: (1) full-precision recommender systems including CF-based methods (NeurCF), and GCN-based models (NGCF, LightGCN), (2) 1-bit quantizationbased models for general item retrieval tasks (LSH, HashNet) and for Top-K recommendation (HashGNN). • LSH [16] is a classical hashing method. LSH is ﬁrstly proposed to approximate the similarity search for massive high-dimensional data and we introduce it for Top-K recommendation by following the adaptat io n in [40]. • HashNet [8] is a stat e-of-the-art deep hashing method that is originally proposed fo r multimedia retrieval tasks. Similar to [40], we adapt it for graph data mainly by replacing the used AlexNet [27] with the general graph convolutional network. • HashGNN [40] is the state-of-the-art 1-bit quantization-based recommender system method with GCN framework. We use HashGNN to denote the vanilla version with hard encodi ng proposed in [40], where each element of quantized user-item embeddings is strictly quantized. We use HashGNN-soft to represent the relaxed version proposed in [40], where it adopts a Bernoulli random variable to provide the probability of replacing the quantized digits with continuous values in the original embeddings. • NeurCF [19] is one st ate-of-the-art neural network model for collaborative ﬁltering. NeurCF models latent features of users and items to capture their nonlinear feature interactions. • NGCF [43] is one of the state-of-the-art GCN-based recommender models. We compare L𝑄-GCN with NGCF mainly to study their performance capabilities in To p-K recommendation. • LightGCN [18] is the latest state-of-the-art GCN-based recommendation model that has been widely evaluated. We include LightGCN in our experiment mainly to set up the benchmarking as a reference of full-precision recommendation capability. • Quant-gumbe l is a variance of LQ-GCN with the implementation of Gumbel-softmax for quantization [23, 32, 51]. We ﬁrst expand each embedding bit to a size-two one-hot encoding. Then Quant-gumbel utilizes the Gumbel-softmax trick to replace sign function as relaxation for binary code generation. In the evaluation of Top-K recommendation, we apply the learned user-item representations to rank 𝐾 items for each user with the highest predicted scores, i.e., ˆ𝑦. We choose two widely-used evaluation protocols Recall@𝐾 and NDCG@𝐾 to evaluate Top-K recommendation capability. We implement LQ-GCN model under Python 3.7 and PyTorch 1.14.0 with non-distributed training. The exp eriments are run on a Linux machine with 4 NVIDIA V100 GPU, 4 Intel Core i7-8700 CPUs, 32 GB of RAM with 3.20GHz. For all the baselines, we follow the oﬃcial hyper-parameter settings from original papers or as default in corresponding codes. For methods lacking recommended settings, we apply a grid search for hyper-parameters. The embedding dimension is searched in {32, 64, 128, 256, 512}. T he learning rate 𝜂 is tuned within {10, 5 ×10, 10, 5 ×10} and the coeﬃcient of 𝐿2 normalizat ion 𝜆 is tuned among {10, 10, 10}. We initialize and optimize all models with default normal initializer and Adam optimizer [25]. To guarantee repro ducibility, we report all the hyper-parameter settings in Appendix C. In this section, we present a comprehensive performance analysis between L𝑄-GCN with two layers and competing recommender models of full-precision-based and quantization-based. We evaluate Top-K recommendation over four datasets by varying K in {20, 40, 60, 80, 100}. To achieve a more detailed performance comparison, we summarize the results of Top@20 and Top@100 recommendation in Table 2. We also curve their complete results o f Recall@K and NDCG@K metrics and attach them in Appendix D. Generally, L𝑄-GCN has made great improvements over quantizationbased models and shows the competitive performance compared to full-precision models. We have the following observations: • The results demonstrate the superiority of L𝑄-GCN over all quantization-based mo d els. (1) As shown in Table 2, the state-of-the-art quantization-based GCN model, i.e., HashGNN (and HashGNN-soft), works better than trad itional quantizationbased baselines, e.g., LSH, HashNet. This shows the eﬀectiveness of graph convolutional architecture in capturing latent information within interaction graphs for quantization preparation and indicates that a direct adaptation of conventional quantization methods may not well handle the Top-K reco mmendation task. (2) Furthermore, thanks to our proposed quantization-based graph convolution design, both L𝑄-GCNand L𝑄-GCNconsistently outperform HashGNN and its relaxed version HashGNNsoft. The main reason is that, our topology-aware quantization signiﬁcantly enriches the user-item representations and alleviates the feature smoothing issue caused by the numerical quantization. We conduct the ablation study on t his in the later section. • Compared to full-precision models, L𝑄-GCN presents a competitive performance recovery. (1) L𝑄-GCNshows the performance su periority over traditional collaborative ﬁlt ering model, i.e., NeurCF. Considering the large improvement of two GCN-based models, i.e., NGCF and LightGCN, against Neur CF, we can infer the performance gap between L𝑄-GCNand NeurCF mainly comes from the proposed quantization-based graph convolution. (2) Compared to the best model LightGCN, taking Recall metric as an example, L𝑄-GCNshows about 82∼85% and 87∼90% performance capacity in terms of Top@20 and Top@100. This indicates that, as the value of K increases, L𝑄-GCNcan further improve the recommendation accuracy and narrow the gap to LightGCN. (3) Moreover, by utiliz ing the quantization approximation and annealing training strategy, L𝑄-GCNcan achieve better performance than NGCF on Gowalla and Yelp2018 datasets. Compared to our basic implementation L𝑄-GCN, L𝑄-GCNfurther improves the performance recovery by 8∼10% and 9∼10% in terms of Recall@20 and R ecall@100 across all benchmarks, proving the eﬀectiveness of our proposed modiﬁcat ion in L𝑄-GCN. (4) In additio n, with K increasing up to 100, L𝑄GCNpresents a similar trend with L𝑄-GCNsuch that it performs even closely t o LightGCN, i.e., 93∼98% and 96∼99% in terms of Recall and NDCG, respect ively. In a nutshell, the prediction capability of both L𝑄-GCNand L𝑄-GCNdevelops from ﬁne-grained r anking tasks to coarse-grained ones, e.g., Top-20 to Top-100 recommendation. • Both L𝑄-GCNand L𝑄-GCNshow the deployment ﬂexibility towards diﬀerent application scenarios. In the pipeline of industrial recommender systems, recall and re-ranking are two important stages that substantially inﬂuence recommendation quality. Recall refers to the process of quickly retrieving candidate items from the whole it em pool that a given user may interest. Based on the more complex scoring algorithms, reranking outputs a precise ranking list of candidate items. On the one hand, as we will show later, L𝑄-GCNcan speed up abo ut 40% for candidate generation. Considering its performance improvement on coarse-grained ranking tasks, e.g., Top100 reco mmendation, L𝑄-GCNactually provides an alternative option to accelerate the recall stage. On the other hand, L𝑄GCNfurther optimizes the recommendation accuracy that performs similarly t o LightGCN. Since L𝑄-GCNreduces the embedding storage cost to about 9×, we can treat L𝑄-GCN as a substitute for the best model, i.e., LightGCN, as a trade-oﬀ between space cost and prediction accuracy. We report the details of space and time cost for embedding storage and online inference in the following section. In this section, we study the resource consumption in embedding storage and inference time cost. We compare our models with the state-all-the-art full-precision model and quantization-based model, i.e., LightGCN and HashGNN. We take their two-layer structures with the same 128-dimensional embeddings as reference and illustrate with the largest dataset, i.e., Yelp2018, in Figure 5(a). Observations in this section can be popularized to other three datasets and we report the complete results in Appendix D. Embedding storage compression. Quantize d emb eddings can largely reduce the space consumption for oﬄine disk storage. To measure the emb edding size, we save these embeddings to the disk such that they can recover the well-trained user-item representations for inference. As we can observe, after the binarization for user-item embed dings, LQ-GCNand LQ-GCNcan achieve the space reduction with a factor of about 11 × and 9×, respectively. This basically follows the t heoretical bounds that are computed in Equation 14, i.e., 𝑟𝑎𝑡𝑖𝑜and 𝑟𝑎𝑡𝑖𝑜when 𝐿 = 2. Furt hermore, considering the performance improvement, the space usage of approximation factors of LQ-GCNis actually acceptable, which may dispel the concerns of large additional storage overhead. Online inference acceleration. We evaluate the time cost including the score estimation and sorting. LQ-GCN predicts the scores between users and items by conducting embedding multiplications. At the stage of online inference, we ﬁrst interpret these binary embeddings by signed integers, e.g., int8, and then conduct integer arithmetics including integer summations and matrix multiplications. Please notice that we leave the development o f bit wise operations for online inference for future work. To give a fair comparison on the inference time cost, we disable al l arithmetic optimization such as BLAS, MKL, and condu ct the experiments using the vanilla NumPy provided by. As shown in Figure 5(a), purely based on the quantized embeddings, LQ-GCN can achieve 39.83% (118.23→196.49) of inference acceleration. LQGCNtakes a similar running time with LightGCN as it introduces the approximation factors in score estimation. Furthermore, Figure 5(b) visualizes the overall evaluation in terms of resource consumption and recommendation accur acy. As the cube’s fronthigh corner means the ideal optimal performance, our proposed methods make a good balance w.r.t consumption and accuracy. We evaluate the necessity of each model comp onent in both LQ-GCNand LQ-GCN. Due to the page limits, we report the Top-20 recommendation results as a reference in Table 3. Figure 5: Results of two-layer networks with 128-dimension embeddings (E.S. and I.T. are the abbreviations of embedding size (MB) and inference time (s). Best view in color). 5.6.1 Eﬀect of Topology-aware antization. To substantiate the impact of topology-aware quantization in graph convolution, we give a variant of LQ-GCN, i.e., w/o TQ, by disabling the layer-wise quantization and setting it as the ﬁnal encoder of fullprecision graph convolution. As we can observe in Table 3, variant w/o TQ remarkably underperforms LQ-GCN. This demonstrates that simply using the latest-updated embeddings from the GCN framework may not suﬃciently model the unique latent features of both users and items, especially for the quantization-based ranking. Via capturing the intermediate information for representation enrichment, our topology-aware quantization can eﬀectively alleviate the ranking smoothness issue caused by the limited expressivity of discrete embeddings. T his helps to make the quantized representations of both users and items mo re discriminative, which leads to the performance improvement on Top-K recommendation. 5.6.2 Eﬀect of Multi-loss in Optimization. To stu dy the effect of BPR loss Land graph reco nstruction loss L, we set two variants, termed by w/o Land w/o L, to optimize LQGCNseparately. As shown in Table 3, with all other mod el components, partiall y using one of Land Lproduces large performance decay to LQ-GCN. T his conﬁrms the eﬀectiveness of our proposed muti-loss design: while Lassigns higher prediction values to observed interactions, 1.e., 𝑦= 1, than the unobserved user-item pairs, Ltransfers the graph reconstruction problem to a classiﬁcation task by using the full-precision embeddings in training. By col lectively optimizing these two lo ss fu nctions, LQ-GCNcan learn precise intermediate embeddings from L, and produce qu antized representations with high-quality relative order information regularized by Laccordingly. 5.6.3 Eﬀect of Rescaling Approximation. We now discuss the eﬀect of approximation factors in LQ-GCN. We create two variants, namely w/o RAF and w/in LF. w/o RAF directly removes the rescaling approximation factors and w/in LF means replacing our original approximation factor s with learnable ones. We optimize both variants w/o RAF and w/in LF with the annealing training strategy. (1) The performance decline of w/o RAF proves the eﬀectiveness of rescaling approximation for user-item representations. Although these factors are directly calculated and may not be theoretically optimal, they reﬂect the numerical uniqueness of embeddings for both users and items, which substantially improves LQGCN’s prediction capability. (2) As for w/in LF, the design of VariantR@20 N@20 R@20 N@20 R @20 N@20 R@20 N@20 w/o TQ17.73 35.31 1 1.63 15.58 10.29 11.60 4.33 8.58-13.60% -8.07% -20.45% -26.65% -17.81% -16.67% -15.10% -10.81% w/o L19.67 37.22 8.66 13.33 5.12 6.01 3.52 7.33-4.14% -3.10% -40.77% -37.24% -59.11% -56.82% -30.98% -23.80% w/o L16.98 32.76 8.32 9.28 10.86 11.55 3.33 6.60-17.25% -14.71% -43.09% -56.31% -13.26% -17.03% -34.71% -31.39% Best 20.52 38.41 14.62 21.24 12.52 13.92 5.10 9.62 w/o RAF20.86 38.14 1 0.29 12.10 11.19 12.11 4.25 8.09-8.55% -10.13% -36.17% -48.77% -19.32% -20.90% -25.96% -24.18% w/in L F20.05 38.25 1 4.53 21.23 12.35 13.65 5.56 10.20-12.10% -9.87% -9.86% -10.12% -10.96% -10.84% -3.13% -4.40% w/o AT21.24 40.05 1 5.09 22.70 13.37 14.75 5.27 9.96-6.88% -5.63% -6.39% -3.90% -3.60% -3.66% -8.19% -6.65% Best 22.81 42.44 16.12 23.62 13.87 15.31 5.74 10.67 learnable rescaling factors does not achieve good performance as expected. One explanation is that, our proposed model currently does not post a direct mathematical constraint to learnable factors (𝑙 𝑓 ), e.g., 𝑙 𝑓= argmin(𝒗, 𝑙 𝑓𝒒), mainly because they have diﬀerent embedding dimensionality. T his means that purely relying on the stochastic optimization may hardly reach the optimum. In a word, considering the additional search space introduced by this regularization term, we argue that our deterministic rescaling method is simple but eﬀective in practice. 5.6.4 Eﬀect of Annealing Training Strategy. We disable the annealing training strategy by only adapt ing the rescaling approximation design to LQ-GCNand denote the variant as w/o AT. The performance of w/o AT well demonstrate the usefulness of our utilized annealing training strategy in avoiding unnecessary optimization directions and the numerical stability for producing better recommendation accuracy. In conclusion, the ablation study well justiﬁes the necessity of each model component in bot h LQ-GCNand LQ-GCN. We also discuss the eﬀect of diﬀerent hyperparameter settings, e.g., layer depth 𝐿, quantization dimension 𝑑, to model performance and attached the results in Appendix D. In this work, we propose LQ-GCN to study the problem of 1bit representation qu antization for Top-K recommendation. While LQ-GCNimplements the basic framework of LQ-GCN to generate the quantized user-item representations, LQ-GCNfurther improves the recommendation capability by at taining 90∼99% performance recovery compared to the state-of-the-art model. The extensive experiments over four real benchmarks not only prove the eﬀectiveness of our proposed models but also justify the necessity of each model co mponent. As for future work, we point out two po ssible directio ns. (1) Instead of relying on integer arithmetics, how to develop the bit wiseoperation-supported computation for eﬃcient inference is an import ant topic to investigate. (2) It is also worth studying the problem of complete network binarization for the GCN framework, as it is more fundamental to many GCN-related methods for model compression and computation acceleration.