pxfvintage@163.com, {shilou.syb, keren.ykr, qinggan.wh, shui.lius, chengjun.mcj, zhizhao.cb}@alibaba-inc.com Abstract—Promotions are becoming more important and prevalent in e-commerce platforms to attract customers and boost sales. However, Click-Through Rate (CTR) prediction methods in recommender systems are not able to handle such circumstances well since: 1) they can’t generalize well to serving because the online data distribution is uncertain due to the potentially upcoming promotions; 2) without paying enough attention to scenario signals, they are incapable of learning different feature representation patterns which coexist in each scenario. In this work, we propose Scenario Adaptive Mixtureof-Experts (SAME), a simple yet effective model that serves both promotion and normal scenarios. Technically, it follows the idea of Mixture-of-Experts by adopting multiple experts to learn feature representations, which are modulated by a Feature Gated Network (FGN) via an attention mechanism. To obtain high-quality representations, we design a Stacked Parallel Attention Unit (SPAU) to help each expert better handle user behavior sequence. To tackle the distribution uncertainty, a set of scenario signals are elaborately devised from a perspective of time series prediction and fed into the FGN, whose output is concatenated with feature representation from each expert to learn the attention. Accordingly, a mixture of the feature representations is obtained scenario-adaptively and used for the ﬁnal CTR prediction. In this way, each expert can learn a discriminative representation pattern. To the best of our knowledge, this is the ﬁrst study for promotion-aware CTR prediction. Experimental results on real-world datasets validate the superiority of SAME. Online A/B test also shows SAME achieves signiﬁcant gains of 3.58% on CTR and 5.94% on IPV during promotion periods as well as 3.93% and 6.57% in normal days, respectively. Index Terms—Recommender System, Click-Through Rate Prediction, E-commerce Promotions, Scenario Adaptive, Mixture-ofExperts A well-performing recommender system needs to discover valuable items from massive available options for customers not only accurately but also timely [12], [14], [25], [28], [35], [36], [42]. To achieve this goal, most of advanced ClickThrough Rate (CTR) prediction models use high-order interactions of features to improve their representation capacity [3], [10], [21], [34], [40], and leverage sequential user behaviors to model users in a dynamic manner [8], [38], [39], [44], [45]. Most of the previous works assume that users’ preferences are coherent and change smoothly along time. However, there exists scenarios where users’ behaviors can be driven , Keren Yu, Hong Wen, Shui Liu and Bo Cao differently by different occasions [33]. Such scenarios refer to time periods with different data distributions and are related to particular time or events. Especially with the intensiﬁcation of e-commerce competition, various online promotions become quite frequent, which leads to signiﬁcant distribution shift of online data as illustrated in Figure 1. Concretely, promotions can be grouped into three categories: 1) annual shopping festivals, such as Black Friday and Double 11; 2) promotions for Women’s Day, Spring Festival, Christmas Day, and so forth, which are derived by certain culture; and 3) promotions initiated according to speciﬁc strategies of e-commerce platform or emerging consumption trends, such as Jingdong’s Digital Festival and Pinduoduo’s Zhenxiang Festival, which may be irregular in time. Impacts of promotions on e-commerce are partly investigated in [41] by a statistical analysis about Double 11. However, no suitable approach is proposed and promotions of the other categories draw little attention. As far as we know, existing approaches to deal with recommendations for e-commerce promotions are mostly based on temporarily customizing or adding extra models. It highly relies on expert experience to decide when to switch models for online serving because it’s uncertain that when online data distribution will change. Besides, different from MultiScenario CTR prediction [20], [26], it’s not easy to build scenario-speciﬁc training data since scenarios in this work are time-variant and intertwined with each other in time, which makes medium and long term statistical features unavailable. As a result, it takes considerable efforts to manually handle every promotion in this way, while the improvements are uncertain because the inﬂuence of promotions varies according to many factors, such as the promotion intensity and the capacity of e-commerce platforms. For example, only super promotions like 618 and Double 11 could impact Taobao remarkably while almost every single promotion may have a signiﬁcant impact on small e-commerce businesses, such as Tmall Global and Kaola. Up to now, it remains a challenge to handle recommendations for both normal days and promotions using a uniﬁed model. Although there exist a few works [16], [31]– [33] paying attention to this challenge, they are incapable of modeling the aforementioned complex scenarios sufﬁciently. In [33], a model is developed to adapt to different scenarios by learning representations indexed by timestamp. However, timestamps are not enough to describe all promotions of the second and third category mentioned above because calendar in different cultures varies and promotions of the third category may not be time-dependent. In [16], item behaviors [7], i.e., a set of users who interact with this item, are introduced via a carefully designed time-sensitive neural structure, which models items in a dynamic manner to strengthen the ability to predict users’ emerging interests. Methods proposed in [31], [32] follow the similar idea of modeling items in a dynamic manner by adopting a hypergraph. However, the mutual interference between non-identically distributed data from different scenarios is not tackled, which may impose extra difﬁculties on the learning process. Last but not the least, existing sequential modeling methods may not perform well when serving small e-commerce businesses due to the highly sparse and incoherent user behaviors. Based on the above analysis, it is non-trivial to develop a promotion-aware CTR prediction method for e-commerce recommendations. Inspired by works on time series prediction [5], [37] and multi-task learning [11], [17], [27], we propose a novel Scenario Adaptive Mixture-of-Experts (SAME) model. It explicitly models each sample from different scenarios as a mixture of experts, each of which could learn a discriminative feature representation pattern under the modulation of Feature Gated Network (FGN) and thus facilitates mitigating the mutual interference between non-identically distributed data. Our contribution is threefold: CTR prediction in the context of e-commerce promotions. We propose a simple yet effective SAME model to serve both promotion and normal scenarios. multiple experts modulated by a FGN. The FGN can efﬁciently adapt to different scenarios by elaborately processing scenario signals so that the distribution uncertainty is handled. Besides, a Stacked Parallel Attention Unit (SPAU) is designed and applied in each expert for effective sequential modeling. demonstrate the superiority of our SAME model over representative methods. We also perform visualization analysis for better interpretability. In this part, we brieﬂy review the most related works to our SAME model, i.e., CTR prediction and multi-task learning. We also discuss the existing methods for Multi-domain Learning to further clarify the speciﬁcity of promotion-aware CTR prediction. CTR prediction has become a crucial part of many online applications, such as search engines, recommender systems, and online advertising. In recent years, Deep Neural Networks (DNN) have shown powerful capacity of learning feature representations and modeling high-order feature interactions. For CTR prediction, there are some representative DNN models have been proposed, including Wide&Deep [3], PNN [21], PIN [22], DeepFM [10], and DCN [34]. More recently, since the sequence of user behaviors contains rich information of users’ interests and preferences, there is increasing attention on sequential modeling in online systems. Given a target item, DIN [45] introduces the attention mechanism to activate the historical behaviors and capture the diversity characteristic of user interests. ATRank [43] proposes an attention-based framework modeling the inﬂuence between heterogeneous behaviors of a user. To capture dependencies between sequential behaviors, DIEN [44] adopts a two-layer RNN structure to model the evolving process of speciﬁc interest for different target items. For better modeling of the temporal effects, HPMN [24] is proposed to capture the periodic patterns of users with a hierarchical recurrent memory network, while TIEN [16] models items in a dynamic manner by using item behaviors to strengthen the ability to predict users’ emerging interests. To sum up, learning higher-order representations of features and introducing sequential behaviors improve the expressive ability of models and the prediction accuracy signiﬁcantly. However, none of these models pay enough attention to the issues caused by e-commerce promotions. Multi-task learning has been used successfully across various applications of machine learning, from natural language processing [2], [4] to computer vision [9], [23], and recommender systems [19]. By sharing representations between related tasks, multi-task learning allows to exploit common underlying factors and transfer knowledge across different tasks, where more data can be leveraged to learn better-shared representations. Multi-task learning is typically done in either a hard or a soft way by sharing parameters of hidden layers. The hard parameter sharing method [1] shares the hidden layers between all tasks and keeps several task-speciﬁc output layers. Such framework has been widely used in online advertising and recommender systems [18], [35], [36], which are based on the explicit decomposition of user sequential behavior graph from impression to purchase. For soft parameter sharing, each task has its own model with its own parameters, where the distance between the parameters is regularized [11], [17], [27]. With advanced techniques including expert model, gate mechanism, and connection routing, such models achieve parameter sharing between different experts while learning speciﬁc experts for different tasks. Partially inspired by these prior works, we borrow the idea of Mixture-of-Experts. Nevertheless, we deal with a single task (i.e., CTR prediction) in the context of time-variant scenarios, including promotion events and normal days. Multi-domain learning [6], [13], [15] enables knowledge transfer between domains to improve learning. The difference between multi-domain learning and multi-task learning is that multi-domain learning makes predictions for multiple domains addressing the same problem. Multi-Scenario CTR prediction [20], [26] can be seen as a case of multi-domain learning, in which each scenario corresponds to a business domain and the task is the CTR prediction. Sheng et al. [26] propose the Star Topology Adaptive Recommender (STAR) model to leverage data from all domains simultaneously and exploit the domain relationship so that the model can serve all domains. With shared centered parameters and multiple sets of domainspeciﬁc parameters, STAR can achieve effective information transformation across multiple domains to learn domain commonalities and distinctions. Niu et al. [20] propose a treeguided mixture of expert networks named TREEMS to accommodate all sharing scenarios under a single uniﬁed recommendation model, where each scenario obtains its speciﬁc context information from a scenario tree to determine the combination of experts. In general, Multi-Scenario CTR prediction tackles spatially different scenarios in different venues, where each scenario has its explicit indicators (e.g., scenario id). Each scenario can be treated as stable and time-independent so data of each scenario are accessible for training. In contrast, promotion-aware CTR prediction handles timevariant scenarios which are intertwined with each other in time, so it’s uncertain that when online data distribution will change. Promotion-aware CTR models are expected to generalize well to the upcoming different scenarios automatically, whose data have never been seen during training, which makes existing Multi-Scenario CTR models not applicable. Besides, explicit scenario indicators are not available so it’s not easy to build scenario-speciﬁc training data, which makes the design of scenario-speciﬁc experts [26] infeasible. In CTR prediction, the model takes input as (x, y) ∼ (X, Y ), where x is the feature and y ∈ {0, 1} is the click label. Speciﬁcally, the input features of CTR models mainly consist of six parts. The ﬁrst part is the user behavior sequence, which records user history of clicked/purchased items. The second part consists of the user features, including user proﬁle (e.g., age and gender) and statistic features from user history. The third part consists of the item features, e.g., item id, category, brand, and related statistic features. The fourth part is the interaction features of the target item and user, e.g., clicks of the user in the category during the last 24 hours, and purchases of the user in the shop during the last 24 hours. The ﬁfth part consists of context features, such as position, device and time information. The sixth part of features contains scenario signals that are sensitive to promotions, including global statistics such as the amount of active users and the GMV (Gross Merchandise Volume) of several recent time windows, and the change rate of corresponding features. The goal of CTR prediction is to learn a model fwith parameter θ that minimizes the the generalization error: where L is the loss function. Note that the training datasets D consist of samples from both normal days and promotions, so in this work, the assumption of identical data distribution does not hold. Besides, taking the irregular promotions into consideration, the upcoming online data distribution is also uncertain. Finally, we formulate the problem of promotion-aware CTR prediction as training a model on the datasets D which can predict the click probability given a user-item pair and generalize well to the upcoming different distribution automatically. The focus of this work is to develop a uniﬁed CTR prediction method serving both promotions and normal days. As mentioned in the introduction, promotion, a widespread phenomenon in e-commerce that can cause certain degrees of data distribution shift, has not been explored thoroughly in existing recommendation works. It can be observed that different interaction tendencies of users and items coexist in various scenarios, e.g., normal days and promotions. For example, some users may tend to interact with items of their intrinsic preference while some may tend to interact with items globally or locally hot. Despite the coexistence, it’s obvious that the impact of different interaction tendencies varies in different scenarios, which may be the main reason for the data distribution shift. We assume that different interaction tendencies lead to discriminative user-item feature representation patterns. Intuitively, to mitigate the mutual interference between non-identically distributed data and adapt to the potentially different online distribution, we need to learn different representation patterns and mix them according to the detailed context to make the ﬁnal prediction. Besides, we also need to consider that whether the existing sequential modeling methods can work well when it comes to recommendations for e-commerce businesses of which the scale is much smaller than platforms such as Taobao and Amazon. The more severe sparsity and incoherence in user behaviors may introduce new challenges on representation learning when handling the user behavior sequence. In the following sections, we present the details of our proposed SAME model. As shown in Figure 2(a), scenario signals, a set of features elaborately devised from a perspective of time series prediction, are fed into the FGN, while the other features are fed into each expert via a shared embedding layer. For each expert, the SPAU is used for generating a representation of the target item and user. The FGN aims at generating a high-quality scenario representation to capture shifted data distribution so that different scenarios can be distinguished automatically. With the output of the experts network and the FGN, attention weights are calculated to combine different experts for the ﬁnal CTR prediction. With scenario signals carefully processed, we try to make the FGN more efﬁcient in learning scenario representation than the gate mechanism of multi-task learning, and provide better guidance of representation learning for experts. The shared embedding layer is devised to handle the input of experts network, which includes all features mentioned in Section III-A except for scenario signals. They can be further grouped into two kinds of features: categorical feature and numerical feature. We discrete the numerical features based on their boundary values, transforming them into the categorical type. Then each categorical feature is encoded as a one-hot vector. After raw feature processing, x, x, x, xand xdenote user features, user behavior sequence, item features, user-item interaction features and context features respectively. Due to the sparseness nature of one-hot encoding, we apply linear fully connected layers to obtain low dimensional embedding e, e, eand e, according to: e= [Wx, ..., Wx, ...] , W∈ R, e= [Wx, ..., Wx, ...] , W∈ R, where W∈ Rdenotes the embedding matrix of the kfeature, i.e., x, and dis the embedding dimension of xwhile vis the vocabulary size, and ∗ ∈ {u, i, ui, c}. Then the embedding of user behavior sequence is formed with item embedding in the sequence, i.e., e= {e, ..., e} where e denotes the item embedding of tuser behavior and t is the sequence length. The basic principle of designing the experts network is to make every single expert be able to generate a high-quality representation from the output of the shared embedding layer, i.e., e, e, e, eand e. The embedding of user behavior sequence is of great importance since it contains rich information about user interest. When online shopping, users may browse some unrelated items, which would somehow inﬂuence the representation learning of the sequence. Besides, how active the users are in e-commerce depends a lot on the scale of business and the abundance of items, which means user behaviors tend to be more sparse and incoherent if the scale of business is smaller. Under such circumstances, the existing sequential modeling methods, such as target attention mechanism [45] and RNNbased network [44], may not be able to learn well and thus lead to unreliable personalized recommendations. Due to the above reason, we propose the SPAU structure to handle the user behavior sequence, in which three kinds of attention weights are calculated. For all of them, we apply Layer Normalization on Query and Key (LN-QK) vectors as shown in Figure 2(b), which can relieve the vanishing gradient problem so better attention weights can be learned. Layer normalization is not applied on Value for the reason that we don’t expect to change its original distribution. In this way, the difﬁculty of learning from the sparse and incoherent sequence is alleviated and the training performance is boosted. First, we use a multi-head [30] self-attention network to model user preference from multiple views of interest and decrease the inﬂuence of unrelated and incoherent behaviors, according to: output = Concat(head, ..., head)W, head= Attention(Q, K, V)(3) where h represent the number of heads and Wdenotes the weight matrix of output linear transformation. LN(.) denotes layer normalization operation and dis the dimension of K. Q, Kand Vare calculated by linear projection: where Q, K and V refer to Query, Key and Value respectively, and W, Wand Wdenote linear projection weight matrix for Q, Kand Vof the ihead respectively. For self-attention, Q, K and V all refer to e, and ˆe= {ˆe, ..., ˆe} is the output in which ˆedenotes the output of multi-head self-attention at tposition. Then, on top of the multi-head self-attention, user attention and target attention are performed in parallel. To mine more ﬁne-grained personalized information and suppress noisy behavior, eis used as the query vector attending to ˆe. In Equation 4, eis used as Q and ˆeis used as K and V , and then the user attention scan be calculated according to Equation 3. Similarly, we perform target attention to activate historical interests related to the target item, with eattending to ˆe. By replacing e with e, target attention sis calculated in the same way as user attention. It can be observed that different users in different context usually behave differently even to similar items, which implies that users’ behaviors are biased. Therefore, we concatenate eand e, and feed them into a Multi-Layer Perception (MLP), which is called Bias Net. Meanwhile, all information, including s, s, e, e, eand e, are concatenated and fed into another MLP, i.e., Main Net. Finally, we concatenate the outputs of Main Net and Bias Net to obtain the output of an expert, i.e., the user-item feature representation. F. Feature Gated Network (FGN) Scenario signals, described in Section III-A, are devised following the idea of trend features in time series prediction. As the input of the FGN, it should be noted that the number of unique feature values of most scenario signals relies on the amount of related time windows in training data. For example, given training data sampled from 30 days, there are only 30 unique feature values for the feature of the amount of active users in the last day. In this situation, direct usage or discretization and embedding of these features may lead to poor online performance because of the over-ﬁtting problem and unstable distribution of feature value. To address this issue, we adopt the Gaussian log1p transformation [46] to process scenario signals, which helps keep sensitive to feature values while improving the stability of feature transformation. Speciﬁcally, for scenario signals x, this process can be formulated as: where D denotes training set and |D| denote the number of samples in D. x[i] denotes scenario signals of the isample in D and z[i] is the corresponding output of Gaussian log1p transformation. As shown in the right part of Figure 2(a), zis fed into a MLP to get the higher-order scenario representation p. Then, the gated (attention) vector α is derived according to: where αis the gated weight for iexpert and f is a function that projects input into a scalar. N denotes the number of experts and odenotes the output of iexpert. The ﬁnal CTR prediction layer is formulated as follows: where F is a function implemented as a MLP, of which the last layer uses Sigmoid as activation function while the other layers use ReLU. We adopt the widely-used cross-entropy loss during training our SAME model, which is deﬁned as: L = −1|D| In this section, we conduct a series of experiments to answer the following research questions: RQ1 How does SAME perform compared to the state-of-theart models for the CTR prediction task in the context of both promotion and normal scenarios? RQ2 Can FGN adapt to different scenarios and guide each expert to learn discriminative user-item feature representation? RQ3 How does the number of experts in SAME affect the performance? RQ4 Does the SPAU component necessarily contribute to the learning of user behavior and the improvement of the performance? A. Experimental Setup 1) Datasets: We establish the datasets by collecting the users’ interaction logsfrom our online e-commerce platform, where promotions are highly frequent and have a considerable impact on the recommender system. Logs are sampled from 2020/10/01 to 2020/12/31, including Double 11, Black Friday, Double 12, and three monthly promotions (21st of every month). The detailed statistics are summarized in Table I. We split the entire datasets into non-overlapped training set and testing set according to the timestamp of the prediction behavior, effectively avoiding feature leakage. In this way, the training set is about 80% of the whole datasets and the left 20% of data is used as the testing set. To comprehensively evaluate the performance of the proposed model under scenarios of promotions and normal days, the testing set is further divided into two parts accordingly. All the data we use have been anonymously processed by the log system and users’ information is protected. 2) Evaluation Metrics: To compare with the SOTA methods, area under ROC curve (AUC) is used as the ofﬂine evaluation metric. For online A/B testing, we choose clickthrough rate (CTR) and average number of user clicks (IPV), which are widely adopted in industrial recommender systems for evaluating online performance. Improving CTR and IPV simultaneously implies not only more accurate recommendation but also more active users. 3) Comparison Methods: We compare our SAME model with three classes of the previous methods: 1) methods that capture high-order feature interactions; 2) sequential user behaviors based methods; and 3) multi-task learning based methods. We also include two variants of our model for ablation study. They are brieﬂy described as follows: while only adding negligible extra complexity to a DNN model. mechanism. It models interests evolving process from user behaviors and calculates attention values to control the second RNN layer to activate the most relative interests to the candidate item. learning methods. In this work, MMoE is trained with CTR prediction task and CVR prediction task, using the same expert structure as SAME. The CVR task is chosen because conversion is more sensitive to promotions. calculate the importance weight for each user in item behaviors and captures the popularity of the items by a time-aware evolution layer. It strengthens the ability to predict users’ emerging interests by modeling items in a dynamic manner. simpliﬁed version of SAME, i.e., using a single expert without FGN. It can also be regarded as a variant of DIEN, of which the two-layer RNN structure with an attention is replaced with the SPAU. without LN-QK. Note that for all the above methods, scenario signals are processed as the other input features, i.e., discretized and embedded into low dimension vectors. 4) Implementation Details: We implement these deep learning models in distributed Tensorﬂow 1.4. During training, we use 3 parameter servers and 6 Nvidia Tesla V100 16GB GPU workers. Item ID, category ID and brand ID have an embedding size of 32 while 8 for the other categorical features. We use 8-head attention structures with a hidden size of 128. Both Main Net and Bias Net are MLPs with 3 layers. As for the FGN, a 3-layer MLP is used after Gaussian log1p transformation. Adagrad optimizer with a learning rate of 0.01 and a mini-batch size of 256 is used for training. The number of experts N varies from 2 to 5 and is set to 2 by default. We report the results of each method under its empirically optimal hyper-parameters settings. B. Experimental Results: RQ1 1) Ofﬂine Results: Table II presents the results of all methods in both promotion and normal scenarios. The major observations are summarized as follows: in promotions, and outperforms DIEN in both scenarios, validating the effectiveness of the proposed SPAU. runner-up method in promotion scenarios, outperforming SAME-OSE slightly. However, it’s not comparable to SAME-OSE in normal days, implying that MMoE fails to adapt to different scenarios without paying enough attention to scenario signals. degree of user activity in our business is low so the huge user embedding parameters introduced by item behaviors can’t be sufﬁciently trained. the best performance, outperforming the runner-up methods by a large margin. 2) Online A/B testing: From 2021/01/18 to 2021/01/31, online A/B testing was conducted in our online recommender system. DCN has been ofﬂine for a long time since it was outperformed by SAME-OSE, while SAME-OSE-NoLN and TIEN can’t perform well in ofﬂine evaluation, so they were not selected for online tests. SAME-OSE was the main online deployed model and served as the baseline model, while MMoE was another choice because we observed that the CVR prediction is more sensitive to promotions. As shown in Table III, compared to the baseline model, SAME improves 3.58% on CTR and 5.94% on IPV during promotions while 3.93% and 6.57% in normal days respectively, outperforming all the other models. The narrowed gap of CTR and IPV between different scenarios demonstrates that our model has a better adaption ability, which helps to recommend more attractive items and improves the amount of purchases. C. Visualization of the FGN and Experts Network: RQ2 In this section, we conduct an analysis on FGN and Experts Network to study how they contribute to the ﬁnal performance. We feed samples from different scenarios into SAME and generate gated weights for all samples. To be speciﬁc, scenarios of normal days (20210105-20210106), days right before promotion (20201219-20201220), middle of promotion (20201222) when users’ primary demands have been released, and the other days of promotion (20201221, 20201223) are included. Then, distributions of the gated weights of different days are plotted in different colors as shown in Figure 3(a), of which the x-axis refers to the value of gated weights and the y-axis refers to the amount of samples. It can be observed that gated weights of days of the same scenario are close while for different scenarios, e.g., normal scenario, middle of promotion and the other scenarios, the gated weights distinguish from each other obviously, which implies that the FGN can adapt to different scenarios. To validate the effectiveness of the Experts Network, we randomly sample hundreds of samples and feed them into SAME. The output of the two experts is visualized using tSNE [29]. As shown in Figure 3(b), representations of the two experts clearly distinguish from each other, which implies that different experts in the Experts Network are able to learn discriminative user-item feature representation patterns under the modulation of FGN. D. Inﬂuence of the Number of Experts: RQ3 We investigate the inﬂuence of the number of experts in our SAME model. We retrain SAME with the number of experts varying from 2 to 5 and evaluate these models with the whole test set, observing a decreasing trend of AUC which is detailed in Table IV. The visualization displayed in Figure 3(b) and Figure 4 indicates that representations between experts become more indistinguishable as the number of experts increases from 2 to 5, i.e., two experts can capture clearly distinguishable representation patterns while as experts increase the captured representation patterns mix with each other. Intuitively, we conclude that with too many experts the learned representation pattern of each expert may entangle with each other, leading to the ﬁnal performance degradation. In the promotion-aware CTR prediction task, two experts can get the best results. When SAME is applied to the other similar tasks, we consider that best performance may be achieved when each expert corresponds to a certain representation pattern empirically. E. Study of Attention: RQ4 The ofﬂine and online results in Table II and Table III indicate that SAME-OSE outperforms DIEN consistently. This ablation study conﬁrms the superiority of the SPAU in sequential modeling. Another ablation study is conducted by comparing SAME-OSE and SAME-OSE-NoLN. The results shown in Table II conﬁrm the effectiveness of LN-QK. To be more intuitive, we visualize the attention weights of the multi-head target attention corresponding to a user randomly sampled from the test datasets. As shown in Figure 5, given the foundation make-up as the target item, the SAME-OSE-NoLN fails to attend to the related cosmetic items in the user’s historical behaviors, while the SAME-OSE is able to highlight the related historical interactions correctly for both clothing and foundation make-up. The above case further illustrates that LN-QK can contribute to extracting information related to the target item and eliminating the impact of irrelevant items especially when user behaviors are highly sparse and incoherent, while the existing sequential modeling methods cannot perform well. In this paper, we investigate the difﬁculties of CTR prediction on e-commerce platforms with frequent promotions and propose a simple yet effective SAME model for both promotion and normal scenarios, i.e., Scenario Adaptive Mixtureof-Experts. Our SAME model consists of a Feature Gated Network (FGN) and Experts Network, where the former can learn distinct gated weights for different scenarios and guides the latter to learn distinguishing feature representation patterns by different experts. In this way, our SAME model can adapt to both promotion and normal scenarios and outperforms representative CTR methods on both realworld ofﬂine datasets as well as online A/B testing. The empirical study of gated weights distribution and feature representation visualization further conﬁrms the effectiveness of the proposed FGN and Experts Network. With the help of SPAU, SAME can overcome the shortcoming of existing sequential modeling methods in situations of highly sparse and incoherent user behaviors. The exploration on the number of experts shows useful insights for future work, i.e., adapting SAME to scenarios in the other context beyond promotions.