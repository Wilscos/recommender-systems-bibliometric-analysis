Keywords Automatic feature engineering · Click-through rate prediction · Combinatorial features · Feature selection In general machine learning problems, feature engineering (FE) is a crucial step that inﬂuences the ﬁnal performance. Studies have shown that feature engineering is often more important and time-consuming than models training [ Automatic feature engineering frameworks may be used to automate this process to saves a considerable amount of time and discover more novel features. Nevertheless, just like the No Free Lunch theorem of model selection [ universally applicable automatic feature engineering solution. Another focused area is deep learning, which emphasizes {crfcolf, jasonyang8119}@gmail.com, yacongma@foxmail.com, {sbdong, jlhu}@scut.edu.cn The challenge of solving data mining problems in e-commerce applications such as recommendation system (RS) and click-through rate (CTR) prediction is how to make inferences by constructing combinatorial features from a large number of categorical features while preserving the interpretability of the method. In this paper, we propose Automatic Embedded Feature Engineering(AEFE), an automatic feature engineering framework for representing categorical features, which consists of various components including custom paradigm feature construction and multiple feature selection. By selecting the potential ﬁeld pairs intelligently and generating a series of interpretable combinatorial features, our framework can provide a set of unseen generated features for enhancing model performance and then assist data analysts in discovering the feature importance for particular data mining tasks. Furthermore, AEFE is distributed implemented by task-parallelism, data sampling, and searching schema based on Matrix Factorization ﬁeld combination, to optimize the performance and enhance the efﬁciency and scalability of the framework. Experiments conducted on some typical e-commerce datasets indicate that our method outperforms the classical machine learning models and state-of-the-art deep learning models. end-to-end learning. In this area, feature engineering in traditional machine learning is replaced by representation learning. However, there is no ubiquitous deep learning model for all tasks, which means different feature extraction layers need to be designed for different original features. Some important tasks of data mining, such as recommendation system and click-through rate prediction, usually contain a large number of categorical ﬁelds including students, programmers, etc.). A ﬁeld containing encoding. In a dataset with m of them is 1 while the rest is 0, which causes high-dimensional sparsity. Therefore, how to effectively represent categorical features is a core task of these applications. Among various features, the combinatorial feature, which is a combination of different raw features through a certain paradigm, or the crossing feature learned by models, is the most crucial. For example, in the advertisement CTR problem, a speciﬁc user prefers to click on the snack advertisement in the afternoon since he or she becomes hungry at tea time. Shan [ that they are often the strongest features of many models, while Cheng [ of manual crossing features in the wide part. The crossing feature " brings a more reasonable inference than the one-hot encoded feature " separately. The combinatorial features can bring great precision improvement to prediction; Its high interpretability is helpful to dig deep into the underlying relationship of the data. In order to reduce the time-consuming artiﬁcial feature engineering of combinatorial features and improve the prediction accuracy, researchers and practitioners invested a lot of time and effort into this aspect. The research substantially focuses on the improvement of machine learning models and automatic feature engineering. To learn the crossing features, some machine learning models such as Factorization Machines (FM) [ categorical features as latent vectors and model the feature crossing by the inner product of latent vectors. Inspired by the successful use of deep learning in computer vision and natural language processing, studies [ categorical features into embedding vectors, use FM to train the embedding layer, and then use Deep Neural Networks (DNN) to obtain higher-order nonlinear interactions of features. The key point of these studies is to design a novel feature crossing layer, So a technique named Neural architecture Search (NAS) from AutoML was applied to design it automatically [ models, they cannot make full use of all active combinatorial features and have less interpretability. The automatic feature engineering frameworks can also construct speciﬁc features. Some of them are oriented to various data mining tasks. They can perform feature engineering on different types of features (e.g., categorical features, numerical features) while maintaining acceptable interpretability. For example, Data Science Machine(DSM) [ which iteratively constructs combinatorial features in relational data tables. Based on DSM, The follow-up works emphasize a more complex construction paradigm [ in-depth studies of combinations for categorical features nor attempts to embed high-dimensional sparse categorical features by automatic feature engineering techniques. In this paper, we propose Automatic Embedded Feature Engineering(AEFE), which can perform feature engineering on sparse categorical features automatically. It generates complex but interpretable combinatorial features, as shown in Table 1, and requires no manual intervention. AEFE is generally applicable to most Internet data mining tasks that contain a large number of categorical features. Overall, the contribution of this paper is as follows: Categorical ﬁeld and categorical feature, which are from the perspective of data form and machine learning respectively, are similar in most contexts of this paper. 9,10,11]. Although how to handle the high-dimensional sparse features is solved by deep learning We attempt to apply automatic feature engineering technology on the feature combination problem of categorical features and devise AEFE, a novel solution to represent combinatorial features in machine learning. Rich feature construction paradigms and operation sets are provided by AEFE to generate various meaningful statistical features, which can signiﬁcantly enhance model performance. Meanwhile, the multi-module cascaded feature selection unit can select the most useful features efﬁciently. Diverse optimization techniques like data sampling, ﬁeld combination search, and distributed implementation are leveraged to ensure that the framework is scalable and efﬁcient. By experiments on several typical datasets, it is empirically demonstrated that AEFE cascaded with GBDT(Gradient Boosting Desicion Tree) achieves better performance than state-of-the-art deep learning models. In addition, AEFE has a stronger capture capability of combination information, as well as better interpretability, than some learning models such as FM, AFM, DeepFM, xDeepFM. The remainder of the paper is organized as follows. In Section 2, relevant previous works about machine learning models and automatic feature engineering are introduced. In Section 3, we present our AEFE in detail and illustrate it from different perspectives. In Section 4, some optimization techniques of AEFE are described. We conduct several experiments in Section 5 to show the performance of our methods, and ﬁnally, a brief conclusion and prospects are presented in Section 6. The early feature crossing technique applied to high-dimensional sparse features is derived from the optimization of the binomial kernel method [ achieve feature crossing. However, due to the high feature dimension, such models encounter the problem of excessive parameter space and over-ﬁtting. To alleviate the excessively large parameter space of the crossing feature, FM [ feature to the inner product between the latent vectors of corresponding features. Owing to its pleasing effect, improving FM becomes an important research direction. Higher-order Factorization Machines (HOFM) [ the matrix factorization of FM to tensor factorization to achieve higher-order feature crossing. Xiao [ attention mechanism to obtain the weight of second-order terms and propose Attentional Factorization Machines(AFM). Field-aware Factorization Machines(FFM) [ Each feature uses different latent vectors to calculate the interaction with corresponding ﬁelds. Pan [ Field-weighed Factorization Machine(FwFM) to model the weights of ﬁeld pairs, reducing the number of parameters compared with FFM. Furthermore, FM can be regarded as a network model with only an embedding layer and a hidden crossing layer [21]. From this perspective, FM may be used as an approach to train the embedding layer, which not only maps high-dimensional sparse features to dense vectors but also captures second-order feature crossing. Zhang et al. [ proposed Factorization-Machine Supported Neural Networks (FNN), which pre-train the embedding vectors of features with FM and feed them into DNN next. Similar to FNN, Neural Factorization Machines (NFM) uses the Bi-interaction layer to subdivide the inner product of FM, generalizes FM with a deep learning structure [ wide part of the Wide&Deep model with FM. The commonness of these models is that after embedding the categorical features, the second-order crossing information and the high-order nonlinear crossing information are captured by FM and DNN, respectively. These models may be referred to as "FM-embedding deep learning models". Training the embedding vector containing feature crossing information does not necessarily require the use of FM. For example, Product-based Neural Network (PNN) replaces the FM layer with a product layer, which captures different crossing patterns by inner product or outer product [ Factorization Machines (xDeepFM) [ learn speciﬁc orders of explicit feature crossing, while the former interact features at the bit-wise level and the latter at the vector-wise level. XDeepInt [ and bit-wise level feature interactions. In addition, it is acceptable to use CNN [ model feature interaction. What’s more, extensive research proposed various models to learn feature crossing, e.g., Deep Crossing [ to name a few. The models mentioned above can learn different order feature crossing, especially the state-of-the-art deep learning models that can capture non-linear crossing. However, the large amount of parameters makes them only suitable for applications with a large amount of data. Moreover, they have problems of low prediction stability and difﬁcult tuning, compared with the pattern of feature engineering plus traditional machine learning. To handle it, recently, Display advertisement prediction Proportion of video vin user u’s records 3], Deep and Shallow Layers (DSL) [28], and Fi-GNN [29] which applied Graph Neural Network, just NAS was used to ﬁnd appropriate layers for different feature interactions. It focuses on leveraging mining of the high-order feature interactions and the complexity of model architectures. Inspired by DARTs [ for differentiable NAS, AutoFIS [ ﬁnd useful combinational features in inference step. AutoFeature [ of feature ﬁelds based on their pre-deﬁned operations including Pointwise Addition, Hadamard Product, Concatenation, Generalized Product, and Null. On the contrary, AutoGroup [ from FM but explores which feature is useful in which order interaction. The application of NAS hasn’t changed the functions of basic feature interaction. A benchmark conducted by Zhu [ indicates that designing a novel architecture meets the bottleneck in pursuing better performance. Besides, either the implicit crossing feature learned by DNN or the explicit crossing feature learned by PNN, CIN, GNN, PIN, etc., makes the model’s interpretability weak, which leads to the fact that in some business scenarios, the prediction results cannot give valuable feedback to the data collection and preprocessing stages. Taking the online recommendation system as an example, the deep learning model can give accurate recommendation items, but it is difﬁcult to know which context information or what combinatorial features are working to select this item. Guo [ the understanding of DNN on CTR prediction problems through visualization, yet its work mainly analyzes the internal mechanism of DNN and gives no guiding analysis from the business level. The study of automatic feature engineering and deep learning models is quite different, but they are similar in the goals of reducing time-consuming manual feature engineering and improving prediction accuracy just like NAS. Some of the early automatic feature engineering studies mostly focus on ﬁnding effective transformation functions, which are applied to the appropriate features, thus improving the prediction effect [ to ﬁnd features in the construction space, and takes advantage of Information Gain (IG) as a proxy function to judge whether the feature is valid. FCtree [ and selects the structural features that can bring the gain at the same time. Unlike the way we attach weight to ﬁeld combination, FCTree assigns weights to the transformation function and constantly updates them. FEADIS randomly selects features to construct and uses cross-validation to verify the efﬁcacy of features, which can be regarded as a greedy strategy [ sample, 2) the construction paradigm is simple, and 3) they only apply to small datasets. Some studies attempt to use the learning model to improve automation and enhancement in the process of automatic feature engineering. ExploreKit [ whether to choose a particular transformation as a binary classiﬁcation task, so a Multilayer Perceptron (MLP) was trained on the meta-features of plenty datasets for prediction. It is worth noting that the above two studies adopt meta-learning in their framework. Besides, AutoLearn [ regression model, which makes the machine learning task improve obviously. Our work integrates a gradient descent process of Matrix Factorization (MF) into the ﬁeld combination search, which improves efﬁciency. Cognito [ construction features, and each edge is a transformation function. AutoCross [ didn’t focus on feature engineering. Khurana [ uses reinforcement learning to trade off factors like effectiveness and efﬁciency. These frameworks can iteratively use transformation functions to make the constructed features more complex. Features complicated by iterative transformation, while improving the effectiveness, are more difﬁcult to understand. How to explain the meanings of a feature after multiple conversions? In order to construct more valuable features, Data Science Machine (DSM) [ by using aggregation operations (Groupby) in the relational table, which makes DSM more practical in data mining tasks. Lam’s OneBM [ on complex feature paradigms, which deﬁned multiple families of features such as the family of social graph-based features [13]. These work for realistic data mining scenarios are more practical than some previous works. The above works consider constructing different forms of features with as many data types as possible to provide universality. However, the actual data mining tasks vary widely, so the uniﬁed solution has a limited effect, and it is challenging to introduce prior knowledge guidance. Furthermore, for most of the automatic feature engineering research, the construction process only uses the feature information, yet the label information is only used for verifying the validity of features, which makes the information captured by these frameworks different from that of the deep learning model (since deep learning model directly ﬁts the relationship between features and labels). 36]. What the above work has in common is that 1) feature construction only uses data of a single 40] expresses the automatic feature problem as a tree-structured task: Each node is a dataset that joins the The interpretability of features or models, the complexity of ﬁnding high-order feature interactions, and the capability of handling high-dimensional sparse features are all points that need to be considered. To the best of our knowledge, most existing related works can only satisfy partial demands mentioned above, for example, deep learning models [ loss the interpretability of the features, and some automatic feature engineering framework [ combinatorial features from categorical features. The pros and cons of different methods are summarized in Table 2. Feature Crossing by Model Architectures Traditional machine learning models, e.g., FM [5] FFM [19] Deep Learning models, e.g., NFM [8] xDeepFM [24] NAS-based models, e.g., AutoFeature [10] AutoGroup [11] Automatic Feature Engineering The proposed AEFE focuses on categorical data that is not highlighted in other studies. For speciﬁc tasks, users can specify a series of custom options on background knowledge to obtain generated features. In addition, partial features constructed by AEFE revealed the relationship between categorical features and prediction target, so that it can also capture information similar to that learned by models. In other words, our framework explores the answers to the following problem: 1. How to generate features automatically on a large dataset with high-dimensional sparse features? 2. How to design universal paradigms to exploit the potential of combinatorial features fully? 3. How to ensure the automatic feature engineering framework to work efﬁciently? 4. How to maintain the interpretability of the method to beneﬁt data analysis? First, we generalize the machine learning problem to be solved: Given a dataset containing speciﬁc forms of data, our goal is to automatically transform the raw features into effective combinatorial features to enhance model performance. More formally, we have: • Indicator set I, including: • A custom framework parameter collection H, including: Categorical ﬁeld setF = {F, F, · · · , F}, which is the dominant features in the dataset.mis the number of categorical ﬁelds. – Predicted target y, a.k.a label of a sample ; – Time value T S (optional); – Other continuous ﬁelds{I, I, · · · , I} (optional). nis the number of other continuous ﬁelds. A datasetD = (X, Y), whereX = {x}, y = {y}. Each sample contains the above categorical ﬁeld and optional continuous ﬁelds x= (x, · · · , x, x, x, · · · , x), and a label y; – Time windows W = {W, W, · · · , W}; – Operator set O = {O, O, · · · , O}; Figure 1: The structure of an E-commerce website purchase prediction dataset and the parameters setting. Then, the role of our proposed framework is to construct a batch of features information, and convert the raw dataset into a generated dataset: The features of the generated samples are of the framework will be described in the following sections. We take purchase prediction on an e-commerce website as an example to illustrate the data form mentioned above. As shown in Figure 1, it is a task to predict whether a user will purchase a speciﬁc item by exploiting user information and item information. This dataset includes three categorical ﬁelds—Occupation, Country, Item category and three indicators—Timestamp, Impression, and Purchase. Timestamp represents the time a user viewed the item. Impression indicates whether the user viewed the item, and Purchase is the predicted target, take 0 or 1. The meaning of the framework parameters is explained in the following sections. An overview of AEFE is shown in Figure 2, and the key modules are explained in Table 3. The workﬂow is as follows: Firstly, a certain proportion of data is sampled from the raw dataset, which is used to construct features and verify – Construction paradigms P = {P, P, · · · , P}; n, n, nare the number of corresponding sets respectively. Figure 2: AEFE Overview. It shows how the framework transforms the raw dataset to one with generated features. effectiveness. Secondly, at the feature construction phase, ﬁeld combination search module generates a categorical ﬁeld pair based on the feedback information from the previous iteration of feature selection, then various indicators and parameters are coupled to generate combinatorial features. Thirdly, these combinatorial features are passed through a multi-module cascaded feature selection phase, which ﬁlters ineffective features, and the remaining features are added to the effective combinatorial features list. Completing the feature construction and feature selection of a ﬁeld pair is considered to be one iteration. Subsequently, continue to select another ﬁeld pair and repeat the above process (as indicated by the closed-loop made up by the dotted arrow in the ﬁgure) until the ﬁeld combination search module has no longer generated new ﬁeld pair. Finally, a complete effective combinatorial features list is obtained, which is used as a template for the full raw data to generate a new dataset. Feature structure, in essence, is a process of combining categorical ﬁelds, indicator sets, operation sets, and time windows to generate combinatorial features speciﬁed by the construction paradigm. It can be summarized as follows: 1) Groupby, 2) Aggregating, and 3) Paradigm combination. For sample as follows: Groupby: After obtaining a ﬁeld pairF, F∈ F, a set of samples that are equivalent toxon these ﬁelds is recalled: Or after setting a time window W∈ W, the sample set is represented as: Ois an aggregation operator such as mean and sum. For simplicity, we mark the calculation process of groupby-then-aggregating as follows: Paradigm combination: Finally, the above calculation result is combined with speciﬁed paradigmP ∈ Pto obtain the construction feature f, for example: In the purchase prediction task of Figure 1, we set to clarify the meaning of these steps. In step 1, Groupby, all the samples of programmers viewing book items in the last three days— samples sets is aggregated into of books purchased by programmers and the total sales of books in the past three days respectively. By combining these two aggregation values through the paradigm purchase in the last three days is obtained in step 3 Paradigm combination. More combinatorial features can be generated by using the other framework parameters in Figure 1. Some examples of features of different paradigms are shown in Table 4. Annotate: Occ., Cat., Imp., Pur. and TS are short for Occupation, Item Category, Impression, Purchase, and Timestamp. Framework parameters include time window, operator set, and construction paradigm. Time windows need to be selected according to the speciﬁc situation. Furthermore, the operator set contains the common aggregation function [sum, mean, std, max, min] by analyzing the manual feature engineering in various application scenarios, we summarize some general feature construction paradigms, which can be divided into four categories, and they are shown in the column "paradigm" of Table 4 in order. They are deﬁned as: 1. Single-ﬁeld statistical feature is a feature obtained by aggregating the historical samples with the same value value of the current sample. A large number of combinatorial features generated in the feature construction phase will bring huge space overhead to the subsequent model training, and the expressiveness of these features may be redundant. Therefore, feature selection of the combinatorial features is needed. The feature selection algorithm consists of three modules: Filter, Embedded and Wrapper. Feature Selection Algorithm is shown in Algorithm 1 and the detailed description of the three modules is as follows: These features with smaller weights may even contain noise, affecting prediction accuracy. S(x| Occ.&Cat., 3ds)—would be gathered. Then in step 2 Aggregating, with functionsum, the Table 4: Construction paradigms and corresponding features of the E-commerce purchase scenario. (F, I(Occ., Imp.)Times user whose occupation isFview items in the lastd (F, I(Occ.&Cat., P ur.)Times user whose occupation isFpurchase items with (F, I)GA(O c c.&Cat., P ur.)Proportion of occupationF’s purchase in sales of category (F, I)GA(Cat., P ur.)Fin the last d days (F, I)GA(Cat., T S) − xThe difference between the average time of item with catefor a certain ﬁeld, which can reﬂect the relationship between a category ﬁeld and an indicator. Multi-ﬁeld combination statistical feature is constructed in a similar form to the single-ﬁeld statistical feature, except that the samples to be computed are determined by two ﬁelds. This type of feature can effectively mine effective categorical ﬁeld combination. Nondimensionalization featureis a multi-ﬁeld feature divided by a single-ﬁeld feature, which represents the proportion of an indicator aggregate value in another. Distance measurement feature is the relationship between the single(multi)-ﬁeld feature and the indicator : It calculates the variance of each feature and drops the features whose variance is less thant because features with small variances tend to be less distinguishable. GBDT and RandomForest), and that with weight less thantare dropped. In this way, taking into account the relative importance of all features, features that contribute less to the prediction can be removed. Algorithm 1: Feature Selection Algorithm(FSA) Input :D, G Output :G An ordered combination of multiple feature selection strategies can complement each other. Filter has the highest efﬁciency and can quickly ﬁlter out the features with lower discrimination because it does not need to train the model. However, it is not enough to use the variance to judge whether the feature is potent. Embedded takes into account speciﬁc prediction tasks and with acceptable speed. With the view of diminishing the features redundancy and substantially scale down the ﬁnal number of features, the most rigorous but time-consuming method — Wrapper, is used in the last step. Since some features have been dropped in the ﬁrst two steps, the overall time of this step is acceptable. In the previous section, the entire framework has been described in detail. Here, we illustrate AEFE from two perspectives. One perspective is as a work in the AutoML domain, which is the characteristic of AEFE. Another perspective is the difference between AEFE and machine learning methods as a feature extractor. In the study of AutoML, AEFE is a work for automating the feature engineering phase, which is characterized by constructing continuous combinatorial features for multi-ﬁeld categorical features. As [ AutoML problems can be deﬁned as iteratively using Optimizer search conﬁguration on a speciﬁc Search space and then evaluating the feedback with Evaluator to complete the optimization task. Here, we interpret the meaning of AEFE in AutoML form: ← SplitData(D, rate); in descending order of feature importance ; ← score; Update pred; ← G− {g}; : Finally, the wrapper module sorts the features in descending order of feature importance. Using the cascaded method, it adds each feature to train the model one by one, to ﬁnd out whether the prediction effect can be improved after a feature is added. The feature is retained if it can bring improvement larger than t; otherwise, it is dropped since it means that the current feature has redundancy with the selected feature set. The procedure terminates when all features have been tried. Search space: search space is determined by the combination of all possible generated features if the number of generated features is Q: Figure 3: AEFE vs. FM-embedding. They use different techniques to capture feature combinations. In general, due to the large search space, we use tightly coupled tools and techniques in the process of Optimize-Evaluate to improve the efﬁciency of the proposed framework. Among them, the ﬁeld combination search module plays an important role in this process. See Section 4.2 for internal optimization details. As a feature extractor, AEFE converts high-dimensional sparse features into continuous features with practical meanings. This characteristic that is similar in form to representation learning is the essence of AEFE’s "Embedded". When the indicator and operator used by AEFE are limited to label and sum/mean, it can be considered as a method to extracts the relationship between the combination of categorical features and label, from the perspective of the data information utilized, which is consistent with FM. From the aspect of the output form, AEFE and FM both generate a series of continuous variables. The difference is that AEFE performs aggregation operations on the training samples, and FM uses these samples to update parameters through optimization such as stochastic gradient descent(SGD). The similarities and differences between AEFE and FM are shown in Figure 3. In the classiﬁcation task, the expression of FM is: σ(x)means Logistic Regression(LR). Therefore, FM can be decomposed into obtaining the latent vectors by FMembedding, get combinatorial features through inner product, and then LR is used as the classiﬁer. From this perspective, AEFE, like FM-embedding, plays the role of feature extractor. In addition to methodological differences, AEFE shows supplementary strengths compared to FM. Beneﬁt from the setting of the time windows, the features constructed by AEFE can change over time on the grounds that it utilizes timing information to treat the historical samples in a discriminative manner. Moreover, AEFE does not only use label as indicator and sum/mean as operator. When more elements are added to each set, AEFE can mine more data patterns. Conversely, the analogy of Figure 3 also helps to contemplate the use of representation learning to capture more forms of features similar to ones constructed in our framework without using label. Optimizer: A greedy search optimization strategy is used in generating and selecting feature sets phase, which is embodied in two aspects. One is to select the current optimal ﬁeld pair in ﬁeld combination search, and the other is the criterion of feature selection—whether a feature can bring improvement in the current stage. Evaluator: On the one hand, the evaluation techniques used in this work include data sampling and a direct training model to evaluation. On the other hand, after feature selection, the feedback information is transmitted back to the optimizer by means of proxy evaluation. Reasonable optimization techniques and efﬁcient implementation are necessary for an automatic feature framework to be used under acceptable resource constraints. In this section, we will discuss the optimization and implementation of AEFE. First, optimization techniques include data sampling and Matrix Factorization-based ﬁeld combination search are described in Section 4.1 and Section 4.2. Then, a distributed implementation will be introduced in Section 4.3. Prior to feature construction, proportional sampling will be performed on the raw dataset. These sampled data are used for successive phases to generate the valid combinatorial features list. Finally, the complete dataset is reused to generate the new dataset. The primary purpose of data sampling is to make our proposed framework suitable for large datasets since the groupbythen-aggregating operation in the feature construction phase brings a large memory overhead. At the same time, data sampling can make related phases faster. However, a low sampling rate may lead to the problem of constructing features ineffective, so choosing the appropriate sampling rate is a trade-off between efﬁciency and effectiveness. There are some works to apply sampling on similar problems, such as the automatic feature frameworks OneBM [14] and Cognito [40], which can also be used on large datasets, but these works do not carry out experimental analysis on the impact of sampling. Additionally, some machine learning models use a negative down-sampling strategy on CTR tasks to increase speed and slightly improve accuracy [ we do not conduct negative down-sampling, because it would seriously damage the original distribution and affect the calculation of indicators such as mean. Assuming that the time complexity of constructing a feature is from Equation 2 and Section 3.3, the time complexity of the whole feature construction phase is O(|F|×|I|×|W|×|O|×|P|×C) |F|, which is generally tens(then ﬁelds is most needed for optimization. Hence we use a heuristic strategy, named Matrix Factorization-based ﬁeld combination search, to search for valid feature combinations as much as possible, avoiding wasting time on searching for invalid feature combinations. The core idea of our proposed search strategy is to model the combining ability of a categorical ﬁeld (hereafter called "ﬁeld"), which is deﬁned as the effectiveness of combinatorial features generated from this ﬁeld with other ﬁelds. The best-performing ﬁeld pair is selected in each step for feature constructing. After that, the parameters are updated by evaluating the validity of features. To do this, the following assumptions are made: The combining ability of a ﬁeld is quantiﬁable under Assumption 1, while Assumption 2 is made for the distributed implementation as mentioned in Section 4.3. Our proposed AEFE with MF-based ﬁeld combination search can be brieﬂy described in Algorithm 2 and the details are as follows: The combining ability of a ﬁeld is migratable. That is, if the combinatorial features of ﬁeld A and B work well, the combination of ﬁeld A and other ﬁelds is more likely to be effective; The combining ability of features generated by different operations is independent. That is, if ﬁeld A and B get a positive result with mean as the operator, it does not mean that operator std is equally good for ﬁeld A and B. Latent vectorsvrepresents the combining ability ofF, and the effectiveness of combinatorial features ofF and Fis expressed as p= v· v, then the matrix of all ﬁeld combinations is¯M: ¯Mis a symmetric matrix with diagonal elementsp= 0. In order to ﬁnd the initial value ofv,pis initialized by the Information Gain Ratio, speciﬁcally: Algorithm 2: AEFE with MF-based Field Combination Search Input :D, F Output :G ← N with other elements(operators, indicators etc.). The total number of combinatorial features for these two ﬁelds we regard the real effectiveness of combinatorial features of F 5. Following the idea of iterative computation matrix factorization [45], with the current ﬁelds pair as a sample, Multiple feature selection and MF-based ﬁeld combination search make AEFE feasible. Further, we propose a distributed implementation scheme using task-parallelism to improve the speed of the framework. The most time-consuming phase of the whole framework is to obtain the valid combinatorial features list through feature combination and feature selection, so we divide it into a combination of operator set and indicator set, and acquire ¯M∈ Rusing Information Gain ratio(as Equation 4, Equation 5 and Equation 6); ← |G|; ← FSA(D, G ← |G|; ← evaluation score of G; > scorethen ← score; does not improve within iteriterations then HereCis the Cartesian product of the features in ﬁeldsiandj,Lis the set of target values (labels), and H(X)represents entropy of the random variableX. After calculating the information gain ratio of all elements, the entire matrix is transformed into [0, 1] interval. Matrix factorization of¯Myields the matrixV = MF (¯M) = [v, · · · , v], andVroughly satisﬁes the equation¯M= VV . Then the latent vectors representing the combining ability of F are obtained. Select the largestpthat is unused for combination in matrixM, and useFandFto construct features The number of combinatorial features that can be retained after the feature selection algorithm isN; then, p, ˙pare the predicted value and the target value, respectively, andvandvare updated by gradient descent: vrepresents thekth component ofvin thetth iteration, andηis the learning rate. By updating the latent vectors, the combining ability ofFandFcan be changed in time, so that ﬁeld combination is directed to the highest yield in each iteration. Repeat Step 3 to Step 5 until the termination condition, which is the evaluation metric not improving within continuous iteriterations or completing all ﬁeld combinations. N< |I|×|O| tasks will be less than |I|×|O|). These tasks are assigned to different compute nodes. Each node independently performs the two main phases - Feature Construction, Feature Selection - of AEFE. The hypothesis that the combining ability of features generated by different operations is independent supports MF-based ﬁeld combination search can be used independently after task partitioning. In addition, due to the larger number and the greater difference of the candidate features constructed by each task are summarized and there is still redundancy, Global Feature Selection need to perform, which is similar to Algorithm 1. In this section, we will ﬁrst introduce the experiment settings in Section 5.1, and then answer the following questions through a series of experiments: • (RQ1) • (RQ2) • (RQ3) We conduct experiments on two public datasets and one private dataset, which are ad click-through rate prediction datasets. The summary statistics of three datasets are shown in Table 5. • Iqiyi: A private online video display ad dataset. A 20-day impression log of an advertising platform on Iqiyi We uniformly use the last day’s log as a test set and all the previous data as a complete training set. In order to select the best model hyper-parameters, hyper-parameter tuning. AUC(Area Under ROC) is adopted as the main metric for evaluating the performance of the models. AUC can be understood here as the probability of correct ranking of a random “positive”-“negative” pair. AUC is a commonly used https://tianchi.aliyun.com/dataset/dataDetail?dataId=56 https://www.kaggle.com/c/avazu-ctr-prediction tasks(Partial aggregation functions are not meaningful for some indicators, so the total number of How much improvement does our proposed AEFE bring to the models, and how does our method perform compared to state-of-the-art deep learning models? Can AEFE effectively capture ﬁeld combination information? What is the interpretability of this captured information? What are the effects of two optimization techniques—data sampling and MF-based ﬁeld combination search? online video website from July 4, 2016, to July 23, 2016. The ﬁnal dataset is obtained by pre-processing the deletion of missing values, fraudulent click record ﬁltering, and the like. : A public e-commerce platform displays advertising data sets. We useraw_sampleas the main table, andraw_behavior_logis simply processed and connected to the main table to get btag, cat, brand 3 ﬁelds, then right joinad_feature, user_profilewith the main table to get the advertising and user proﬁle information for each log. : A Kaggle competition dataset contains the click log from October 21st to 29th, 2014 (because the logs on October 30th have no label information, so they are dropped), we group the features that appear less than 10 times in a ﬁeld to one feature, and then perform some other simple preprocessing. classiﬁcation task evaluation indicator because it is not sensitive to class imbalance and can better reﬂect the needs of the practical scene. If the class of a sample is randomly predicted, the value of AUC is 0.5. In addition, for the purpose of clearly comparing the performance differences between our method and other comparison models, the Relative Improvement of AUC (RelaImpr) [46] will also be shown. The expression of RelaImpr is as follow: To observe the performance of AEFE, we chose some classical machine learning models and state-of-the-art deep learning models in the same problem domain as the baseline models, and most of them modeling feature crossing. Other automatic feature engineering frameworks are not compared because they are not suitable for a dataset that is almost all categorical ﬁelds. The baseline models are introduced as follows: • Logistic Regression(LR) • LightGBM(GBDT) • Factorization Machine(FM) • Field-aware Factorization Machine(FFM) • Attentional Factorization Machines(AFM) • Neural Factorization Machines(NFM) • Deep Factorization Machines(DeepFM) • eXtreme Deep Factorization Machines(xDeepFM) We use AEFE-generated features to train LR and GBDT (denoted as against the above model to verify the validity of AEFE. For fairness, the key hyper-parameters of these models are determined by grid search, and the hyper-parameters with the best effect on the validation set are selected for testing. The hyper-parameter search scope is shown in Table 6. Except for the model that explicitly speciﬁes the implementation, other models are implemented using Tensorﬂow rate selected from 0.0001 to 0.0005. The main parameters of AEFE in the experiments are shown in Table 7. Time windows is determined by the time span of the dataset while operator set and construction paradigms are ﬁxed to the same for all dataset. Sampling rate will be discussed in Section 5.3.1. In addition, XGBoost [ Meanwhile t AEFE is implemented and experimented with Python 3.6. It is deployed on an 8-node platform with each node conﬁgured as CPU: Intel Xeon E5-2670 @2.6Ghz*16core*2, memory: 128GB. Other models implemented by non-Tensorﬂow are also running on these nodes. https://github.com/Microsoft/LightGBM https://github.com/guestwalk/libffm https://www.tensorflow.org It is widely used to handle sparse categorical features because of its simplicity. We add L2 regularization to avoid overﬁtting. tation of GBDT, and it supports categorical features well. We implements it using [47]’s code. generally applied for recommender system, etc. It is the basis of many deep learning models in these domains since it can learn the embedding vectors with feature crossing information. and features interact with different ﬁelds’ features using different embedding vectors. We used LibFFM packagein our experiments, and for the Avazu dataset, we use the hyper-parameters of the original paper: learningrate = 0.2, L= 0.00005, k= 4. mechanism. It can learn different weights for different crossing features. interaction layer and fully-connected layers to learn higher-order nonlinear interactions between features. DNN to train the embedding layers in parallel and can take advantage of both second-order and higher-order feature crossing information. explicit feature crossing by using DNN and Compressed Interaction Network (CIN) at the same time. We choose this model as a representative of non-FM-embeddings deep learning models for comparison. , t, and tare set as 0.00001, 0.02, and 0 respectively. Annotate: Dt=dropout ratio of speciﬁc layers, The version of Tensorﬂow for the deep learning model is r1.12.0. The machine used is conﬁgured as CPU: Intel Xeon e5-2603 v4.@1.70ghz, GPU: NVIDIA GTX 1080Ti*2, and memory: 64GB. 5.2.1 Comparison with Baseline Models(RQ1) The performance of different models on the test set is summarized in Table 8, which contains the evaluation metric AUC and ReleImpr(original features vs. AEFE’s features). All results are the average of 10 replicates. The following points are known from Table 8: L=L2 regulization coefﬁcient on speciﬁc parameters,k=embedding size,k=attention factors of AFM, Features generated by AEFE make LR and GBDT superior to those trained with raw features, which shows that our proposed framework can signiﬁcantly enhance the performance of machine learning models. Especially on the Iqiyi dataset, the effect of LR or GBDT alone is not satisfying, while AUC can be sharply improved by using AEFE’s combinatorial features. AEFE+GBDT outperforms state-of-the-art models in three datasets. Compared to the best baseline, RelaImprs are1.53%,15.65%and0.92%respectively. Meanwhile, we conduct 10 repeated experiments on baseline Note: bold indicates the best result, and underline indicates the best result of baseline models. The content in italics in AEFE on each dataset is signiﬁcant. (sufﬁcient to cover the expression of FM), they do not achieve the effect of FM on some datasets in our This is why we still need to focus on automatic feature engineering. Experiments in this section show that our proposed AEFE improves the performance of models signiﬁcantly and AEFE+GBDT achieves better AUC compared with the state-of-the-art deep learning models. 5.2.2 Comparison on Field Combination(RQ2) In addition to the accuracy comparison with deep learning models, we compare the capabilities to capture second-order feature interaction of AEFE and FM in this section. Then we study the interpretability of the features generated by AEFE. To analyze the effectiveness of the features interaction of different ﬁeld pairs, we deﬁne the Combination Strength Matrix(CS weights, and then we divide and accumulate the feature weights by ﬁeld combination to get the combined strength of different ﬁelds for the prediction, as in the form of Equation 10; for FM, which cannot directly obtain the importance of ﬁeld combination, we take the practice of calculating the ﬁeld mean embedding in [ represented as that paper, since we need to know the importance of the ﬁeld combination to the prediction without distinguishing its positive and negative, we will take the absolute value of the matrix. parentheses indicates the improvement of AEFE+GBDT relative to this model on RelaImpr. models and AEFE+GBDT for t-test, and the p-value is shown in Table 9. The results show that the increase of FM, which can capture the second-order feature interaction information, brings about obvious improvement compared with LR. However, although various improved models of FM have a larger Hypothesis Space experiments. For example, AFM is worse than FM on Iqiyi and Ali datasets. On the contrary, our method is more stable in all experiments. AEFE+LR, using the simplest classiﬁer, outperforms all baseline models on the Ali dataset, which shows that effective constructed features are more powerful than a complex learning model in some datasets of such tasks. ) of AEFE and FM respectively: for our proposed AEFE+GBDT, GBDT can easily output the feature ¯v=v/d, the ﬁeld combination strength expression is shown in Equation 11. Different from Figure 4: Field combination heat map of AEFE and FM on Iqiyi, Ali and Avazu datasets. Darker represents stronger. As [50] mention, we can assume that the interaction of ﬁelds should be sparse. So, ideally, only a few values are larger and mostly smaller in the combination strength matrix. We plot the combination strength matrix of AEFE and FM into heat maps(Figure 4). The two rows from top to bottom are the results of AEFE and FM, with ﬁeld number on the x-axis and y-axis. The grid the intensity. The experimental results show that only a few ﬁeld combinations can have higher intensity, and both methods are consistent with the "hypothesis" of sparseness. On the one hand, it shows that the combinatorial features are of considerable importance to the model. On the other hand, the number of ﬁeld combinations that contribute to the prediction is limited. Besides, AEFE’s strength matrix is more sparse than FM’s, because many invalid ﬁeld combinations are directly discarded by AEFE, and their weight is the comparison of the heat maps: the important ﬁeld combinations they capture rarely overlap. Therefore, we use Ali dataset, a public dataset with a clear meaning for each ﬁeld, as an example for further analysis. Field names of the Ali dataset in Figure 4 are shown in Figure 10, while the meaning and group of some ﬁelds are listed in Figure 11. In Figure 4, the ﬁeld combination that contributes the most to the prediction in AEFE is cate ad_cate and brand and advertisement information features, which means that user’s recent interaction record with a speciﬁc brand or a speciﬁc category of goods implies the user’s interest in the current advertisement. However, FM considers that ﬁnal_gender_code features—are most important, which means that more detailed user group division (such as gender and consumption grades should be jointly divided) can bring better personalized prediction. Although we cannot absolutely point out which combinations of ﬁelds in this scenario are better than another, generally, the combination of users and items is beneﬁcial and needs to be fully mined in tasks like recommender systems. On the other hand, some industrial models with real-time requirements will deliberately omit intra-group combinations to reduce computational complexity because they tend to have less impact on accuracy. Therefore, we believe that AEFE can be more accurate in identifying potential data patterns at the ﬁeld-level. As evidence of our judgment, our approach also results in FM. Furthermore, with the visualization, the interpretability of learning models such as FM is limited to which ﬁeld combinations are more important. AEFE can further reveal the paradigm of important combinatorial features. As shown in Table 12, there are four most weighted features of the above two ﬁelds in the Ali dataset. First, the ﬁrst two important features are about the click sequence variance rather than the most intuitive click rate (click:mean), which means that the divergence of click sequence of a user who recently interacted with a particular merchandise category and a particular ad category is a strong predictor. Second, 3 of the 4 features are complex ratios or difference features, indicating that this complicated paradigm is more effective. Finally, in this experiment, time windows are set to and the higher weights in Table 12 are all 5 days, which means that a more extended time window is more suitable in this dataset. The elaborated exploration of generated features above is a demonstration of AEFE as an expert and intelligent system at the analysis level. For one thing, it is helpful for mining important features to guide data collection and to preprocess. For another, it enlightens data analysts to conceive stronger features based on them—in fact, AEFE is not a complete replacement for the work of data scientists (this is a goal that other similar work cannot achieve now) but makes the work more efﬁcient and inspiring. 5.3 Optimization Analysis(RQ3) AEFE uses a series of optimization to improve its efﬁciency. We conduct two experiments to analysis whether these techniques can bring acceptable results. The classiﬁer used in this section is GBDT. new_user_class_level city level of user Table 12: Weights of some features belonging to two important ﬁeld pairs of the Ali dataset. GA(ad_brand&brand, click)/GA GA(ad_brand&brand, impression)/GA(ad_brand, impression) 1.38% It is mentioned in Section 4.1 that data sampling is used to speed up feature constructing and save system resources. Obviously, the lower the sampling rate brings faster speed, but at the same time, it damages the effect. Our experiment focuses on the relationship between the sample rate and feature quality. Here we deﬁne Deviation of Weights’ Gap(DoWG) to measure whether a sampling rate is effective enough, the expression is as follows: Wherew denominator to be 0. Apparently, the closer ratesis to unsampling, and the better the simulation of this sample rate is for the whole dataset. In particular, We sort the generated features in descending order of feature importance and select the ﬁrst feature, the two trisection points, and the last one, labeled [0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0] stable, the results obtained at each sampling rate are the average of 10 repeated experiments. The experimental results are shown in Figure 5. From Figure 5, we can observe the following phenomena: 1) The larger the sampling rate, the closer the DoWG is to 0, which is consistent with intuition. When the sampling rate reaches 0.25 or higher, the effect is close to perfect ﬁtting. 2) Comparing the datasets with different sizes, it can be found that the larger the sample size, the lower the sampling rate requirement. Even with a sampling rate of the experiments of Iqiyi and Ali, there is still an apparent ﬂuctuation at higher sample rates. 3)DoWG between features with lower weights (such as randomness. However, the features with higher weights are stable and can be guaranteed to be retained. A higher sample rate should be chosen to achieve good simulation results. However, we only want to ensure that the valid features can be maintained after feature selection, so the deviation of feature weights without severe distortion can meet our requirement. And the feature weights need to re-train after generation. Conversely, lower sampling rates are more efﬁcient. We design experiments to compare the performance of the complete AEFE and AEFE without MF-based ﬁeld combination search to observe 1) whether this technique can improve efﬁciency, and 2) whether it affects accuracy. In Figure 6, there are AUC change curves of single-node tasks of generating effective combinatorial features lists. We have selected 4 out of 8 tasks, and their indicators and operators are click:mean, click:std, impression:sum and timestamp:max respectively. In addition, Table 13, Table 14, Table 15 contain comparisons of the results of the two methods. First, in Figure 6, we can see that most of AEFE’s tasks terminate early, which reveals that MF-based search does improve efﬁciency. And from Table 13, Table 14, Table 15, column "Avg. time" demonstrates that by implementing Figure 5: Comparison of different data sampling rate. The black dotted line represents the best ﬁt. ∈ [0, 1]denotes the weight of featureiin the model trained with sample rates, andis10to avoid the Figure 6: AUC changes for different tasks with or without MF-base search. Solid dots represent early stopping points. (clk, imp, and ts are short for click, impression and timestamp.) MF-based search, the running time of each task is reduced, respectively, to solid line representing AEFE almost always leads the dotted line representing AEFE(noMF) in the ﬁrst ten rounds of iterations, indicating that MF-based search can ﬁnd out stronger features faster. But in the experiments of Ali and Avazu, after AEFE(noMF) exhaustively searching from all combinatorial features, some tasks’ AUC of AEFE(noMF) eventually exceed AEFE. Does it mean that AEFE(noMF) can perform better in the end? It can be seen from Table 13, Table 14, Table 15 that AUC of AEFE is larger than AEFE(noMF), and more feature generated is not better. The reason may be that features that perform well on a single task probably have higher redundancy and are discarded in the global feature selection phase subsequently. What’s more, it is worth noting that in some tasks, AEFE fails to trigger early stop on Avazu, and AEFE’s single task AUC is not better than AEFE(noMF)’s. However, AEFE achieves better AUC with less than 100 features compared to AEFE(noMF), which shows that even if the early stop mechanism is nearly ineffective, the good feature quality brought by MF-based search can make the model perform well. In this paper, we propose an automatic feature engineering framework for categorical features, named AEFE, which solves two fundamental problems: data sparsity and combinatorial feature construction, in e-commerce applications, and keeps good interpretability. In order to avoid time-consuming transversal overall combinatorial feature schemes, we propose an MF-based ﬁeld combination search strategy. It effectively reduces about half of the running time and improves the accuracy of learning models. Furthermore, combined with data sampling, distributed implementation, and other technologies, AEFE becomes an efﬁcient and easy-to-use feature framework. Our method support decision-making at the application level and analysis level. It not only increases the prediction accuracy apparently even cascaded with shallow models like LR but also generates interpretable features for the follow-up data analysis. In fact, the custom paradigm, from a certain perspective, is a kind of domain knowledge and also a necessary factor for the interpretability of the complex generated feature while the proposed method is nearly data-driven. With small changes of custom conﬁgurations, AEFE can handle a large quantity of data as materials to generate features that are not easily constructed by humans and then select the most effective ones through speciﬁc criteria. In this way, our study is an attempt to combine domain-related knowledge with a data-driven method. The experimental results show that AEFE cascaded with GBDT outperforms state-of-the-art deep learning models on several datasets. Compared to directly training LR or GBDT with raw features, they achieve a large relative improvement of AUC with features generated by AEFE. Through visual analysis, AEFE can mine different but valuable combinatorial features compared to FM. Further experiments conﬁrmed the validity of data sampling and MF-based ﬁeld combination search. Table 13: Statistic of AEFE with or without MF-based search on the Iqiyi dataset. There are some limitations of our study for different reasons. Firstly, though higher-order interaction is helpful in some tasks, we only consider the combination of two categorical ﬁelds in AEFE because the complexity of the search space grows exponentially with the order. However, experiments reveal that this weakness can be compensated by GBDT, which learns feature interactions of any order, and outperform deep learning models that learn higher-order nonlinear interactions. Secondly, the scheme "Feature Engineering + Model Training" is not an end-to-end solution like deep learning models. But at present, we cannot tightly couple the entire process while maintaining equivalent interpretability. This could be an area of future research. Thirdly, the lack of experimental comparison of related automatic feature engineering work is another limitation. The main reason is that the related work investigated is not expert in handling such features. However, the state-of-the-art deep learning models used for comparison are inﬂuential and effective. Aside from the potential research areas arising from the limitations of the current study, we would like to explore this work in three more directions next. The ﬁrst is how to combine AEFE with FM and other FM-embedding deep learning models. As mentioned in Section 3.5.2, AEFE and FM capture utterly different information about ﬁeld combinations. Therefore prediction accuracy may be improved by fusing the two methods properly. The general model ensemble may be a rough but effective way, but there are more suitable approaches left to explore. The second is to study how to design representation learning modules for learning more rich forms of constructed features (such as timestamp-type features generated by AEFE) for better model performance. Most of the deep learning models currently only capture the relation of original features and labels, which is not enough for the data mining tasks we are concerned with. The third is to use meta-learning in automatic feature engineering for categorical features so that the feature generation capabilities can be transferred to more unseen datasets. To archive this, we need to determine how to form meta-features and how to design the meta-model, which requires a further understanding of the deeper connotations of categorical features. Table 14: Statistic of AEFE with or without MF-based search on the Ali dataset. Table 15: Statistic of AEFE with or without MF-based search on the Avazu dataset.