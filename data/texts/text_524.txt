With the recent success of graph convolutional networks (GCNs), they have been widely applied for recommendation, and achieved impressive performance gains. The core of GCNs lies in its message passing mechanism to aggregate neighborhood information. However, we observed that message passing largely slows down the convergence of GCNs during training, especially for large-scale recommender systems, which hinders their wide adoption. LightGCN makes an early attempt to simplify GCNs for collaborative ltering by omitting feature transformations and nonlinear activations. In this paper, we take one step further to propose an ultra-simplied formulation of GCNs (dubbed UltraGCN), which skips innite layers of message passing for ecient recommendation. Instead of explicit message passing, UltraGCN resorts to directly approximate the limit of innite-layer graph convolutions via a constraint loss. Meanwhile, UltraGCN allows for more appropriate edge weight assignments and exible adjustment of the relative importances among dierent types of relationships. This nally yields a simple yet eective UltraGCN model, which is easy to implement and efcient to train. Experimental results on four benchmark datasets show that UltraGCN not only outperforms the state-of-the-art GCN models but also achieves more than 10x speedup over LightGCN. • Information systems → Recommender systems;Collaborative ltering. Recommender systems; collaborative ltering; graph convolutional networks ACM Reference Format: Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. 2021. UltraGCN: Ultra Simplication of Graph Convolutional Networks for Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3459637.3482291 Nowadays, personalized recommendation has become a prevalent way to help users nd information of their interests in various applications, such as e-commerce, online news, and social media. The core of recommendation is to precisely match a user’s preference with candidate items. Collaborative ltering (CF) [11], as a fundamental recommendation task, has been widely studied in both academia and industry. A common paradigm of CF is to learn vector representations (i.e., embeddings) of users and items from historical interaction data and then perform top-k recommendation based on the pairwise similarity between user and item embeddings. As the interaction data can be naturally modelled as graphs, such as user-item bipartite graph and item-item co-occurrence graph, recent studies [10,13,24,27] opt for powerful graph convolutional/neural networks (GCNs, or GNNs in general) to learn user and item node representations. These GCN-based models are capable of exploiting higher-order connectivity between users and items, and therefore have achieved impressive performance gains for recommendation. PinSage [31] and M2GRL [26] are two successful use cases in industrial applications. Despite the promising results obtained, we argue that current model designs are heavy and burdensome. In order to capture higher-order collaborative signals and better model the interaction process between users and items, current GNN-based CF models [1,24,27,32] tend to seek for more and more sophisticated network encoders. However, we observed that these GCN-based models are hard to train with large graphs, which hinders their wide adoption in industry. Industrial recommender systems usually involve massive graphs due to the large numbers of users and items. This brings eciency and scalability challenges for model designs. Towards this end, some research eorts [4,10,17] have been made to simplify the design of GCN-based CF models, mainly by removing feature transformations and non-linear activations that are not necessary for CF. These simplied models not only obtain much better performance than those complex ones, but also brings some benets on training eciency. Inspired by these pioneer studies, we performed further empirical analysis on the training process of GCN-based models and found that message passing (i.e., neighborhood aggregation) on a large graph is usually time-consuming for CF. In particular, stacking multiple layers of message passing could lead to the slow convergence of GCN-based models on CF tasks. Although the aforementioned models such as LightGCN [10] have already been simplied for training, the message passing operations still dominate their training. For example, in our experiments, three-layer LightGCN takes more than 700 epochs to converge to its best result on the AmazonBooks dataset [9], which would be unacceptable in an industrial setting. How to improve the eciency of GCN models yet retain their eectiveness on recommendation is still an open problem. To tackle this challenge, in this work, we question the necessity of explicit message passing layers in CF, and nally propose an ultra-simplied form of GCNs (dubbed UltraGCN) without message passing for ecient recommendation. More specically, we analyzed the message passing formula of LightGCN and identied three critical limitations: 1) The weights assigned on edges during message passing are counter-intuitive, which may not be appropriate for CF. 2) The propagation process recursively combines dierent types of relationship pairs (including user-item pairs, item-item pairs, and user-user pairs) into the model, but fails to capture their varying importance. This may also introduce noisy and uninformative relationships that confuse the model training. 3) The over-smoothing issue limits the use of too many layers of message passing in LightGCN. Therefore, instead of performing explicit message passing, we seek to directly approximate the limit of innite-layer graph convolutions via a constraint loss, which leads to the ultra-simplied GCN model, UltraGCN. The loss-based design of UltraGCN is very exible, allowing us to manually adjust the relative importances of dierent types of relationships and also avoid the over-smoothing problem by negative sampling. This nally yields a simple yet eective UltraGCN model, which is easy to implement and ecient to train. Furthermore, we show that UltraGCN achieves signicant improvements over the state-of-the-art CF models. For instance, UltraGCN attains up to 76.6% improvement in NDCG@20 and more than 10x speedup in training over LightGCN on the Amazon-Books dataset. In summary, this work makes the following main contributions: •We empirically analyze the training ineciency of LightGCN and further attribute its cause to the critical limitations of the message passing mechanism. •We propose an ultra simplied formulation of GCN, namely UltraGCN, which skips innite layers of explicit message passing for ecient recommendation. •Extensive experiments have been conducted on four benchmark datasets to show the eectiveness and eciency of UltraGCN. In this section, we revisit the GCN and LightGCN models, and further identify the limitations resulted from the inherent message passing mechanism, which also justify the motivation of our work. GCN [14] is a representative model of graph neural networks that applies message passing to aggregate neighborhood information. The message passing layer with self-loops is dened as follows: withˆ𝐴 = 𝐴 + 𝐼andˆ𝐷 = 𝐷 +𝐼.𝐴,𝐷,𝐼are the adjacency matrix, the diagonal node degree matrix, and the identity matrix, respectively. 𝐼is used to integrate self-loop connections on nodes.𝐸and𝑊 denote the representation matrix and the weight matrix for the𝑙-th layer. 𝜎 (·) is a non-linear activation function (e.g., ReLU). Despite the wide success of GCN in graph learning, several recent studies [4,10,17,29] found that simplifying GCN appropriately can further boost the performance on CF tasks. LightGCN [10] is one such simplied GCN model that removes feature transformations (i.e.,𝑊) and non-linear activations (i.e.,𝜎). Its message passing layer can thus be expressed as follows: It is worth noting that although LightGCN also removes self-loop connections on nodes, its layer combination operation has a similar eect to self-loops used in Equation 2, becauase both of them output a weighted sum of the embeddings propagated at each layer as the nal output representation. Given self-loop connections, we can rewrite the message passing operations for user𝑢and item𝑖as follows: where𝑒and𝑒denote the embeddings of user𝑢and item 𝑖at layer𝑙.N(𝑢)andN(𝑖)represent their neighbor node sets, respectively. 𝑑denotes the original degree of the node 𝑢. As shown in the left part of Figure 1, LightGCN performs a stack of message passing layers to obtain the embeddings and nally uses their dot product for training. We argue that such message passing layers have potential limitations that hinder the eective and ecient training of GCN-based models in recommendation tasks. To illustrate it, we take the𝑙-th layer message passing of LightGCN in Equation 3 and 4 for example. Note that𝑢and𝑣denote users while𝑖and𝑘denote items. LightGCN takes the dot product of the two embedding as the nal logit to capture the preference of user𝑢on item𝑖. Thus we obtain: where 𝛼, 𝛼, 𝛼, and 𝛼can be derived as follows: Therefore, we can observe that multiple dierent types of collaborative signals, including user-item relationships (𝑢-𝑖and𝑘-𝑣), item-item relationships (𝑘-𝑖), and user-user relationships (𝑢-𝑣), are captured when training GCN-based models with message passing layers. This also reveals why GCN-based models are eective for CF. However, we found that the edge weights assigned on various types of relationships are not justied to be appropriate for CF tasks. Based on our empirical analysis, we identify three critical limitations of the message passing layers in GCN-based models: • Limitation I: The weight𝛼is used to model the item-item relationships. However, given the user𝑢, the factors of item 𝑖and item𝑘are asymmetric (for item𝑘while for item𝑖). This is not reasonable since it is counter-intuitive to treat the item𝑘and item𝑖unequally. Similarly,𝛼that models the user-user relationships also suer this issue. Such unreasonable weight assignments may mislead the model training and nally result in sub-optimal performance. • Limitation II: The message passing recursively combine dierent types of relationships into the modeling. While such collaborative signals should be benecial, the above message passing formula fails to capture their varying importance. Meanwhile, stacking multiple layers of message passing as in Equation 5 likely introduce uninformative, noisy, or ambiguous relationships, which could largely aect the training eciency and eectiveness. It is desirable to exibly adjust the relative importances of various relationships. We validate this empirically in Section 4.4. • Limitation III: Stacking more layers of message passing should capture higher-order collaborative signals, but in fact the performance of LightGCN begins to degrade at layer 2 or 3 [10]. We partially attribute it to the over-smoothing problem of message passing. As graph convolution is a special form of Laplacian smoothing [16], performing too many layers of message passing will make the nodes with the same degrees tend to have exactly the same embeddings. According to Theorem 1 in [5], we can derive the innite powers of message passing which take the following limit: where𝑛and𝑚are the total numbers of nodes and edges in the graph, respectively. The above limitations of message passing motivate our work. We question the necessity of explicit message passing layers in CF and Figure 1: Illustrations of training of LightGCN (left) and UltraGCN (right). LightGCN nee ds to recurrently perform 𝐿layers message passing to get the nal embeddings for training, while UltraGCN can “skip” such message passing to make the embeddings be directly trained, largely improving training eciency and helping real deployment. further propose an ultra-simplied formulation of GCN, dubbed UltraGCN. In this section, we present our ultra-simplied UltraGCN model and demonstrate how to incorporate dierent types of relationships in a exible manner. We also elaborate on how it overcomes the above limitations and analyze its connections to other related models. Due to the limitations of message passing, in this work, we take one step forward to question the necessity of explicit message passing in CF. Considering that the limit of innite powers of message passing exists as shown in Equation 6, we wonder whether it is possible to skip the innite-layer message passing yet approximate the convergence state reached. After repeating innite layers of message passing, we express the nal convergence condition as follows: That is, the representations of the last two layers keep unchanged, since the vector generated from neighborhood aggregation equals to the node representation itself. We use𝑒(or𝑒) to denote the nal converged representation of user𝑢(or item𝑖). Then, Equation 3 can be rewritten as: After some simplications, we derive the following convergence state: In other words, if Equation 9 is satised for each node, it reaches the convergence state of message passing. Instead of performing explicit message passing, we aim to directly approximate such convergence state. To this end, a straightforward way is to minimize the dierence of both sides of Equation 9. In this work, we normalize the embeddings to unit vectors and then maximize the dot product of both terms: which is equivalent to maximize the cosine similarity between𝑒 and𝑒. For ease of optimization, we further incorporate sigmoid activation and negative log likelihood [2], and derive the following where𝜎is the sigmoid function. The loss is optimized to fulll the structure constraint imposed by Equation 9. As such, we denoteL as the constraint loss and denote 𝛽as the constraint coecient. However, optimizingLcould also suer from the over-smoothing problem asLrequires all connected pairs (𝛽>0) to be similar. In this way, users and items could easily converge to the same embeddings. To alleviate the over-smoothing problem, conventional GCN-based CF models usually x a small number of message passing layers, e.g., 2∼4 layers in LightGCN. Instead, as UltraGCN approximates the limit of innite-layer message passing via a constraint loss, we choose to perform negative sampling during training. This is inspired from the negative sampling strategy used in Word2Vec [19], which provides a more simple and eective way to counteract the over-smoothing problem. After performing negative sampling, we nally derive the following constraint loss: where𝑁and𝑁represent the sets of positive pairs and randomly sampled negative pairs. Note that we omit the summation over𝑈for ease of presentation. The constraint lossLenables UltraGCN to directly approximate the limit of innite-layer message passing to capture arbitrary high-order collaborative signals in the user-item bipartite graph, while eectively avoiding the troublesome oversmoothing issue via negative sampling. Furthermore, we note that 𝛽acts as the loss weight inL, which is inversely proportional to𝑑and𝑑with similar magnitudes. This is interpretable for CF. If a user interacts with many items or an item is interacted by many users, the inuence of their interaction would be small, and thus the loss weight of this (𝑢, 𝑖) pair should be small. 3.1.1 Optimization. Typically, CF models perform item recommendation by applying either pairwise BPR (Bayesian personalized ranking) loss [22] or pointwise BCE (binary cross-entropy) loss [11] for optimization. We formulate CF as a link prediction problem in graph learning. Therefore, we choose the following BCE loss as the main optimization objective. It is also consistent with the loss format of L. where𝑁and𝑁represent positive and randomly sampled negative links (i.e.,𝑢-𝑗pairs). Note that for simplicity, we use the same sets of sample pairs withL, but they could also be made dierent conveniently. AsLandLdepends only on the user-item relationships, we dene it as the base version of UltraGCN, denoted as UltraGCN, which has the following optimization objective. where𝜆is the hyper-parameter to control the importance weights of two losse terms. As Equation 5 shows, except for user-item relationships, some other relationships (e.g., item-item and user-user relationships) also greatly contribute to the eectiveness of GCN-based models on CF. However, in conventional GCN-based models, these relationships are implicitly learned through the same message passing layers with user-item relationships. This not only leads to the unreasonable edge weight assignments as discussed in Section 2.2, but also fails to capture the relative importances of dierent types of relationships. In contrast, UltraGCN does not rely on explicit message passing so that we can separately learn other relationships in a more exible way. This also enables us to manually adjust the relative importances of dierent relationships. We emphasize that UltraGCN is exible to extend to model many dierent relation graphs, such as user-user graphs, itemitem graphs, and even knowlege graphs. In this work, we mainly demonstrate its use on the item-item co-occurrence graph, which has been shown to be useful for recommendation in [26]. We rst build the item-item co-occurrence graph by linking items that have co-occurrences, which produces the following weighted adjacent matrix 𝐺 ∈ R. where each entry denotes the co-occurrences of two items. We follow Equation 9 to approximate innite-layer graph convolution on 𝐺 and derive the new coecient 𝜔: where𝑔and𝑔denote the degrees (sum by column) of item𝑖and item 𝑗 in 𝐺, respectively. Similar to Equation 12, we can derive the constraint loss on the item-item graph to learn the item-item relationships in an explicit way. However, as the adjacency matrix𝐺of the item-item graph is usually much denser compared to the sparse adjacency matrix𝐴of the user-item graph, directly minimizing the constraint loss on𝐺 would introduce too many unreliable or noisy item-item pairs into optimization, which may make UltraGCN dicult to train. This is also similar to theLimitation IIof conventional GCN-based models described in Section 2.2. But thanks to the exible design of UltraGCN, we choose to select only informative pairs for training. Specically, to keep sparse item connections and retain training eciency, we rst select top-𝐾most similar items𝑆 (𝑖)for item𝑖 according to𝜔. Intuitively,𝜔measures the similarity of item𝑖 and item𝑗, since it is proportional to the co-occurrence number of item𝑖and item𝑗, yet inversely proportional to the total degrees of both items. Instead of directly learning item-item pairs, we propose to augment positive user-item pairs to capture item-item relationships. This keeps the training terms of UltraGCN being unied and decrease the possible diculty in multi-task learning. We also empirically show that such a way can achieve better performance in Section 4.4. For each positive (𝑢,𝑖) pair, we rst construct𝐾 weighted positive (𝑢,𝑗) pairs, for𝑗 ∈ 𝑆 (𝑖). Then, we penalize the learning of these pairs with the more reasonable similarity score 𝜔and derive the constraint lossLon the item-item graph as follow: where|𝑆 (𝑖)| = 𝐾. We omit the negative sampling here as the negative sampling inLandLhas already enabled UltraGCN to counteract over-smoothing. With this constraint loss, we extend UltraGCN to better learn item-item relationships, and nally derive the following training objective of UltraGCN, where𝜆and𝛾are hyper-parameters to adjust the relative importances of user-item and item-item relationships, respectively. Figure 1 illustrates the simple architecture of UltraGCN in contrast to LightGCN. Similarly, in inference, we use the dot product ˆ𝑦= 𝑒𝑒between user𝑢and item𝑖as the ranking score for recommendation. 3.3.1 Mo del Analysis. We rst analyze the strengths of our UltraGCN model: 1) The weights assigned on edges in UltraGCN, i.e., 𝛽and𝜔, are more reasonable and interpretable for CF, which are helpful to better learn user-item and item-item relationships, respectively. 2) Without explicit message passing, UltraGCN is exible to separately customize its learning with dierent types of relationships. It is also able to select valuable training pairs (as in Section 3.2), rather than learn from all neighbor pairs indistinguishably, which may be mislead by noise. 3) Although UltraGCN is trained with dierent types of relationships in a multi-task learning way, its training losses (i.e.,L,L, andL) are actually unied, following the form of binary cross entropy. Such unication facilitates the training of UltraGCN, which converges fast. 4) The design of UltraGCN is exible, by setting𝛾to 0, it reduces to UltraGCN, which only learns on the user-item graph. The performance comparison between UltraGCN and UltraGCNis provided in Table 2. Note that in the current version, we do not incorporate the modeling of user-user relationships in UltraGCN. This is mainly because that users’ interests are much broader than items’ attributes. We found that it is harder to capture the user-user relationships from the user-user co-occurrence graph only. In Section 4.4, we empirically show that learning on the user-user co-occurrence graph does not bring noticeable improvements to UltraGCN. In contrast, conventional GCN-based CF models indistinguishably learn over all relationships from the user-item graph (i.e., Limitation II) likely suer from performance degradation. The user-user relationships may be better modeled from a social network graph, and we leave it for future work. 3.3.2 Relations to Other Models. In this part, we discuss the relations between our UltraGCN and some other existing models. Relation to MF. UltraGCN is formally to be a new weighted MF model with BCE loss tailored for CF. In contrast to previous MF models (e.g., NeuMF [11]), UltraGCN can more deeply mine the collaborative information using graphs, yet keep the same concise architecture and model eciency as MF. Relation to Network Embedding Methods. Qiu et al. [21] have proved that many popular network embedding methods with negative sampling (e.g., DeepWalk [20], LINE [25], and Node2Vec [7]) all can be unied into the MF framework. However, in contrast to these network embedding methods, the edge weights used in UltraGCN are more meaningful and reasonable for CF, and thus lead to much better performance. In addition, the random walk in many network embedding methods will also uncontrollably introduce uninformative relationships that aect the performance. We empirically show the superiority of UltraGCN over three typical network embedding methods on CF in Section 4.2. Relation to One-Layer LightGCN. We emphasize that UltraGCN is also dierent from one-layer LightGCN with BCE loss, because LightGCN applies weight combination to embeddings aggregation while our constraint coecients are imposed on the constraint loss function, which aims to learn the essence of innitelayer graph convolution. On the contrary, UltraGCN can overcome the limitations of one-layer LightGCN as described in Section 3.2. 3.3.3 Model Complexity. Given the embedding size𝑑,𝐾similar items for each𝑆 (𝑖),𝑅as the number of negative samples for each positive pair, and|𝐴|as the number of valid non-zero entries in the user-item interaction matrix, we can derive the training time complexity of UltraGCN:O((𝐾 +𝑅 +1) ∗ |𝐴| ∗ (𝑑+1)). We note that the time complexities to calculate𝛽and𝜔areO(1), since we can pre-calculate them oine before training. As we usually limit 𝐾to be small (e.g., 10 in our experiments) in practice, the time complexity of UltraGCN lies in the same level with MF, which is O((𝑅 +1) ∗ |𝐴| ∗ 𝑑). Besides, the only trainable parameters in UltraGCN are the embeddings of users and items, which is also the same with MF and LightGCN. As a result, our low-complexity UltraGCN brings great eciency for model training and should be more practically applicable to large-scale recommender systems. We rst compare UltraGCN with various state-of-the-art CF methods to demonstrate its eectiveness and high eciency. We also perform detailed ablation studies to justify the rationality and effectiveness of the design choices of UltraGCN. Datasets and Evaluation Protocol.We use four publicly available datasets, including Amazon-Book, Yelp2018, Gowalla, and MovieLens-1M to conduct our experiments, as many recent GCNbased CF models [10,27,28,32] are evaluated on these four datasets. We closely follow these GCN-based CF studies and use the same data split as them. Table 1 shows the statistics of the used datasets. For the evaluation protocol, Recall@20 and NDCG@20 are chosen as the evaluation metrics as they are popular in the evaluation of GCN-based CF models. We treat all items not interacted by a user as the candidates, and report the average results over all users. Baselines.In total, we compare UltraGCN with various types of the state-of-the-art models, covering MF-based (MF-BPR [15], ENMF [3]), metric learing-based (CML [12]), network embedding methods (DeepWalk [20], LINE [25], and Node2Vec [7]), and GCNbased (NGCF [27], NIA-GCN [24], LR-GCCF [4], LightGCN [10], and DGCF [28]). Parameter Settings.Generally, we adopt Gaussian distribution with 0 mean and 10standard deviation to initialize embeddings. In many cases, we adopt𝐿regularization with 10weight and we set the learning rate to 10, the batch size to 1024, the negative sampling ratio𝑅to 300, and the size of the neighbor set𝐾to 10. In particular, we x the embedding size to 64 which is identical to recent GCN-based work [10,24,27,28] to keep the same level of the number of parameters for fair comparison. We tune𝜆in [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4], and𝛾in [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3, 3.5]. For some baselines, we report the results from their papers to keep consistency. They are also comparable since we use the exactly same datasets and experimental settings provided by them. For other baselines, we mainly use their ocial open-source code and carefully tune the parameters to achieve the best performance for fair comparisons. To allow for reproduciblility, we have released the source code and benchmark settings of UltraGCN at Github. Table 2 reports the performance comparison results. We have the following observations: •UltraGCN consistently yields the best performance across all four datasets. In particular, UltraGCN hugely improves over the strongest GCN-based baseline (i.e., DGCF) on AmazonBook by 61.4% and 71.6% w.r.t. Recall@20 and NDCG@20 respectively. The results of signicance testing indicates that our improvements over the current strongest GCN-based baseline are statistically signicant (𝑝-value<0.05). With additional learning on the item-item graph, UltraGCN performs consistently better than its simpler variant UltraGCN. We attribute such good performance of UltraGCN to the following reasons: 1) Compared with network embedding models and the other GCN-based models, UltraGCN can respectively lter uninformative user-item and item-item relationships in a soft way (i.e., optimize with𝛽) and a hard way (i.e., only select𝐾most similar item pairs). The edge weights for the learning of user-item and item-item relationships in UltraGCN are also more reasonable; 2) Compared with other baselines, UltraGCN can leverage powerful graph convolution to exploit useful and deeper collaborative information in graphs. These advantages together lead to the superiority of UltraGCN than compared state-of-the-art models. •Overall, network embedding models perform worse than GCN-based models, especially on Gowalla. The reason might be that the powerful graph convolution is more eective than traditional random walk or heuristic mining strategies in many network embedding methods, to capture collaborative information for recommendation. •Since UltraGCN is a special MF which only needs the dot product operation for embeddings, its architecture is orthogonal to some state-of-the-art models (e.g., DGCF). Therefore, similar to MF, UltraGCN can be deemed as an eective and ecient CF framework which is possible to be incorporated with other methods, such as enabling disentangled representation for users and items as DGCF, to achieve better performance. We leave such study in future work. As highlighted in Section 3.3, UltraGCN is endowed with high training eciency for CF thanks to its concise and unied designs. We have also theoretically demonstrated that the training time complexity of UltraGCN is on the same level as MF in Section 3.3.3. In this section, we further empirically demonstrate the superiority of UltraGCN on training eciency compared with other CF models, especially GCN-based models. To be specic, we select MF-BPR, ENMF, LightGCN, and LR-GCCF as the competitors, which are relatively ecient models in their respective categories. To be more convincing, we compare their training eciency from two views: •The total training time and epochs for achieving their best performance. •Training them with the same epochs to see what performance they can achieve. Note that the validation time is not included in the training time. Considering the fact that the ocial implementations of the compared models can be optimized to be more ecient, we use a uniform code framework implemented by ourselves for all models for fair comparison. In particular, our implementations refer to their ocial versions and optimize them with uniform acceleration methods (e.g, parallel sampling) to ensure the fairness of comparison. We will release all of our code. Experiments are conducted on AmazonBook with the same Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz machine with one GeForce RTX 2080 GPU for all compared models. Results of the two experiments are shown in Table 3 and Table 4 respectively. We have the following conclusions: (1) Table 3 shows that the training speed (i.e., Time/Epoch) of UltraGCN is close to MF-BPR, which empirically justies our analysis that the time complexities of UltraGCN and MF are on the same level. UltraGCN needs 75 epochs to converge which is much less than LR-GCCF and LightGCN, leading to only 45 minutes for total training. Finally, UltraGCN has around 14x, 4x, 4x speedup compared with LightGCN, LR-GCCF, and ENMF respectively, demonstrating the big eciency superiority of UltraGCN. (2) Table 4 shows that when UltraGCN converges (i.e., train the xed 75 epochs), the performances of all the other compared models are much worse than UltraGCN. That is to say, UltraGCN can achieve much better performance with less time, which further Table 2: Overall performance comparison. Improv. denotes the relative improvements over the best GNN-base d baselines. Table 3: Eciency comparison from the rst view. Table 4: Eciency comparison from the second view. All models are trained with the xed 75 epochs except MF-BPR. Since MF-BPR needs less than 75 epochs to converge, we report its actual training time. demonstrates the higher eciency of UltraGCN than the other GCN-based CF models. We perform ablation studies on Amazon-Book to justify the following opinions: (i) The designs of UltraGCN is eective, which can exibly and separately learn the user-item relationships and item-item relationships to improve recommendation performance; (ii) Augmenting positive user-item pairs for training to learn itemitem relationships can achieve better performance than optimizing between item-item pairs; (iii) User-user co-occurrence information is probably not very informative to help recommendation. For opinion (i), we compare UltraGCN with the following variants to show the eectiveness of our designs in UltraGCN: •UltraGCN(𝜆 =0,𝛾 =0): when setting𝜆and𝛾to 0, UltraGCN is simply reduced to MF training with BCE loss function, which does not leverage graph information and cannot capture higher-order collaborative signals. •UltraGCN(𝛾 =0): this variant is identical to UltraGCN, which only learns on the user-item graph and lacks more eective learning for item-item relationships. •UltraGCN(𝜆 =0): this variant lacks the graph convolution ability for learning on the user-item graph to more deeply mine the collaborative information. Results are shown in Figure 2. We have the following observations: (1) UltraGCN(𝛾 =0) and UltraGCN(𝜆 =0) all perform better than UltraGCN(𝜆 =0,𝛾 =0), demonstrating that the designs of UltraGCN can eectively learn on both the user-item graph and item-item graph to improve recommendation; (2) Relatively, UltraGCN(𝜆 =0) is inferior to UltraGCN(𝛾 =0), indicating that user-item relationships may be better modeled than item-item relationships in UltraGCN; (3) UltraGCN performs much better than all the other three variants, demonstrating that our idea to disassemble various relationships, eliminate uninformative things which may disturb the model learning, and nally conduct multi-task learning in a clearer way, is eective to overcome the limitations (see Section 2.2) of previous GCN-based CF models. For opinion (ii), we change Lto L: which is instead to optimize between the target positive item and its most𝐾similar items. We compare the performance of UltraGCN usingLandLrespectively with careful parameters tuning. Results are shown in Figure 3. It is clear that no matter incorporatingLor Table 5: Performance comparison of whether learning on the user-user co-occurrence graph. Figure 2: Performance comparison of variants of UltraGCN. Figure 3: Performance comparison of using Land L. not, usingLcan achieves obvious better performance than using L, which proves that our designed strategy to learn on item-item graph is more eective. Furthermore, the performance gap between usingLand usingLbecomes large when incorporatingL, indicating that our strategy which makes the objective of UltraGCN unied can thus facilitate training and improve performance. For opinion (iii), we derive the user-user constraint lossL with the similar method of Section 3.2 and combine it to the nal objective. We carefully re-tune the parameters and show the comparison results of whether usingLin Table 5. As can be seen, incorporatingLto learn user-user relationships does not bring obvious benets to UltraGCN. We attribute this phenomenon to the fact that the users’ interests are broader than items’ properties, and thus it is much harder to capture user-user relationships just from the user-user co-occurrence graph. Therefore, we do not introduce the modeling of user-user relationships into UltraGCN in this paper, and we will continue to study it in the future. Figure 4: Performance comparison of setting dierent 𝐾. Figure 5: Performance comparison with dierent 𝜆 and 𝛾. We investigate the inuence of the number of selected neighbors𝐾 and the weights of the two constraint losses (i.e.,𝜆and𝛾) on the performance for a better understanding of UltraGCN. 4.5.1 Impact of𝐾. We test the performance of UltraGCN with dierent𝐾in [5, 10, 20, 50] on Amazon-Book and Yelp2018. Figure 4 shows the experimental results. We can nd that when𝐾 increases from 5 to 50, the performance shows a trend of increasing rst and then decreasing. This is because that when𝐾is 5, the item-item relationships are not suciently exploited. While when 𝐾becomes large, there may introduce some less similar or less condent item-item relationships into the learning process that aect model performance. Such phenomenon also conrms that conventional GCN-based CF models inevitably take into account too many low-condence relationships, thus hurting performance. 4.5.2 Impact of𝜆and𝛾. We rst set𝜆 =0 and show the performance of dierent𝜆from 0.2 to 1.4 (0.2 as the interval). Then we test with dierent𝛾in [0.1, 0.5, 1, 1.5, 2, 2.5, 3, 3.5] based on the best𝜆. Experiments are conducted on Amazon-Book, and we show results in Figure 5. For𝜆, we nd that the small value limits the exertion of the user-item constraint loss, and a value of 1 or so would be suitable for𝜆. For the impact of𝛾, its trend is similar to 𝜆but is more signicant, and 2.5 is a suitable choice for𝛾. In general, our investigations for𝜆and𝛾show that these two parameters are important to UltraGCN, which can exibly adjust the learning weights for dierent relationships and should be carefully set. In this section, we briey review some representative GNN-based methods and their eorts for model simplication toward recommendation tasks. With the development and success of GNN in various machine learning areas, there appears a lot of excellent work in recommendation community since the interaction of users and items could be naturally formed to a user-item bipartite graph. Rianne van den Berg et al. [1] propose graph convolutional matrix completion (GCMC), a graph-based auto-encoder framework for explicit matrix completion. The encoder of GC-MC aggregates the information from neighbors based on the types of ratings, and then combine it to the new embeddings of the next layer. It is the rst work using graph convolutional neural networks for recommendation. Ying et al. [31] rst applys GCN on web-scale recommender systems and propose an ecient GCN-based method named Pinsage, which combines ecient random walks and graph convolutions to generate embeddings of items that incorporate both graph structure as well as item feature information. Then, Wang et al. [27] design NGCF which is a new graph-based framework for collaborative ltering. NGCF has a crafted interaction encoder to capture the collaborative signals among users and items. Although NGCF achieves good performance compared with previous non-GNN based methods, its heavy designs limit its eciency and full exertion of GCN. To model the diversity of user intents on items, Wang et al. [28] devise Disentangled Graph Collaborative Filtering (DGCF), which considers user-item relationships at the ner granularity of user intents and generates disentangled user and item representations to get better recommendation performance. Although GNN-based recommendation models have achieved impressive performance, their eciencies are still unsatisfactory when facing large-scale recommendation scenarios. How to improve the eciency of GNNs and reserve their high performance for recommendation becomes a hot research problem. Recently, Dai et al. [6] and Gu et al. [8] extend xed-point theory on GNN for better representation learning. Liu et al. [18] propose UCMF that simplies GCN for the node classication task. Wu et al. [29] nd the non-necessity of nonlinear activation and feature transformation in GCN, proposing a simplied GCN (SGCN) model by removing these two parts. Inspired by SGC, He et al. [10] devise LightGCN for recommendation by removing nonlinear activation and feature transformation too. However, its eciency is still limited by the time-consuming message passing. Qiu et al. [21] demonstrate that many network embedding algorithms with negative sampling can be unied into the MF framework which may be ecient, however, their performances still have a gap between that of GCNs. We are inspired by these instructive studies, and propose UltraGCN for both ecient and eective recommendation. In this work, we propose an ultra-simplied formulation of GCN, dubbed UltraGCN. UltraGCN skips explicit message passing and directly approximate the limit of innite message passing layers. Extensive experimental results demonstrate that UltraGCN achieves impressive improvements over the state-of-the-art CF models in terms of both accuracy and eciency. This work was supported in part by the National Natural Science Foundation of China (61972219), the Research and Development Program of Shenzhen (JCYJ20190813174403598, SGDX20190918101201696), the National Key Research and Development Program of China (2018YFB1800601), and the Overseas Research Cooperation Fund of Tsinghua Shenzhen International Graduate School (HW2021013). To further demonstrate the eectiveness of UltraGCN, we additionally provide the results compared to some more recent stateof-the-art CF models, including NBPO [33], BGCF [23], SCF [34], LCFN [32], and SGL-ED [30]. For simplicity and fairness of comparison, we use the same dataset and evaluation protocol provided by each paper. We also duplicate the results reported in their papers to keep consistency. The results in Table 6 again validate the eectiveness of UltraGCN, which outperforms the most recent CF models by a large margin. Table 6: Performance comparison with some more models, including SCF, LCFN, NBPO, BGCF, and SGL-ED.