Telecom SudParis, Institut Polytechnique de ParisTelecom SudParis, Institut Polytechnique de Paris Abstract—This paper proposes a new recommendation system preserving both privacy and utility. It relies on the local differential privacy (LDP) for the browsing user to transmit his noisy preference proﬁle, as perturbed Bloom ﬁlters, to the service provider. The originality of the approach is multifold. First, as far as we know, the approach is the ﬁrst one including at the user side two perturbation rounds - PRR (Permanent Randomized Response) and IRR (Instantaneous Randomized Response) - over a complete user proﬁle. Second, a full validation experimentation chain is set up, with a machine learning decoding algorithm based on neural network or XGBoost for decoding the perturbed Bloom ﬁlters and the clustering Kmeans tool for clustering users. Third, extensive experiments show that our method achieves good utility-privacy trade-off, i.e. a 90% clustering success rate, resp. 80.3% for a value of LDP  = 0.8, resp.  = 2. Fourth, an experimental and theoretical analysis gives concrete results on the resistance of our approach to the plausible deniability and resistance against averaging attacks. Index Terms—Local differential privacy, recommendation, privacy, RAPPOR, proﬁles perturbation, Bloom ﬁlters, neural networks, XGBoost, Kmeans With the increase of online services, individuals are confronted with a lot of choices when making purchases. For better user experience, service providers rely on recommender systems helping users ﬁnd items of interest. For this purpose, service providers are massively collecting and analyzing users’ data which may threaten users’ privacy. Hence, for individuals to get tailor-made services but not at the price of their privacy, as exposed in the survey of 2.000 people [16], and for recommenders to monetize the attention of consumers with targeted advertising, products and services selling, there is a Brice.Mazeau@telecom-sudparis.eu strong need to elaborate new recommendation systems taking privacy and utility into account. Contributions This paper proposes a new recommendation system preserving both privacy and utility. The idea is to ensure protection against honest-but-curious entities - service providers and outsiders - while still getting useful recommendations. With that objective in mind, our approach relies on the LDP principle for the user preference proﬁle to be perturbed at the browsing user side through a two-round processing - PRR (Permanent Randomized Response) and IRR (Instantaneous Randomized Response). That perturbation is an adaptation of the LDP-based RAPPOR approach [9] being made suitable for both classiﬁcation and clustering tasks under local differential privacy. As far as we know, this is the ﬁrst time that a tworound perturbation processing has been applied to a complete user proﬁle. With the objective of getting experimental utility vs privacy validation results, a full validation experimentation chain is set up with a recommender being implemented. At the recommender side, two successive mechanisms are performed: a machine learning decoding algorithm based on neural networks or XGBoost for decoding the perturbed Bloom ﬁlters and a clustering Kmeans tool for clustering users. It has to be noted that an appropriate user clustering leads straight to relevant recommendations for the user. The idea of the experiment is thus to assess how much users clustering is successful according to their perturbed preferences and a related privacy budget quantifying how much noise is included into their preferences. Our approach is validated through both extensive experiments and a security analysis. The experiments show that our method achieves a good utility-privacy trade-off with a 90% clustering success rate, resp. 80.3% for a value of LDP  = 0.8, resp.  = 2. The experimental and theoretical security analysis demonstrates that our approach supports both plausible deniability and resistance against averaging attacks. Paper organization. Section II gives the useful background about Local Differential Privacy (LDP). Section III surveys existing LDP-related works highlighting their deﬁciencies. Then our approach is described in Sections IV and V. Section IV introduces the system model with the actors, the utility metrics, the privacy properties and the threat model. Section V details the step-by-step processing phases at both the user side and the recommender side. The three following sections provide a full evaluation of our approach. Section VI studies the utility performance achievements of our decoding and clustering algorithms according to several experimental conditions (privacy budget, Bloom ﬁlter parameters). Section VII gives a security analysis of our scheme with regard to the plausible deniability and averaging attacks. Section VIII discusses the utility vs privacy trade-off. Conclusions are given in Section IX. Local Differential Privacy (LDP) has its roots in Differential Privacy (DP) works [8], [13]. DP matches the global model where a trusted third party uses DP to produce statistics over a dataset, while withholding information about individuals in the dataset. LDP [18] matches the local model where data can be perturbed right at the source, locally to the user, thus leading to higher privacy guarantees as the trusted third party is no longer necessary. Deﬁnition. A randomized algorithm M satisﬁes the  local differential privacy [18] where  > 0 if for all pairs of the client ’s values x and y and for all S ⊆ range(M): The deﬁnition introduces , known as the privacy budget or the privacy loss. It controls to what extent the output of an algorithm depends on the input, and thus it reﬂects the desired level of privacy. The smaller is the privacy budget ,  being a positive value, the higher is the privacy level. This section makes an overview ﬁrst on some LDP wellknown use cases, and then on some privacy preserving systems including LDP recommender systems. LDP use cases. LDP concepts are applied in various domains in the literature. For instance, Google proposed RAPPOR [9] so as to collect statistics about malicious URLS from users browsers without causing privacy threats. The proposed solution relies on encoding the values to be sent, by applying two -LDP perturbation levels. In Microsoft, LDP enables collecting data about the time spent by users in different applications, therefore, identifying its favorite ones and improving the users’ experience [7], while still preserving privacy. For reducing possible privacy leakage in deep learning models, [6] proposes LATENT as an intermediate layer designed to satisfy LDP in deep learning models. The suggested solution enables a data owner to perturb data at the owner’s device before the data reach out an untrusted machine learning service. Privacy preserving recommender systems. The need for privacy preservation in recommender systems triggered research efforts in the last decades. Two main axes based on cryptography and data perturbation are investigated. Kaaniche et al. [11] designed a privacy-preserving framework for recommender systems. They suggested that a user perturbs his proﬁle relying on a collaborative secure computation, that incorporates intermediate nodes between end-users and service providers. Their framework generates additional computation overhead. In [4], [5], Canny proposes cryptographic protocols to preserve privacy in recommender systems. The suggested scheme uses matrix projection and factor analysis. Both techniques result in heavy computational and communication overhead. A¨ımeur et al. [1] designed the Alambic system where users private data are shared between the service provider and a semi-trusted third party. The whole public key infrastructure is adopted so as to ensure data protection. Yet, only if the service provider doesn’t collude with the semi-trusted third party, the user privacy is protected. Recently, Kim et al. [12] suggested SPIREL, a location based recommender system under LDP. The framework uses the Optimized Randomized Response [18] approach to perturb the transition patterns and refers to the piecewise mechanism [17] in order to perturb gradients. Both suggested algorithms are  − LDP . The solution drawback is its high communication cost, as the transition patterns are encoded in a n-sized bit-array (n denoting the domain of possible locations) by the user before perturbing them with the Optimized Randomized Response. Moreover, the framework takes into account only one location transition from the user’s check-in history. Nevertheless, using LDP to protect only one transition doesn’t ﬁt well in practice as the user may have many transitions to report. The extension of SPIREL to report many items under LDP is far from obvious as the process relies on the gradient perturbation. While most of the existing works tackle the issue of only one item perturbation, BLIP [2] focuses on perturbing a whole proﬁle using the LDP mechanism. However, the achieved utility in terms of recall does not exceed 0.26 for  = 3. Moreover, the utility computation is performed on perturbed Bloom ﬁlters, and does not refer to any decoding algorithm, which is not compatible with an integration into a recommender system. Furthermore, as the similarity computation between proﬁles is done at user side each time a new node joins the system, the solution can be considered as cost-prohibitive. This section details our full system model, and an overview of our approach. It presents the actors, the metrics for measuring utility, the supported privacy properties along with the considered threat model. Our system model is composed of the following entities: Search Engine W SE along with his proﬁle made of individual attributes (preferences). C who cares about his privacy, sends to W SE his perturbed preferences according to the LDP method and a privacy budget . the client’s query according to the transmitted preference proﬁle. W SE can suggest its own recommendations to C, and can take on the role of a proxy between C and third parties T P by relaying the proﬁle and the T P ’s recommendations back to C. Note that the intermediate W SE between C and T P could be any Service Providers. recommendation provider. Its main goal is to provide users with targeted recommendations. To avoid leaking his preferences while still getting appropriate recommendations, a user C can decide to send a search query to W SE along with his perturbed preferences. The full processing for perturbing the preferences is detailed in subsection V-A and includes the following steps which are depicted in Figure 1: 1) C preferences using a Bloom ﬁlter [9] 2) Execute the permanent randomized response (PRR) step over the Bloom ﬁlter [9] 3) Execute the Instantaneous Randomized Response (IRR), as a second perturbation step [9], over the modiﬁed Bloom ﬁlter 4) Send the query along the perturbed preferences to W SE Upon receiving the request, W SE forwards the perturbed proﬁle to the recommender, either T P or W SE, and next referred for clarity as T P . T P next needs to decode the given preferences using a machine learning algorithm, resulting in a reconstructed approximate user proﬁle (cf. subsection V-B). Then based on the obtained proﬁle and the similarity among proﬁles, T P executes a clustering algorithm (cf. subsection V-C) and classiﬁes C in one of the group of users, for next delivering targeted recommendations suitable for that group of users to C via W SE. Our recommendation system should preserve the utility i.e. the adequacy between C’s expectations and the returned recommendations - for maintaining the user experience. This means that the user clustering being performed by recommenders W SE or T P on the perturbed C proﬁle should be as close as possible to the one achieved with the unperturbed C proﬁle. To measure the utility, the metrics well known for clustering algorithms - accuracy, precision, recall, and F1 score - rely on the following values: and is predicted to be in the class. class, but it is predicted as a class member. but it is predicted as not being a member of that class. The following metrics are used for measuring utility: 1) Accuracy. The rate of the correct predictions out of all the predictions. Accuracy is sensitive to class imbalance, and is expressed as follows: 2) Precision. The proportion of items correctly identiﬁed to be within a class i out of all the items identiﬁed to belong to that class. A low precision helps to identify where the model made incorrect classiﬁcation. It is deﬁned as:P 3) Recall. The proportion of items that should have been annotated with a given label, and that were actually annotated with that label. This is a measure of completeness and is formally deﬁned as follows:P 4) F1 score. A weighted average of recall and precision. Thereby, the metric takes into account both false positives and false negatives, as follows: This section deﬁnes the basic privacy properties supported by our approach. the use of an algorithm that satisﬁes the LDP deﬁnition as an obfuscation scheme. In fact, by observing the output of the algorithm, an adversary can not tell with high conﬁdence whether a particular preference was deﬁnitely used as an input of the algorithm. having knowledge of several versions of the perturbed Bloom ﬁlters can not ﬁnd out the original preferences through an averaging attack. Our threat model considers a honest-but-curious adversary attempting to learn the preferences of client C from the perturbed Bloom ﬁlters sent by client C. We next deﬁne three privacy games, with regard to two adversaries provided with the following abilities: 1) Basic Adversary BA. BA is not provided with any Machine Learning algorithms. BA is an outsider, i.e. external to our system. 2) Advanced Adversary AA. AA is provided with Machine Learning algorithms. AA is an honest-but-curious WSE or an honest-but-curious T P . 1) The Plausible Deniability Game Conducted over one Preference by a Basic Adversary: An outsider BA is challenged with the following privacy game: a set of N preferences, the Bloom ﬁlter size M, k hash functions and the privacy budget . eters, BA is given a polynomial computation time in order to compute some Bloom ﬁlters for the transmitted N preferences. Therefore, he can reconstruct a database DBF of perturbed Bloom ﬁlters. C a related perturbed Bloom ﬁlter. BA sends back to C his guess about the preference. BA wins the game if he is able to correctly guess the encoded preference. If the probability to win the game is negligible, then our approach is said to be resistant against the plausible deniability attack conducted over one preference by a basic adversary. 2) The Plausible Deniability Game Conducted over multiple preferences by the Advanced Adversary: The insider AA is challenged with the following privacy game: perturbed preferences along with their original values. vided values. his dataset, and sends to C a related perturbed Bloom ﬁlter. AA sends back to C his guess about the set of preferences, which comes down to correctly decoding the perturbed Bloom ﬁlters. The adversary wins the game if he gets high precision and recall. If the probability to win the game is negligible, then our approach is said to be resistant against the plausible deniability attack conducted over multiple preferences by an advanced adversary. 3) Averaging Game: Any BA or AA adversary is challenged to ﬁnd out the original preferences from a set of perturbed Bloom ﬁlters issued over the same set of C’s preferences. In this section, we detail the main three phases of the new recommendation system, an overview of which is described in Section IV-B. The perturbation is handled at C’s device with the objective to obfuscate C’s preferences. It has to be noted that the set of preferences forms a proﬁle composed of one or many categories where each category is denoted by C, i ∈ {1...l} where l denotes the maximum number of supported categories. Each category is composed of groups of interests I(i.e. j ∈ {1...g}) where g represents the maximum number of groups of interests for category i. 1) Encoding. As the ﬁrst step of the perturbation process, the encoding algorithm maps C’ preferences into bits in a Bloom ﬁlter. The ﬁrst operation is to compute the optimal Bloom ﬁlter size m, given the maximum number of preferences encoded into the Bloom ﬁlter (i.e. n = l∗g if each category includes the same number of groups of interests), and a ﬁxed false positive rate f, according to equation 6 [3]: Then, the optimal number of hash functions [14] is computed as given by equation 7. After selection of m and k appropriate values, the Bloom ﬁlter B is initialized with all ”0” values, as given in Figure 2. For feeding B with the set of preference values v, C ﬁrst applies the k hash functions to v, and feeds B with the hash output providing indices. For illustration, let us consider a Bloom ﬁlter of size m = 10 bits, with k = 2 hash functions (h1, h2), and two preferences {Action, Fantasy} to be included into the Bloom ﬁlter. As given in Figure 3, C ﬁrst computes the two hashes of the preference Action, and gets the results h(Action)= 3 and h(Action)= 6, thus leading to positioning the 3and the 6bits of the Bloom ﬁlter to value 1. The same applies for the preference F antasy as depicted in Figure 4 where h(Fantasy)= 2 and h(Fantasy)= 8. 2) Permanent Randomized Response (PRR). This ﬁrst level perturbation applies over the Bloom Filter B obtained through the encoding phase. This step is executed once over a set of preferences v, whatever the number of search requests done by C to W SE. A noisy bit is derived from each bit of B thus resulting in a perturbed Bloom ﬁlter vector B. The derivation is compliant with the RAPPOR works [9] and considers the following probabilistic processing: The obtained bit vector Bremains stored and known to C only. This ﬁrst level perturbation PRR algorithm is -differential privacy with the following quantiﬁed  privacy budget: 3) Instantaneous Randomized Response (IRR). To guarantee stronger short-term privacy, this second level perturbation [9] is executed for each request done by C to W SE. After getting B, the user initializes a bit vector S with all zeros and then applies the following probabilistic processing: Where p denotes the probability of ﬂipping a bit that equals to 0 into 1 whereas q represents the probability of keeping bits equal to 1. This second level perturbation IRR algorithm is differential privacy with the following quantiﬁed  privacy budget [9]: Where p, resp. q, is the probability of observing 1 given that the same Bloom ﬁlter bit was set to 0, resp. 1, as deﬁned in the following equations. Upon receiving C’s perturbed preferences, T P decodes the received C’s perturbed preferences into some approximate preferences, based on a machine learning algorithm. The problem is modeled as a multiclass classiﬁcation task, aiming at predicting the classes of perturbed Bloom ﬁlters. For instance, given the music category, which contains eight classes (groups of interest): classical, jazz, pop... T P should identify for each perturbed Bloom ﬁlter its right label. Two machine learning algorithms - neural network and XGBoost - were selected for their ability to work on perturbed data. Both algorithms are calibrated to ﬁt the speciﬁcities of the two following datasets, thus resulting into 2 conﬁgurations as detailed below: categories - movies, music and sports - and 7 classes for movies, 8 for music and 12 for sports. gories - destination and ﬂight class type - which are made up 11 classes for destination and 3 for ﬂight class type. 1) Neural network conﬁguration. The neural network is composed of an input layer which is fed with the perturbed preferences, two hidden layers, and an output layer. Two dropouts are introduced to mitigate the overﬁtting issue. For both of our datasets, the layers of the algorithm are conﬁgured according to the parameters given in Table I. 2) XGBoost conﬁguration. XGBoost is a gradient boosting algorithm. Table II gives the parameters calibrated for each dataset to optimize the model’s performances. As can be shown, the conﬁguration is slightly the same, except for parameter Subsample. Clustering is done with Kmeans, considering K = 4 clusters for a number of proﬁles that is equal to 80.000 and K = 5 clusters for 90.000 proﬁles (used later to study the privacy utility trade-off). This choice of K is based on the elbow method [15]. The main idea behind the technique is to run the Kmeans algorithm for different values of clusters and to calculate the Within Cluster Sum of Squares (WCSS). According to this metric, the variability of observations is computed in each cluster. A cluster that has a low value of WCSS is more homogeneous than a cluster with a high WCSS value. Formally, the objective is to minimize the following function. Where Yrepresents the centroid for observation X. Then, for a range of k numbers, the WCSS variation is plotted with respect to k. The optimal value of k = 4 was obtained through an experiment we did over 80.000 cleaned traces from a Qwant dataset to the Kmeans, and a range of k varying between 1 and 15. As depicted in Figure 5, there is a sudden huge drop in the WCSS value when increasing the number of clusters from 1 to 2, and a second drop - not as huge as the ﬁrst one at the cluster number 4. As the WCSS maintains a minimum value starting from k = 4, we deduce an optimal value of k = 4. Unlikely to usual recommendation systems, our LDPrecommendation system works on perturbed proﬁles instead of the true users proﬁles. It is therefore necessary to adapt the accuracy measurement for evaluating the ability of the algorithm to group same-proﬁle users into the same cluster. In our case, in a ﬁrst experience, Kmeans is applied over a set of original (unperturbed) proﬁles, thus resulting in some cluster labels, as classically done. In a second experience, Kmeans is fed with proﬁles which have been ﬁrst perturbed and then decoded. The clustering results are recorded and then used in a ﬁnal step for comparing the clustering results with/without perturbation and for measuring accuracy. The more matches we get, the higher our accuracy. As shown in Table III, neural network outperforms XGBoost for both datasets with regard to the measured accuracy (for K = 4 clusters). This result is conﬁrmed in Table III for low perturbation level ( = 2) as well as high perturbation level ( = 0.80). It gives higher precision, recall and f1-score and thus higher clustering success rates. This subsection analyses the inﬂuence of different parameters on the clustering results, including the privacy budget value , the Bloom ﬁlter size M and the number of hash functions k. results in higher classiﬁcation accuracy, as the Bloom ﬁlter collision rate decreases. Yet, this comes at a cost in memory since increasing M leads to larger Bloom ﬁlter sizes. As shown, the beneﬁt for high values of M diminishes when M exceeds 144 for this experimental setup. This observation is due to the hash collisions starting to vanish. Next observations are thus considering M = 144. minimum value of hash functions of 3, which corresponds to the optimal number of hash functions for M = 144, according to the equation 7. As depicted in the ﬁgure, the classiﬁcation accuracy decreases when the number of hash function increases. This stems from an increasing number of hash collisions. accuracy is an increasing function of the privacy budget. Indeed, the higher the privacy budget, the lower the perturbation level, and the higher the accuracy. The preference dataset achieves better clustering results. This might be due to the dataset characteristics, the number of categories, the number of classes per category... This section is dedicated to the security analysis with regard to the threat model deﬁned in subsection IV-E. [Bloom ﬁlter size for  = 0.85, f= 0.1 and k = 3] [Hash functions for M = 144,  = 0.85 and f= 0.1] Referring to the game described in Section IV-E, given the perturbed Bloom ﬁlter y sent by the challenger C and the preference universe D, the strategy for the basic adversary BA is to compute: guess(y) = arg maxP r[v|y] Where φdenotes the perturbation mechanism, D is the universe of the preferences to enter into a Bloom ﬁlter, π(v) is the prior probability of v and is equal tofor all v ∈ D. For computing for each value v the probability that the perturbation mechanism outputs y, BA can refer to the following probability expressed in Equation 16 [10]. hiif B[i] = 1 P rB[i] = φ(B[i]) = 1= Where 2∆ denotes how many positions can change in neighboring vectors at most. In Bloom ﬁlter encoding, ∆ is equal to k the size of the hash functions domain. Brepresents the perturbed version of B. Experimental results for quantifying the chance of BA of winning the game. The success rate of BA for winning the game can directly be derived from the probability of Equation (16) as the probability of getting B[i]=1. This probability can be experimentally measured on our preference dataset, according to  and k values and gives the results depicted in Figure 8. As expected, the lower  and the higher k, the lower the probability for winning the game is. For  ranging from [0.1, 0.85] value, the probability that the adversary wins the game is in average below 0.22. Higher the k values, more difﬁcult it is for BA to win the game as the number of hash collisions increases, thus leading to higher wrong guess. As a conclusion, the success rate of BA can be low according to the selected  and k values. As such, a suitable trade-off utility vs privacy, as detailed in Section VIII, needs to be found. The attack is managed by an advanced adversary AA as presented in Subsection IV-E. This section provides experimental results, and an in-depth discussion, about the  impact on the success rate of the adversary. The experiment is conducted over 10.000 samples and 20.000 samples given to AA for training. Results are provided in two ﬁgures, Figure 9 for the confusion matrices, and Table IV for the classiﬁcation result statistics which enables to evaluate the decoding ability of the adversary. Table V gives the success rate of an AA adversary to win the game. Figure 9.a shows that for low privacy budget ( = 0.1), the adversary has mistaken the majority of the classes as the values outside the diagonal are relatively high. His overall precision and recall are below 29 % as demonstrated by the classiﬁcation report in Table IV. An increase of the privacy budget from  = 0.1 to  = 1.2, resp.  = 2.4, improves the classiﬁcation ability of the adversary. Indeed, the precision and recall reach only 31 %, resp. 49 % in average as depicted in Table IV. Thus the success rate for winning the game by AA  = 1.2, resp.  = 2.4, is equal to 33 %, resp. 52 % (see Table V). One can also notice that for  = 2.4, values inside the diagonal of the confusion matrix in Figure 9.c are higher than for matrices in Figures 9.a and 9.b. As described in Section IV-E, the adversary is provided with a number of perturbed Bloom ﬁlters corresponding to the same set of C’s preferences. The adversary is only able to compute the ﬁrst PRR output value Bwhich is the ﬁrst-level perturbed Bloom ﬁlter. As the same Bvalue is used from one session to another (cf. Equation 10), the adversary is unable to ﬁnd out the original Bloom ﬁlter B. As such, he is not able to win the averaging attack game. This analysis measures the utility in terms of classiﬁcation accuracy, which is deﬁned as the ability of the recommender to perform correct classiﬁcation of users, despite the perturbation scheme. Privacy is measured as the success rate of an advanced adversary for decoding the preferences. The experimental setup considers k = 15, 90.000 different proﬁles, and K = 5 clusters. Figures 10.a and 10.b illustrate the achieved trade-off for both datasets with speciﬁc parameters k = 15 and K = 5. As expected, the privacy is a decreasing function of the privacy budget and the utility is a rising function of the privacy budget. There is a privacy vs utility trade-off (curves intersection) happening for 84% of privacy level and 80% of utility for both datasets for a privacy budget equal to 0.58. This point is a kind of optimum when both utility and privacy are equally important. This paper proposes a privacy-preserving recommandation system, which lets the user decide on the amount of preference data he wants to communicate to a recommender, and the amount of LDP noise he wants to introduce into his data. Thus the user can decide how much privacy vs user experience he wants to keep. Through our speciﬁc experiment conducted over two datasets, our solution exhibits good performances in terms of privacy and utility, i.e. a 90% clustering success rate, resp. 80.3% for a value of LDP  = 0.8, resp.  = 2, and it shows that a privacy vs utility trade-off can be found for  = 0.58, with 84% privacy level and 80% utility. This paper is supported in part by Institut Mines-Telecom chair VP-IP for Values and Policies of Personal Information and in part by the European Union’s Horizon 2020 research and innovation program under grant agreement No 830892, SPARTA project. Authors are also thankful to Qwant for providing a dataset of cleaned traces which helped to achieve results closer to the ground.