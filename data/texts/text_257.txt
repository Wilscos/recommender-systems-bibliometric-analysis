An essential part of the scientiﬁc research process is the publication of the obtained results at a suitable venue, i.e., a particular conference, workshop, or journal. The related selection problem for the best ﬁtting scientiﬁc venue has many diﬀerent aspects, such as the ﬁt of the research topics, the prospects of acceptance, and the prestige of the venue. The complexity of the selection is further exacerbated by the growing number of publication venues, the increasing granularity of research topics, and the exponentially surging number of publications. To support researchers with this task, diﬀerent methods have been proposed, e.g., based on Latent Dirichlet Allocation [9], hybrid approaches incorporating social networks [14, 13], or procedures that draw from background ontologies [20, 16]. Moreover, recent approaches based on deep learning methods achieved high accuracy in recommendations [6]. All these methods have in common that their recommendations are insuﬃciently explained. For example, Kobs et al. [6] solely highlight words from the input article that were essential for a recommendation. With the present work we show a new approach for recommending venues that improves on explainability. From the information a scientist provides, such as paper title, abstract and, possibly, a list of keywords, our method creates a ranking over Knowledge and Data Engineering Group, University of Kassel {schaefermeier, stumme, hanika}@cs.uni-kassel.de AbstractSelecting the best scientiﬁc venue (i.e., conference/journal) for the submission of a research article constitutes a multifaceted challenge. Important aspects to consider are the suitability of research topics, a venue’s prestige, and the probability of acceptance. The selection problem is exacerbated through the continuous emergence of additional venues. Previously proposed approaches for supporting authors in this process rely on complex recommender systems, e.g., based on Word2Vec or TextCNN. These, however, often elude an explanation for their recommendations. In this work, we propose an unsophisticated method that advances the state-of-the-art in two aspects: First, we enhance the interpretability of recommendations through non-negative matrix factorization based topic models; Second, we surprisingly can obtain competitive recommendation performance while using simpler learning methods. Keywords: topic models, recommender systems, matrix factorization kthematically ﬁtting venues. Our method further generates a list of the most important research topics found in the input article. We provide a contrasting juxtaposition to the venues by generating a similar list of research topics for each recommended venue. The research topics are found automatically through a topic model, speciﬁcally a non-negative matrix factorization (NMF), that is precomputed on a corpus of suitable research articles. The thereof resulting recommendations entail two major advantages. First, they are more comprehensible due to their annotations using research topics. Second, their explanations go beyond the mere occurrence of certain terms in the input article. From a procedural point of view, through NMF, we identify research topics in a corpus and represent venues as a combination of these topics. This approach has been used successfully before to map trajectories of venues within thematic spaces [17] in an interpretable manner. Based on that our speciﬁc contribution with the present work is threefold. mendation procedure based on tf-idf, logistic regression and non-negative matrix factorization. or even better results with respect to the state-of-the-art in venue recommendation. We base this claim on the comparability accomplished by using the data from Kobs et al. [6]. recommendations may be enhanced using research topics discovered by NMF. When researchers are looking for a publishing venue for their recent research of said venues emphasized research topics, to be considered for submission. Problem 1 (General Submission Problem). Given paper (document) The order of this ranking should reﬂect the similarity between elements ofVand the input paper (document) topics. Furthermore, other properties of of importance. The resulting topvenue recommendations for paper the submission problem we resort to a paper corpus This corpus is constituted by a set of papers labeled with their publishing venues v ∈ V. In the following, we consider the aspect of ﬁnding good topical ﬁts as the submission problem. We require that all venues inC, i.e., for any topics of d must be from the same particular research domain as the corpus C. Polonioli [12] pointed out that transparency is a necessary feature of search engines for research. The most prominent reasons are accountability of the search results and the disclosure of possible biases (e.g., in algorithms or data). In our work, we hence do rather not focus on outperforming previous benchmarks, but on advancing methods that are intrinsically comprehensible. Hence, our Second,we demonstrate that our setup can achieve competitive , they encounter an increasing number of conferences and journals. Each v ∈ Vhas propertiesp ∈ P, such as prestige, acceptance rate, and approach is based on a topic model that is derived from non-negative matrix factorization (NMF). This method has been used in previous works to gain explainable insights into research topics [17]. Using NMF we annotate venues, i.e., conferences and journals, with their respective topics. Furthermore, we attempt to use the discovered topics as an input to the ranking method. What is more, our method, as presented in this work, is useful to ﬁnd similar research for a given piece of work, although our focus here is with the ﬁrst application in mind. Yang and Davison [21] recommend venues from ACM digital library using a collaborative ﬁltering approach incorporating topics and writing style. Medvet, Bartoli, and Piccinin [9] test three diﬀerent methods based on n-grams and Latent Dirichlet Allocation. More complex, hybrid methods integrate social network analysis [14, 13]. All of the above do not add explanations to recommendations. Smart Book Recommender (SBR) represents books, journals and conference proceedings as vectors of topic counts extracted through the Computer Science Ontology (CSO) [20, 16]. Due to the dependence on CSO, SBR is not directly applicable to other research domains. Recommendations are only available for items in a pre-computed database. Iana et al. [4] develop a conference recommender based on SciGraph, a taxonomy by Springer Nature. The authors experiment with author information, paper abstracts and paper keywords. For abstracts, diﬀerent representations are evaluated, such as TF-IDF, Latent Semantic Analysis (LSA), probabilistic LSA and diﬀerent embedding methods (Word2Vec [10], GloVe and FastText). For keywords, SciGraph marketing codes are used, categories deﬁned by Springer. The best performing method is based on these marketing codes, which, however, may not be available from other than the used data sources. In “Where To Submit” (WTS, Kobs et al. [6]) self-trained Word2Vec embeddings and TextCNN [5] are employed. Enhancements are made by incorporating paper titles and keywords and applying convolutions separately to them. Explanations through the integrated gradients method are added [19]. This computes words inﬂuential for the classiﬁer, which are visually highlighted. These, however, do not necessarily improve the interpretability. For example, often, only few words are highlighted. Examples for non-interpretable black-box recommenders can be found online. We approach the submission problem as a supervised learning task. For every paper label. Based on that we train a supervised classiﬁer to predict the venue. The output of such classiﬁers are probabilities for, or fractions of, class membership, which impose a linear order on V . https://journalsuggester.springer.com cin a corpusC, we use its publication venuev ∈ Vas its ground truth Our aim is to generate both, recommendations and explanations comprised of interpretable, automatically derived research topics. Our reasoning is that research topics provide more informative explanations for venue recommendations than single words [17]. In short, the envisioned NMF approach allows for incorporating words that are not part of the input document and keywords) into the explanation. This additional information is pre-extracted from co-occurrences in a paper corpus of the research domain. The research topics allow for a uniﬁed view on venues as well as documents. For the rest of this work, a document is comprised of the following features: a title; an abstract; a set of keywords. Each of these attributes is a string (or set of strings). Every document is labeled with its publication venue v ∈ V . Deﬁnition 1 (Paper Corpus). A (research) paper a set of keywords s ⊆ S and is labeled with a venue name v ∈ V . We use non-negative matrix factorization [8] to create a topic model from a corpus of papers and to obtain a lower-dimensional representation of the papers in topic space. It has previously been shown that NMF is able to reconstruct research topics that are similar to research categories produced by human experts [17]. For an input matrix as tf-idf vectors), NMF ﬁnds an approximate factorization matrices desired lower-dimensional representation of the input documents (represented as a weighted sum of the basis vectors). The as vectors of term importances, i.e. topics. The lower-dimensional document representations can be interpreted as proportions of topics in a document. We may stress that NMF, due to its additive nature, allows for human interpretable topics as well as document representations as vectors. We will use research topics calculated through NMF as an attempt towards recommendation explanations. The proposed approach for creating explanations is independent from the concrete choice of the classiﬁcation method. Hence, we are able to evaluate a multitude of procedures. We also evaluate diﬀerent methods for feature extraction and paper representation. In particular, we use tf-idf vectors [15] as well as the aforementioned NMF topics as features. This being said, we emphasize in our investigation the logistic regression approach, since this has already been successful in previous work [6, 4]. Additionally, logistic regression is a robust procedure that does not require a large number of parameter values to be identiﬁed. Furthermore, the resulting classiﬁcation function can be computed fast, unlike e.g., similarity-based approaches [20]. Ultimately, linear models such as logistic regression are already interpretable to some extent [2]. In contrast to [6], we prefer tf-idf over tf (i,e., W ∈ RandH ∈ Rcontain a set of basis vectors as well as the term frequencies), because we assume, as being one de-facto standard in text representation, it may enable better classiﬁcation results. In our experiments we compare our results to Kobs et al. [6]. We use the following methods: in random order. prevalence in the data set. multi-class logistic regression from scikit-learn [22, 11]. We apply these classiﬁers to diﬀerent document representations. For every abstract and keywords before calculating the representations: the representation calculated through NMF [8] using a reasonable topic number. We adapt the choice for the maximum training iterations per batch for matrices values in Section 5. the product of tf and inverted document frequency (idf). concatenate both of the above representations to a single vector. We evaluate our methods on two diﬀerent domains: Artiﬁcial intelligence (AI) and medicine (MED). More speciﬁcally, we use the data sets described by Kobs et al. [6], which are based on Semantic Scholar [1]. Each data set is comprised of papers from 78 non-uniformly distributed classes (i.e., venues). The AI corpus contains 245,573 papers; the MED corpus 2,924,609 papers. To facilitate comparability we calculate the the mean reciprocal rank (MRR) on a test data set of papers. For ranking for a tested paper is counted as correct, when its true label is contained in the top Iana et al. [4] used the term calculates the same. MRR is calculated from a test set rank(x, C generated by algorithmP to account for its larger size. We set to 300 and h_max_iter to 100. This led to better training convergence. Table 1 (left) depicts our scores in comparison to the state-of-the-art WTS. Notably, on AI, we ﬁnd that logistic regression achieves higher values for all performance measures. This is remarkable, given the simplicity of logistic regression and the complexity of WTS. Kobs et al. [6] also evaluated logistic regression, however, using mere term frequencies instead of tf-idf. We hence added an experiment on AI to conﬁrm that tf-idf is favorable. On the larger MED corpus, WTS https://github.com/konstantinkobs/wts kranked venues. This count is divided by the total size of the test set. )∈ Nbe the rank of the true label of publicationx ∈ Tin a ranking Table 1: Left: Results of diﬀerent recommendation methods. Results for WTS are taken from Kobs et al. [6]. Right: Top 3 recommendations and top 3 topics for the BERT paper [3]. Our recommendation NAACL is the true publication venue. exhibits the best performance scores. This shows that the employed TextCNN and Word2Vec proﬁt from large amounts of training data, a fact that has been frequently stated for neural networks before. MED is a very diverse data set covering venues from the ﬁelds chemistry, medicine, physics and more. In practice, the domain-focussed AI corpus is a more realistic recommendation scenario. Adding NMF-topics to tf-idf vectors led to slightly higher scores for AI and MED. The eﬀect might be larger when NMF is trained on an additional corpus or when NMF variants incorporating class information are used [18]. Pure NMF representations led to lower scores than tf-idf. Yet, despite their few vector components they are in the range of the tf representations. To a certain degree, NMF topics allow for an interpretation and assessment of recommendations. Table 1 (right) depicts recommendations and topics for an example paper The true venue is ranked second. The identiﬁed topics of top-weighted terms, are interpretable as question answering, natural language processing and neural networks. The venue topics are often strongly related and, where not, may give a hint for further recommendation assessment. We presented methods towards explainable scientiﬁc venue recommendations. First, we showed that logistic regression with tf-idf is a competitive recommendation setup. Second, we illustrated a principled approach for annotating recommendations with topics derived from a research corpus. The so-provided contrasting juxtaposition using topics allows for explanations that exceed the Demonstration available at https://sci-rec.org content of the input paper (i.e., query). We envision that future work should target automated topic labeling [7] to further increase topic comprehensibility.