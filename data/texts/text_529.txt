<title>Two-sided fairness in rankings via Lorenz dominance</title> <title>Abstract</title> We consider the problem of generating rankings that are fair towards both users and item producers in recommender systems. We address both usual recommendation (e.g., of music or movies) and reciprocal recommendation (e.g., dating). Following concepts of distributive justice in welfare economics, our notion of fairness aims at increasing the utility of the worse-off individuals, which we formalize using the criterion of Lorenz efﬁciency. It guarantees that rankings are Pareto efﬁcient, and that they maximally redistribute utility from better-off to worse-off, at a given level of overall utility. We propose to generate rankings by maximizing concave welfare functions, and develop an efﬁcient inference procedure based on the Frank-Wolfe algorithm. We prove that unlike existing approaches based on fairness constraints, our approach always produces fair rankings. Our experiments also show that it increases the utility of the worse-off at lower costs in terms of overall utility. <title>1 Introduction</title> <title>arXiv:2110.15781v1  [cs.IR]  28 Oct 2021</title> of the least served users (e.g., so that least served users do not support the cost of item-side fairness), once again at the expense of average user utility. The goal of this paper is to provide an algorithmic framework to generate rankings that achieve a variety of these trade-offs, leaving the choice of a speciﬁc trade-off to the practitioner. The leading approach to fairness in rankings is to maximize user utility under constraints of equal item exposure (or equal quality-weighted exposure) [ 54 ] or equal user satisfaction [ ]. When these constraints imply an unacceptable decrease in average user utility, so-called “trade-offs between utility and fairness” [ 65 41 ] are obtained by relaxing the fairness constraints, leading to the optimization of a trade-off between average user utility and a measure of users’ or items’ inequality. Thinking about fairness in terms of optimal utility/inequality trade-offs has, however, two fundamental limitations. First, the optimization of a utility/inequality trade-off is not necessarily Pareto-efﬁcient from the point of view of users and items: it sometimes chooses solutions that decrease the utility of some individuals without making anybody else better off. We argue that reducing inequalities by decreasing the utility of the better-off is not desirable if it does not beneﬁt anyone. The second limitation is that focusing on a single measure of inequality does not address the question of how inequality is reduced, and in particular, which fraction of the population beneﬁts or bears the cost of reducing inequalities. In this paper, we propose a new framework for two-sided fairness in rankings grounded in the analysis of generalized Lorenz curves of user and item utilities. Widely used to study efﬁciency and equity in cardinal welfare economics [ 53 ], these curves plot the cumulative utility obtained by fractions of the population ordered from the worst-off to the best-off. A curve that is always above another means that all fractions of the populations are better off. We deﬁne fair rankings as those with non-dominated generalized Lorenz curves for users and items. First, this deﬁnition guarantees that fair rankings are Pareto efﬁcient. Second, examining the entirety of the generalized Lorenz curves provides a better understanding of which fractions of the population beneﬁt from an intervention, and which ones have to pay for it. We present our general framework based on Lorenz dominance in usual recommendation settings (e.g., music or movie recommendation), and also show how extend it to reciprocal recommendation tasks such as dating applications or friends recommendation, where users are recommended to other users. We present a new method for generating rankings based on the maximization of concave welfare functions of users’ and items’ utilities. The parameters of the welfare function control the relative weight of users and items, and how much focus is given to the worse-off fractions of users and items. We show that rankings generated by maximizing our welfare functions are fair for every value of the parameters. Our framework does not aim at deﬁning what parameters are suitable in general — rather, the choice of a speciﬁc trade-off depends on the application. From an algorithmic perspective, two-sided fairness is challenging because items’ utilities depend on the rankings of all users, requiring global inference. Previous work on item-side fairness addressed this issue with heuristic methods without guarantees or control on the achievable trade-offs. We show how the Frank-Wolfe algorithm can be leveraged to make inference tractable, addressing both our welfare maximization approach and existing item-side fairness penalties. We demonstrate that our welfare function approach enjoys stronger theoretical guarantees than existing methods. While it always generates rankings with non-dominated generalized Lorenz curves, many other approaches do not. We show that one of the main criteria of the literature, called equity of attention by Biega et al. [7] , can lead to decrease user utility, while increasing inequalities of exposure between items. Moreover, equal user satisfaction criteria in reciprocal recommendation can lead to decrease the utility of every user, even the worse-off. Our notion of fairness prevents these undesirable behaviors. We report experimental results on music and friend recommendation tasks, where we analyze the trade-offs obtained by different methods by looking at different points of their Lorenz curves. Our welfare approach generates a wide variety of trade-offs, and is, in particular, more effective at improving the utility of worse-off users than the baselines. We present our formal framework in Section 2. We discuss the theoretical properties of previous approaches in Section 3, and present our ranking algorithm in Section 4. Our experiments are described in Section 5, and the related work is discussed in Section 6. <title>2 Two-sided fairness via Lorenz dominance</title> Terminology and notation. We identify an item with its producer, so that “item utility” means “item producer’s utility”. The main paper focuses on fairness towards individual users and items. We describe in Appendix B the extension of our approach to sensitive groups of users or items. |X| denotes the cardinal of the set . Given n ∈ N , we denote by [[n]] = {1, . . . , n} . The set of users is identiﬁed with {1, ..., |N|} and the set of items is identiﬁed with {|N| + 1, ..., n} where n = |N| + |I|. For (i, j) ∈ N × I, we denote by µ the value of item j to user i. A (deterministic) ranking σ : I → [[|I|]] is a one-to-one mapping from items to their rank σ(j) Following [ 54 ], we use stochastic rankings because they allow us to perform inference using convex optimization (see Section 4). The recommender system produces one stochastic ranking per user, represented by a 3-way ranking tensor where is the probability that is recommended to at rank k. We denote by P the set of ranking tensors. Figure 1: Generalized Lorenz curves for usual (left) and reciprocal (right) recommendation. Utilities of users and items are deﬁned through a position-based model, as in previous work [ 54 63 ]. Let v ∈ R , where is the exposure weight at rank . We assume that lower ranks receive more exposure, so that ∀k ∈ [[|I| − 1]], v ≥ v ≥ 0 Given a user and a ranking , the user-side utility of is the sum of the s weighted by the exposure weight of their rank (j) (σ ) = . Given an item , the item-side utility of is the sum over users of the exposure of to i. These deﬁnitions extend to stochastic rankings by taking the expectation over rankings, written in matrix form: user-side utility: u (P ) = v item-side utility (exposure): u (P ) = We denote by u(P ) = (u (P )) the utility proﬁle for , and by U = {u(P ) : P ∈ P} the set of feasible proﬁles. For u ∈ U = (u and = (u denote the utility proﬁles of users and items respectively. Two-sided fairness in rankings. In practice, values of are not known to the recommender system. Ranking algorithms use an estimate ˆµ of based on historical data. We address here the problem of inference: the task is to compute the ranking tensor given ˆµ , with the goal of making fair trade-offs between (true) user and item utilities. Notice that the user-side utility depends only on the ranking of the user, but for every item, the exposure depends on the rankings of all users. Thus, accounting for both users’ and items’ utilities in the recommendations is a global inference problem. More general item utilities We consider exposure as the item-side utility to follow prior work and for simplicity. Our framework and algorithm readily applies in a more general case of two-sided preferences, where items also have preferences over users (for instance, in hiring, job seekers have preferences over which recruiters they are recommended to). Denoting the value of user to item j, the item side-utility is then u (P ) = v. Our notion of fairness aims at improving the utility of the worse-off users and items. Since this does not prescribe exactly which fraction of the worse-off users/items should be prioritized, the assessment of trade-offs requires looking at all fractions of the population. This is captured by the generalized Lorenz curve used in cardinal welfare economics [ 53 ]. Formally, given a utility proﬁle , let (u be the sorted values in from smallest to largest, i.e., ≤ . . . ≤ u , then the generalized Lorenz curve plots (U where = u + . . . + u . To assess the fairness of trade-offs, we rely on the following dominance relations on utility proﬁles: Pareto-dominance  . u  ⇐⇒ (∀i ∈ [[n]], u ≥ u and ∃i ∈ [[n]], u > u ). Lorenz-dominance  . Then u  ⇐⇒ U  We write for non-strict Lorenz dominance (i.e., ∀i, U ≥ U ). Notice that Pareto-dominance implies Lorenz-dominance. Our notion of fairness, which we call Lorenz efﬁciency, states that a ranking is fair if the utility proﬁles for users and for items are not jointly Lorenz-dominated: Deﬁnition 1 (Lorenz efﬁciency) A utility proﬁle u ∈ U is Lorenz-efﬁcient if there is no ∈ U such that either (u and u ) or (u and u ). We consider that Lorenz-dominated proﬁles are undesirable (and unfair) because the utility of worseoff fractions of the population could have been increased at no cost for total utility. Examples of Lorenz-curves of users and items are given in Fig. 1. The blue solid, green dotted and orange dashed curves are all non-dominated (the blue solid ranking has higher user utility but high item inequality, the green dotted and orange dashed curves have similar item exposure proﬁles, but user curves that intersect). On the other hand, the red dot/dashed curve is an unfair ranking: compared to the green dotted and orange dashed curve, all fractions of the worse off users have lower utility, together with less exposure for worse-off items. A fundamental result from cardinal welfare economics is that concave welfare functions of utility proﬁles order proﬁles according to Lorenz dominance [ 53 ]. The choice of the welfare function speciﬁes which (fair) trade-off is desirable in a speciﬁc context. This result holds when all utilities are comparable. In our case where there are users and items, we propose the following welfare function parameterized by θ = (λ, α , α ): Inference is carried out by maximizing W (an efﬁcient algorithm is proposed in Section 4): In λ ∈ [0, 1] controls the relative weight of users and items. The motivation for the speciﬁc choice of is that it appears in scale invariant welfare functions [ 43 ], but other families can be used as long as the functions are increasing and concave. Monotonicity implies that maxima of are Paretoefﬁcient. For < 1 and < 1 is strictly concave. Then, exhibits diminishing returns, which is the key to Lorenz efﬁciency: an increment in utility for a worse-off user/item increases welfare more than the same increment for a better-off user/item. The effect of the parameters is shown in Fig. 1 (left): For item fairness we obtain more item equality by using < 1 (here, = 0.5 ) and incrasing (see blue solid vs orange dashed curve). The parameter controls user fairness: smaller values yield more user utility for the worse-off users at the expense of total utility, with similar item exposure curve (green dotted vs orange dahsed curves). Let Θ = {(λ, α , α ) ∈(0, 1) × (−∞,1) For every θ ∈ Θ, W is strictly concave, and users and items have non-zero weight. We then have (the result is a straightforward consequence of diminishing returns, see Appendix C): Proposition 1. ∀θ ∈ Θ, ∀P ∈ argmax (u(P )), P is Lorenz-efﬁcient. Relationship to inequality measures A well-known measure of inequality is the Gini index, deﬁned as 1 − 2 × AULC , where AULC is the area under the Lorenz curve. The difference between Lorenz and generalized Lorenz curves is that the former is normalized by the cumulative utility. This difference is fundamental: we can decrease inequalities while dragging everyone’s utility to 0. However, this would lead to dominated generalized Lorenz curves. Interestingly, for item-side fairness, the cumulative exposure is a constant and thus trade-offs between user utility and item exposure inequality are not really problematic. However, for user-side fairness, the total utility is not constant and reducing inequalities might require dragging the utility of some users down for the beneﬁt of no one. Additional theoretical results In App. C.2, we show that as , α → −∞, utility proﬁles tend to leximin-optimal solutions [ 43 ]. Leximin optimality corresponds to increasing the utility of the worst-off users/items one a a time, similarly to a lexical order. In App. C.3, we present an excess risk bound, which provides theoretical guarantees on the true welfare when computing rankings based on estimated preferences, depending on the quality of the estimates. In reciprocal recommendation problems such as dating, the users are also items. The notion of fairness simpliﬁes to increasing the utility of the worse-off users, which can in practice be done by boosting the exposure of worse-off users. Our framework above applies readily by taking N = I and n = |N| . The critical step however is to redeﬁne the utility of a user to account for the fact that (1) the user utility comes from both the recommendation they receive and who they are recommended to, and (2) users have preferences over who they are recommended to. With this deﬁnition of two-sided utility, our previous framework can be readily applied using N = I A (two-sided) utility proﬁle u ∈ U is Lorenz-efﬁcient if there is no ∈ U such that . The welfare function simpliﬁes to (u) = ψ(u , α) , and Proposition 1 also holds true in this setting: maximizing the welfare function always yields Lorenz-efﬁcient rankings. Fig. 1 (right) illustrates how decreasing increases utilities for the worse-off users at the expense of total utility. It also shows a Lorenz-dominated (unfair) proﬁle, in which all fractions from the worst-off to the better-off users have lower utility. From now on, we refer to one-sided recommendation for non-reciprocal recommendation. <title>3 Comparison to utility/inequality trade-off approaches</title> As stated in the introduction, leading approaches to fairness in ranking are based on utility/inequality trade-offs. We describe here the representative approaches we consider as baselines in our experiments. We then present theoretical results illustrating the undesirable behavior of some of them. One-sided recommendation In one-sided recommendation, the leading approach is to deﬁne exposure-based criteria for item fairness [ 54 ]. The ﬁrst criterion, equality of exposure, aims at equalizing exposure across items. The second one, quality-weighted exposure , which is advocated by many authors, deﬁnes the quality of an item as the sum of user values and aims for item exposure proportional to quality. The motivation of quality-weighted exposure is to take user utilities into account in the extreme case where the constraint is strictly enforced. Interestingly, as we show later, this approach has bad properties in terms of trading off user and item utilities. equality of (u) = − β D(u) with D(u) = exposure |I| Some authors use (u) = instead of 55 42 ]. and have qualitatively the same behavior. We propose (u) as a computationally efﬁcient alternative to since it involves only a linear number of terms and is convex and differentiable except on 0. Reciprocal recommendation For reciprocal recommendation, we consider as competing approach a trade-off between total (two-sided) utility and inequality of utilities, as measured by the standard deviation: equality of (u) = − β D(u) with D(u) = utility |I| We point out here to two deﬁciencies of previous approaches. For every d ∈ N and every N ∈ N , there is a one-sided recommendation problem, with d + 1 items and N(d + 1) users, such that ∀θ ∈ Θ, we have: ∃β > 0, u and u and lim −−−→ Second, in reciprocal recommendation, striving for pure equality can even lead to utility for every user, even that of the worst-off user. More precisely, we show that in some cases, compared to the welfare approach with any choice of parameter θ ∈ Θ , there exists β > 0 such that equality of utility has lower utility for every user, eventually leading to 0 utility for everyone in the limit β → ∞. Proposition 3. For β > 0 , let = argmax (u) . The claim below holds irrespective of the choice of ∈ U . Let n ≥ 5 . There is a reciprocal recommendation task with users such that: ∀θ ∈ Θ, u , ∃β > 0 : ∀i ∈ [[n]], u > u and lim = 0. Proofs and additional results All proofs are deferred to App. D, where we provide several additional results regarding the use of quality-weighted exposure and equality of exposure in reciprocal recommendation: We show in Prop. 8 that there are cases where both approaches lead to user utility proﬁles with Lorenz-dominated curves, and signiﬁcantly lower total user utility than the welfare approach for any choice of the parameters. <title>4 Efﬁcient inference of fair rankings with the Frank-Wolfe algorithm</title> We now present our inference algorithm for (1) . Appendix E contains the proofs of this section and describes a similar approach for the objective functions of the previous section. From an abstract perspective, the goal is to ﬁnd a maximum P such that: rankings of all users, so a naive approach would require |N||I| parameters and 2|N||I| linear constraints. The same problem arises with the penalties of previous work. In the literature, authors either considered applying the item-fairness constraints to each ranking individually [ 54 ], which leads to inefﬁciencies with our deﬁnition of utility (see Appendix H), or resort to heuristics to compute the rankings one by one without guarantees on the trade-offs that are achieved [42, 7]. Our approach is based on the Frank-Wolfe algorithm [ 18 ], which was previously used in machine learning in e.g., structured output prediction or low-rank matrix completion [ 30 ], but to the best of Figure 2: Summary of results on Lastfm-2k, focusing on the user utility/item inequality trade-off. our knowledge not for ranking. Denoting hX |Y i = the dot product between tensors, the algorithm creates iterates by ﬁrst computing P = argmax hP |∇W (P )i and then updating = (1 − γ )P + γ with 13 ]. Starting from an initial solution the algorithm always stays in the feasible region without any additional projection step. Our main contribution of this section is to show that argmax hP |∇W (P )i can be computed efﬁciently, requiring only one sort operation per user after computing the utilities. In the result below, for a ranking tensor P and a user i, we denote by S(P ) the support of P in ranking space. Theorem 1. Let ˜µ = Φ (P + Φ (P . Let P such that: ∀i ∈ N, ∀˜σ ∈ S( ): ˜σ (j) < ˜σ (j ) =⇒ ˜µ ≥ ˜µ . Then P ∈ argmax hP |∇W (P )i. Moreover, it produces a compact representation of the stochastic ranking as a weighted sum of permutation matrices. The number of iterations of the algorithm allows to control the trade-off between memory requirements and accuracy of the solution. Using previous convergence results for the Frank-Wolfe algorithm [13], assuming each Φ is bounded, we have: Proposition 4. Let B = max kΦ and U = max kuk . Let be the maximum index of a nonzero value in (or |I| ). Then ∀t ≥ 1, W (P ) ≥ max W (P ) − O( . Moreover, for each user, an iteration costs O(|I|ln K) operations and requires O(K) additional bytes of storage. <title>5 Experiments</title> We ﬁrst present experiments on movie recommendation task. We report here our experiments with the Lastfm-2k dataset [ 47 ], which contains the music listening histories of 1.9k users. We present in App. F.2 experiments on a larger portion of the Last.fm dataset, and in App. F.3 results using the MovieLens-20m dataset [24]. Our results are qualitatively similar across the three datasets. We select the top 2500 items most listened to, and estimate preferences with a matrix factorization algorithm using a random sample of 80% of the data. All experiments are carried out with three repetitions for this subsample. The details of the experimental protocol are in App. F.1. Since the goal is to analyze the behavior of the ranking algorithms rather than the quality of the preference estimates, we consider the estimated preferences as ground truth when computing user utilities and comparing methods, following previous work. We compare our welfare approach (welf) to three baselines. The ﬁrst one is the algorithm of [ 47 ] (referred to as Patro et al. in the ﬁgures), who consider envy-freeness for user-side fairness and, for item-side fairness, a constraint that the minimum exposure of an item is where is the trade-off parameter. The other baselines are quality-weighted exposure (qua.-weighted) and equality of exposure (eq. exposure) as described in Sec. 3. Item-side fairness We ﬁrst study in isolation item-side fairness, deﬁned as improving the exposure of the worse-off item (producers). To summarize the trade-offs, we show the trade-offs by looking at exposure inequalities as measured by the Gini index (see Sec. 2.2). The results are given in Fig. 2: Generating user utility/item inequality trade-offs is performed with our approach by keeping = α = 0 and varying the relative weight of items . Fig. 2a plots some trade-offs achieved by our approach. As expected, the user utility curve degrades as we increase the weight of items, while at the same time the curve of item exposure moves towards the straight line, which corresponds to strict equality of exposure. Fig. 6 in the appendix provides analogous curves for all methods, obtained by varying the weight β of the inequality measure. qua.-weighted yields unfair trade-offs Fig. 2c shows a welf ranking that dominates a qua.-weighted ranking on both user and item curves. This is in line with the discussion of Section 3, qua.-weighted can lead to unfair rankings on utility/item inequality trade-offs. welf dominates the user utility/item inequality (Gini) trade-offs as seen on Fig. 2b: while all methods have the same total user utility when accepting high item inequality, welf dominates Patro et al., eq. exposure and qua.-weighted as soon as Gini ≤ 0.5 . Note, however, that the Gini index is only one measure of inequality. When measuring item inequalities with the standard deviation, eq. exposure becomes optimal since our implementation optimizes a trade-off with this measure (see Fig. 8 in App. F.1). Overall, welf and eq. exposure yield different fair trade-offs. Two-sided fairness Fig. 3 shows the effect of the user curvature ∈ {−2, 0, 1} , keeping = 0 Fig. 8 in App. F.1 shows similar plots when the item inequality is measured by the standard deviation rather than the Gini index. Smaller reduce user inequalities at the expense of total user utility, at various levels of item inequality. This is observed by comparing the results for α ∈ {−2, 0, 1} in Fig. 3a and Fig. 3b. welf = 0 is better than Patro et al., which can be seen by jointly looking at Fig. 3c, 3d and Fig. 3b which give the cumulative utility at different points of the Lorenz curve ( 10% 25% and 100% of the users respectively). We observe that welf = 0 is similar to Patro et al. at the 10% and 25% levels, but has higher total utility. Example curves are given in Fig. 3e and 3f which plot welf = 0 and Patro et al. at two levels of item inequality. welf = 0 obtains similar curves to Patro et al., except that it performs better at the end of the curve. A similar comparison can be made with welf α = 1 and eq. exposure. More user inequalities is not necessarily unfair as seen in Fig. 3a comparing welf = 0 and Patro et al.. We observe that welf = 0 has slightly higher Gini index, but this is not unfair: as seen in Fig. 3e and 3f, this is due to the higher utility at the end of the generalized Lorenz curve of welf, but the worse-off users have similar utilities with welf and Patro et al.. We now present results on a reciprocal recommendation task, where fairness refers to increasing the utility of the worse-off users (this can be done by boosting their exposure at the expense of total utility). Since there is no standard benchmark for reciprocal recommendation, we generate an artiﬁcial task based on the Higgs Twitter dataset [ 15 ], which contains follower links, and address the task of ﬁnding mutual followers (i.e., “matches”). We keep users having at least 20 mutual links, resulting in a subset of 13 k users. We build estimated match probabilities using matrix factorization. The experimental protocol is detailed in App. F.4. We also present in App. F.5 additional experiments using the Epinions dataset [49]. The results are qualitatively similar. Our main baseline is equal utility (eq. utility) deﬁned in Section 3. We also compare to qualityweighted exposure, and equality of exposure as baselines that ignore the reciprocal nature of the task. The results are summarized in Fig. 4: Example of trade-offs obtained by varying are plotted in Fig. 4a. As decreases, the utility increases for the worse-off users at the expense of better-off users. We note that increasing the utility of worse-off users has a massive cost on total user utility: looking at the exact numbers we observe that α = −5 has more than doubled the cumulative utility of the 10% worse off users compared to α = 1 (120 vs 280), but at the cost of more than 60% of the total utility ( 17 k vs 6.4 k). Fig. 6 in Appendix F.4 contains plots of the trade-offs achieved by the other methods. qua.-weighted and eq. exposure are dominated by welf on a large range of hyperparameters. An example is given in Fig. 4b, where welf α = 0.5 already dominates some of their models, even though in this region of there is little focus on worse-off users. More generally, all values of β ≥ 0.1 for qua.-weighted and eq. exposure lead to rankings with dominated curves. This is expected since they ignore the reciprocal nature of the task. eq. utility is dominated by welf near strict equality as illustrated in Fig. 4c: for large values of it is not possible to increase the utility of the worse off users, and eq. utility only drags utility of better-off users down. welf is more effective at increasing utility of the worse-off users as can be seen in Fig. 4e-g, which plots the total utility as a function of the cumulative utility at different points of the Lorenz curve 10% 20% 50% worse-off users respectively). For total utilities larger than 50% of the maximum achievable, welf signiﬁcantly dominates eq. utility in terms of utility of worse-off users (10% and 25%) at a given level of total utility. welf also dominates eq. utility on the 50% worse-off users (Fig. 4h) in the interesting region where the total utility is within 20% of the maximum. More inequality is not necessarily unfair As shown in Fig. 4d, we see that for the same utility for the 10% worse-off users, welf models have higher inequalities than eq. utility. As seen before, this higher inequality is due to a higher total utility (and higher total utilities for the 25% worse-off users. The analysis of these Lorenz curves allow us to conclude that these larger inequalities are not due to unfairness. They arise because welf optimizes the utility of the worse-off users at lower cost in terms of average utility than eq. utility. <title>6 Related work</title> The question of fairness in rankings originated from independent audits on recommender systems or search engines, which showed that results could exhibit bias against relevant social groups 57 33 21 40 35 ] Our work follows the subsequent work on ranking algorithms that promote fairness of exposure for individual or sensitive groups of items [ 10 54 42 65 ]. The goal is often to prevent winner-take-all effects, combat popularity bias [ ] or promote smaller producers [ 39 41 ]. Section 3 is devoted to the comparison with this type of approaches. Most of these works use a notion of fairness oriented towards items only. Towards two-sided fairness, Wang and Joachims [60] promote user-side fairness using concave functions of user utilities, similarly to us. Other works use equality constraints to deﬁne user-side fairness [ 63 ]. These three approaches rely on the deﬁnitions of item-side fairness discussed in Section 3. Patro et al. [47] generate rankings that are envy-free on the user side, and guarantees the fair min-share for items. This approach is not amenable to controllable trade-offs between user and item utilities. We are the ﬁrst to address one-sided and reciprocal recommendation within the same framework. There is less existing work studying the fairness of rankings in the reciprocal setting. Xia et al. [64] aim at equalizing user utility between groups, which suffers from the problems discussed in Figure 4: Results on the twitter dataset. Section 3. Jia et al. [31] generate rankings using a welfare function approach, but optimizing only the utility of users being recommended. Paraschakis and Nilsson [46] postprocess rankings to correct for inconsistencies between estimated and declared preferences of users. In contrast, we aim at fair trade-offs between user and item utilities, under the assumption that biases in the preference estimates have been addressed earlier in the recommendation pipeline. Fairness is also studied in the context of ridesharing applications [62, 37, 44], but they address matching rather than ranking problems. There is growing interest in making the relationship between fairness in machine learning and social choice theory [ 25 59 20 27 12 16 17 ], and welfare economics in particular [ 56 28 34 36 67 ]. In line with Hu and Chen [28] , who focused on classiﬁcation and parity penalties, we argue that Pareto-efﬁciency should be part of fairness assessments. We are the ﬁrst to propose concave welfare functions and Lorenz dominance to address two-sided fairness in recommendation. <title>7 Conclusion</title> We view fairness in rankings as optimizing the distribution of user and item utilities, giving priority to the worse-off. Following this view, we deﬁned fair rankings as having non-dominated generalized Lorenz curves of user and item utilities, and develop a new conceptual and algorithmic framework for fair ranking. The generality of the approach is showcased on several recommendation tasks, including reciprocal recommendation. The expected positive societal impact of this work is to provide more principled approaches to mediating between several parties on a recommendation platform. Yet, we did not address several questions that are critical for the deployment of our approach. In particular, true user preferences are often not directly available, and we only observe proxies to them, such as clicks or likes. Second, interpersonal comparisons of utilities are critical in this work. It is thus necessary to make sure that the proxies we choose lead to meaningful comparisons of utilities between users. Third, estimating preferences or their proxies is itself not trivial in recommendation because of partial observability. The true fairness of our approach is bound to a careful analysis of (at least) these additional steps. <title>Acknowledgments and Disclosure of Funding</title> We thank David Lopez-Paz, Jérôme Lang, as well as the anonymous NeurIPS reviewers for their constructive feedback on early versions of the paper. <title>References</title> [1] H. Abdollahpouri, M. Mansoury, R. Burke, and B. Mobasher. The unfairness of popularity bias in recommendation. arXiv preprint arXiv:1907.13286, 2019. [2] S. Agarwal. Surrogate regret bounds for bipartite ranking via strongly proper losses. The Journal of Machine Learning Research, 15(1):1653–1674, 2014. [3] A. B. Atkinson. On the measurement of inequality. Journal of economic theory, 2(3):244–263, 1970. [4] M.-F. Balcan, T. Dick, R. Noothigattu, and A. D. Procaccia. Envy-free classiﬁcatoion. arXiv preprint arXiv:1809.08700, 2018. [5] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006. [6] K. Basu, C. DiCiccio, H. Logan, and N. E. Karoui. A framework for fairness in two-sided marketplaces. arXiv preprint arXiv:2006.12756, 2020. [7] A. J. Biega, K. P. Gummadi, and G. Weikum. Equity of attention: Amortizing individual fairness in rankings. In The 41st international acm sigir conference on research & development in information retrieval, pages 405–414, 2018. [8] R. Burke. Multisided fairness for recommendation. arXiv preprint arXiv:1707.00093, 2017. [9] I. Cantador, P. Brusilovsky, and T. Kuﬂik. 2nd workshop on information heterogeneity and fusion in recommender systems (hetrec 2011). In Proceedings of the 5th ACM conference on Recommender systems, RecSys 2011, New York, NY, USA, 2011. ACM. [10] L. E. Celis, D. Straszak, and N. K. Vishnoi. Ranking with fairness constraints. arXiv preprint arXiv:1704.06840, 2017. [11] O. Celma. Music Recommendation and Discovery in the Long Tail. Springer, 2010. [12] A. Chakraborty, G. K. Patro, N. Ganguly, K. P. Gummadi, and P. Loiseau. Equality of voice: Towards fair representation in crowdsourced top-k recommendations. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 129–138. ACM, 2019. [13] K. L. Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):1–30, 2010. [14] D. Cossock and T. Zhang. Statistical analysis of bayes optimal subset ranking. IEEE Transactions on Information Theory, 54(11):5140–5154, 2008. [15] M. De Domenico, A. Lima, P. Mougel, and M. Musolesi. The anatomy of a scientiﬁc rumor. Scientiﬁc reports, 3(1):1–9, 2013. [16] V. Do, S. Corbett-Davies, J. Atif, and N. Usunier. Online certiﬁcation of preference-based fairness for personalized recommender systems. arXiv preprint arXiv:2104.14527, 2021. [17] J. Finocchiaro, R. Maio, F. Monachou, G. K. Patro, M. Raghavan, A.-A. Stoica, and S. Tsirtsis. Bridging machine learning and mechanism design towards algorithmic fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 489–503, 2021. [18] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95–110, 1956. [19] C. Gini. Measurement of inequality of incomes. The economic journal, 31(121):124–126, 1921. [20] P. Gölz, A. Kahng, and A. D. Procaccia. Paradoxes in fair machine learning. Advances in Neural Information Processing Systems (NeurIPS). Forthcoming, 2019. [21] A. Hannak, G. Soeller, D. Lazer, A. Mislove, and C. Wilson. Measuring price discrimination and steering on e-commerce web sites. In Proceedings of the 2014 conference on internet measurement conference, pages 305–318, 2014. [22] M. Hardt, E. Price, N. Srebro, et al. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems, pages 3315–3323, 2016. [23] G. H. Hardy, J. E. Littlewood, and G. Pólya. Inequalities. 2nd ed. Cambridge, Engl.: At the University Press. XII, 324 p. (1952)., 1952. [24] F. M. Harper and J. A. Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1–19, 2015. [25] H. Heidari, C. Ferrari, K. Gummadi, and A. Krause. Fairness behind a veil of ignorance: A welfare analysis for automated decision making. In Advances in Neural Information Processing Systems, pages 1265–1276, 2018. [26] H. Heidari, M. Loi, K. P. Gummadi, and A. Krause. A moral framework for understanding fair ml through economic models of equality of opportunity. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 181–190, 2019. [27] S. Hossain, A. Mladenovic, and N. Shah. Designing fairly fair classiﬁers via economic fairness notions. In Proceedings of The Web Conference 2020, pages 1559–1569, 2020. [28] L. Hu and Y. Chen. Fair classiﬁcation and social welfare. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 535–545, 2020. [29] Y. Hu, Y. Koren, and C. Volinsky. Collaborative ﬁltering for implicit feedback datasets. In 2008 Eighth IEEE International Conference on Data Mining, pages 263–272. Ieee, 2008. [30] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International Conference on Machine Learning, pages 427–435. PMLR, 2013. [31] Y. Jia, X. Liu, and W. Xu. When online dating meets nash social welfare: Achieving efﬁciency and fairness. In Proceedings of the 2018 World Wide Web Conference, pages 429–438, 2018. [32] C. C. Johnson. Logistic matrix factorization for implicit feedback data. Advances in Neural Information Processing Systems, 27(78):1–9, 2014. [33] M. Kay, C. Matuszek, and S. A. Munson. Unequal representation and gender stereotypes in image search results for occupations. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pages 3819–3828. ACM, 2015. [34] J. Kleinberg, J. Ludwig, S. Mullainathan, and A. Rambachan. Algorithmic fairness. In Aea papers and proceedings, volume 108, pages 22–27, 2018. [35] A. Lambrecht and C. Tucker. Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads. Management Science, 65(7):2966–2981, 2019. [36] M. S. A. Lee, L. Floridi, and J. Singh. Formalising trade-offs beyond algorithmic fairness: lessons from ethical philosophy and welfare economics. AI and Ethics, pages 1–16, 2021. [37] N. S. Lesmana, X. Zhang, and X. Bei. Balancing efﬁciency and fairness in on-demand ridesourcing. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [38] D. Lim, J. McAuley, and G. Lanckriet. Top-n recommendation with missing implicit feedback. In Proceedings of the 9th ACM Conference on Recommender Systems, pages 309–312, 2015. [39] W. Liu, J. Guo, N. Sonboli, R. Burke, and S. Zhang. Personalized fairness-aware re-ranking for microlending. In Proceedings of the 13th ACM Conference on Recommender Systems, pages 467–471, 2019. [40] R. Mehrotra, A. Anderson, F. Diaz, A. Sharma, H. Wallach, and E. Yilmaz. Auditing search engines for differential satisfaction across demographics. In Proceedings of the 26th international conference on World Wide Web companion, pages 626–633, 2017. [41] R. Mehrotra, J. McInerney, H. Bouchard, M. Lalmas, and F. Diaz. Towards a fair marketplace: Counterfactual evaluation of the trade-off between relevance, fairness & satisfaction in recommendation systems. In Proceedings of the 27th acm international conference on information and knowledge management, pages 2243–2251, 2018. [42] M. Morik, A. Singh, J. Hong, and T. Joachims. Controlling fairness and bias in dynamic learning-to-rank. arXiv preprint arXiv:2005.14713, 2020. [43] H. Moulin. Fair division and collective welfare. MIT press, 2003. [44] V. Nanda, P. Xu, K. A. Sankararaman, J. Dickerson, and A. Srinivasan. Balancing the tradeoff between proﬁt and fairness in rideshare platforms during high-demand hours. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 2210–2217, 2020. [45] I. Palomares, C. Porcel, L. Pizzato, I. Guy, and E. Herrera-Viedma. Reciprocal recommender systems: Analysis of state-of-art literature, challenges and opportunities towards social recommendation. Information Fusion, 69:103–127, 2021. [46] D. Paraschakis and B. J. Nilsson. Matchmaking under fairness constraints: a speed dating case study. In International Workshop on Algorithmic Bias in Search and Recommendation, pages 43–57. Springer, 2020. [47] G. K. Patro, A. Biswas, N. Ganguly, K. P. Gummadi, and A. Chakraborty. Fairrec: Twosided fairness for personalized recommendations in two-sided platforms. arXiv preprint arXiv:2002.10764, 2020. [48] P. Ravikumar, A. Tewari, and E. Yang. On ndcg consistency of listwise ranking methods. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, pages 618–626. JMLR Workshop and Conference Proceedings, 2011. [49] M. Richardson, R. Agrawal, and P. Domingos. Trust management for the semantic web. In International semantic Web conference, pages 351–368. Springer, 2003. [50] J. E. Roemer and A. Trannoy. Equality of opportunity: Theory and measurement. Journal of Economic Literature, 54(4):1288–1332, 2016. [51] A. Sen. Equality of what? The Tanner lecture on human values, 1, 1979. [52] A. F. Shorrocks. The class of additively decomposable inequality measures. Econometrica: Journal of the Econometric Society, pages 613–625, 1980. [53] A. F. Shorrocks. Ranking income distributions. Economica, 50(197):3–17, 1983. [54] A. Singh and T. Joachims. Fairness of exposure in rankings. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2219–2228. ACM, 2018. [55] A. Singh and T. Joachims. Policy learning for fairness in ranking. In Advances in Neural Information Processing Systems, pages 5427–5437, 2019. [56] T. Speicher, H. Heidari, N. Grgic-Hlaca, K. P. Gummadi, A. Singla, A. Weller, and M. B. Zafar. A uniﬁed approach to quantifying algorithmic unfairness: Measuring individual &group unfairness via inequality indices. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2239–2248. ACM, 2018. [57] L. Sweeney. Discrimination in online ad delivery. Queue, 11(3):10, 2013. [58] P. D. Thistle. Ranking distributions with generalized lorenz curves. Southern Economic Journal, pages 1–12, 1989. [59] B. Ustun, Y. Liu, and D. Parkes. Fairness without harm: Decoupled classiﬁers with preference guarantees. In International Conference on Machine Learning, pages 6373–6382, 2019. [60] L. Wang and T. Joachims. Fairness and diversity for rankings in two-sided markets. arXiv preprint arXiv:2010.01470, 2020. [61] M. Wang, M. Gong, X. Zheng, and K. Zhang. Modeling dynamic missingness of implicit feedback for recommendation. Advances in neural information processing systems, 31:6669, 2018. [62] O. Wolfson and J. Lin. Fairness versus optimality in ridesharing. In 2017 18th IEEE International Conference on Mobile Data Management (MDM), pages 118–123. IEEE, 2017. [63] Y. Wu, J. Cao, G. Xu, and Y. Tan. Tfrom: A two-sided fairness-aware recommendation model for both customers and providers. arXiv preprint arXiv:2104.09024, 2021. [64] B. Xia, J. Yin, J. Xu, and Y. Li. We-rec: A fairness-aware reciprocal recommendation based on walrasian equilibrium. Knowledge-Based Systems, 182:104857, 2019. [65] M. Zehlike and C. Castillo. Reducing disparate exposure in ranking: A learning to rank approach. In Proceedings of The Web Conference 2020, pages 2849–2855, 2020. [66] T. Zhang et al. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. The Annals of Statistics, 32(1):56–85, 2004. [67] M. Zimmer, C. Glanois, U. Siddique, and P. Weng. Learning fair policies in decentralized cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pages 12967–12978. PMLR, 2021. <title>A Outline of the appendices</title> These appendices are structured as follows: In Appendix B, we present how our fairness framework can be applied to sensitive groups of users or categories of items. In Appendix C, we present a deeper analysis of the trade-offs achieved by the welfare approach. We also provide a theoretical guarantee relating the true welfare obtained by maximizing the welfare using estimated preferences, depending on the quality of the estimates. In Appendix D, we present the proofs for the theoretical results comparing our results and previous criteria of fairness in rankings. In addition, in Appendix D.3, we describe how to extend the criteria of equality of exposure and quality-weighted exposure in a reciprocal recommendation setting. This is the extension used in our experiments on reciprocal recommendation. In Proposition 8, we present an additional result regarding the inefﬁciency of these criteria in reciprocal recommendation. In Appendix E, we present the more general version of the Frank-Wolfe algorithm, which we use both to optimize the welfare function over stochastic rankings, as well as the penaltybased baselines. This appendix also contains the proofs of the results in Section 4. In addition, this appendix contains fundamental lemmas that are used in other appendices. Appendix G brieﬂy discusses the difference between the penalty we use in our implementation of the baseline approaches and an alternative penalty used by some authors. Finally, Appendix H discusses the difference between applying item-side fairness criteria for every ranking, compared to what we do in the paper, which deﬁnes item-side utility as an aggregate over the rankings of all users. <title>B Fairness towards sensitive groups rather than individuals</title> In all the paper we focus on fairness towards individual users and items rather than groups of users or items. Prior work [ 54 42 55 ] considered the utlity of a group as the sum or the average utility of its members. Using this deﬁnition of group utility, our framework dirrectly extends to groups rather than individuals. In this section we describe the case of one-sided recommendation with groups of users and item categories. The case of reciprocal recommendation (with user groups only) is similar but simpler. Let S = (s be (possibly overlapping) user groups, i.e., ∀p ∈ [[|S|]], s ⊆ N and . Similarly, let C = (c be (possibly overlapping) item categories, i.e., ∀q ∈ [[|C|]], c ⊆ I and = I . On the user side, such groups would typically correspond to demographic groups considered sensitive for the application at hand [ 57 ]. On the item side, groups can represent a single producer for the case where we want to be fair to producers based on the aggregate utility they obtain from their products [41], or demographic groups as well [33]. In all cases, we redeﬁne the user-side utility for groups and the item-side utility for categories: (P ) = (P ) u (P ) = (P ) Let (P ) = (u (P )) and (P ) = (u (P )) be the utility proﬁles of user groups and item categories associated to respectively. The two-sided Lorenz efﬁciency for groups and categories is deﬁned as: 2. u (P )  (P ) and u (P )  (P ). Finally, the extension of Proposition 1 is straightforward. Its proof is similar to the proof presented in Appendix C. Proposition 5. ∀θ ∈ Θ, ∀P ∈ argmax (P ), P is (S, C)-Lorenz efﬁcient. Note that this way of treating groups is not necessarily optimal. In particular, in does not account for within-group fairness. The separate consideration of within-group and between-group fairness has been studied extensively in the literature on equality of opportunity [ 50 ], which has inspired several works on algorithmic fairness [ 22 26 ]. Yet, how to apply these principles to two-sided fairness in recommendation is still open, and is left as future work. <title>C More on welfare functions</title> This appendix provides an in-depth analysis of the trade-offs that are achievable by the welfare approach. We ﬁrst pove the proposition of Section 2.2, and analyze the utilitarian rankings (obtained with = α = 1 ). We then analyze how to obtain leximin optimal solutions on the side of the items in Appendix C.2, as mentioned in Section 2.2. Finally, we prove Theorem 2 in Appendix C.3, which provides a regret bound relating the true welfare achieved when maximizing welfare on estimated preferences. Some results in this section use Lemma 3 of Appendix 4, which is proved in Appendix 4. Throughout the appendices, we use the more general version of item utilities (two-sided preferences), described at the end of Section 2.1. Moreover, to clarify the notation, we remind that a ranking tensor is a three-way tensor where is the probability that item is recommended to user at rank We consider as an n × n × |I| tensor, where irrelevant entries are set to . With this notation, the utility for both users and items can be written with the same formula: ∀i ∈ [[n]], u (P ) = (P + P )v. Note that this formula also corresponds to the two-sided utility in reciprocal recommendation. In general, the results in this appendix can be extended to reciprocal recommendation with minimal changes to their proofs, using N = I = [[n]] and the formula above for the utility. We ﬁrst prove Proposition 1: Proposition 1. ∀θ ∈ Θ, ∀P ∈ argmax (u(P )), P is Lorenz-efﬁcient. Proof. It is well known that if is increasing and strictly concave, then F (u) = Φ(u is monotonic with respect to Lorenz dominance [53, 58]: u  =⇒ F (u) > F (u ). In the case of , for every θ = (λ, α , α ) ∈ Θ , both ψ(., α and ψ(., α are strictly concave by the deﬁnition of Θ (recall that in Θ, we have α , α < 1). or (u and . Let us assume (u and , the other case is dealt with similarly. We then have: which contradicts the maximality of u. The analogous for Proposition 1 for reciprocal recommendation is a direct consequence of standard results that concave welfare functions are monotonic with respect to Lorenz dominance [53, 58]. ∈ argmax (P ) = argmax (P ). When mutual preferences are symmetric (i.e., = µ ), the utilitarian ranking is the same as the usual sort by decreasing . This also obviously holds when we consider exposue as item utility = 1 ). This means that without considerations of two-sided fairness ( , α < 1 ), the optimal ranking for two-sided utilities is the same as the usual ranking. This might explain why the two-sided utility has never been studied before, even in reciprocal recommendation [45]. For the proof of Proposition 6, the main part is the following lemma: Moreover, if ∀k ∈ [[n −1]], v > v ≥ 0, then the reciprocal is true. Proof. Notice that, thanks to the completion of with zeros on irrelevant entries and formula C, F (u(P )) can be rewritten as: where the last equality is obtained by swapping and in the second sum, which is possible since and j span the same range. The ﬁrst of statement of Proposition 6 assumes that the exposure weights are non-negative and strictly decreasing as per the second point of Lemma 1. Lemma 1 above gives the statement for the more general case of non-increasing v. Proof of Proposition 6. The ﬁrst statement is the consequence of Lemma 1 above, noticing that F (u(P )) in Lemma 1 always has the same argmax. The second statement is obvious from the assumptions. The most egalitarian trade-off achievable by our method is described by the leximin order [ 51 ]. Given two utility proﬁles and u ≥ if is greater than according to the lexicographic order. The leximin optimal proﬁle is egalitarian in the sense that it maximizes the utility of individuals in sequence, from the worse-off to the better-off. Depending on the set of feasible proﬁles, this may not lead to equal utility for everyone, but any further reduction of inequality can only be achieved by making people worse off for the beneﬁt of no other, in violation of Pareto-dominance. The proposition below formalizes how leximin optimal solutions on the side of items are found. It shows that item-side leximin solutions are obtained by having → −∞ and λ → 1 at the same time. The proposition gives a formal statement of the rate at which should converge to relative to α. In the statement of the proposition, given two functions and , we use F (α) ≥ G(α) as a shorthand for F (α) ≥ G(α) for α sufﬁciently small. Proposition 7. Let U = {u ∈ U : ∀u ∈ U, u } and let u = argmax ψ(u , α ). ∀η > max(1, ku ), ∀u ∈ U : W (u ) ≥ (u). This means that among the leximin-optimal item-side utility proﬁles, still controls the redistribution proﬁle on the user side, since it is possible that |U | > 1 in one-sided recommendation. A similar result holds for user-side item leximin. Proof. Let u = argmax ψ(u , α ) and u ∈ U. Let θ = (λ, α , α) and take α < min(0, α ). Let m = max{k ∈ [[|I|]] ∪{0} : ∀` ≤ k, u = u }+ 1 , be the last index (+1) such that the smallest values of u and u are equal (m = 1 if the smallest values are different). Let C(α) = W (u ) − W (u). case 1: m = |I|+ 1. Then C(α) = (1 −η )K ≥ 0 since = u and maximizes the user-side welfare. case 2: m < |I|. Then, we have u < u by the leximin optimality of u . We then have: = −(1 − η +1 + 1 − η | {z } | {z } | {z } | {z } −−−−−→ −−−−−→ −−−−−→ which implies lim C(α) = +∞ and thus the desired result. estimate ˆµ and an optimality guarantee for u( P ) (i.e., the true welfare of the ranking inferred on the estimated values). We prove the result for θ = ( , α, α) for α ≤ 1 to simplify notation. Theorem 2. Let α ≤ 1 and θ = ( , α, α) ∈ Θ . Let ˆµ ∈ R P = argmax u(P )) and P = argmax (u(P )). Let furthermore B(ˆµ) = max max (u P ), α), max (ˆu (P ), α) . We have: The existing results closest to our Theorem 2 are Theorem 2 of [ 14 ]. Here the result is substantially more difﬁcult to prove because of the concave function and the fact that utilities are two-sided, calling for considering the rankings of multiple users at once. Proof. We have: (u(P )) − W (u( P )) = W (u(P )) − W u( P )) +W u( P )) − W (u( P )) | {z } + W u( P )) − W (u( P )) ≤ W (u(P )) − W u(P )) | {z } | {z } Let B (ˆµ) = max (P ), α). We ﬁrst prove: To prove (2), we start by using the concavity of ψ(., α) for α ≤ 1. Let Φ(.) = ψ(., α). We have: (P ))(µ − ˆµ ) + Φ (P ))(µ − ˆµ | {z } where, similarly to the proof of Lemma 1, we swapped the indexed (i, j) in the (P ))µ )v which is possible because i and j span the same range in the sum. Notice that the terms are all zero except if i ∈ N and j ∈ I (because = 0 otherwise). For i ∈ N , let be a ranking which ranks (A in decreasing order, i.e., (j) < σ (j ) =⇒ ≥ A . Using Lemma 3 in Appendix E, we have: ≤ max v = − ˆµ ), leading to: which proves (2). Similarly, using B (ˆµ) = max (u P ), α) and the same arguments as above, we obtain: which yields the desired result. <title>D Comparison to utility/inequality trade-offs</title> In this appendix, we provide the proofs of Section 3, and describe more precisely how we applied quality-weighted exposure and equality of exposure in reciprocal recommendation. For every d ∈ N and every N ∈ N , there is a one-sided recommendation problem, with d + 1 items and N(d + 1) users, such that ∀θ ∈ Θ, we have: ∃β > 0, u and u and lim −−−→ Proof. We prove it for N = 1 , the more general case is just obtained by repeating the pattern with d + 1 items and d + 1 users. Let , ..., i be the indexes of the users and , ..., j the indexes of the items. The preferences have the following pattern: all other (for user and item ) are set to (note that we are in a problem with one-sided preferences, which means µ = 1 for every item j and user i. We consider a task with a single recommendation slot ( = 1, v = . . . = v = 0 ). On that problem, the optimal ranking for every θ ∈ Θ is to show item to user , which leads to perfect equality in terms of item exposure, and maximizes every user utility. It is thus leximin optimal for both users and items for every θ ∈ Θ. Then, the qualities are equal to: ∀k ∈ [[d]], q = 1 q d + 1 the target exposure is thus t for k ∈ [[d]] and t = (d + 1) Since the problem is symmetric in the users , ..., i , by the concavity of (u(P )) with respect to P , there is an optimal ranking described by a single probability p as: Now, as β → ∞, p is such that exposure equals its target, which leads to the following equation: d + 1 dp + 1 = (d + 1) d + 1 We thus get p = −−−→ , which gives the result u (P ) = 1 − p −−−→ Notice that similarly to Proposition 3, the result does not depend on the choice of because the sum of user utilities converges. We now prove Proposition 3. Proposition 3. For β > 0 , let = argmax (u) . The claim below holds irrespective of the choice of ∈ U . Let n ≥ 5 . There is a reciprocal recommendation task with users such that: ∀θ ∈ Θ, u , ∃β > 0 : ∀i ∈ [[n]], u > u and lim = 0. Proof. The example is given in Figure 5. We still consider a recommendation task with a single recommendation slot. Let us rename the users by , i , ..., i . The preference patterns are = µ = 1 and = 1 Apart from = µ , other s are . In this proof, we show that = 2u for every , which implies that −−−−→ because utility for every user is feasible. On this task, the leximin ranking also maximizes the sum of users utilities (as shown in Figure 5), so the optimal ranking is the same for every θ ∈ Θ, and every user has a two-sided utility of at least 1.5. Since (u) is stricly Schur-concave for β > 0 and always have the same utility in an optimal utility proﬁle (because they play a symmetric role). and also have the same utility. Note that the interest of and in that problem is to make it possible to recommend them to , which has value. Similarly to the problem in one-sided recommendation, the only way to decrease the penalty is to reduce the utility of , i , i . However, reducing the utility of can only be done by either recommending or to , or recommending /i to /i . In all cases, decreasing ’s utility decreases i /i ’s utilities. More precisely, because of the symmetries and the concavity of (u(P )) with respect to , for every β > 0, there is an optimal ranking tensor described by three probabilities p, q, q such that: (P ) = p + 2q and u (P ) = q + |{z} |{z} Thus, in an optimal ranking for (u) , we must have (P ) = 2u (P ) . Equality, which is achieved at β → ∞ can then only be at 0 utility for every user (since 0 is feasible). The task used in the proof contains only users. Any number of users can be added to the group {i , i }, with a “complete” preference proﬁle (µ = 1 for all pair i, j in that group). The Lorenz efﬁciency of our welfare approach guarantees that it cannot exhibit the undesirable behaviors of equality or quality-weighted exposure penalties described in Propositions 2 and 8. In one-sided recommendation with one-sided preferences, equality of exposure is the same as equality of utility. More generally, let (P ) = the total exposure of item . Equality of exposure is deﬁned by: |N| (P ) = (P ) − β (P ) − kvk |I| In one-sided recommendation, parity of exposure is relatively well behaved because the exposure target kvk is constant. Driving towards equality can thus not lead to a decrease of the total exposure budget, which was the problem with equality of utility in settings with two-sided preferences (driving towards equality of utility leads to a decrease of total utility), as we described in Section 3. The formula allows us to extend parity of exposure in the next section and in our experiments, since it is also valid in reciprocal recommendation. Likewise, the formula of quality-weighted exposure that is also valid in reciprocal recommendation is given by: (P ) − β (P ) = (P ) − The result below shows that equality of exposure and quality-weighted exposure lead to inefﬁciencies in reciprocal recommendation settings: Proof. An example of extreme case is with users when there is a “leader” who is the only possible match with other users. We consider a single recommendation slot. The preferences are: ∀j ∈ {2, . . . , n}, µ = µ = 1 ∀(i, j) ∈ {2, . . . , n} , µ = 0. On this task, the for every θ ∈ Θ, the optimal ranking is given by: The reason it is the only possible optimal ranking is because it is leximin optimal and has the maximum achievable sum of utilities. The utilities are then (P ) = n and (P ) = 1 + , which leads to = 2n. Equality of exposure Driving towards equality of exposure requires to reduce the exposure of user which in turn reduces the utility of user and the utilities of those who user 1 is less exposed to. Thus, there is β > 0 such that because of the loss of efﬁciency. Finally, by the concavity of the objective with respect to , and by the symmetry of the problem with respect to , . . . , i we can conclude that an optimal way to achieve perfect equality of exposure is to recommend, to every user , every user j 6= i with probability . The utility is then (P ) = 1 + (n −1) and (P ) = for i ≥ 2, leading to = 4, which gives the result. Figure 5: Left: Example of a reciprocal recommendation task where equality of utility leads to utility (see the proof of Prop. 3 in App. D). There is one recommendation slot per user. We give the recommendation probabilities and utilities for the utilitarian ranking and three users, the other ones are obtained by the symmetry of the problem. The utilitarian ranking is also leximin optimal, so our approach yields the same recommendations for all Right: Example where quality-weighted exposure reduces user utility while increasing inequalities between items. . By the symmetry of the problem, as β → ∞ , quality weighted exposure is achieved by setting: 1 − ∀j ∈ {2, . . . , n}, j 6= j, P n − 2 2(n − 1) The utilities are then (P ) = 1 + (n −1) = 1 + and (P ) = The total utility is thus 2 + n, which gives the result. The Lorenz efﬁciency of our welfare approach guarantees that it cannot exhibit the undesirable behaviors of parity or quality-weighted exposure penalties described in Propositions 2 and 8. <title>E A generic Frank-Wolfe algorithm for ranking</title> In this section, we present a general form of our algorithm presented in Section 4, as well as the proofs of the claims. The stepsize is from Clarkson [13 , Section 3 , which avoids a line search and in our experiments seemed to yield acceptable results. Irrespective of the step size, the fundamental results which allows to use Frank-Wolfe in the setting of (3) are the two following lemmas: Lemma 2. Recall that (P ) = (P + P )v . Let denote the derivative of with respect to its i-th argument and (u(P )) the value of this derivative at u(P ). Then, ∀i ∈ N, ∀j ∈ I, ∀k ∈ [[|I|]], we have: Proof. The result is a consequence of the chain rule: (P ) = (P + P )v = (µ + µ )v , which gives the desired result. Thus Lemma 3. Let be an n × n matrix with ∈ R (not necessarily non-negative). Let v ∈ R with non-negative and non-increasing entries, i.e., ∀k ∈ [[|I|−1]] ≥ v ≥ 0 . Let be the last index such that v > 0 (or K = |I| if there is no such index). Let P ∈ P such that: Then P ∈ argmax hP |Xi. Moreover, if ∀k ∈ [[|I| −1]], v > v ≥ 0, then for every P ∈ argmax hP |Xi, we have: Proof. The result stems from the rearrangement inequality (also known as the Hardy-Littlewood inequality [ 23 ]), which states that for two vectors a ∈ R , and b ∈ R argmax where spans the permutations of [[n]] , is the set of permutations such that is ordered similarly to (a . If the s are non-increasing, then every permutation that sorts in decreasing order is in the argmax. We need the reciprocal statement for the second part of our Lemma: if the s are strictly decreasing, then only the permutations that sort in decreasing order are in argmax Note that these arguments are well-known in learning to rank [see, e.g., 14]. hP |Xi = The maximization over can then be performed over each user (and thus each bistochastic matrix separately). Now, if is such that every ∈ S(P orders in decreasing order, then by the rearrangement inequality ∈ argmax . Notice that if only the ﬁrst elements of v are non-zero, we only need a top-K ranking. This gives us the ﬁrst part of the thoerem. Lemma 2 and 3 together are sufﬁcient to give algorithms for the inference of stochastic rankings using our welfare function (1) and using the penalties of Section 3, by computing the partial derivatives . The main result of Section 4, which we prove now, instantiates this principle for the welfare function approach: Theorem 1. Let ˜µ = Φ (P + Φ (P . Let P such that: ∀i ∈ N, ∀˜σ ∈ S( ): ˜σ (j) < ˜σ (j ) =⇒ ˜µ ≥ ˜µ . Then P ∈ argmax hP |∇W (P )i. Proof. Notice that with W (P ) = F (u(P )) = (u (P )) , then (u(P )) = Φ (u (P )) . By Lemma 2, we have that hP |∇F (P i) is of the form hP |Xi with = A with = ˜µ so the result is implied by Lemma 3. E.2 Proof of Proposition 4 Proposition 4. Let B = max kΦ and U = max kuk . Let be the maximum index of a nonzero value in (or |I| ). Then ∀t ≥ 1, W (P ) ≥ max W (P ) − O( . Moreover, for each user, an iteration costs O(|I|ln K) operations and requires O(K) additional bytes of storage. ≤ sup (u − u W ( u)(u − u ) ≤ sup ku − u ≤ 2BU. For the computation cost, we use Lemma 3, which is more precise than Theorem 1, to see that ﬁnding the argmax only requires a topranking. While technically any P ∈ P should contain a whole bistochastic matrix, it is not necessary to store a completion of the toprankings because they have no impact on the utility. As such, storing each only costs O(K) bytes per user, which contain the indices of the top-K items in the ranking found by Theorem 2. Computing the two-sided utilities costs O(|N||I|) , and thus O(|I|) per user. Moreover, computing the top-K ranking costs O(|I|ln K) in the worst case, with a streaming method that maintains a min-heap of the top-K elements seen so far, and ﬁnish with sorting the top-K elements. Notice that for faster average performance, the top-K sort can be performed using a fast selection algorithm (such as quickselect), to obtain the topelements with O(|I|) expected time complexity, and then sorting, yielding O(|I|+ K ln K) expected time complexity per user at each iteration. <title>F Additional experimental results</title> Our experiments are fully implemented in Python 3.9 using PyTorch . We provide the code as supplementary material. We compare our welfare maximization approach with the fairness penalties presented in Section 3. We also compare ourselves to the algorithm FairRec from Patro et al. [47] (referred to as Patro et al. in the ﬁgures and description), who consider envy-freeness as user-side fairness criterion, and max-min share of exposure as item-side fairness criterion. Envy-freeness states that every user should prefer their recommendation list to that of any other user. The max-min exposure criterion on the item side means that each user should receive an exposure of at least where is a parameter allowing to control how much exposure is guaranteed to items. We vary this parameter in our experiments to show the trade-offs achieved by Patro et al.. Since Patro et al. does not produce rankings, we took the recommendation list with the given order as a ranked list. We describe in this section the details of the experiments presented in Section 5.1. We use a dataset from the online music service Last.fm . In the main paper, we presented results on Lastfm-2k from Cantador et al. [9] which contains real play counts of 2k users for 19k artists, and was used by Patro et al. [47] who also study two-sided fairness in recommendation. We ﬁlter the top 2, 500 items most listened to. Following [ 32 ], we pre-process the raw counts with log -transformation. We split the dataset into train/validation/test sets, each including 70%/10%/20% of the user-item play counts. We create three different splits using three random seeds. One-sided preferences are estimated using the standard matrix factorization algorithm of Hu et al. [29] trained on the train set, with hyperparameters selected on the validation set by grid search. The number of latent factors is chosen in [16, 32, 64, 128] , the regularization in [0.1, 1., 10., 20., 50.] , and the conﬁdence weighting parameter in [0.1, 1., 10., 100.] . The estimated preferences we use are the positive part of the resulting estimates. Rankings are inferred from these estimated preferences. The exposure weights we use in the computation of utilities are the standard weights of the discounted cumulative gain (DCG) (also used in e.g., [ 54 42 ]): ∀k ∈ [[|I|]], v . For each ranking approach, the Frank-Wolfe algorithm is run with 5000 iterations to make sure we are close to convergence, and the number of recommendation slots is set to 40. We evaluate rankings on estimated preferences, considered as ground truth, following many works on fair recommendation [ 54 47 60 63 ]. This is because the goal is to evaluate the fairness of ranking algorithms themselves, rather than biases in preference estimates. All results are averaged over three random seeds. To obtain various tradeoffs, for welf we vary in [0.001, 0.01, 0.05, 0.075, 0.1, 0.125, 0.15, 0.2, 0.3, 0.325, 0.35] and [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.9, 0.95, 0.99, 0.999] . For Patro et al. we vary in [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4] and [0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1] , and for other methods we vary in [0.001, 0.005, 0.01, 0.015, 0.0175, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06] and [0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1, 0.105, 0.11, 0.2, 0.5, 1, 2, 5, 10, 20, 30, 40, 50, 70, 100] Item-side fairness Figure 6 presents the various trade-offs achieved by each method in one-sided recommendation, as discussed in Section 5.1. We observe that only qua.-weighted is unable to reach equal exposure because of its quality-weighted exposure target: perfectly equal exposure is only permitted when all items have the same quality. Two-sided fairness Figure 7 shows the effect of varying and on user fairness as in Figure 3 of the main paper, but with results repeated over three random seeds. We observe the same trade-offs and conclude again that welf is better than Patro et al. and eq. exposure, in terms of its impact on worse-off users. Figure 6: representative trade-offs achieved by the various compared methods on Lastfm-2k. The trade-offs achieved by the different methods look alike, except that qua.-weighted does not aim at reaching equality of exposure for exteme values of . See Section 5.1 for the discussions on the differences between the trade-offs achieved by the different approaches. Figure 7: Focus on user fairness on Lastfm-2k: effect of varying (user-side curvature of the welfare function) keeping = 0 . The ﬁgure shows all the results obtained with a repetition of three seeds. Overlapping points correspond to the same model parameter across different seeds. We can see that the variance is negligible compared to the observed differences. The importance of considering the whole Lorenz curve In Fig. 8 we show the results of the same models as before, but changing the way we measure the item inequality: using the standard deviation of exposure rather than the Gini index. Now, eq. exposure dominates the total utility/item inequality plot, since the plot corresponds exactly to the objective function of the algorithm. Comparing eq. exposure with welf = 1 , we now see that the trade-offs are different, with eq. exposure performing better on the worse-off users. Comparing welf = 0 and Patro et al., we see that they still exhibit similar behaviors, with welf = 0 being better for better off users. Finally, welf = −2 still dominates the othe methods in terms of performance on the worse-off users. We replicate the experiments on a larger dataset to verify our conclusions at a larger scale. We consider another Lastfm dataset from Celma [11] , which includes 360k users and 180k items (artists). We select the top 15, 000 users and items having the most interactions, so we refer to this dataset as Lastfm-15k. We apply exactly the same experimental protocol as for Lastfm-2k, with the same range of hyperparameters for the different methods. Figure 8: Focus on user fairness on Lastfm-2k, measuring item inequality with standard deviation rather than Gini index. We observe a similar relative behavior between welf and Patro et al., but now equality of exposure is optimal on the total utility/item inequality trade-off since it corresponds exactly to the objective of the algorithm. Nonetheless, welf = −2 still obtains higher performance on 10%-25% worse-off users, showing that welf offers a larger range of trade-offs than eq. exposure. Figure 9: Results on Lastfm-15k when measuring the inequality between items with the Gini coefﬁcient. Results Fig. 11 and 10 show the results obtained by welf, Patro et al. and eq. exposure. The conclusions are similar to those on Lastfm-2k, with the results of welf α = 0 being more uniformly better than those of Patro et al., even though overall similar. welf = −2 dominates in terms of user utility on worse-off users. welf and eq. exposure still ﬁnd different trade-offs, with welf dominating eq. exposure when inequality between items is measured by the Gini index, and eq. exposure dominating welf when inequality is measured by the standard deviation. Figure 10: Results on Lastfm-15k when measuring inequalities between items with the standard deviation. Figure 11: Results on Movielens when measuring the inequality between items with the Gini coefﬁcient. Figure 12: Results on Movielens when measuring inequalities between items with the standard deviation. to Last.fm, we consider missing ratings as negative feedback and the task is to predict positive values. Since ratings < 3 are usually considered as negative [ 38 61 ], we set ratings < 3 to zero, resulting in a dataset with preference values among {0, 3, 3.5, 4, 4.5, 5} . As for Lastfm-15k, we select the top 15, 000 users and items with the most interactions. For the inference and evaluation of rankings, we follow the same protocols as for Last.fm. The experimental protocol is the same as for Lastfm-2k and Lastfm-15k except that we do not run the algorithm by [47] because its runtime was prohibitive. results The results are qualitatviely similar to those on Lastfm-2k and Lastfm-15k except that the trends are magniﬁed. welf α = 1 and eq. exposure seem more similar, with welf α = 0 dominating the trade-off total utility/item iniequality when item inequality is measured with the Gini index, and eq. exposure dominating this trade-off when item inequality is measured with standard deviation. welf α = −2 has great performance on worse-off users compared to eq. exposure or welf with larger , but also comes at a signiﬁcant cost in terms of total user utility, which is very rapidly driven down. We now provide the full details of the experiments on Twitter presented in Section 5.2 of the main body. Given the lack of common benchmark for reciprocal recommendation [ 45 ], we generate a reciprocal recommendation task for people-to-people recommendation problems based on the social network Twitter. We use the Higgs Twitter-13k dataset which includes (directed) follower relationships between users. We keep users having at least 20 mutual follows, resulting in a subset of 13 k users. We use the directed links to estimate the probability that follows , and the (symmetric) probability of a mutual follow, which is = φ × φ . As in the experiments for one-sided recommendation, we split the dataset into train/validation/test sets, each including 70%/10%/20% of the directed follower links. We create three random uniform splits, corresponding to three different seeds. Figure 13: representative trade-offs achieved by the various compared methods on Twitter-13k. Exposure-based approaches (qua.-weighted and eq. exposure) do not yield interesting trade-offs as they are unable to increase the utility of worse-off users. The trade-offs achieved by the welf and eq. utility are different. Equal utility rapidly generates near-ﬂat curves without really focusing on the very ﬁrst users, while welf increases the utility of the worst-off users while keeping the total utility relatively high. Estimates are built with logistic matrix factorization 32 ] trained on the train set with hyperparameter selection on the validation set. The number of latent factors is chosen in [16, 32, 64, 128] , the regularization in [0.1, 1., 10., 20., 50.] , and the conﬁdence weighting parameter in [0.1, 1., 10., 100.] Rankings are inferred from all estimated mutual preferences ˆµ = max( , 0) . For each ranking method, the Frank-Wolfe algorithm is run with 5000 iterations, and the number of recommendation slots is set to 40 . As for one-sided recommendation, rankings are estimated on estimated mutual preferences taken as ground truth. We generate different trade-offs with welf by varying in [0.99, 0.9, 0.75, 0.5, 0.25, 0, −0.25, −0.5, −0.6, −0.7, −0.8, −0.9, −1.0] [−1.1, −1.25, −1.5, −1.75, −2.0, −2.5, −3, −5, −10, −15, −16, −17, −18] . For all other methods, we vary β in [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.25, 1.5, 2, 5, 10, 50, 100]. All presented results are obtained by averaging performance over the three seeds. Results Figure 13 presents the trade-offs achieved by the different methods on Twitter-13k. As expected, qua.-weighted and eq. exposure do not exhibit a good behavior: stronger penalties lead to more dominated curves where the utility of every user is decreased. This is because constraining item exposure is not meaningful in reciprocal recommendation, where the relevant utility is the two-sided utility. The trade-offs achieved by the welf and eq. utility are different. Equal utility rapidly generates near-ﬂat curves without really focusing on the very ﬁrst users, while welf increases the utility of the worst-off users while keeping the total utility relatively high. We present additional experiments on reciprocal recommendation with the Epinions dataset [ 49 ]. Epinions.com is a consumer review site with a who-trust-whom network, and the dataset gathers (directed) trust relationships between members of the platform. Here, we consider the task of ﬁnding mutual trust links. We keep users having at least 20 mutual trust links, resulting in a subset of 800 entities. For the inference and evaluation of rankings, we use the same protocols as for the Twitter experiments described in the previous subsection. The experimental parameters are the same as for the Twitter-13k dataset. Results Figure 14 presents the trade-offs achieved by the different methods on Epinions. As expected, qua.-weighted and eq. exposure do not exhibit a good behavior: stronger penalties lead to more dominated curves where the utility of every user is decreased. In Figure 15 plots the equivalent of Fig. 4. The results are similar: all of qua.-weighted, eq. exposure and eq. utility have dominated curves. We also observe that in the more interesting region where we are closer to the maximum achievable utility, welf optimizes better the utility of worse-off users. Yet, in that region, there is no strict dominance of welf over eq. utility. Figure 14: representative trade-offs achieved by the various compared methods on Epinions. The results are qualitatively similar to those on Twitter-13k. Figure 15: Results on the epinions dataset. <title>G Pairwise vs pointwise penalties</title> Our penalty-based approach uses the penalty D(u) with: D(u) = |I| Some authors use (u) = |u − u instead of D(u) 55 42 ], but it is less computationally efﬁcient than our penalty because it involves a quadratic number of terms. The penalties are similar in that they are related to well-known measures of inequalities: is the Gini index of 19 ], which, up to an afﬁne transform is the area under the Lorenz curve. inequality measures [52]. We use D(u) to scale the penalty with the sum of users’ utilities. factor). Since both penalties drive towards equality, it is straightforward to show that the results of Section 3 as β → ∞ also apply to D (u). <title>H Exposure constraints at the level of every ranking</title> The notions of fairness of exposure are sometimes deﬁned with item-side constraints deﬁned at the level of every ranking [ 54 ]. We give here the examples of constraints for equality of exposure and quality-weighted exposure: kvk ∈ argmax (P ) u.c. ∀(i, j) ∈ N × I, P v = |I| kvk ∈ argmax (P ) u.c. ∀(i, j) ∈ N × I, P v = The advantage of this formulation is that it leads to optimization problems that can be solved locally for every user, since there is no dependency between user rankings through item utility anymore. However, applying the exposure criterion at the level of every ranking effectively applies a different notion of fairness. In our setting, this corresponds to deﬁning a separate recommendation task for every user, i.e., taking |N| = 1 . The welfare function then mediates, within a single ranking, between the user utility and the utility of the different items. When evaluated on exposures aggregated over all users, as we do in the paper, applying the fairness constraints at the level of individual rankings can lead to drastic reductions of user utility for no beneﬁt in terms of total item exposure. This is summarized in the following result, which shows that there exists problems for which the optimal rankings for every θ ∈ Θ satisfy the constraints of equality of exposure and quality-weighted exposure as we deﬁne them in Section 3, but when applying the constraints at the level of every ranking, it has the effect of reducing user utility. In the proposition, we use the notation of the objective function for parity of exposure and of Section 3. In other words, applying the constraints at the level of every ranking might lead to a drastic decrease of user utilities, even in tasks where satisfying the constraints on average over users (as we do in this paper) does not conﬂict with the optimal ranking. Proof. We describe the problem with N = 1 , the general case is obtained by repeating the preference pattern. Let us consider a task with d + 1 users, d + 1 items and a single recommendation slot. Let , . . . , i be the user indexes, and j , . . . , j the item indexes. The preferences are deﬁned as: All items have the same quality. For every θ ∈ Θ is given by the utilitarian ranking, which gives probability to item for user , which leads to optimal user utility = 1 and equal exposure to every item = 1 . Since the quality is the same for all items (equal to 1 + d ), the ranking for satisﬁes both equality of exposure and quality-weighted exposure constraints. Thus, for every β > 0 ∈ argmax (u) and u ∈ argmax (u). On the other hand, satisfying equality of exposure at the level of every ranking requires for every user i and item j, which leads to u (P ) = + d × for every user. For quality-weighted exposure for every ranking, it leads to: ∀k ∈ [[d + 1]], P ∀j 6= j , P and thus a user utility u (P ) = + d × Notice that in the examples of the proof, the global exposure of items is constant in and as well as in the ranking given by optimal welfare. So from the point of view of our deﬁnitions of utility, applying the constraints at the level of every ranking only decreased user utility for the beneﬁt of no items. Yet, we re-iterate that applying item-side fairness at the level of every ranking might be meaningful in some contexts. The goal of this section is to highlight the difference between using global and local deﬁnitions of item utilities.