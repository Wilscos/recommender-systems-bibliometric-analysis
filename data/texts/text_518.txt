With the prevalence of deep learning based embedding approaches, recommender systems have become a proven and indispensable tool in various information ltering applications. However, many of them remain dicult to diagnose what aspects of the deep models’ input drive the nal ranking decision, thus, they cannot often be understood by human stakeholders. In this paper, we investigate the dilemma between recommendation and explainability, and show that by utilizing the contextual features (e.g., item reviews from users), we can design a series of explainable recommender systems without sacricing their performance. In particular, we propose three types of explainable recommendation strategies with gradual change of model transparency: whitebox, graybox, and blackbox. Each strategy explains its ranking decisions via dierent mechanisms: attention weights, adversarial perturbations, and counterfactual perturbations. We apply these explainable models on ve real-world data sets under the contextualized setting where users and items have explicit interactions. The empirical results show that our model achieves highly competitive ranking performance, and generates accurate and eective explanations in terms of numerous quantitative metrics and qualitative visualizations. • Information systems → Recommender systems;Personalization. Recommendation plays an important role in many information ltering systems, such as e-commerce (Amazon, Walmart), streaming services (YouTube, Netix), and business review services (Yelp, Trip Advisor), etc. Modern recommender systems (RS) provide personalized recommendation suggestions by learning from the historical user-item interactions, e.g., explicit interactions such as ratings or implicit interactions such as clicks. Recently, with the prevalence of embedding-based approaches, many recommender systems can accurately uncover the preference of users over unseen items. By learning the ranking model merely from the historical interactions University of Illinois Urbana Champaign data between users and items, a lot of successes have been observed in both industry and academia. Nevertheless, most (if not all) of these models suer from a lack of model explainability. Over the last few years, the demand for explainability of recommender systems drastically increased because customers are no longer satised with only high-quality recommendations, but they also require intuitive explanations. Explaining the relationship, in a human-interpretable way, between the users and its ranking decisions is critical for transparency, eectiveness, and trustworthiness. By providing a personalized recommender algorithm that explains why the recommendation results are presented in such a specic way, could signicantly improve user satisfaction. Without using the explicit user-item interaction features (e.g., review text), the pioneering work of recommender systems that utilized item-based collaborative ltering methods are explainable to some degrees, e.g. “similar movies other users also watched” or “this product is similar to the products you purchased before”, etc. Later on, the content-based recommender system becomes prevalent since it is also human interpretable by modeling users and items with various shared content information, for example, “genre”, “actors”, and “duration” in the movie recommendation problem. Besides using simple models with good explainability such as regression-based or tree-based models, there exist multiple strategies to build content-based explanation recommender systems. With the success of more business-driven models being applied into productions, such as matrix factorization based models [20,25], neural collaborative ltering based models [16,17], generative adversarial network based models [44,52], and graphbased models [15], the performance of recommender systems has dramatically improved over the last decade, but the lack of explainability is becoming a more severe concern. The available solutions to the aforementioned issues mostly rely on incorporating contextualized features into the model learning process. In recent years, multiple eorts have been made in this direction, such as FM [32], VBPR [14], DeepFM [13], etc. The majority of such eorts are devoted to designing the learning mechanism of the high-order feature interactions. Typical contextualized features for users could be their meta-information (e.g., locations, age, gender), and those for items could come from other sources (e.g., images, item titles, item descriptions). However, since their contextualized features are isolated between users and items, these models are more suitable for “cold start” recommendations and personalized ranking with rich features. Another line of research, such as EFM [51], utilized the user-item interaction features (text reviews) and aimed to learn the sentiment latent factors using matrix factorization. However, their learning objective is explicitly modeled for personalized sentiment attention instead of user preference. Therefore, it is dicult to directly apply to personalized ranking problems. In this work, we propose to achieve the explainability of personalized ranking by building a series of models with various architecture transparency: an intrinsically interpretable model (whitebox model with feature attention); a model with partial transparency (greybox model with adversarial training); and a post-hoc explanations model (blackbox model with counterfactual augmentations). Intuitively, we could easily build a fully transparent model such as logistic regression or decision tree that has good explainability. However, they mostly have limited performance (due to model linearity or inclination to overtting) under many application scenarios. In this work, we mainly focus on providing insights into the explainability regarding existing neural network based recommendation models and perform a horizontal comparison among all proposed strategies in terms of their characteristics and relationships. The main contribution of this paper can be summarized as follows: •We provide a systematic overview of the explainable recommendation models based on the level of model transparency requirement in a descending order: intrinsic attention based recommendation, adversarial explainable recommendation, and counterfactual explainable recommendation. •We compare the advantages and disadvantages of these three types of models in terms of their eectiveness, suitable domains, as well as their explainability via various qualitative visualizations. •We verify our claims and quantitatively evaluate all three models on ve publicly accessible data sets in terms of user explanation oriented metrics and user ranking oriented metrics. The rest of the paper is organized as follows. Section 2 is the related work. Section 3 describes the proposed three explainable recommendation models, and Section 4 presents the analysis of these models from various perspectives. The experimental results are demonstrated in Section 5, and we conclude the paper in Section 6. In this section, we introduce the background knowledge regarding the explainable recommendation and other highly related domains: counterfactual explanation, adversarial ranking, and attention-based recommendation. Recommender systems point the online users to a certain list of items with potential interests, thereby increasing the cross-sales and customer engagement. Many eorts have been devoted to this direction with the booming growth of e-commerce and online streaming services. Recent years have seen signicant improvement in performance with the emergence of more sophisticated models. However, only showing the recommended ranking list of items can hardly gain the trust and satisfaction of users. The reason is that many modern recommendation models are dicult to support the end-users in the decision-making processes by providing useful and meaningful explanations [9,38,50]. Traditional explainable models such as collaborative ltering [18,34], factorization-based model [29,45,51], and tree-based models [46] can leverage the explanations from user/item features [31], opinions [28], and summarized topics [49]. However, they usually have limited performance and lack the ability to extend to complicated scenarios. Recently, various types of neural network based explainable recommendation models have been proposed. A2CF [5] is an attribute-aware model that uses the residual network to model user-item reviews with joint sentiment analysis; SAERS [19] leveraged the power of gradient-weighted class activation mapping [35] and used the backpropagated ranking loss to generate item image’s saliency map as visual explanations; AMCF [30] maps the uninterpretable features into the interpretable aspect features by minimizing both ranking loss and interpretation loss, etc. Various kinds of explanation models are emerging every year, e.g., set operation based model [2], knowledge graph based explanation model [1], disentangled embedding explanation [26], and many others. These existing approaches generate explanations in dierent ways but lack the systemic analysis regarding explainable recommendation models from multiple perspectives. Based on the classication of explainable models [50], they can be either information source dependent (displaying explainable content that is humanreadable as the justication of the ranking model’s decision) or model architecture-dependent (deliberately designing the machine learning model with the ability to explain). In our work, we have considered both directions and studied three types of models that are coherently connected as well as complying with this classication taxonomy. Attention models are one type of input processing technique that allows the neural network to focus on certain parts of a complex input by assigning various modular learned weights. In this way, it allows the neural networks to approximate the visual attention mechanism humans use and the output can be calculated as an attention-weighted sum of the input. It has gained a lot of successes in multiple domains especially natural language processing and computer vision. In recent years, various works [4,8,22,23,48] have been proposed to use attention in recommender systems. Intuitively, dierent users may interact with the same item with attention to dierent aspects, e.g., words in the text, saliency areas in images, etc. NPA [48] and NRPA [24] use two-level attention models of both word-attention and document-attention to personalize news recommendation; NETE [22] learned template-controlled sentences from data to describe word features; VECF [6] utilized both attentionweighted visual features and GRU-based weak supervised learning to increase the model explanation on personalized ranking. In this work, we adopt a similar strategy where the attention weights are learned with ID embeddings and feature embeddings. However, for the sake of explanation, the attentions are applied to features instead of their embeddings. Despite the successes of recommendation models, numerous works have reported that they can be vulnerable when exposed to adversarial perturbations or even random perturbations [7,16]. Thus, two types of adversarial recommendation models have been proposed: adversarial learning [11,27] based RS and GAN-based RS. Among the rst type: APR [16] proposed to add adversarial perturbations on user/item embeddings to improve BPR[33] performance; AMR [37] leverages the adversarial training by adding adversarial perturbations to target image features. For the second type: IRGAN [44], for the rst time, formulated the IR tasks as a minimax game where the generator learns the discrete relevance distribution of entities and the discriminator is the ranking model that decides whether the user-item pair is relevant or not; CFGAN [3] relaxed the discrete constraints of IRGAN by introducing a generator with the ability to perform continuous embedding space synthesizing; PURE [52] further enriched the CFGAN by performing a positive-unlabeled sampling strategy and reached state-of-the-art performance. Nevertheless, none of them have the ability to provide explanations for the decisions made by the ranking model. Our work bridges the gap by introducing an explanation to the ranking model via adversarial training. Counterfactual explanation is a specic class of explanation model that provides the reasoning between decision making and input modications. In recommendations, counterfactual explanations provide perturbations to user input features and these modications can help the ranking model’s transition to the desirable decision boundary, e.g., ipping the original relevance preference over pair of items. Multiple eorts have been made to categorize and evaluate the counterfactual explanation models [41,43] and they have found successful applications in multiple domains, including natural language processing, computer vision [12], and recommender systems [10,36,39,47]. This is an emerging domain in recent years, and our work extends the existing work with multi-pass counterfactual augmentations. In this paper, we study the contextual recommendation problem whereU,I, andFare the set of users, items, and features, respectively. Given a user𝑢 ∈ U, a list of relevant items and their corresponding interactions are recorded. For each user-item pair (𝑢, 𝑖), there exists a group of raw features that can be used as the source of explanation to describe the interaction between the user 𝑢and the item𝑖. A good source of features is the review text from the user for the specic items. Compared with many existing works that only utilized the isolated features from either the user (e.g., biographical information) or the item (e.g., item title description or item image), the text review features have the explicit interactions between user and item, thus, it is more informative in terms of building an explainable ranking model. We further assumeΩto be the index set of observed user-item-feature interactions(𝑢, 𝑖, 𝒇) ∈ Ω where we denote𝒇as the observed review features within one specic application domain (e.g., electronics, fashion). Here, the corresponding user id, item id, as well as their review features should satisfy 𝑢 ∈ U, 𝑖 ∈ I and 𝒇⊂ F . Then, our recommendation problem is formulated as follows: Definition 1 (Contextualized Explainable Recommendation). Given:A set of usersU = {𝑢, 𝑢, ..., 𝑢}, a set of itemsI = {𝑖, 𝑖, ..., 𝑖}, a set of raw featuresF = {𝑤, 𝑤, ..., 𝑤}, the observed user-item-feature interaction sets Ω. Output:The estimated interaction scores of the unobserved items for each user𝑢inUas well as their corresponding feature explanations. It should be noted that under this problems setting, the features being utilized for users are not isolated from these ones from items, this is dierent from the traditional contextualized recommender systems [13,14,32]. Similar to [36,51], we decompose the review features𝒇∈ R(where𝑝 < 𝑃) into user feature vector𝒇∈ R and item feature vector𝒇∈ Rw.r.t. the𝑘-th review feature𝑤as shown in Equation (1). Intuitively, the user tends to comment on the features they care more about, thus, the user feature is merely a frequency-based vector. However, for items, their qualities can be accurately reected in the review sentiments from multiple users. Therefore, we formulate the item features by considering both review feature frequency and average sentiment. ! where𝑇is the maximum review scores a user can give to an item, 𝒇is the frequency that user𝑢mentioned review feature𝑘toward item𝑖, and𝑠is the average sentiment on review feature𝑘toward item𝑖. After transformation, all entries of these feature vectors are mapped into the range of[1,𝑇 ]and the the indexed set becomes Ω = {(𝑢, 𝑖, 𝒇, 𝒇)}. In this subsection, we introduce the neural attention recommendation (NAR) in detail. NAR is a white box recommendation model with intrinsically explainable designs and it has two major components: a user (item) network to learn user (item) representations, and a nal prediction network to predict the relevance scores. The overview of the NRA model is shown in Figure 1. 3.2.1 User (item) embedding network. For the embedding network, it maps each user𝑢and item𝑖into lower-dimensional representations𝒆∈ Rand𝒆∈ Rvia an embedding layer based on their IDs. Under the contextualized setting, we also have the decomposed feature vectors𝒇and𝒇available for each user𝑢and each item𝑖. Given one feature vector𝒇or𝒇, we embed each word𝑤within it to a lower dimensional representation𝒘∈ R(denoted in bold) via word embeddings. Within a specic application domain, each user will have attentions on a few features. Based on this intuition, we highlight the crucial features by designing the attention network as: where𝑀∈ Ris the attention mapping matrix between user embedding and word embedding. Thus, the attention weights of each user on𝑘-th word will be𝛼∈ (0,1). Each item can also obtain its attention weights 𝛽∈ (0, 1) in a similar manner with a dierent attention mapping matrix 𝑀∈ R. 3.2.2 Prediction network. To incorporate the review features into the prediction, one naive solution is to let the user and item share the same feature vector𝒇. In our design, dierent mapping functions, i.e., Equation (1), are applied to transform𝒇to the user as𝒇and to the item as𝒇, respectively. After the embedding network, we obtain the attention aggregated representations of users and items as follows. In the end, we concatenate the user and item embeddings and attention aggregated representations. Then, the nal relevance prediction between the user and item can be retrieved by passing the concatenated vector representation into a multi-layer perceptron (MLP) layer where⊕is the concatenation of two vectors and⊙is the elementwise product of two vectors. In this section, we introduce the contextualized adversarial ranking model (CAR) in detail. CAR is a gray box recommendation model with a partial transparency requirement on the feature gradients and it allows the ranking model to be retrained with adversarial augmentations. Specically, CAR has the same user (item) embedding network as NAR. In addition, CAR also has two distinct components: an adversarial augmentation network, and the nal prediction network. The overview of the NRA model is shown in Figure 2. 3.3.1 Prediction network. To incorporate the feature signal into the ranking prediction, a common practice is rst to convert user features and item features to latent embeddings, then, infuse them with the user (item) ID embeddings as a single latent vector representation. where𝐸and𝐸are the mapping matrices that convert the normalized user feature vector and item feature vector into the same latent space. 3.3.2 Adversarial augmentation network. Considering that we aim to use adversarial perturbation for generating the explainable perturbations, the rst step is to learn the base recommendation model. Then, we generate the adversarial perturbations that aim to maximize the loss of our model. Assuming that the BPR loss is𝐿=Í −log[𝜎 (𝑅− 𝑅)], then, the corresponding perturbations could be generated using the fast gradient sign method (FGSM [11]): whereΔis the perturbation estimated by calculating the gradient of the BPR loss w.r.t. the user feature vectors. This assumes that our loss is the rst-order derivative around𝒇. Meanwhile, the adversarial perturbation is under the max-norm constraint. In other words, after the L2 normalization, each dimension of𝜹should be constrained under the magnitude of𝜖in order to get a scalecontrollable minimum perturbation. Next, we augment the original training set with adversarial perturbations on top of the user features. Notice that adversarial augmentation with retraining could in general make the ranking model less sensitive to the adversarial perturbations, hence increasing the model robustness. In our contextualized setting, we could augment the data set with item features or even user (item) id embedding as well. However, as the target is to provide explanations for personalized rankings, we only need to perturb the user feature vector to serve this purpose. Eventually, our adversarial augmented prediction network will be trained in the following manner: From another perspective, we can think of the loss of the adversarial augmentations as a regularization term to control the trade-o between the ranking model that optimizes the pairwise loss and the attacking model that nds the most eective perturbations against the current ranking model. This trade-o can be balanced by introducing a hyper-parameter 𝜆: where𝜃is the ranking model’s parameter andΩis the useritem-feature set where the user feature vector has its adversarial perturbation 𝛿being added on user features. In this section, we introduce the counterfactual neural recommendation model (CNR) in detail. CNR is a blackbox recommendation model with model-agnostic designs to generate explanations for rankings. Similar to the CAR model, CNR has exactly the same user (item) embedding network and nal prediction network. Nevertheless, its augmentation network is designed by following the counterfactual thinking [36,42]:“User X purchased item A over item B because feature variables have values(𝑓, 𝑓, ...) associated with X. If user X has a few feature variables (𝑓, 𝑓, ...)changed and all other variables remained the same, user X would purchase item B over item A instead”. One should note that many such explanations exist, our objective is to nd a counterfactual perturbation that alters features as little as possible but returns the closest world where the ranking decision has been ipped. Based on the above principle, we can nd the counterfactual perturbation𝜹and an optimum solution𝜃for the ranking model by solving the following minimax problem: (𝜹, 𝜃) = arg minmax𝐿(Ω|𝜃) + 𝜉 dist(𝒇, 𝒇+ 𝜹) (9) where𝜹is the counterfactual perturbation of user features by xing the model parameter𝜃and minimizing this objective.𝑑 (·, ·)is the distance function that measures the dierence between original user features and perturbed user features. Meanwhile, the ranking loss should be maximized since the ranking decision is designed to be ipped after the feature perturbations are applied. In practice, we should initialize𝜃by training the ranking model without any perturbations, and then we can iteratively update𝜹and𝜃 until a perturbation has been found that is suciently small to ip the ranking predictions. For example, under the pairwise learning setting,𝑅> 𝑅will become𝑅< 𝑅if the counterfactual perturbation is applied on user features 𝒇+ 𝜹. The distance termdist(·, ·)is critical in counterfactual recommendations, especially when we aim to generate human interpretable explanations. Namely, only a small number of features should be changed and the rest remain untouched. In this way, these counterfactual perturbations are easier for humans to interpret. Thus, various distance functions can be dened to support our goal. One straightforward denition is L2 distancedist = ||𝜹||, which can be used to guarantee the user features are minimally perturbed. For the sake of introducing sparsity to𝜹so that humans can understand such explanation, we also introduce the L1 norm dist = ||𝜹||+||𝜹||. This is also called elastic net regularization, which combines ridge and lasso regularization. We compare all three types of explanation models in various aspects: model transparency, data manipulation, explanation constraints, learning objectives, and extensibility. Specially they all share certain commonalities but have a signicant dierence in terms of their explanation generating mechanisms. The high-level summary is presented in Table 1. Model transparency. From the intrinsic model to the adversarial model and then the counterfactual model, the requirements for model transparency gradually decrease. The intrinsic model (NAR) usually requires full transparency of the ranking model architecture and a well-designed attention mechanism with extra attention weights on user features. The adversarial model (CAR) only requires the gradient w.r.t. the features that need to be explained, and it is more like a partially transparent model. The counterfactual model (CNR) bypasses the substantial challenges of exploring the internal logic of the complex ranking systems. Therefore, counterfactual models can explain the rationale of the ranking decision-making process to a certain extent without opening the “black box”, and it is a model agnostic explanation approach. Learning objectives. The learning objectives of all three types of models will be the same as any popular ranking models where we usually minimize the log loss of the observed user-item interactions regardless of whether it is under the pointwise or the pairwise setting. The major dierence is reected in their explanation components: for the intrinsic model, the explanation is instantiated using attention weights, thus, no modication is needed in the objective; for the adversarial model, the perturbation is designed to be the gradient w.r.t. the user features that can maximize the log loss; for the counterfactual model, the perturbation is learned to serve the purpose of ipping the pre-trained ranking model’s decision, do it does not necessarily need to be the largest gradient w.r.t. the log loss. Explanation constraints. The intrinsic model has no constraints on the learning process. The adversarial model’s perturbation is maxnorm bounded by the magnitude of𝜖on the loss’s gradient. The counterfactual model’s perturbation is usually L2-norm regularized (and L1-norm regularized sometimes for the sake of explanation sparsity) but not strictly bounded. Data manipulations. The intrinsic model does not require any data augmentations since it is inherently designed to be explainable by attention weights. Both the adversarial model and counterfactual model require data augmentation to increase the model expressiveness and alleviate the data sparsity issue by adding more data points near the ranking decision boundary in latent feature space. Extensibility. The adversarial model and counterfactual model require very little or no eort to modify the existing model architectures. Therefore, they can be easily extended from any contextualized recommender systems. On the other hand, the intrinsic model requires a dedicated design for the attention mechanism. In this section, we aim to answer the following questions: RQ1:Do the learned models give competitive and human interpretable results? RQ2:Are the explainable mechanisms helpful with the ranking performance? RQ3:How do we evaluate the explanation results both quantitatively and qualitatively? 5.1.1 Data sets. We conduct the experiments on ve publicly accessible data setswith various sizes: Instrument, Video, Music, Beauty, and Clothing. For the preprocessing steps, we keep the user-item interaction pairs that have text reviews. To perform the negative sampling, we treat the 4-star and 5-star reviews as the positive feedback and the rest are unlabeled feedback [44,52]. Following this pro-processing protocol, all user and item interactions are eventually stored into the interaction matrix with entries of values 0 and 1. The details of all data set are summarized in Table 2. We utilize the Sentirepackage to further process each data set in order to get the word and sentiment pairs{(𝑤, 𝑠), ..., (𝑤, 𝑠)}for each user from the review text. 5.1.2 Evaluation protocols. For the purpose of evaluation, we have adopted the 4 : 1 train and test random split on each user when the number of positive examples is larger than 5. Otherwise, there will be no positive examples in testing. Similar to the existing work, we also perform the sampled evaluation [17,21] to speed up the computation where each positive test item will be randomly assigned with a candidate item pool of size 100. The nal performance evaluation uses both classication and ranking-based metrics, i.e., Precision, Recall, F1, Hit rate, NDCG, and MRR. Specically, MRR has the cutting threshold of 1 (i.e., MRR@1) and all the rest of the evaluation metrics have the cutting threshold of 10 (e.g., NDCG@10). It is also very important that we can quantitatively evaluate the explanations generated by our models. Therefore, besides the modelbased performance evaluation, we also perform the user-based evaluation. Intuitively, we aim to evaluate how good our model’s explanation is when matched with users’ sentiment in terms of the features. Let us assume that user𝑢has𝑘ground truth features 𝒈= {𝑔, 𝑔, ..., 𝑔}. Here, a feature will be added to𝒈only if user𝑢has already given sentiments (positive and negative) on this feature regarding all items. Meanwhile, for our ranking model, we have generated an explanation vector𝝓= {𝛿, 𝛿, ...𝛿}, i.e., attention weight vector in NAR or perturbation vector in CAR and CNR. We can sort this explanation vector𝝓in descending order and compare with users’ sentiment vector𝒈in terms of precision and recall at dierent cutting thresholds 𝑘. Precision@𝑘 =|𝝓∩ 𝒈||𝝓|and Recall@𝑘 =|𝝓∩ 𝒈||𝒈| Furthermore, F1@k is computed on top of precision and recall and NDCG@k will be straight forward to compute as well. 5.1.3 Baselines. We have considered various state-of-the-art baselines for the model performance evaluations: • Neural collaborative ltering (NCF)[17]: The most popular two tower based neural network recommendations. • Visual Bayesian personalized ranking (VBPR)[14]: A modied version of the original VBPR by using the neural network based embeddings along with the contextual features. • Counterfactual explainable recommendation (CER)[36]: The latest counterfactual explainable recommendation model. • Neural attention recommendation (NAR): Our proposed attention based intrinsic recommendation model. • Contextualized adversarial recommendation (CAR): Our proposed personalized ranking model with adversarial explanation. • Counterfactual neural recommendation (CNR): Our proposed iterative counterfactual explainable recommendation model. 5.1.4 Reproducible seing. To guarantee a fair comparison, we x both ID embedding and feature embedding to be 350 for all models on all data sets. Meanwhile, all models’ feature input regarding users and items will be kept the same across dierent models. We optimize ball baselines using Adam with a xed learning rate of 0.001 and the number of training epochs is set to 50. For CNR, the number of the outer loop iteration for the minimax problem (9) is set to 20. For the regularization parameters𝜆 =1 for CAR and 𝜉 =0.001 for CNR. The source code (pre-processing and modeling) will be released upon acceptance. For the recommendation evaluation, the comparison results are summarized in Tables 4-8. We observe that regardless of the data set size or the application domain, the adversarial perturbation and augmentation based method CAR always outperforms all other baselines. Meanwhile, we also observe that the counterfactual-based models, such as CER and CNR, also have competitive results. Specifically, CNR consistently beats CER in every aspect. This is because CNR is the minimax version of CER and it has multiple rounds of optimizations to nd the best counterfactual perturbations and keeps improving the ranking decision boundaries constantly. As a direct comparison, CAR also generates perturbations and retrains the ranking model on top. Nevertheless, the objective of CAR is to directly optimize the ranking performance. With more nearboundary perturbation examples generated, the embedded feature space will be further covered for the scenarios where the data set is extremely sparse. Similar phenomena have been observed in multiple recent works that use adversarial augmentation [3,16,52]. For the attention-based model NAR, due to its high model complexity, it does not perform well for small-scaled data sets because of model overtting. But its performance gets signicantly improved on large-scale data sets and becomes very competitive on them. The VBPR and NCF models in general are quite stable in terms of their performance, but due to the lack of a module for explanation, their prediction results are hardly interpretable. GT{vocal, tone, display,{character, drama, sea-{hits, artist, performance, pedal, sound}son, performances, ad-playing, charm} NAR{tune, hum, tab, tube,{eng, murder mystery,{lyricist, outtakes, comestyles}outdoors, storytelling,back, hook, studio} CAR{processor, bucks, neck,{york, grooves, ris, inoverdrive, plugs}cabin, twists, performer}tro, sounds} CNR{noise, epiphone, cord,{adventures, seasons,{album, smash, guitar holder, pickup}artistry, dramas, series}playing, guitar work, In terms of the user-orientated evaluation w.r.t. the explanations, we mainly focus on horizontally comparing all three proposed models. As shown in Figure 3, we observe that neural attention model NAR and counterfactual model CNR perform relatively well, but the adversarial perturbation model CAR has low scores across all ve data sets. The reason is that NAR and CNR are more suitable for model explanations: NAR has an explicitly designed architecture for learning the attention weights of the input features; CNR uses the counterfactual perturbation to alter the ranking decision, therefore, these perturbations are also explainable in terms of making a ranking decision. As a sharp contrast, CAR learns its adversarial perturbations to maximize the ranking loss and they have weak correlations with explanation because these perturbations do not guarantee to change the model ranking. In this section, we compare and connect all three explanation models, NAR, CAR, and CNR, by conducting various visualizations. These visualization show that quantitative evaluations and qualitative visualization are well aligned. Since the users are inclined to give reviews to items with positive or negative sentiment, then, the word-sentiment pairs of each user are one reliable proxy of ground truth (GT) for explanation evaluations. 5.4.1 Explanation correlation analysis. In Figure 4, we compute the average Pearson correlations of GT, NAR, CAR, and CNR in a pairwise manner and demonstrate them as the heatmaps. We observe that comparing GT, NAR and CNR have the most correlated explanations. Since the feature dimension is relatively high, and GT features are extremely sparse (only a few explanation words per user), therefore, the correlation computed w.r.t. GT are in general low as expected. However, the correlations between the proposed explanation models are relatively high. NAR and CNR are also strongly correlated with each other. 5.4.2 Explanation clustering analysis. In Figure 5, we transform all the explanation vectors of users into two-dimensional representations using t-SNE [40]. We observe that the users group together within the same model. CNR usually has a cluster of users that are close to the GT user clusters. The NAR clusters are slightly further distanced compared with CNR. CAR always has the most distanced group of users. 5.4.3 Top explainable words. As we can see in Table 9, we randomly pick the users for visualization and observe that the top-5 words of various explanation models are quite dierent. Nevertheless, they belong to the specic application domains as expected. In general, the retrieved explainable words for all three models also have large variations compared with GT. However, since we are only focusing on the top words, with large𝑘values being selected, we begin to see more overlaps of explainable words. In this paper, we propose three explainable recommendation models with various transparencies and perform a systematic and extensive comparison between them in terms of both quantitative evaluations and qualitative visualizations. The overall takeaway message for all these proposed models is: CAR has outstanding recommendation performance but does not have sucient ability to explain; NAR explains well but could easily overt in small-scale data sets and has reasonably recommendation results on large-scale data sets; CNR is the winner model which balances well in terms of both recommendation performance and explanation. Besides those, CNR is also a blackbox explainable ranking model and it can be easily extended to any existing ranking architecture.