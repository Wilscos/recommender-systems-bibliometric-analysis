O-policy Evaluation (OPE), or oine evaluation in general, evaluates the performance of hypothetical policies leveraging only oine log data. It is particularly useful in applications where the online interaction involves high stakes and expensive setting such as precision medicine and recommender systems. Since many OPE estimators have been proposed and some of them have hyperparameters to be tuned, there is an emerging challenge for practitioners to select and tune OPE estimators for their specic application. Unfortunately, identifying a reliable estimator from results reported in research papers is often dicult because the current experimental procedure evaluates and compares the estimators’ performance on a narrow set of hyperparameters and evaluation policies. Therefore, it is dicult to know which estimator is safe and reliable to use. In this work, we develop Interpretable Evaluation for Oine Evaluation (IEOE), an experimental procedure to evaluate OPE estimators’ robustness to changes in hyperparameters and/or evaluation policies in an interpretable manner. Then, using the IEOE procedure, we perform extensive evaluation of a wide variety of existing estimators on Open Bandit Dataset, a large-scale public real-world dataset for OPE. We demonstrate that our procedure can evaluate the estimators’ robustness to the hyperparamter choice, helping us avoid using unsafe estimators. Finally, we apply IEOE to realworld e-commerce platform data and demonstrate how to use our protocol in practice. o-policy evaluation, recommender systems, counterfactual estimation ACM Reference Format: Yuta Saito, Takuma Udagawa, Haruka Kiyohara, Kazuki Mogi, Yusuke Narita, and Kei Tateno. 2021. Evaluating the Robustness of O-Policy Evaluation. In Fifteenth ACM Conference on Recommender Systems (RecSys ’21), September 27-October 1, 2021, Amsterdam, Netherlands . ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/3460231.3474245 Interactive bandit and reinforcement learning algorithms have been used to optimize decision making in many real-life scenarios such as precision medicine, recommender systems, advertising, etc. We often use these algorithms to maximize the expected reward, but they also produce log data valuable for evaluating and redesigning future decision making. For example, the logs of a news recommender system record which news article was presented and whether the user read it, giving the decision maker a chance to make its recommendation more relevant. Exploiting log data is, however, more dicult than conventional supervised machine learning. This is because the result is only observed for the action chosen by the algorithm but not for all the other actions the system could have taken. The logs are also biased, as the logs overrepresent the actions favored by the algorithm used to collect the data. Online experiment or A/B test is a potential solution to this issue. It compares the performance of counterfactual algorithms in an online environment, enabling unbiased evaluation and comparison. However, A/B testing counterfactual algorithms is often dicult, since deploying a new policy to a real environment is time-consuming and may damage user satisfaction [7, 19]. This motivates us to study O-policy Evaluation (OPE), which aims to estimate the performance of an evaluation policy using only log data collected by a behavior policy. Such an evaluation allows us to compare the performance of candidate policies safely and helps us decide which policy to deploy in the eld. This alternative oine evaluation approach thus has the potential to overcome the above issues with the online A/B test approach. With growing interest in OPE, the research community has produced a number of estimators, including Direct Method (DM) [2], Inverse Probability Weighting (IPW) [17,21], Self-Normalized IPW (SNIPW) [25], Doubly Robust (DR) [4], Switch-DR [29], and Doubly Robust with Optimistic Shrinkage (DRos) [22]. One emerging challenge with this trend is that there is a need for practitioners to select and tune appropriate hyperparameters for OPE estimators for their specic application [23,28]. For example, DM rst estimates the expected reward function using an arbitrary machine learning method, then uses its estimate for OPE. Therefore, one has to identify a good machine learning method to estimate the expected reward before the oine evaluation phase. Identifying the appropriate machine learning method for DM is dicult, because its accuracy cannot be easily quantied from bandit data [8]. Sophisticated estimators such as Switch-DR [29] and DRos [22] show improved oine evaluation performance in some experiments. However, these estimators have a larger number of hyperparameters to be tuned compared to the baseline estimators. A diculty here is that the estimation accuracy of OPE estimators is highly sensitive to the choice of hyperparameters, as implied in empirical studies [20,28]. When we rely on OPE in real-world applications, it is desirable to use an estimator that is robust to the choice of hyperparameters and achieves accurate evaluations without requiring signicant hyperparameter tuning. Moreover, we want the estimators to be robust to other possible conguration changes such as evaluation policies. An estimator of this type is preferable, because tuning hyperparameters of OPE estimators with only logged bandit data is challenging in nature, and we often apply an estimator to several dierent policies to compare the performance of candidate policies oine. The aim of this paper is thus to enable a safer OPE practice by developing a procedure to evaluate the estimators’ robustness. Current dominant evaluation procedures.The current evaluation procedure used in OPE research is not suitable for evaluating the estimators’ robustness. Almost all OPE papers evaluate the estimator’s performance for a single given set of hyperparameters and an arbitrary evaluation policy [4,6,13–15,20,22,24,27,29]. Even though it is common to iterate trials with dierent random seeds to provide an estimate of the performance, this procedure cannot evaluate the estimators’ robustness to hyperparameter choices or the changes in evaluation policies, which is critical in real-world scenarios. The estimator’s performance derived from this common procedure does not properly account for the uncertainty in oine evaluation performance, as the reported performance metric is a single random variable drawn from the distribution over the estimator’s performance. Consequently, choosing an appropriate OPE estimator is dicult, as their robustness to hyperparameter choices or the changes in evaluation policies are not quantied in existing experiments. Contributions.Motivated towards promoting a reliable use of OPE in practice, we develop an interpretable and scalable evaluation procedure for OPE estimators that quanties their robustness to the choice of hyperparameters and possible changes in evaluation policies. Our evaluation procedure compares several OPE estimators as depicted in Figure 1. This gure compares the oine evaluation performance of IPW and DM by illustrating their accuracy distributions as we vary their hyperparameters, evaluation policies, and random seeds. The x-axis is the squared error in oine evaluation; a lower value indicates that an estimator is more accurate. The gure is visually interpretable, and in this case, we are condent that IPW is better, having lower squared errors with high probability, being robust to the changes in congurations, and being more accurate even in the worst case. In addition to developing the evaluation procedure, we have implemented open-source Python Figure 1: An example output of the proposed evaluation procedure for oline evaluation software,pyIEOE, so that researchers can easily implement our procedure in their experiments, and practitioners can identify the best estimator for their specic environment. Using our procedure and software, we evaluate a wide variety of existing OPE estimators on Open Bandit Dataset [20] (Section 5) and several classication datasets (Appendix A). Through these extensive experiments, we demonstrate that IEOE can provide informative results, in particular the estimators’ robustness to the hyperparameter settings and evaluation policy changes, which could not be obtained using typical experimental procedure in OPE research. Finally, as a proof of concept, we use our procedure to select the best estimator for the oine evaluation of coupon treatment policies on a real-world e-commerce platform. The platform uses OPE to improve its coupon optimization policy safely without implementing A/B tests. However, the platform’s data scientists do not know which OPE estimator is appropriate for their setting. We apply our procedure to provide an appropriate estimator choice for the platform. This real-world application demonstrates how to use our procedure to reduce uncertainty and risk that we face in real-world oine evaluation. Our contributions are summarized as follows. •We develop an experimental procedure called IEOE that is useful for identifying robust estimators and avoid the use of estimators sensitive to conguration changes. •We have implementedpyIEOE, open-source Python software, that facilitates the use of our experimental procedure both in research and in practice. •We conduct comprehensive benchmark experiments on public datasets and demonstrate that IEOE is useful for identifying estimators sensitive to conguration changes, and thus can help avoid potential failures in OPE. •We apply IEOE to a real-world OPE application and demonstrate how this procedure helps us safely conduct OPE in practice. We consider a general contextual bandit setting. Let𝑟 ∈ [0, 𝑟] denote a reward or outcome variable (e.g., whether a coupon assignment results in an increase in revenue) and𝑎 ∈ Abe a discrete action. We let𝑥 ∈ Xbe a context vector (e.g., the user’s demographic prole) that the decision maker observes when picking an action. Rewards and contexts are sampled from unknown probability distributions𝑝 (𝑟 | 𝑥,𝑎)and𝑝 (𝑥), respectively. We call a function𝜋:X → Δ(A)a policy. It maps each context𝑥 ∈ Xinto a distribution over actions, where𝜋 (𝑎 | 𝑥)is the probability of taking action 𝑎 given context vector 𝑥. LetD:= {(𝑥, 𝑎, 𝑟)}be a historical logged bandit feedback with𝑛observations.𝑎is a discrete variable indicating which action inAis chosen for individual𝑖.𝑟and𝑥denote the reward and the context observed for individual𝑖. We assume that a logged bandit feedback dataset is generated by a behavior policy 𝜋as follows: where each context-action-reward triplet is sampled independently from the identical product distribution. Then, for a function𝑓 (𝑥, 𝑎, 𝑟 ),Í we useE[𝑓 ]:= 𝑛𝑓 (𝑥, 𝑎, 𝑟)to denote its empirical expectation over𝑛observations inD. We also use𝑞(𝑥, 𝑎):= E[𝑟 | 𝑥, 𝑎]to denote the mean reward function for a given context and action. In OPE, we are interested in using historical logged bandit data to estimate the following policy value of a given evaluation policy 𝜋which might be dierent from 𝜋: Estimating𝑉 (𝜋)before deploying𝜋in an online environment is useful in practice, because𝜋may perform poorly. Additionally, this makes it possible to select an evaluation policy that maximizes the policy value by comparing their estimated performances without incurring additional implementation cost. Given the policy value as the estimand, the goal of researchers is to propose an accurate estimator. OPE estimatorˆ𝑉estimates the policy value of an arbitrary evaluation policy as𝑉 (𝜋) ≈ˆ𝑉 (𝜋;D,𝜃), whereDis an available logged bandit feedback dataset, and𝜃is a set of pre-dened hyperparameters ofˆ𝑉 . Below, we summarize the denitions and properties of several existing OPE estimators. We also summarize their built-in hyperparameters in Table 1. Direct Method (DM). DM [2] rst trains a supervised machine learning method, such as ridge regression, to estimate the mean reward function 𝑞. DM then estimates the policy value as whereˆ𝑞(𝑥, 𝑎)is the estimated mean reward function. Ifˆ𝑞(𝑥, 𝑎)is a good approximation to the mean reward function, this estimator accurately estimates the policy value of the evaluation policy. If ˆ𝑞(𝑥, 𝑎)fails to approximate the mean reward function well, however, the nal estimator tends to fail in OPE. Inverse Probability Weighting (IPW). To alleviate the issue with DM, researchers often use IPW [17,21]. IPW re-weights the rewards by the ratio of the evaluation policy to the behavior policy, as where𝜌 (𝑥, 𝑎):= 𝜋(𝑎 | 𝑥)/𝜋(𝑎 | 𝑥)is called the importance weight. When the behavior policy is known, IPW is unbiased and consistent for the policy value. However, it can have high variance, especially when the evaluation policy deviates signicantly from the behavior policy. To reduce the variance of IPW, the following weight clipping is often applied. where𝜆 ≥0 is a clipping hyperparamter. A lower value of𝜆greatly reduces the variance while introducing a large bias. Following Su et al. [22], we call IPW with weight clipping as IPW with Pessimistic Shrinkage (IPWps). When 𝜆 = ∞, IPWps is identical to IPW. Doubly Robust (DR). DR [4] combines DM and IPW as follows. DR uses the estimated mean reward function as a control variate to decrease the variance of IPW. It is also doubly robust in that it is consistent to the policy value if either the importance weight or the mean reward estimator is accurate. The weight clipping can also be applied to DR as follows. := E[E[ˆ𝑞(𝑥, 𝑎)] + min{𝜌 (𝑥, 𝑎), 𝜆}(𝑟−ˆ𝑞(𝑥, 𝑎))], where𝜆 ≥0 is a clipping hyperparamter. DR with weight clipping is called DR with Pessimistic Shrinkage (DRps). When𝜆 = ∞, DRps is identical to DR. Self-Normalized Estimators. SNIPW [25] is an approach to address the variance issue of IPW. It estimates the policy value by dividing the sum of weighted rewards by the sum of importance weights as: SNIPW is more stable than IPW, because the policy value estimated by SNIPW is bounded in the support of rewards, and its conditional variance given action and context is bounded by the conditional variance of the rewards [12]. IPW does not have these properties. We can dene Self-Normalized Doubly Robust (SNDR) in a similar manner as follows. Switch Estimator. DR can still be subject to the variance issue, particularly when the importance weights are large due to low overlap between behavior and evaluation policies. Switch-DR [29] aims to further reduce the variance by using DM where the importance weight is large: Note:ˆ𝑞 is an estimator for the mean reward function constructed by an arbitrary machine learning method. 𝐾 is the number of folds in the cross-tting procedure.ˆ𝜋is an estimated behavior policy. This is unnecessary when we know the true behavior policy, and thus it is in parentheses. 𝜏 and 𝜆 are non-negative hyperparameters for dening the corresponding estimators. whereI{·}is the indicator function and𝜏 ≥0 is a hyperparameter. Switch-DR interpolates between DM and DR. When𝜏 =0, it is identical to DM, while 𝜏 → ∞ yields DR. Doubly Robust with Optimistic Shrinkage (DRos). Su et al. [22] proposes DRos based on a new weight functionˆ𝜌:X × A → R that directly minimizes sharp bounds on the mean-squared-error (MSE) of the resulting estimator. DRos is dened as where𝜆 ≥0 is a hyperparameter andˆ𝜌is dened asˆ𝜌 (𝑥, 𝑎;𝜆):= 𝜌 (𝑥, 𝑎). When𝜆 =0,ˆ𝜌 (𝑥, 𝑎;𝜆) =0 leading to DM. On the other hand, as 𝜆 → ∞,ˆ𝜌 (𝑥, 𝑎; 𝜆) = 𝜌 (𝑥, 𝑎) leading to DR. Cross-Fitting Procedure. To obtain a reward estimator,ˆ𝑞, we sometimes use cross-tting to avoid the substantial bias that might arise due to overtting [16]. The cross-tting procedure constructs a model-dependent estimator such as DM and DR as follows: (1)Take a𝐾-fold random partition(D)of size𝑛of logged bandit feedback datasetDsuch that the size of each fold is𝑛= 𝑛/𝐾. Also, for each𝑘 =1,2, . . . 𝐾, we deneD:= (2)For each𝑘 =1,2, . . . 𝐾, construct reward estimators{ˆ𝑞} using the subset of data D. (3)Given{ˆ𝑞}and model-dependent estimatorˆ𝑉, estimateÍ the policy value by 𝐾ˆ𝑉 (𝜋; D,ˆ𝑞). Hyperparameter Tuning Proce dure. As Table 1 summarizes, most OPE estimators have hyperparameters such as𝜆,𝜏,𝐾, andˆ𝑞that should appropriately be set. Su et al. [22] proposes to select a set of hyperparameters based on the following criterion. whereV(𝜃;D)is the sample variance in OPE, andBiasUB(𝜃;D) is the upper bound of the bias estimated usingD. There are several ways to derive the bias upper bound as stated in Su et al. [22]. One way is the direct bias estimation: where𝛿 ∈ (0,1]is the condence delta to derive the high probability upper bound, and𝜌:= max𝜌 (𝑥, 𝑎)is the maximum importance weight.ˆ𝜌 (𝑥, 𝑎;𝜃)is the importance weight modied by a hyperparameter. For example, for IPWps and DRps,ˆ𝜌 (𝑥, 𝑎;𝜆) = min{𝜌 (𝑥, 𝑎), 𝜆}, and for Switch-DR,ˆ𝜌 (𝑥, 𝑎;𝜏) = 𝜌(𝑥, 𝑎)I{𝜌 (𝑥, 𝑎) ≤ 𝜏}. So far, we have seen that the OPE community has developed a variety of OPE estimators. What every OPE research paper should do in their experiments is to compare the performance (estimation accuracy) of the existing estimators and report the results. A typical and dominant method to do so is to estimate the following meansquared-error (MSE) as the estimator’s performance metric: MSE(ˆ𝑉 ; 𝜋, 𝜃 ) := E𝑉 (𝜋) −ˆ𝑉 (𝜋; D, 𝜃), where𝑉 (𝜋)is the policy value andˆ𝑉is an estimator to be evaluated. MSE measures the squared distance between the policy value and its estimated value; a lower value means a more accurate OPE byˆ𝑉. Researchers often calculate the MSE of each estimator several times with dierent random seeds and report its mean. The issue with this procedure is that most of the estimators have some hyperparameters that should be chosen properly before the estimation process. Moreover, the estimation performance can vary when evaluating dierent evaluation policies (especially in nite sample cases). However, the current dominant procedure for evaluating OPE estimators uses only one set of hyperparameters and an arbitrary evaluation policy for each estimator, and then discusses the derived results [1,6,24,27,29].This type of simplied experimental procedure does not accurately capture the uncertainty in the performance of OPE estimators. Specically, it cannot evaluate the robustness to hyperparameter choices and evaluation policy settings, as the reported score is for a single arbitrary set of hyperparameters and for a single evaluation policy. What is often critical in oine evaluation practices is to identify an estimator that performs well for a variety of evaluation policies without problem-specic hyperparameter tuning. An estimator robust to the changes in such congurations is usable reliably in uncertain real-life scenarios. In contrast, an estimator which performs well only on a narrow set of hyperparameters and evaluation policies entails a higher risk of failure in its particular application. Therefore, we want to avoid using such sensitive estimators as these estimators are more likely to fail. In the next section, we describe an experimental procedure that can evaluate the estimators’ robustness to experimental congurations, leading to informative estimator comparisons in OPE research and a reliable estimator selection in practice. Here, we outline our experimental protocol, Interpretable Evaluation for Oine Evaluation (IEOE). As we have discussed, the expected value of performance (e.g., MSE) alone is insucient to properly evaluate the real-world applicability of an estimator, as it discards information about its robustness to hyperparameter choices and changes in evaluation policies. We can conduct a more informative experiment by estimating the cumulative distribution function (CDF) of an estimator’s performance, as done in some studies on reinforcement learning [5,9,10]. CDF is the function,𝐹:R → [0,1], where 𝑍 is a random variable representing the performance metric of an estimator (e.g., the squared error).𝐹(𝑧)maps a performance metric𝑧to the probability that the estimator achieves a performance better or equal to that score, i.e., 𝐹(𝑧) := P(𝑍 ≤ 𝑧). When we have size𝑚of realizations of𝑍, i.e.,Z:= {𝑧, . . . , 𝑧}, we can estimate the CDF by Using the CDF for evaluating OPE estimators allows researchers to compare dierent estimators with respect to their robustness to the varying congurations. Specically, we can use the CDF to evaluate OPE estimators by examining the CDF of the estimators’ performance visually or computing some summary scores of the CDF as the estimators’ performance metric. For example, we can score an estimator by the area under the CDF curve (AU-∫ CDF):AU-CDF(𝑧):=𝐹(𝑧)𝑑𝑧.Another possible summary score is conditional value-at-risk (CVaR) which computes the expected value of a random variable above a given probability𝛼: CVaR(𝑍 ):= E[𝑍 | 𝑍 ≥ 𝐹(𝛼)],where𝐹(𝛼):= argmin{𝑧 | 𝐹(𝑧) ≥ 𝛼 }is the inverse of the CDF. When using CVaR, the estimators are evaluated based on the average performance of the bottom 100× (1− 𝛼)percent of trials. For example,CVaR(𝑍 ) is the average performance of the worst 30% of trials. In addition, we can use standard deviation (Std),E[(𝑍 − E[𝑍 ])], and some other moments such as the skewness ofˆ𝐹 (𝑧) as summary scores. IEOE with Synthetic or Classication Data. In research papers, it is common to use synthetic or classication data to evaluate OPE estimators [4,11,12,22,29]. We rst present how to apply the IEOE procedure to synthetic or classication data in Algorithm 1. To evaluate the estimation performance ofˆ𝑉, we need to specify a candidate set of hyperparametersΘ, a set of evaluation policies Π, a hyperparameter sampling function𝜙, and a set of random seedsS. Then, for every seed𝑠 ∈ S, the algorithm samples a set of hyperparameters𝜃 ∈ Θbased on sampler𝜙. What kind of𝜙 we use can change depending on the purpose of the evaluation of OPE. For example, we can use a hyperparameter tuning method for OPE estimators such as the method described in Section 2.2 as 𝜙, assuming practitioners use it in real-world applications. When we cannot implement such a hyperparameter tuning method for OPE due to its implementation cost or risk of overtting, we can be conservative and use the uniform distribution as𝜙in the evaluation of OPE. Next, the IEOE algorithm samples an evaluation policy𝜋∈ Πfrom the discrete uniform distribution. Then, it replicates the data generating process using the b ootstrap sampling fromD. A bootstrapped logged bandit feedback dataset is dened asD:= {(𝑥, 𝑎, 𝑟)}where each tuple(𝑥, 𝑎, 𝑟)is sampled independently fromDwith replacement. Finally, for sampled tuple (𝜋, D, 𝜃 ), it computes a performance metric (e.g., the squared error). After applying Algorithm 1 to several estimators and obtaining the empirical CDF of their evaluation performances, we can visualize them or compute some summary scores to evaluate and compare the estimators’ robustness. IEOE with Real-World Data. It is also possible to apply IEOE to real-world logged bandit data. Algorithm 2 presents IEOE that can be used in real-world applications. To evaluate the performance ofˆ𝑉with real-world data, we need to prepare several logged bandit feedback datasets{D}where each datasetDis collected by a policy𝜋. Then, for every seed𝑠 ∈ S, the algorithm samples a set of hyperparameters𝜃 ∈ Θbased on a sampler𝜙. Next, the algorithm samples an evaluation policy𝜋∈ Πfrom the discrete uniform distribution. Then, the evaluation and test sets are denedÐ asD= DandD=Dwhere the evaluation set is used in OPE and the test set is used to calculate the ground-truth performance of𝜋. Then, the algorithm replicates the environment using the bootstrap sampling fromD. A bootstrapped logged bandit feedback dataset is dened asD:= {(𝑥, 𝑎, 𝑟)}where each tuple(𝑥, 𝑎, 𝑟)is sampled independently fromDwith replacement. Finally, for a sampled tuple(𝜋, D, 𝜃 ), it computes the squared error as follows. where𝑉(𝜋;D) = E[𝑟]is the on-policy estimate of the policy value of 𝜋estimated with the test set. Following Algorithm 2, researchers can benchmark the robustness of OPE estimators using public real-world data. In addition, practitioners can avoid using unstable estimators by applying Algorithm 2 to their own bandit data. In this section, we use IEOE and evaluate the robustness of a wide variety of OPE estimators on Open Bandit Dataset (OBD). We run the experiments using our pyIEOE software. By using it, anyone can replicate the results easily. OBD is a set of logged bandit feedback datasets collected on a largescale fashion e-commerce platform provided by Saito et al. [20]. There are three campaigns, "ALL", "Men", and "Women". We use size 30,000 and 300,000 of randomly sub-sampled data from the "ALL" campaign. The dataset contains user context as feature vector𝑥 ∈ X, fashion item recommendation as action𝑎 ∈ A, and click indicator as reward𝑟 ∈ {0,1}. The dimensions of the feature vector𝑥is 20, and the number of actions is 80. The dataset consists of subsets of data collected by two dierent policies, the uniform random policy and the Bernoulli Thompson Sampling policy [26]. We letDdenote the dataset collected by uniform random policy𝜋andDdenote that collected by Bernoulli Thompson Sampling policy𝜋. We apply Algorithm 2 to obtain a set of SEs as the performance metric of the estimators. We use our protocol and evaluate DM, IPWps, SNIPW, DRps, SNDR, Switch-DR, and DRos in an interpretable manner. In the experiment, we use the true behavior policy contained in the dataset to derive importance weights. In this setting, SNIPW is hyperparameter-free, while the other estimators need to be tested for robustness to the choice of the pre-dened hyperparameters and changes in evaluation policies. In addition, we use the hyperparameter tuning method described in Section 2.2 to tune estimator-specic hyperparameters such as 𝜆 and 𝜏. Then, we use RandomizedSearchCV implemented in scikit-learn withn_iter =5 to tune hyperparameters of reward estimatorˆ𝑞. Tables 2 and 3 describe hyperparameter spacesΘfor each estimator. Finally, we set S = {0, 1, . . . , 499}. Note: LR/RR means that LogisticRegression (LR) is used when 𝑌 is binary and RidgeRigression (RR) is used otherwise. RF stands for RandomForest. ˆ𝜋is an estimated behavior policy. This is unnecessary when we know the true behavior policy. We estimate the behavior policy only in the experiments with classication data in Appendix A. Therefore, 𝜋is in parentheses. 𝐾 = 1 means that we do not use cross-tting and train the whole D. Note: We follow the scikit-learn package as to the names of the hyperparameters As default, we use max_iter = 10, 000 for LogisticRegression, n_estimators = 100 for RandomForest, and max_iter = 100 for LightGBM. Figure 2 visually compares the CDF of the estimators’ squared error. Table 4 reports AU-CDF, CVaR, and Std as summary scores. When the dataset size is small (𝑛 =30,000), we see that the typical way of reporting only the mean of the squared error cannot tell which estimator is accurate or robust. However, some other summary scores show that DM has more robust and stable estimation performance than other estimators, having lower CVaRand Std. Moreover, Figure 2 provides more detailed information about the estimators’ performance. Specically, DM performs better in the worst case while the other estimators show better performance ˆ𝑞 (and behavior policy estimatorˆ𝜋) in the region where squared error is lower than 0.2. Thus, when we are conservative and prioritize the worst case performance, DM is the most appropriate choice. Otherwise, other estimators might be a better choice. We cannot obtain this conclusion by comparing only the mean (typical metric) of the squared error. When the dataset size is large (𝑛 =300,000), we conrm that IPWps and SNIPW are more accurate than other model-based estimators. In particular, Figure 2 shows that IPWps performs better than other estimators in all region, meaning that we should use it whether we prioritize the best or the worst case performance. Overall, the results indicate that an appropriate estimator can drastically change depending on the situation such as the data size. Note: Larger value is better for AU-CDF and lower value is better for Mean, CVaR, and Std. Note that we normalize the scores by dividing them by the best score among all estimators. We use 𝑧= 1.0 × 10for 𝑛 = 30, 000 and 𝑧 redand greenfonts represent the best and second-best estimators, respectively. The blue Therefore, we argue that identifying a reasonable estimator before conducting OPE is essential in practice. Moreover, we demonstrate that the IEOE procedure can provide more informative insight as to the estimators’ performance compared to the typical metric. In this section, we apply the IEOE procedure to a real-world application. To show how to use IEOE in a real-world application, we conducted a data collection experiment on a real e-commerce platform in September 2020. The platform wants to use OPE to improve the performance of its coupon optimization policy safely without conducting A/B tests. However, it does not know which estimator is appropriate for its specic application and environment. Therefore, we apply the IEOE procedure with the aim of providing a suitable estimator choice for the platform. During the data collection experiment, we constructedD,D, andDby randomly assigning three dierent policies (𝜋,𝜋, and𝜋) to users on the platform. In this application,𝑥is a user’s context vector,𝑎is a coupon assignment variable (where there are four dierent types of coupons, i.e.,|A| =4), and𝑟is either a user’s content consumption indicator (binary outcome) or the revenue from each user observed within the 7-day period after the coupon assignment (continuous outcome). The total number of users considered in the experiment was 39,687, and each ofD, D, and Dhas approximately one third of the users. Note that, in this application, there is a risk of overtting due to the intensive hyperparameter tuning of OPE estimators, as the size of the logged bandit feedback data is not large. Moreover, the data scientists want to use an OPE estimator to evaluate the performance of several candidate policies. Therefore, we aim to nd an estimator that performs stably for a wide range of evaluation policies with fewer hyperparameters. To apply our evaluation procedure, we need to dene a performance metric (in step 8 of Algorithm 2). We can do this by using our realworld data. We rst pick one of the three policies as evaluation policy𝜋and regard the others as behavior policies. When we choose𝜋as the evaluation policy, we deneD= D∪ D andD= D. Then, by applying Algorithm 2, we obtain a set of SEs to evaluate the robustness and real-world applicability of the estimators. We use the IEOE protocol to evaluate the robustness of DM, IPWps, SNIPW, DRps, SNDR, Switch-DR, and DRos. Then, we utilize the experimental results to help the data scientists of the platform choose an appropriate estimator. During the data collection experiment, we logged the true action choice probabilities of the three policies, and thus SNIPW is hyperparameter-free. We use the hyperparameter spaces dened in Tables 2 and 3 for our real-world application. In addition, we use the hyperparameter tuning method described in Section 2.2 to tune estimator-specic hyperparameters such as𝜆and𝜏. Then, we use the uniform distribution as𝜙to sample hyperparameters of reward regression modelˆ𝑞. Finally, we setS = {0,1, . . . ,999}and We applied Algorithm 2 to the above estimators for the binary and continuous outcome data, respectively. Figure 3 compares the CDF of the estimators’ squared error for each outcome. First, it is obvious that SNIPW is the best estimator for the binary outcome case, achieving the best accuracy in almost all regions. We can also argue that SNIPW is preferable for the continuous outcome case, because it reveals the most accurate estimation in the worst case and is hyperparameter-free, although it underperforms DM in some cases. On the other hand, IPWps performs poorly for both outcomes, because our dataset is not large and some behavior policies are near deterministic, making IPWps an unstable estimator. Moreover, Switch-DR fails to accurately Note: Binary Outcome is the results when the outcome is each user’s content consumption indicator. Continuous Outcome is the results when the outcome is the revenue from each user observed within the 7-day period after the coupon assignment. Larger value is better for AU-CDF and lower value is better for Mean, CVaR, and Std. Note that we normalize the scores by dividing them by the best score among all estimators. We use 𝑧= 1.0 × 10for the binary outcome and 𝑧= 1.0 × 10for the continuous outcome to calculate AU-CDF. The red represent the best and second-best estimators, respectively. The bluefonts represent the worst estimator. evaluate the performance of the evaluation policies. Thus, it is unsafe to use these estimators in our application, even though we tune their hyperparameters (𝜆 or 𝜏). We additionally conrm the above observations in a quantitative manner. For both binary and continuous outcomes, we compute AU-CDF, CVaR, and Std of the squared error for each OPE estimator. We report these summary scores in Table 5, and the results demonstrate that SNIPW clearly outperforms other estimators in almost all situations. In particular, SNIPW is the best with respect to CVaRand Std for both binary and continuous outcomes, showing that this estimator is the most stable estimator in our environment. Moreover, SNIPW is hyperparameter-free, and overtting is less likely to occur compared to other estimators having some hyperparameters to be tuned. Through this evaluation of OPE estimators, we concluded thatthe e-commerce platform should use SNIPW for its oline evaluation.After comprehensive accuracy and stability verication, the platform is now using SNIPW to improve its coupon optimization policy safely. In this paper, we argued that the current dominant evaluation procedure for OPE cannot evaluate the robustness of the estimators’ performance. Instead, the IEOE procedure can provide an interpretable way to evaluate how robust each estimator is to the choice of hyperparameters or changes in evaluation policies. We have also developed open-source software to streamline our interpretable evaluation procedure. It enables rapid benchmarking and validation of OPE estimators so that practitioners can spend more time on real decision making problems, and OPE researchers can focus more on tackling advanced technical questions. We perform an extensive evaluation of a wide variety of OPE estimators and demonstrated that our experiments are more informative than a typical procedure, showing which estimators are more sensitive to conguration changes. Finally, we applied our procedure to a real-world application and demonstrated its practical usage. Although our procedure is useful to evaluate the robustness of estimators, we need to prepare at least two logged bandit feedback datasets collected by dierent policies to apply it to real-world applications, as described in Algorithm 2. Thus, it would be benecial to construct a procedure to enable the evaluation of OPE estimators with only logged bandit data collected by a single policy. The authors would like to thank Masahiro Nomura, Ryo Kuroiwa, and Richard Liu for their help in reviewing the paper. Additionally, we would like to thank the anonymous reviewers for their constructive reviews and discussions.