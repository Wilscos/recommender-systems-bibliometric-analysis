Recent advancements in reinforcement learning (RL) have found widespread application in many real-world domains. Often, these domains are built from rich data sources or real-world environments, which could contain sensitive information of many individuals. This is evident in domains such as autonomous driving, recommendation systems, trading, industrial assembly, and domestic service robots. For example, a recommendation system agent for an online shopping platform not only tracks the purchases made, but also how long the user hovers over an item that he/she did not purchase. Another example is when an autonomous driving agent not only learns the dynamics behind driving, but also identiﬁes people and predicts their behaviours on the streets, and how to respond to such situations. The reward function in these environments is often sensitive, as it is built on people’s private information. RL agents trained in such environments should not expose the private information of individuals. We therefore, need to use privacy-preserving methods to protect the rewards from being memorized in the agent’s policy in such a manner that the agent’s utility is not compromised. Recent works use Differential Privacy (DP) [ privacy guarantee. Like many other areas in AI, RL has also started adopting DP to establish a mathematical way of guaranteeing data privacy in RL environments. However, an important question is: does the privacy guarantee offered by the private policies translate well into protecting the reward function? If not, how can we understand the gap in Submitted to 34th Conference on Neural Information Processing Systems (NeurIPS 2020). Do not distribute. kritika.prakash@research.iiit.ac.infiza.husain@students.iiit.ac.in Reinforcement Learning (RL) enables agents to learn how to perform various tasks from scratch. In domains like autonomous driving, recommendation systems and more, optimal RL policies learned could cause a privacy breach if the policies memorize any part of the private reward. We study the set of existing differentially-private RL policies derived from various RL algorithms such as Value Iteration, Deep Q Networks, and Vanilla Proximal Policy Optimization. We propose a new Privacy-Aware Inverse RL (PRIL) analysis framework, that performs reward reconstruction as an adversarial attack on private policies that the agents may deploy. For this, we introduce the reward reconstruction attack, wherein we seek to reconstruct the original reward from a privacy-preserving policy using an Inverse RL algorithm. An adversary must do poorly at reconstructing the original reward function if the agent uses a tightly private policy. Using this framework, we empirically test the effectiveness of the privacy guarantee offered by the private algorithms on multiple instances of the FrozenLake domain of varying complexities. Based on the analysis performed, we infer a gap between the current standard of privacy offered and the standard of privacy needed to protect reward functions in RL. We do so by quantifying the extent to which each private policy protects the reward function by measuring distances between the original and reconstructed rewards. achieving meaningful privacy? What role does the type of reward function, algorithms (both RL and DP algorithms), and environment play? In other words, does privacy in policy translate well to privacy in reward? To investigate this, we ﬁrst build an autonomous agent whose aim is to learn a private policy using existing privacy techniques. The intent is to reach a goal state that helps maximize the agent’s expected reward in discrete ﬁnite-state environments. We then investigate the true level of reward privacy offered by the existing state-of-the-art privacy techniques for RL algorithms. We evaluate the private policies by estimating an adversary’s ability to reconstruct the original reward function from the agent’s learned policy. The ﬁeld of Inverse RL [ the observed, optimal behaviour. Given the “reverse engineering" nature of inverse RL, the reconstructed reward can be used as an adversarial attack on environments with protected rewards. Building on this key intuition, we propose the PRIL analysis framework that ﬁrst performs the reward reconstruction attack, and then computes its similarity to the original private reward via various distance metrics to test the strength of the attack. We apply this framework over a set of privacy preserving techniques: 1. Bellman update DP 2. Rényi-DP in deep learning (DL) 3. Functional noise DP These are applied to a set of RL algorithms: 1. Deep Q Network (DQN) 2. Vanilla Proximal Policy Optimization (PPO) 3. Value Iteration (VI) We build privacy into the agent from multiple perspectives: a DL perspective, an RL perspective, and a deep RL perspective. Through experiments, we show that there exists a gap between the privacy offered via the current private RL methods, and the privacy needed to protect reward functions. We also present the privacy-utility trade-off achieved by each policy. We show that privacy in policy does not translate to privacy in reward, as the reconstruction error is independent of the gap in privacy. Our experiments demonstrate that there is a need to further inspect the effectiveness of DP policies to protect sensitive reward functions. It is a serious privacy threat if an adversary is able to infer the rewards in spite of using a private policy. In summary, our key contributions are: 1. We study and analyze the existing set of privacy techniques for RL. 2. We introduce a novel reward reconstruction attack and supporting PRIL framework. 3. We empirically evaluate the performance of various private deep RL policies within our framework. 4. We identify and quantify the gap between the privacy offered in policy and the privacy needed in reward. The next section reviews related work. Section 3 provides background on RL, DP, and inverse RL. Section 4 explains the PRIL framework. Section 5 describes the experimental pipeline and setup. Section 6 provides an empirical analysis and discussion on our ﬁndings. Section 7 concludes the work. Deferred proofs and scope appears in the appendix. All source code and experiments are made publicly available Previous works on privacy-preserving RL make the use of DP. [ the upper conﬁdence bound and Q-learning algorithms, where each training episode comes from a different environment. [25] makes the use of functional noise to make Q-learning private. [ (PAC) and regret guarantees for private RL. [ algorithm. [ [14] presents a differentially private critic, and [23] presents a differentially private actor. In RL, a signiﬁcant amount of work has been done to perform adversarial attacks that target the quality of the learned policy ([9 However, very few works have looked into privacy attacks in RL. [ and its private transition dynamics, by performing privacy attacks using genetic algorithms and candidate inference. [8] introduce an adversarial inverse RL algorithm based on an adversarial reward learning formulation to improve robustness. While these previous works tackle building privacy in policies using Differential Privacy, they do not investigate its impact on the underlying private data i.e., the reward function used to learn the policies. Many adversarial attacks such as the membership inference attack, linkage attack, and data-reconstruction attack have been used to evaluate the level of privacy attainable by a data-analysis system. We introduce a new privacy attack that targets the private reward function. Our proposed attack - the reward-reconstruction attack is a special case of the data-reconstruction attack. We use inverse RL to learn the reward, and to assess the quality of private RL algorithms. In this section, we introduce the basics of RL, inverse RL, and DP. We focus on non-deterministic environments with discrete and ﬁnite state and action spaces. Let (S, A, P, R, γ, S nite actions, initial state distribution over starting with state π). The goal of an RL agent is to learn a policy that maximizes the expected cumulative reward. We use the following classes of RL algorithms to learn the optimal policy and value function for our experiments: 10] introduces a private way of using multi-party contextual bandits. For the actor-critic class of algorithms, ], [11], [13], [6]). Building on this, there has been work to make RL policies robust to such attacks [18]. )represent the MDP environment. Here,Sis the set of ﬁnite discrete states,Ais the set of ﬁ- P (s, a, s)is the transition probability of reaching statesby taking an actiona ∈ Ain states, where R(s)is the reward the agent receives in states ∈ S,γis the discount factor for future rewards, andSis the s. State-Action valueQ(s, a)denotes the value of taking actionain states(following some policy VI [21]: VI computes an optimal state value function for an MDP. The method uses Bellman updates to converge to the optimal values. DQN [16]: DQN is a model-free off-policy deep RL approach that uses a deep neural network for the Q-function which uses a batch of past experiences (replay memory) to train the agent to learn the optimal policy. PPO [22]: PPO is a ﬁrst-order optimization based policy-gradient algorithm that uses the actor-critic approach to ﬁnd the best policy. The actor model learns to take an action in an observed state by improving upon the feedback given by the critic model - that takes state as an input, and ﬁnds a value function estimating future rewards with the help of a deep neural network. Inverse RL [ We use the method of inverse RL in ﬁnite-state spaces to reconstruct the private reward function by solving a linear programming (LP) [ sub-optimal policies). Since this is an under-constrained problem, we choose the reward with the smallest L 17] is a method of extracting a reward function, given the observed, optimal behaviour in an environment. DP [7] is considered to be the golden standard of computational privacy. It allows us to quantify the degree of privacy achievable by a mechanism. It is built on the concept of adjacent databases. In the context of our work, the RL agents learn optimal policies by exploring the environment and taking in rewards as a feedback for their actions. Since we care about the privacy of the rewards, we say that two reward functions are adjacent if the maximum point-wise difference is upper bounded by 1. Deﬁnition 1 (, δ) privacy if for any two adjacent inputs d, d Deﬁnition 2 α order α > 1 is where P(x) and Q(x) are the respective probability densities of P and Q at x. Deﬁnition 3 (α, ) order α, or (α, )-RDP for short, if for any adjacent d, d We use the following private RL methods in our experiments: We assume that we share the entire training process including every loss gradient update publicly, (worst-case privacy guarantee). We introduce a novel case of the data-reconstruction attack - the reward reconstruction attack for RL, as we wish to protect the reward function from adversaries. We assume that the adversary has knowledge of the environment and the learned private policy. Using this information, the adversary tries to reverse engineer the reward function. While many methods can be used to do so, we focus on the inverse RL technique in this paper, as it seems to be the best tool at our disposal. Using inverse RL, we perform the reward reconstruction attack, to determine how effective a private policy is at protecting the reward function. It does so by computing (a variety of) distances between the reconstructed reward and the original reward. The framework takes as input the original reward function policyP andR, from The larger the distance, the stronger the RL policy’s privacy guarantee (in protecting the reward function). We use multiple distance metrics, such as - L We will now discuss our overall experimental pipeline and setup. We perform our experiments on 24 custom environments (as shown in Figure 2) in the FrozenLake domain - a discrete-state OpenAI Gym [ -Rènyi Divergence: For two probability distributions P and Q deﬁned overR, the Rènyi divergence of ) of order α between outputs of the algorithm is less than e. Bellman update DP: In this method, noise is added locally to the Bellman update step of VI, such that it satisﬁes the deﬁnition of -DP [7]. Rényi-DP in DL: This is a natural relaxation of DP that we use for multiple DP methods - DQN-DP-SGD, PPO-DP-SGD, and more [1], [20]. Functional noise DP in Q-Learning: In this, functional noise is iteratively added to the value function in the training process. The aim is to protect the value function [25]. trained using the same algorithm. Using the inverse RL algorithm, it predicts the reconstructed rewards,R PandPrespectively. It then computes the distancesd(R, R)andd(R, R), and compares them. Figure 2: All 24 FrozenLake environments used. Here, green: goal (G), red: frozen (F), yellow: start state, white: safe (S), and blue: high-reward (A). environments, the agent controls its movement and navigates in a grid-world. Additionally, the movement direction of the agent is uncertain and is only partially dependent on the direction chosen. The agent is rewarded for ﬁnding the most rewarding walkable path to the goal state. The grid-world environment has ﬁve possible states - safe (S), frozen (F), hole (H), high-reward (A) and goal (G). The agent has four possible actions - up, down, left and right. Half of the 24 environments are of a grid-size 5x5, and the remaining half are of a grid-size 10x10. The agent moves around the grid until it reaches the goal state. If it falls into the hole, it has to start from the beginning and is given a low reward. The process continues until it eventually reaches the goal state. We measure the performance of 8 private algorithms across three algorithm classes - VI, DQN, and PPO: For VI, we evaluate the performance of VI-DP-Bellman (private Bellman update via local DP) as well as non-private VI. For DQN, we evaluate the performance of the following cases (with and without privacy): 1. DQN-DP-SGD: DP-SGD optimizer + ReLU activations 2. DQN-DP-Adam: DP-Adam optimizer + ReLU activations 3. DQN-DP-Shoe: DP-SGD optimizer + tan-h activations 4. DQN-DP-FN: DQN + functional noise For PPO, we evaluate the performance of the following cases (with and without privacy in the actor network): 1. PPO-DP-SGD: DP-SGD optimizer + ReLU activations 2. PPO-DP-Adam: DP-Adam optimizer + ReLU activations 3. PPO-DP-Shoe: DP-SGD optimizer + tan-h activations We evaluate the privacy-utility trade-off by simultaneously measuring the average returns of the the private policy over multiple sample trajectories during test time (as shown in Figure 4). For each private RL algorithm, we consider the non-private version of the RL policy (privacy budget the policy is, the larger the reward distance should be (between the original reward and the reconstructed reward). Figure 3: Original and reconstructed reward heatmaps for two 5x5 FrozenLake environments (1a, 2a) and for two 10x10 FrozenLake environments (3a, 4a) Reward distance as a measure of privacy guarantee: We used four reward distance metrics for our experiments distance, original reward function and the recovered reward function. This is a measure of the degree of privacy of a policy - the larger the reward distance, the more private the policy is. The metrics are calculated as follows: 1. L 2. L 3. L vectors. 4. Sign change count: Measure the number of sign changes from R to R Since each distance metric is in a different space, all the distances evaluated together allow us to get a deeper insight into the reward reconstruction mechanism, and the optimality and privacy of policies. Policy return as a measure of agent utility: We measure how much utility the learned private policies achieve by observing how they perform during test-time, by calculating the average discounted returns over multiple trajectories played by the agent following the policy. Implementation details: We build 24 custom FrozenLake environments using the Open AI gym toolkit. We use an LP solver to solve the objective functions of Inverse RL. We build the Deep RL experiments using TensorFlow 2.4, and add privacy using TensorFlow Privacy. We build VI-DP-Bellman private algorithm from scratch, and use the publicly available code provided for the DQN-DP-FN strategy [ inverse RL that makes the use of cvxopt [ the RL agents with a total of 8 GPUs and 8 CPUs. All experiments spanned across 9 privacy budgets, 24 environments, 8 policy classes, repeating each experiment 10 times (to account for the randomness stemming from private noise mechanisms and DL optimization). The total runtime for the entire set of experiments was 3 weeks. To the best of our knowledge, this is the ﬁrst time such an attack and evaluation is conducted - hence the lack of pre-existing baselines. We evaluate each algorithm against the baseline case of no privacy ( = ∞). Hyper-parameters and assumptions: Ldistance,Ldistance, and the sign change count. The idea is to measure the similarity between the distance: Normalize the rewardsR, RusingL-norm, and then take theLdistance across the 2 vectors. distance: Normalize the rewardsR, RusingL-norm, and then take theLdistance across the 2 vectors. distance: Normalize the rewardsR, RusingL-norm, and then take theLdistance across the 2 We choose 9 different privacy budgets: - ∈ {0.1, 0.105, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, ∞}(including the noprivacy case where = ∞). The way we choose these budgets is to have a diverse order of magnitudes within a reasonable range of budget values. We include0.1and0.105on purpose - to show the increased sensitivity of the private algorithm with slight changes to the budget at lower values. We observed a budget lower bound of0.1- adding anymore noise results in a budget of0. We present the list of standard deviations used for each  budget for each policy class in table 5. Figure 4: Utility (test-time return) vs privacy trade-off for all policies averaged across grid-sizes 5x5 (a) and 10x10 (b). Legend is the same as that in ﬁgure 5. Figure 5: Reward distance vs privacy budget graphs for all strategies: 1,5: distance, 4,8: Sign change counts. 1,2,3,4: averaged over 5x5 grid sized environments, 5,6,7,8: averaged over 10x10 grid sized environments The intuition behind the DP-Shoe set of strategies comes from [20] titled "Making the shoe ﬁt: : Architectures, initializations, and tuning for learning with privacy". Their key result states that SGD optimizer is strictly Figure 6: Variance Violin plots for (1): PPO-DP-Shoe on an MDP of grid-size 5x5 with a privacy budget showing variance over 10 runs; (2): VI-DP-Bellman on an MDP of grid-size 10x10 with a privacy budget showing variance over 20 runs • Learning rate = 0.15 • Mini-batch size = 50 • Number of micro-batches = 5 (each of size 10) • Discount factor γ = 0.99 • Wind factor = 0.0001 • For reproducibility of the experiments, we ﬁxed a random seed and in our case we used seed = 0. better than Adam optimizer, and that using tan-h activations are strictly better than using ReLu activations in the case of differentially private training. Hence, we named the DP-SGD optimizer with tan-h activations as the DP-Shoe method. This is also the reason why we do not perform experiments on the DP-Adam + tan-h combination. For all the DQN and PPO policy classes, we assume anl-sensitivity of1.0- as all the private deep learning techniques allow for us to scale-down the data, to bring it to unit sensitivity. We present the list ofl-sensitivities for each policy class in table 3. For PPO policy class, we assume that only the actor network will be using a DP optimizer (and not the critic network). We based this on the result from [23], which shows that DP-Actor outperforms DP-Critic and DP-Both (where both the networks are made private using DP). (sigmas) for our chosen  budgets, with an additive δ budget of 10 Figure 5 presents the variation in reward distances (y-axis) (of each type: the original reward) with an increase in the at the very end ( 5x5, whereas the second row shows the results corresponding to the environments of grid size 10x10. Each graph shows this relationship for all 8 private algorithms: DQN-DP-SGD, DQN-DP-Shoe, DQN-DP-Adam, DQN-DP-FN, PPO-DP-SGD, PPO-DP-Shoe, PPO-DP-Adam, and VI-DP-Bellman. The graphs show that there is no clear indication of any private strategy improving at reconstructing the reward (wrt all distances) with a relaxation in the privacy budget - thus, rendering all strategies ineffective at being a truly meaningful private strategy. The reward reconstruction distance(s) are independent from the privacy budgets across all policy classes. We observe the same lack of trend across both rows, i.e., both for the 5x5 results in row 1 and 10x10 results in row 2. Figure 4 presents the trade-off between the amount of utility (expected return: y-axis) and the degree of privacy ( budget: x-axis) achieved by a private RL privacy. Graph 1 gives us the average trade-off for 5x5 environments, and graph 2 - for the 10x10 environments. Almost all algorithms exhibit comparable performance - with the exception of DQN-DP-FN, which performs signiﬁcantly worse. Figure 7: Aggregated algorithms: DQN, PPO, and VI. Convergence threshold for value iteration= 1.0 × 10. Alternatively, we enforce the number of iterations to be less than 10000. For each graph in ﬁgure 5, the DQN-DP-FN strategy is giving straight lines. This implies that this strategy is extremely privacy agnostic in this domain. We made sure that there were no issues with the implementation itself. Our method of creating grid-world environment maps aimed for diversity in spatial rewards and richness in reward structure. We built maps that would lead to interesting policies - while some policies would be very speciﬁc (like learning a single route), other policies would have multiple optimal routes. We also show that the absolute reward values themselves do not dictate the policy - it’s their values taken relative to their surrounding reward values that inﬂuences the policy. We used the Rényi-DP privacy engine from the TensorFlow Privacy library to compute standard deviations  = ∞). The ﬁrst row shows results averaged over the 12 FrozenLake environments of grid size Figure 3 shows the heatmaps and reward structures of 4 different MDPs (row 1) and their corresponding reconstructed rewards (row 2) using the VI-DP-Bellman algorithm. From 1a and 1b we can observe that the agent is able to clearly detect the obstacle bar on the right. From 2a and 2b we can infer that the agent ﬁnds a straight line path along the edges that is rewarding. But it also wrongly identiﬁes some states as rewarding, even though they might be dead-ends (bright yellow bottom-right corner). Perhaps, this could be because the agent is able to survive the cost of the state directly above it, if it means that the agent can easily reach the goal state in a short amount of time. In 3a and 3b we learn that even in such a rich environment, the agent is able to reconstruct the reward structure from an implementation of the policy. It can clearly identify rewarding blocks within the entire maze. And ﬁnally, in 4a and 4b, the agent learns to stick to the edges to maximize reward early-on - which is why its evaluation of the central region is poor, and privacy preserving. Figure 7 shows that DQN algorithms give us the strongest reward privacy, followed by PPO algorithms, followed by VI algorithms - that do a very poor job at protecting the reward - despite having very similar utilities (from ﬁgure 4). Figure 6 presents violin plots for speciﬁc instances of PPO-DP-Shoe and VI-DP-Bellman techniques on an MDP of size 5x5 and 10x10 respectively, averaged over multiple runs. The x-axis shows all the states whereas the y-axis shows violin plots of the value of the reconstructed reward. It shows that across multiple runs of our experiments, we are able to bound the reconstructed reward’s variance (for each state) by a term of the order of in most other cases as well. These rewards are learnt by an adversary via the PRIL framework. Based on the experiments performed, we can say that there is a considerable gap between the privacy provided by the existing private methods (in policy), and the level of privacy needed to protect the reward function from the inverse RL attack. We can also infer that techniques using Deep RL methods are able to learn the policy in a more general manner, as compared to non-deep methods. We address the need for better privacy techniques for RL algorithms that can effectively protect the reward function. We hope that our work inspires a deeper theoretical understanding of the limits to minimizing the gap, as well as its consequences in real-world applications. Besides thoroughly testing our code, we perform an extensive span of experiments. Contrasting our results with the baseline (no privacy), we ﬁnd the reward distances to be quite similar. We therefore, believe that the source of the privacy gap is not experimental error. Our survey of papers that experiment on FrozenLake shows that the commonly used grid sizes are while we experimented with slightly larger grid sizes privacy gap upon further increase of grid size since the reward would be richer in information, and the DP sensitivity is independent of the grid size. While we demonstrate our work on a grid-world domain, we believe it is extendible to real-world domains with sensitive data. Our work is the ﬁrst in this direction and serves as evidence that there is a need to inspect further. Deep RL is increasingly being used for recommendation systems (RecSys) in dynamic environments. Consider the case when the recommendation engine for every user is a unique private RL policy whose job is to recommend items to users and learn their preferences in an online fashion (given the user’s historical data). The reward is the user’s feedback (ratings) to the recommended action. While the policy provides privacy guarantees for its training process, it can leak the user’s feedback when subject to the re-identiﬁcation attack via reward reconstruction. PRILcan help assess this threat better. We surveyed a range of Inverse RL (IRL) algorithms - ﬁnite state space LP, sample trajectories ([ and maximum entropy IRL ([ a signiﬁcant privacy gap. The LP method acts as a baseline for other IRL methods. With increased complexity, the reward function would be represented parametrically which would allow the system to evaluate performance on much larger and richer (and potentially continuous) environments. As the performance of IRL as an attacker improves, we expect the issue of privacy gap to become even more important to address. Our work introduces a novel direction of evaluating the privacy guarantees of RL systems. In the future, we hope to build on our work in multiple ways: extending to the multi-agent scenario, extending to a diverse set of domains, assessing the effect of generalization and exploration on privacy, testing the performance of other RL algorithms such as PPO-Clip and PPO-KL, and evaluating the performance of other complex inverse RL algorithms in improving the framework. This paper introduces a new Privacy-Aware Inverse RL analysis framework (PRIL) for enhancing reward privacy in reinforcement learning (RL) that performs a novel reward reconstruction attack and demonstrated its ability to fairly assess the level of privacy achieved in protecting the reward structure from adversarial attacks. We studied the set of existing privacy techniques for RL, performed a detailed evaluation of their effectiveness and identiﬁed that there is a signiﬁcant gap between the current standard of privacy offered and the standard of privacy needed to protect reward functions in RL. We quantify this gap by measuring distances between the original and reconstructed rewards via the reward reconstruction attack.