Machine learning techniques are increasingly be ing used to make consequential decisions in the real world [Konig et al., 2021]. At the same time, research ha s shown machine learning algorithm s are capable of learning unethical biases [Caliskan et al., 2017]. The se two trends have lead researchers to begin to examine machine learning techniques that are intelligible to the human dec ision makers that are using them, so they can e nsure decisions are made in a fair way. Counterfactual explanations are a popular approach to intellig ible machine learning [Verma et al., 2020]. The general approach to these explanations is coming up with a systematic way to determine how a machine learning algo rithm would have behaved given a different input. A virtue of this type of explanation is that it allows p e ople who have received a negative evaluation by an algorithm to understand what a successful applicant would look like. Algorithmic recourse is an extension of counterfactual explanations that seeks to make action recommendations that would allow an applicant to im prove their negative evaluation. One approach to algorithm ic recourse is using nearest counter factu al explanations, which involves ﬁnding the closest example that would have produced the desirable outcome and recomm ending making changes to the feature vector to make it resemble the nearest counterfactual example. This is often set up as the optimization problem (1 ) below [Karimi et al., 2021]. Preprint. Under review. Departmen t of Computer ScienceDepartmen t of Computer Science Philadelphia, PA 19104Philadelphia, PA 19104 ao543@drexel.eduek826@drexel.edu The recent adoption of machine learning as a tool in real world decision making has spurred interest in understanding how these decisions are being ma de. Counterfactual Explanations are a popular inte rpretable machine learning technique that aims to understand how a machine learnin g model would behave if given alternative inputs. Many explanations attempt to go further and recommend actions an individual could take to obtain a more desirable output from the model. These recommendations are known as algorithmic recourse. Past work has largely focused on the effect algor ithmic recourse has on a single agent. In this work, we show that when the assumption of a single agent environment is relaxed, current approa c hes to algorithmic recourse fail to guarantee certain e thically desirable properties. Instead, we propose a new g ame theory inspired framework for providing a lgorithmic r e course in a multi-agent environmen t that does guarantee these properties. The intuition behind the optimization is that we are tying to recommend the lowest cost action , δ that if it w ere taken would cause the agent’s current feature values to shift to a po int where the outcome of the machine learning model would be beyond some threshold , t. The cost function for an agent taking an action δ while at a current feature value x of the agent’s features after taking action δ vector, x An ag e nt often has only a limited number of actions it can take. For example, it may not be feasible for a loan applicant to increase their savings by m ore than 10% to change a model’s outcome from likely default to unlikely default. The set of feasible actions are denoted by the set F . P is a related set that denote s the set of all feature vectors that are actually possible for an agent to achieve. For example, this set will not contain any featu re vectors whereby the agent has a different height. Frequently implicit in the literature on nearest counterfactual explanations and algorithm ic recourse is the assumption that we only ne ed to take into account the effect an agent’s actions have o n their own model evaluation [ K arimi et al., 2020]. In the real world, a benevolent decision making entity would likely be c oncerned with the effects of an agent’s actions on all of the agents it’s advising. In strategic environments, an individual’s utility often depends on its own actions and th e actions of other agents. Therefore, recommended actions that are are optimal from an individual agent’s perspective may not b e optimal when considered from a multi-agent perspective. The following example, based on the classic strategic game the prisoner’s dilemma, is illu stra tive of the problems that can arise when using recourse techniques based on a single-agent per spective. Assume an entity is advising two agents on how to reduce their current priso n sentence. Each agent has the option to collaborate with the police and be tray the other agent or remain silent and refuse to collabo rate. The sentence re ductions for each agent are a function of both their own action and the action of the other agent and are given by table 1. Assume the entity has mod els h perfectly predict the payoffs given in the table below for ag ents one and two respectively. Assume that agent one comes to the entity and notes it’s current sentence reductio n is given by h(x= [0, 1]) = 1 years. P = {[0, 0], [1, 0], [0, 1], [1, 1]} and F = {[1, 0], [0, 0]}. In this example, the optimal rec ourse acc ording to the nearest counterfactual explanation is δ The new predicted payoff for agent one is h has been achieved. There are two additional facts to note. h input x The algorithmic recourse recommendation has made agent two worse off and decreased the sum of beneﬁts to both agents. . The value of the machine learning model at some feature vector x is given by h (x). = [1, 1]. Further more, h(x) + h(x) = 7 < h(x) + h(x) = 11. Now consider the alternate case where agen t one comes to the entity an d notes it’s current senten ce reduction is given by h outcome is δ previous case, the amount it makes agent agent one better off is greater tha n the amount it makes agent two worse off, so the group is better off. The above examples illustrates inherent trade-offs that must be consid e red when making recourse recommendations in strategic situations where an agent’s outcome depends on her actions and those of o ther agents. An ethically ideal recommended ac tion would improve the pred iction of the agent being advised (principle agent), not worsen the predic tion for any other agent, and inc rease the sum of the predictions for all of the agents. The latter two properties are known in ga me theory as Pareto efﬁciency and social welfare efﬁciency [Tadelis, 2013]. When considering the single agent environment, any action that achieves one must trivially satisfy all three. However, when there are at least two agents, an action that a c hieves one goal can fail to achieve one or both of the other two. Our work is not the ﬁrst to examine recourse from a multi-agent perspec tive. Rawal and Lakkaraju [2020] propose a framework for multi-agent recourse but do not incorporate causal relationships between features when making recomm endations. Fur thermore, although they take into account that recourse recommendations might perform differently for different subgroups , they don’t address the idea tha t an agent acting on a recommendation might affect an other agen t’s model outcome. In order to make a multi-agent recourse recommendation, the effect of the recommendation on the predictions for all the agents must be known. Causal and counterfactual knowledge of this kind can be encoded a s a structural causal model M and a corresponding causal graph G <F, X, U> is a th ree tuple of en dogenous variables X, exogenous variables U, and a set of structural equations F = {f In the directed causal graph G An edge con nects a node n argument in th e function f Counterfactual statements can be represented using do-operations of the form do(X do-operation transforms M to M The corresponding causal graph G correspo nding to X A = do({X uniquely determined given any U and vice versa. Hence any structural counterfactual query can be computed in th e following way x The prisoner’s dilemma example can be stated as the following structural causal model. M =< F, X, U > where X = {h = [1, 0] As befo re, this recommendation makes agent two worse off. Unlike the := a}). Assuming no hidden confou nders and an invertible F, any X can be Figure 1: Figure 1: The causal graph corresponding to the prisoner’s dilemma example. Learning causal graphs from data is currently an open area of researc h [Heinze-Deml et al., 2018]. Although progress has b e en made in recent years, the difﬁculty of learning a causal graph or structural causal model f rom d ata is a serious limitation on any approach to recourse, su ch a s ours, that assumes this knowledge. Below we present three different algorithmic recourse optimization problems: single agent efﬁcient, social welfare efﬁcient, and Pareto efﬁcient. Each problems correspond s to a different desirab le ethical property that the recommendation should have. Karimi et al. [2021] showed that incorporating causal models was necessary for making accurate recourse commendations in the case of single agent recommendations. They restate the problem of making recourse recommendations as one of m aking minimal interventions as follows. The intuition behind the problem is that we are trying to recommend an actio n a that minimizes agent i’s cost function while meeting a set o f constraints that ensure action a changes the feature vector to one th at causes the model predicting agent i’s outcome, h The new feature vector is the structural counterfactual outcome and denoted x set of plausible actions. This is the correct statement of the problem when the goal is to maximize the outcome for the princip le agent. In the case that the entity wants to recommend an action that increases the sum o f the predictions for the N-agents it a dvises, the following is the co rrect optimizatio n prob le m. An alter nate variation is one where the sum of the predictions isn’t decreasing and ﬁrst constraint isn’t strict. In the case that the entity wants to recommend an action that makes none o f the N agents the entity advises worse off, the following is the correct optimiz a tion problem. The inequality constraints from 2-4 can be combined. For example, an institution might ﬁnd it ethically desirable to recom mend actions that make agent i strictly better off but make no other agent worse off or the group no worse off. We tested the effects of our multi-agent optimization using experimental data from Bó [2005]. Bó ran an exper iment testing players’ strategies in a simulation of an iterated prisoner ’s dilemma with inﬁnite round s. The iterated prisoner’s dilemma is a multi-round variant of the prisoner’s dilemma whereby a player’s outcome is the sum o f their outcomes in the ind ividual rounds of the game. In his experiment, he split players into a test and control group. The test group, designed to simulate inﬁnite play, would have each game consist of an initial round a nd then an unk nown number of additional rounds where each additional round had a probability δ of occurring. δ was set to 0, 1 /2, or 3/4. The corresponding control group players played a ﬁxed number of games. The ﬁxed number was set to either 1 game if it was a control for δ = 0 , 2 g a mes for δ = 1/2, or 4 games for δ = 3/4, thus ensuring the control and test group played the same nu mber of gam es on average. Test games with δ equal to 0 and control games with one round are equivalent to the single round prisoner’s dilemma from section 2. 329 4 ga mes met one these criterion. Participants were p aid an amount proportional to the number of the points they won in the game. Two different pay-off matrices were used during the game and are displayed below. We examined the effects of recommendations made using single-age nt optim iz ation constrains. Of the rec ommenda tions ma de for the 3 294 single-round prisoner’s dilemma games, 4 34 recommendations resulted in an improvement for the princ iple agent as well as both a loss in socia l we lfare and a violation of Pareto efﬁciency. N o recommendations resulted in either so c ia l welfare or the opposing player improving. When either Pareto efﬁcient or social welfare efﬁcient constrain ts were added or just Pareto efﬁcient constraints were considered alone, no recommendations were made at all because no action can improve the principle agent while improving social welfare o r not har ming the other player. Therefore, optimizatio n setups from our framework would have p revented signiﬁcant third party harm when compar ed with single agent recourse recommendations. When recommen dations were made with just social welfare constraints, 2860 recommen dations resulted in both an increase in social welfare an d a decrease in the principle agent’s we lfare. None of the recommenda tions resulted in an increase in the princ iple agent’s welfare. This result high lights a pressing e thical dilemma that previous work using the single agent framework obscure d, whether or not to advise the principle a gent to do something that makes it worse off becau se it makes the group better off. It is important that the artiﬁcial intellig ence community develops a consensus on how to handle dilemmas such as th is. As machin e learning models become increasing ly important to organizational decision making, the need for realistically understan ding the effects of those decisions will grow. Furthermore, the demand will grow to ensure machine learning powered decision making takes into account its effects on all stakeholders. Pareto superior and social welfare efﬁcient con straints when making recourse recommendations are a step towards this inclusive style of machine learning based decision making. This paper provided both th eoretical and empirical support for the argument that an inherent trade off exists betwe e n making recourse recomm endations that are good for the group and recommendations that are good for the principle agent. In the future, comparing how these different approaches to recourse recommendations perform on addition al real world data sets could demonstrate concrete beneﬁts to this line of research. For example, companies such as Apple, Goog le, and Waze offer GPS services m ake route recommendations to many drivers simultaneously. The time it takes a driver to reach its destination is in part a function of the other drivers on the road. It is estab lish e d that certain network conﬁgurations can result in a prisoner’s dilemma where by individual drivers ta king the fastest route makes a ll of the drivers worse off [Nisan et al., 2007]. There is evidence that this pheno menon is currently contributing to trafﬁc c ongestion in the real world [Caba nnes et al., 2018]. A GPS application making recommendations in a trafﬁc network may have to choose between making recommenda tions that are single agent optimal, social welfare efﬁcient, or Pareto efﬁcient. To the best of the authors’ knowledge, no major GPS com pany has addressed this problem in the literature or othe rwise. Our work can have an immediate impact on real world recourse problems such as this. Finally, other ethical constraints in addition to Pareto efﬁciency and social welfare efﬁciency should be explored. Table 2: First pay off matr ix used in experiments from Bó [2 005]. Table 3: Secon d payoff matrix used in experiments from Bó [2005].