Department of Statistics, The Chinese University of Hong Kong bendai@cuhk.edu.hk; xshen@umn.edu; panxx014@umn.edu A multistage recommender system on a monotonic chain of events predicts a user’s preference of a large collection of items based on only a few user-item feedback at multiple stages, where a user’s positive feedback of subsequent stages can only be observed given his/her positive feedback at present stages. As a result, a user may exhibit an increasing level of intention and preference given the feedback, ranging from the most implicit to the most explicit. It has been widely used for personalized prediction in e-commerce and social networks as well as individual drug responses over multiple phases in personalized medicine (Suphavilai et al., 2018). In such a situation, the objective of a multistage recommender system is to predict a user’s subsequent behavior from his/her previous feedback with a high percentage of missing observations. For instance, as displayed in Figure 1, the Deskdrop A recommender system learns to predict the user-speciﬁc preference or intention over many items simultaneously for all users, making personalized recommendations based on a relatively small number of observations. One central issue is how to leverage three-way interactions, referred to as user-item-stage dependencies on a monotonic chain of events, to enhance the prediction accuracy. A monotonic chain of events occurs, for instance, in an article sharing dataset, where a “follow” action implies a “like” action, which in turn implies a “view” action. In this article, we develop a multistage recommender system utilizing a two-level monotonic property characterizing a monotonic chain of events for personalized prediction. Particularly, we derive a large-margin classiﬁer based on a nonnegative additive latent factor model in the presence of a high percentage of missing observations, particularly between stages, reducing the number of model parameters for personalized prediction while guaranteeing prediction consistency. On this ground, we derive a regularized cost function to learn user-speciﬁc behaviors at diﬀerent stages, linking decision functions to numerical and categorical covariates to model user-item-stage interactions. Computationally, we derive an algorithm based on blockwise coordinate descent. Theoretically, we show that the two-level monotonic property enhances the accuracy of learning as compared to a standard method treating each stage individually and an ordinal method utilizing only one-level monotonicity. Finally, the proposed method compares favorably with existing methods in simulations and an article sharing benchmark. negative Latent Factor Model Figure 1: Demonstration of multistage prediction for Deskdrop dataset, a user may like an article sharing dataset where only less than 0.2% of values are observations with a total of 73, 000 users and more than 3,000 articles. A user likes an article only if this user has viewed it, and the user may follow an article only when he/she has liked it. Thus, three pairwise predictions are made based on three pairs of present and subsequent stages are performed, namely, the prediction of if the user will like an article given that he has viewed, that if he/she will follow given that he has viewed, and that if he/she will follow given that he has liked. level monotonic property, as precisely deﬁned in (3). On the one hand, monotonicity occurs over subsequent stages given a present stage. For instance, based on the fact that a user has viewed the article, this user won’t follow it if he/she doesn’t like it. On the other hand, monotonicity exhibits over present stages given a subsequent stage in a reversed order. That said, if the user has decided to follow only he/she viewed it, then the user will follow an article when he/she has liked. Despite recent progress in recommender systems, multistage personalized prediction on a monotonic chain of events remains largely unexplored. A standard method is to predict a user’s behaviors at a subsequent stage given a present stage individually, ignoring the monotonic property, as described in (11). In this sense, essentially all existing methods of recommender systems for a single-stage are applicable, including latent factor models (Koren, 2008; Dai et al., 2019b), logistic matrix factorization (Johnson, 2014), tensor factorization(Bi et al., 2018), and deep neural networks (He et al., 2017). The worth of note is that ordinal regression/classiﬁcation may be adopted by treating subsequent stages as an ordered class given a present stage, as in (12). For example, in Wan and McAuley (2018); Tran et al. (2012), latent factor models are developed based on an ordered logit model. Yet, a two-level monotonic property introduced by a monotonic chain of events is not met, which considers only forward monotonicity of subsequent stages given the present one but not the backward monotonicity over present stages given the subsequent one. As to be seen, the two-level monotonicity yields not only a high accuracy of prediction but also prediction article only if this user has viewed it, and the user may follow an article only if this user has liked it. Three stagewise predictions based on diﬀerent pairs of a given stage and a subsequent stage are considered: given view to predict like, given view to predict follow, and given like to predict follow. One key characteristic of multistage prediction on a monotonic chain of events is a twoconsistency (3) among diﬀerent stage pairs. In practice, prediction consistency is indeed desirable for decision-making. interactions and integrates the two-level monotonic property into personalized prediction on a monotonic chain of events. Three key contributions in this article can be summarized: Consider a recommender system in which triplets (∆ are observed for user i on item j at stage t; 1 ≤ i ≤ n, 1 ≤ j ≤ m, 1 ≤ t ≤ T . Here n and m are the number of users and items, respectively. ∆ observed or missing and Ω feedback at stage t, where Y stage t, and numerical and categorical features, where u item-speciﬁc numerical predictors, respectively, and normalization is performed for each feature to scale to [0, 1] otherwise, and moreover s and o user-speciﬁc and item-speciﬁc categorical predictor vectors. For instance, in the Deskdrop dataset agent”, “user region”, “user country”, o and Y important characteristic of this kind of data is that subsequent events will not occur if one present event does not occur. For instance, if a user does not view an item, then subsequent events of like and follow will not occur. This article develops a multistage recommender system that models user-item-stage • The proposed method can produce a recommendation for any subsequent stage given observations at any present stage, which is highly demanded in real applications; see Figure 1. Yet, most conventional recommender systems focus on a ﬁxed present stage. • A novel multistage loss function is proposed to treat multistage prediction for any pair of present and subsequent stages. This loss function admits user-item interactions observed at diﬀerent stages and evaluates the prediction accuracy on all subsequent stages. • The two-level monotonic property is fully accounted for by our nonnegative additive latent factor model based on the Bayes rule in Lemma 1. As a result, it substantially reduces model parameters and most importantly ensures prediction consistency across diﬀerent stages. By comparison, none of aforementioned methods can guarantee prediction consistency, c.f., Tables 2 and 3. • An algorithm is developed to implement the proposed method based on blockwise coordinate descent. Moreover, a learning theory is established to quantify the generalization error to demonstrate the beneﬁts of modeling user-item-stage interactions based on the two-level monotonic property. = (o, ··· , o) with o∈ {1, ··· , m} are d-dimensional and d-dimensional , vis a numerical embedding for the content, sconsists of “person Id”, “user and Yindicate if article j is liked and followed by user i, respectively. One which says that event Y which encodes a certain causal relation as deﬁned by the local Markov property (Edwards, 2012) in a directed acyclic graphical model. Multistage classiﬁcation learns to predict the outcome of Y observations at a present stage Y which depends on observations x functions can be expressed as φ decision functions are φ = (φ e(φ) = (nm) where e and P probability and expectation given X predicting the outcome at subsequent stage t based on present stage t reﬂecting the relative importance with respect to prediction at diﬀerent stages in the overall evaluation. In particular, (2) reduces to the next-stage prediction and last-stage prediction when w completely at random, or missing pattern ∆ to the standard misclassiﬁcation error. i on item j in (2) subject to a constraint that predicted Y monotonic behavior chain property (1). Lemma 1 (Multistage Bayes-rule) The optimal multistage pairwise decision function The multistage Bayes rule for predicting Y t< t ≤ T , On this ground, we deﬁne a monotonic behavior chain as follows: = 1. To predict, we introduce a decision function φ= φ(x, y) for classiﬁcation, To evaluate the overall performance of φ, we deﬁne the multistage misclassiﬁcation error (φ) is the pairwise misclassiﬁcation error for user i on item j across T stages, (·) = P(· | X= x) and E(·) = E(· | X= x) denote the conditional = I(t − t= 1) and w= I(t = T ), respectively. Moreover, if missing occurs Lemma 1 gives the multistage Bayes decision function¯φ= argmine(φ) for user minimizing (2) can be written as: for 1 ≤ i ≤ n, 1 ≤ j ≤ m, 0 ≤ t< t ≤ T , (x) satisﬁes the two-level monotonic property (3) iﬀ Yfollows (1), that is, for 0 ≤ Moreover, there exists any ﬁxed t monotonicity). Note that (4) guarantees (3). the additive multistage Bayes rule: for some h Based on the representation of decision function in (5), we rewrite e Note that the indicator function I(·) in (6) is diﬃcult to treat in optimization. Therefore, we replace it by a surrogate loss V (u) for large-margin classiﬁcation, in which V is a function of the corresponding functional margin Y hinge loss V (u) = (1 − u) log(exp(−z)/(1−exp(−z))) (Zhu and Hastie, 2002), and the ψ-loss V (u) = min(1, (1−u) (Shen et al., 2003). On this ground, we propose a multistage large-margin loss function: where Z to f satisﬁes the Bayes rule in Lemma 1. Lemma 2 (Multistage Fisher consistency) The minimizer of l(f) is Fisher-consistent in that it satisﬁes the Bayes rule in Lemma 1 if the surrogate loss V (·) is Fisher consistency in binary classiﬁcation. to incorporate the collaborative information across users, items and stages. In particular, we deﬁne h parametrization. Moreover, a(u linearly to numerical predictors u The two-level monotonic property (3) says that sign(¯f(x)) is decreasing in t for In view of Lemma 1, we introduce our multistage pairwise decision functions to mimic = (∆, Y) and f= (f).P Lemma 2 says that a minimizer of the cost l(f) = (nm)EL(f, Z) with respect Next we parametrize our decision functions based on an additive latent factor model with Figure 2: Architecture of proposed nonnegative additive latent factor models for continuous categorical predictors s subject to A ≥ 0, B ≥ 0, q B ∈ R features to K-dimensional latent vectors, and a factor for s users and items based on an additive model. For example, in Deskdrop dataset, the interaction eﬀect of “userId”-“articleId”, “userId”-“authorId”, “userAgent”-“articleId”, “userAgent”“authorId”, are all captured in (8). Furthermore, nonnegative constraints for A ≥ 0, B ≥ 0, 1, y subject to A ≥ 0, B ≥ 0, a where J and categorical user-item speciﬁc features as well as the user-item-stage eﬀect. ≥ 0; l = 1, ··· , d; h = 1, ··· , m, where ◦ is the Hadamard product, A ∈ Rand Representation (8) is highly interpretable, it leverages all feature-interaction between ≥ 0 and b≥ 0 are enforced to ensure the two-level monotonic property (3). Given observations (x, y)and positive index set Ω= {(i, j) : ∆= = 1} at stage t, we propose a regularized multistage large-margin loss (f) = λkAk+ kBk+ λkak+ kbk+ λkqk, a = (a, ··· , a), b = , ··· , b), q = (q, ··· , q), a= (a, ··· , a), b= (b, ··· , b), λ, λ, λ≥ 0 are tuning parameters controlling the trade-oﬀ between learning and regularization, and F is a space of candidate decision functions in (8), which can be written as where a(·, ·) and b(·, ·) are deﬁned in (8). and 1 ≤ j ≤ m, 0 ≤ t This section compares the proposed method with a standard method treating each stage individually and an ordinal method treating the stage as an ordinal class. combine the prediction results for 0 ≤ t by solving: where F includes latent factor models (Koren, 2008), gradient boosting (Cheng et al., 2014), and deep neural networks (He et al., 2017), and J parameter λ ≥ 0. subject to f stagewise decision functions and J (2002) formulates a parallel decision function f constraint β set f constraints β log-likelihood function, then (12) is a formulation for ordinal regression McCullagh and Nelder (2019); Bhaskar (2016). tonicity. Whereas such monotonicity helps to reduce the size of the parameter space, by F =f = (f) : f(x) = a(u, s)b(v, o) −a(u, s) ◦b(v, o)q, a∈ R, h = 1, ··· , n∈ R, h = 1, ··· , m, q∈ R, B ∈ R Minimization (9) in (A, B, q, b, q) yields an estimated (ˆA,ˆB,ˆa,ˆb,ˆq) thusbfby (8) bφby (5). Finally, the outcome of Yis predicted by sign(bφ(x, Y)); 1 ≤ i ≤ n, Standard. A standard method treats (t, t)-pairwise classiﬁcation separately and then is a parameter space of candidate pairwise decision functions. A standard method Ordinal. An ordinal method treats each stage tseparately, which estimates f= )by solving: = (β−β)x, where the forward monotonicity is ensured by positive Ordinal classiﬁcation incorporates the forward monotonicity but ignores backward monocomparison, the proposed method can further reduce parameters utilizing the backward monotonicity. In contrast, a standard method does not leverage any level of monotonicity. Most critically, both standard and ordinal methods fail to yield prediction consistency. This section develops a computational scheme to solve nonconvex minimization (9). For illustration, consider the hinge loss V (u) = (1 − u) cost function (9) by solving a sequence of relaxed convex subproblems via a block successive minimization (BSM). The scheme uses blockwise descent to alternate the following convex subproblems. For each subproblem, we decompose (9) into an equivalent form of many small optimizations for parallelization and for alleviating the memory requirement. Let where ⊗ is the Kronecker product and vec(A) is the column vectorization of matrix A. = 1−q, we solve (9) as follows. User-eﬀect block A. This convex optimization solves for A: min(nm)wξ+ λkvec(A)k, ξ≥ 0; (i, j) ∈ Ω, 0 ≤ t User-eﬀect block a. This convex optimization solves for (a, ··· , a): ξ≥ 0; (i, j) ∈ Ω; 0 ≤ t the present values for all other variables: min constrains for model parameters, which can be solved via one eﬃcient implementation in our python package varsvm Algorithm 1 (Parallelized version: hinge loss) Step 1 (Initialization). Initialize the values of (A, B, a, b, q) and specify the tolerance error. Step 2 (Update A and a). Update A by solving (13) given present values of the other variables. For l = 1, ··· , d given present values of the other variables. Step 3 (Update B and b). Update B by solving (16) given present values of the other variables. For l = 1, ··· , d given present values of the other variables. Step 4 (Update q). For r = 1, ··· , T , update q other variables. Item-eﬀect block B. This convex optimization solves for B: min(nm)wξ+ λkvec(B)k, ξ≥ 0; (i, j) ∈ Ω, 0 ≤ t Item-eﬀect block b. For l = 1, ··· , d, (b)is solved in a parallel fashion, Stage-eﬀect block q. This convex optimization solves for q, for r = 1, ··· , T , given ξ≥ 1 − y− qa(u, s) ◦ b(v, o)+ (1 −q)a(u, s) ◦ b(v, o), ξ≥ 0; (i, j) ∈ Ω; 0 ≤ t As a technical note, (13)-(18) are standard SVMs with ﬁxed intercept and positive The aforementioned scheme is summarized in Algorithm 1. Step 5. Iterate Steps 2-4 until the decrement of the cost function in (9) is less than the tolerance error. estimated decision functions and t-th stage is sign timization, including weighted SVMs, drifted SVMs, and non-negative SVMs, based on coordinate descent of the dual problem (Wright, 2015). This implementation has been available in varsvm library in Python on the GitHub repository varsvm code is released with the MIT license using GitHub and the release is available via PyPI. The development undergoes an integration of Ubuntu Linux and Mac OS X, in Python 3. Lemma 3 (Convergence of Algorithm 1) A solution ( a stationary point of cost function in (9) is strictly blockwise convex, that is, (13), (15), (16), (17), and (18) are strictly convex in terms of its parameters within each block, yielding a unique minimizer at each step. This aspect diﬀers from the maximum block improvement Chen et al. (2012) that guarantees the convergence of blockwise coordinate descent for a general objective function. One beneﬁt of successive updating is that it signiﬁcantly reduces computational complexity. Note that the solution of Algorithm 1 can be a global minimizer when certain additional assumptions are made Haeﬀele and Vidal (2015). This section investigates the generalization aspect of the proposed multistage recommender ˆf in terms of the accuracy of classiﬁcation, as measured by the classiﬁcation regret, deﬁned as e( although (∆ a same user or multiple users may purchase a same item. Let function, with ( V -loss be V Algorithm 1 returns an estimate (bA,bB,ba,bb,bq) at termination, which in turn yields anP bφ= −1 if Y= −1,bfotherwise. Finally, the prediction at the t-th stage given the To implement Algorithm 1, we develop a Python library to solve an SVM-type of op- The successive update in Algorithm 1 suﬃces to ensure its convergence because the ˆf) − e(¯f), where e(·) is the generalization error deﬁned in (2). Assume that (∆, Y) given X; 1 ≤ i ≤ n; 1 ≤ j ≤ m, are conditionally independent, 1, ··· , m; 0 ≤ t and the truncated cost function is l nical assumptions are assumed: Assumption A (Conversion). There exist constants 0 ≤ α ≤ ∞ and c for all 0 <  ≤ B and f ∈ F, Assumption B (Variance property). There exist constants 0 ≤ µ ≤ 2 and c that for any  ≥ 0, n, m ≥ 1, and f ∈ F, two diﬀerent metrics and the relation between the mean and variance of the regret function in a neighborhood of founded in Zhang and Liu (2014), for example, α = µ = 1 when V (·) is the ψ-loss Shen et al. (2003) or the hinge loss under low-noise assumption. Theorem 4 Let hold, then there exist constants c where where ¯w = When nm ≥ (p gence rate for the proposed method is O dard classiﬁcation in (Tsybakov et al., 2004). property in (3) and the additive property in (4). As illustrated in Table 1, based on the factorization model in (8) while ignoring the two-level monotonicity (3), a standard method and an ordinal method involve ΛKT (T + 1)/2 and ΛKT parameters, which are Assumptions A and B are two local smooth conditions regarding a connection between Let K and¯K be the numbers of latent factor ofˆf and¯f, respectively. = O(Λ + T )Knmlognm ¯wBJ(p+ p+ d+ d)(6T+ TK)(Λ + T )K log()when α = µ = 1, and matches with the minimax rate of the stan- As indicated in (19), the convergence rate is improved by the two-level monotonic chain more than (Λ + T)K and signiﬁcantly impedes the performance when T becomes large. Furthermore, due to the monotonic property in (1), the eﬀective sample size for Y decreases exponentially as t over-ﬁtting at late stages with a small sample size. In contrast, the proposed method leverages the two-level monotonicity to reduce the dimension of the underlying problem. Table 1: Monotonicity and model parameters of the proposed, standard, and ordinal frame- This section examines the proposed method in (9) with the hinge loss V (u) = (1 − u) and compares it with standard methods, including latent factor models (SVD 2008)), gradient boosting (GradBoost) (Friedman, 2001), support vector machine (SVM) (Cortes and Vapnik, 1995), deep neural network (DeepNN) (Schmidhuber, 2015), ordinal method (12), namely ordinal support vector machine (OSVM), in a simulation and an article sharing benchmark. separately for all possible pairs, where we use one-hot encoder (Weinberger et al., 2009) to convert a categorical covariate to 0-1 numerical predictors for training. Moreover, for SVD ﬁrst columns of s if the predicted rating exceeds 0.5 and -1 otherwise; for GradBoost, the minimum number of samples to split an internal node is set to be 2 and the minimum number of samples required to be at a leaf node is 1, and the number of boosting stages is set to be 10. For DeepNN, a ReLU network is used with the number of node in each layer being ﬁxed at 32. For the ordinal method, we set the OSVM classiﬁer in (12) as f since x constraints β OSVM, the Python library sklearn library Surprise as −1 when Y works, denoting (9), (11) and (12). Here forward and backward monotonicity is deﬁned in (3), consistency denotes if the methods can provide a consistent prediction result, and #Parameters denotes the number of model parameters based on (4). For a standard method, SVD, GradBoost, SVM, and DeepNN treat each (t, t) pair , the number of latent factors is set as 20, and latent factors are estimated using the is pre-scaled in [0, 1], where the forward monotonicity is ensured by positivity For implementation, we use our Python library varsvm for the proposed method and In simulations, data {s recommender system on a monotonic chain, with d distribution on 1 degree of freedom, and (s {1, ··· , n deviation for (p randomly with |Ω generated as Ω a partition ratio of 10%, 10%, and 80%, respectively. a grid search based on the overall misclassiﬁcation error on the validation set, where the grid set is chosen as {.001, .005, .2, .4, .6, .8, 1.}. For the neural network, its depth is tuned over a set {1, 2, 3, 5, 7, 9, 11}. For SVM, OSVM, and SVD are tuned based on validation set through {.001, .005, .01, .05, .1, .5, 1, 5, 10, 50, 100, 500}. For the proposed method, K = 20 and three tuning parameters (λ by minimizing the misclassiﬁcation error on a validation set via a grid search over λ {.001, .005, .01}, λ stage weights w one-step, two-step, and three-step forecasting, followed by DeepNN, SVM, and GradBoost. In terms of the overall performance, this same phenomenon is also observed and the amount of improvements of the proposed method are 43%, 22%, 20%, and 13% over Gradient Boosting, OSVM, SVM, and DeepNN. Concerning inconsistency prediction, SVM and OSVM perform the worst, followed by DeepNN and Gradient Boosting. Note that there is no single instance in which prediction by the proposed method is inconsistent, which is ensured by the two-level monotonicity property of the classier. A Deskdrop article sharing dataset contains a sample of 12 months logs and about 73,000 logged user interactions on more than 3,000 public articles. In particular, article-speciﬁc features are “article Id”, “author Id”, “plain text”, as well as “language”, and user-speciﬁc features include: “user Id”, “user agent”, and “user region”, and users’ actions are logged, including view, like, and follow. The goal is to predict user-item interaction over three stage pairs. Mikolov, 2014) of the “plain text” of item j, o “language” of item j, and s = 50, m = m= 100, and m= 40. Then we set K = 20, and for each (l, h, t), ∼ χ(1), b∼ χ(1), and q∼ χ(1) are independently sampled from the chi-squared } and {1, . . . , m}, respectively. Moreover, we set y= 1 at stage 0, set p= (s)b(o) −a(s) ◦ b(o)q, y= signp+ σN(0, .1)at stage 1, and set p= (s)b(t) −a(s) ◦ b(t)q, y= signp+ σN(0, .1)for y= 1, and y= −1 for = −1 at stage 2 to satisfy the monotonic chain property (1), where σis the standard , o, y}. Finally, the data is split into training, validation, and testing sets with To avoid overﬁtting, for gradient boosting, the percentage of features are tuned via As suggested by Table 2, the proposed multistage recommender performs the best for For the proposed method, vis the numerical embedding based on Doc2Vec (Le and Table 2: Test errors of six competitors and their estimated standard deviations in parenthei. For other competing methods, we use all features based on a one-hot encoder to convert categorical covariates to zero-one dummy predictors. Moreover, we predict (t based on the proposed method with T = 1, and predict (t based on the proposed method with T = 2. stage inversely proportional to the number of observations in each class since feedback at each stage is highly imbalanced for this data. Hence, all methods are ﬁtted based on balanced class weights or over- or under-sampling, except SVD regression approach. Yet, for consistency, we focus on balanced zero-one loss for comparison since all methods are ﬁtted and tuned based on a balanced-weighted classiﬁcation loss. based on a nested 15-fold cross-validation with an outer 5-fold loop and an inner 3-fold loop (Cawley and Talbot, 2010). For the proposed method, K = 7 and three tuning parameters grid search over λ network, SVM, OSVM, and SVD manner as simulation. GradBoost, SVM, DeepNN, and OSVM, in terms of the overall performance with the amount of improvement ranging from 75.3% to 8.20%, in terms of the class-balanced zeroone loss. Similarly, for one-step and two-step forward prediction, it is either the best or nearly the best. Concerning inconsistency prediction, all other methods have inconsistent cases ranging from 13.7% to 0.98%. Interestingly, the conditional ordinal method (OSVM) performs similarly to the conditional individual method (SVM), indicating that it does not fully account for the monotonicity property. ses on simulated examples over 50 replications as well as a proportion of inconsistent instances violating monotonicity (3),denoted by %Inconsist. Here proposed, SVD, GradBoost, SVM, DeepNN and OSVM denote the proposed method in (9) with the hinge loss, the latent factor model (Koren, 2008), gradient boosting (Friedman, 2001), support vector machine (Cortes and Vapnik, 1995), deep learner (Schmidhuber, 2015), and conditional ordinal support vector machine in (12). The best performer in each case is bold-faced. For evaluation, we set all stage weights w= 1 and adjust the class weights at each The misclassiﬁcation errors of all competing methods for each (t, t) pair are computed , λ, λ) are tuned by minimizing the misclassiﬁcation error on a validation set via a As suggested by Table 3, the proposed multistage recommender outperforms SVD, Table 3: Class-balanced zero-one losses (CB-01) of six competitors and their estimated stan- Proof of Lemma 1. It suﬃces to show that which yields that 1|Y which implies that P provided that P is ﬁxed because P increases while t is ﬁxed because two-level monotonic property. dard deviations in parentheses on the Deskdrop benchmark, in addition to %Inconsist denoting a proportion of inconsistent instances violating the monotonicity (3). Here proposed, SVD, GradBoost, SVM, DeepNN and OSVM denote the proposed method in (9) with the hinge loss, the latent factor model (Koren, 2008), gradient boosting (Friedman, 2001), support vector machine (Cortes and Vapnik, 1995), deep learner (Schmidhuber, 2015), and ordinal support vector machine (12). The best performance in each case is bold-faced. EI(Yφ(x, Y) ≤ 0)|∆= 1 = P(Y= 1|∆= 1)EI(Yφ(x, 1) ≤ 0)|∆= 1, Y= 1 = 1, ∆= 1) − 1/2minimizes EI(Yφ(x, 1) ≤ 0)|Y= 1, ∆= 1and (x, −1) = −1 minimizes EI(−φ(x, −1) ≤ 0)|∆= 1, Y= −1. Next, we verify the two-level monotonic property (3) of¯f. Note that (Y= 1|∆= 1) = P(Y= 1, Y= 1|∆= 1) + P(Y= 1, Y= −1|∆= 1) Proof of Lemma 3. Note that (13), (15), (16), (17), and (18) are convex minimization Proof of Theorem 4. Our treatment of bounding P argument of empirical process over a suitable partition of F induced by e( in (Dai et al., 2019a; Wong et al., 1995). For u ≥ 1 and v ≥ 0, deﬁne A l(f) − l where g(f λ(2 λJ(f ) = λ Next, we construct an alternative form of the Bayes decision function¯f(x), which (Y= 1|∆= 1)> 0, and c(x) > 0 and α(x) > 0 are two arbitrary posic(x) log2P(Y= 1|Y= 1, ∆= 1)= 1, when P(Y= 1|Y= L(f, Z), when V (·) is Fisher consistency for binary classiﬁcation. ≤ l(f) − l(¯f) ≤ 2, 2¯J ≤ J(f ) ≤ 2¯J} and A= {f ∈ F : 2≤ (¯f) ≤ 2, J(f) ≤¯J}. By Assumption A, To apply Talagrand’s inequality (Gin´e et al., 2006) to I, let ≤ 2E where Z ables drawn from the Rademacher distribution. The third inequality follows from the Talagrand’s inequality and the last inequality follows from the fact that, ≤ aexp(−nmδ2a(2σ+ 16BE(R) + Bδ)) ≤ aexp(−nmδ2a(4cδ+ 5Bδ)) = 32cB+ 9Band the second inequality follows from the fact is an independent copy of Z, R= E(nm)supτL(¯f, Z)− (f, Z)is the Rademacher complexity of A, and τare independent random vari- = {f ∈ F : 2≤ l(f)−l(¯f) ≤ 2, J(f) ≤¯J}, and δ= 2+λ(2− (x) − f(x)=˜a(u, s) ◦˜b(v, o)˜q(t, t) −a(u, s) ◦ b(v, o)q(t, t) , t) = 1 −q. Then we bound Iand Iseparately. For I, where the last inequality follows from the fact that and I≤ 2˜a(u, s) ◦˜b(v, o) − a(u, s) ◦ b(v, o)q(t, t) ≤ 4K + (t − t)kqk˜a(u, s) ◦˜b(v, o) − ˜a(u, s) ◦ b(v, o) + 4K + (t − t)kqk˜a(u, s) ◦ b(v, o) − a(u, s) ◦ b(v, o) 2¯J(T + 1)T K + Tkqkk˜A − Ak+ k˜B − Bk+ k˜a − ak+ k˜b − bk. Therefore, the metric entropy for L where a fore, the Rademacher complexity E(R (Koltchinskii, 2011). Speciﬁcally, there exists a constant a Then there exists a constant a and where the ﬁrst two inequalities follow from non-decreasing of (u, v). This completes the log N(ν, L) ≤ log Nν¯, B(Λ + T )K≤ γ log2JBaν = κ¯w(6T+ (T + 1)T K), B((Λ + T )K) is the unit l-ball in R. There-