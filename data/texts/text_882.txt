Graph representation learning has drawn increasing attention in recent years, especially for learning the low dimensional embedding at both node and graph level for classiﬁcation and recommendation tasks. To enable learning the representation on the large-scale graph data in the real world, numerous research has focused on developing different sampling strategies to facilitate the training process. Herein, we propose an adaptive Graph Policy-driven Sampling model (GPS), where the inﬂuence of each node in the local neighborhood is realized through the adaptive correlation calculation. Speciﬁcally, the selections of the neighbors are guided by an adaptive policy algorithm, contributing directly to the message aggregation, node embedding updating, and graph level readout steps. We then conduct comprehensive experiments against baseline methods on graph classiﬁcation tasks from various perspectives. Our proposed model outperforms the existing ones by 3%-8% on several vital benchmarks, achieving state-of-the-art performance in real-world datasets. Graph neural networks (GNN) are now the de facto standard when it comes to learning the low-dimension embedding on the graph-structured data. It is shown effective in prior works to use the low-dimensional vector embeddings as the input node/graph features for a variety of prediction, recommendation, or any other graph-related analysis[Wu et al., 2020]. The idea behind it is through aggregating and distilling the high-dimensional information of a node as well as its graph neighborhood into a dense vector form, the principle of which is derived from the natural languages processing (NLP) ﬁeld[Zhou et al., 2020]. The rise of GNNs comes from the fact that most dominant neural network architectures in the deep learning ﬁeld focus on dealing with tasks involving grid-like data structures (e.g., images, text corpus). However, it is unpractical not to consider the sheer amount of non-euclidean space data in the case Corresponding Author of social networks, molecular protein networks, or ﬁnancial risk networks[Zhou et al., 2020]. These types of data can be formed into the graph structure and convey a better interpretation from a human’s perspective. Much interesting research has thus emerged over the last several years to embed the graph-structured data using GNNs, and these GNNs can be generalized in two forms, namely conventional and sampling-based ones[Wu et al., 2020]. Speciﬁcally, a conventional GNN refers to optimizing node’s embedding using matrix factorization-based objective in a single, ﬁxed graph, while aggregating knowledge from all connected node pairs. One widely used work is graph convolutional networks (GCN), which could easily lead to the “neighbor explosion” problem in the large-scale and dense graph. The recursive expansion across layers in a top-down way also requires expensive convolution computations for all nodes at each time step. One ﬂagship sampling-based model is GraphSAGE, which randomly samples a ﬁxed size of neighbors for each node instead of taking in the whole neighborhood to avoid “neighbor explosion” problem. Similarly, other sampling works either resort to the layer-wise sampling[Huang et al., 2018]or graph-level sampling[Zeng et al., 2020]. It is considered challenging for representation learning due to the need of generalizing the ones on which the algorithm has already been optimized to the unseen nodes/graphs. To this end, we propose an adaptive graph policy-driven sampling model, namely GPS, to generate both node and graph level embedding inductively. The policy-driven sampling is realized through several key steps. It ﬁrst quantiﬁes the importance of all nodes at each layer recursively, from which a better sense of locality of a given node can be comprehended while sampling the nodes based on the rank of importance. It then integrates the sampling loss into the ﬁnal graph level classiﬁcation task, and the importance of the nodes is thus adjusted adaptively along with the change of gradients towards the minimum of loss. The contributions of the paper can be summarized as the following: • We propose an innovative policy-driven sampling approach to enhance the generation of the embedding on the graph-structured data, in which the node embedding is updated with the guidance of the policy function, and the graph embedding derives from a shallow multi-layer perceptions network (MLP). • We realize the graph representation learning in an end-to-end fashion, including the initial graph construction/vectorization, the adaptive policy-driven sampling/embedding update and graph embedding generation. We quantify the correlations of nodes in the local neighborhood and only sample ones deemed important iteratively. In addition, the sampling process is considered an integral part of the training process and thus adapted dynamically. • We conduct extensive experiments on ﬁve molecular graph datasets for graph level classiﬁcation tasks. Compared with the other four state-of-the-art methods, the proposed GPS achieves superior performance among all. We also demonstrate the inﬂuence of sampling parameter K on the overall performance of the GPS and implement variants of GPS for ablation study. While the sheer number of graph-structured data have been found particularly useful in many learning tasks in recent years (e.g., ﬁnancial risks analysis for costumers social connection graphs), designing effective and efﬁcient graph neural networks has become a popular topic in the graph representation learning ﬁeld. [Bruna et al., 2014]ﬁrst introduces the convolution operation into the graph domain and propose both spatial construction and spectral construction during the convolution process. To speed up the graph convolution computation, both[Kipf and Welling, 2017]and[Defferrard et al., 2016]design localized ﬁlters based on Chebyshev expansion and verify their approach on relatively small datasets, where the training is able to happen in full batch. Apart from that,[Duvenaud et al., 2015]adopts the convolution by using the spatial connections on the graph directly, where a wight matrix is learned for each node degree. Additionally,[Cao et al., 2019]develops the channel-wise convolution to learn a series of graph channels at each layer, which is able to shrink the graph hierarchically to encode the pooled structures. These GCNbased models have achieved state-of-the-art performance at that time and outperformed the traditional node embedding approaches such as node2vec[Grover and Leskovec, 2016] and DeepWalk[Perozzi et al., 2014]. However, despite the success of GCN-based models, one major limitation is that these models are all based on knowledge of the entire graph Laplacian matrix amid training, making it unpractical to large-scale graph data with millions of nodes and edges. Following that, various sampling approaches are proposed to avoid operating on the entire Laplacian matrix[Hamilton et al., 2017; Ying et al., 2018; Huang et al., 2018; Zeng et al., 2020]. Speciﬁcally,[Hamilton et al., 2017]ﬁrst introduces the uniform layer-wise sampling algorithm, which applies a ﬁxed size sampling window to each layer from inside out iteratively.[Ying et al., 2018] takes advantage of random walk and considers sampling from different perspective. It considers nodes with highest visited count as important nodes and constructs the neighborhood based on this. Both[Huang et al., 2018]and[Zeng et al., 2020]focus on the variance reduction-based sampling. While the former proposes a condition-based sampling where the lower layer is sampled conditionally on the top one, the latter samples at the graph level, suggesting to construct different subgraphs as the minibatches while decoupling the sampling from the forward and backpropogation processes. Another line of work aims to improve the model capacity. For instance,[Veliˇckovi´c et al., 2018]ﬁrst integrates the attention mechanism into the graph representation learning. They introduce an attention-based architecture and the use self-attention to assign edge weights to the neighborhood. Following that,[Zhang et al., 2018]adopts gated attention network to further enhance the control over attention heads. While our proposed method shares the same concept of quantifying the correlations to the neighbor nodes, it differentiates in coefﬁcients calculations, the adaptive policy-driven sampling for neighbor nodes, and the forward message passing process. The target of graph classiﬁcation task is to map a structural graph data into a set of labels. In this work, we consider a full undirected graph G = (V, E), where V = {v, ..., v} is the set of nodes, and E = {(v, v) |v, v∈ V} denotes the set of edges in the graph. We use an adjacency matrix A ∈ {0, 1}to encompass the edge sets with all connectivity information in which A= 1 if (v, v) ∈ E and A= 0 otherwise. Since GNN takes both adjacency matrix and vectorized node feature as input to learn the node embedding, we herein have initial node features denoted by X∈ Rwith x∈ Xcorresponding to the d-dimensional feature vector for node v. Simply, the neighbor nodes of vare represented as N (v). For the graph classiﬁcation task, we use {G, ..., G} as the whole graph datasets and {y, ..., y} as corresponding labels, before splitting into train/val/test set. Solution to Initial Node Embedding GNN requires both vectorized feature input and adjacency matrix[Yuan et al., 2020; Rong et al., 2020b], yet original node features in the real-world structural graphs are not vectorized in the ﬁrst place (e.g., citation network, molecular graph, and social network)[Lou et al., 2021]. To this end, we demonstrate the process of turning raw node feature input to initial node embeddings in Figure 1(a), including the steps of graph construction as well as vectorization. At the graph construction step, the graph is assembled with both raw node feature (e.g., paper/author name in citation graph, or carbon atom name in molecular graph) and structural connectivity (i.e., a list of pair-wise connections using node ids), transforming the graph into sequential representation known as SMILES format[Weininger et al., 1989]. It is not uncommon to use SMILES format to encode the contextual information of the graph on which the NLP-based models can be easily applied to generate the node embeddings[Wang et al., 2019; Liu et al., 2019]. To better encode the structural information of the graphs that other NLP-based models fail to do, we use a pre-trained GNN Transformer called GTransformer to leverage both the node features and structural information in SMILES sequence when it comes to generating the initial feature embeddings, which is then used as the input to GPS. Figure 1: The overview of node feature vectorization, and embedding update via GPS, and graph-level embedding generation (from left to Brieﬂy, the key component of the GTransformer is the graph multi-head attention component, which is the attention blocks tailored to latent structural data and reports the state-of-theart initial feature embedding results. The detailed model architecture can be found in[Rong et al., 2020a]. Embedding Update and Learning Instead of drawing the full expansion of the graph like GCN[Kipf and Welling, 2017]in the feedforward computation, our method updates the embedding of each node iteratively (Figure 1(b)). Specifically, we deﬁne the sampled node set as V(V⊆ V) whose embedding only need to be updated at aggregation layer l. Note Vessentially contains all nodes at the root layer, while sampled nodes at other layers are adaptively chosen through the designed policy-driven sampling process (details unveiled in eq.(4) and Algorithm 1). The unduplicated neighbor set of Vis denoted as V=Nv: ∀v∈ V in which N (·) returns the neighbor nodes of v. To show a concrete example, Figure 1(b) demonstrates one of the sampled nodes vat the root layer (l = 0) with neighbor node set {v, v, v}. We use words aggregation layer and message passing layer interchangeably in this paper, the notation of which is l ∈ {1, ..., L}, and L indicates the largest aggregation/message passing layer. Since the node index are in ﬁxed order in full adjacency matrix A, we can deﬁne both row ﬁlter matrix R∈ Rkkand column ﬁlter matrix C∈ Rkkto select rows or columns from the adjacent matrix at any message passing layer l. Simply put, the left multiplication with R can be regarded as selecting sampled nodes from full adjacency A, and the right multiplication with Cindicates selecting corresponding neighbor nodes. Taking the row ﬁlter matrix as an example, Ris expanded ase, e, ..., e, where e∈ {0, 1}and i =V. Speciﬁcally, e serves as the position indicator and tells the ith sampled node position at layer l in the full adjacency matrix A, e.g., e= [0, 1, 0, ..., 0]∈ Rmeans the ﬁrst sampled node is at the second row in A (node 2). Herein, the column ﬁlter matrix Cfollows the same logic to ﬁlter out the unique neighbor nodes of the sampled nodes. Consequently, the layer-wise adjacency matrix A∈ Rkkkkcan be formulated as: Similarly, we can retrieve the embedding representation for both sampled nodes and neighbor nodes at layer l. Say the hidden embedding matrix of all nodes at hop layer l is represented as H∈ R, and H= Xsimply means the node embedding is assigned with the initial node feature matrix at the root layer. whereˆH∈ Rand˜H∈ Rrepresent sampled nodes embedding and unique neighbor nodes, respectively. We then calculate the correlation coefﬁcients for each connected node pair and proceed with the sampling process, which is an integral part of the policy strategy in the proposed model. The adaptive policy-driven sampling process can be written as: δ (·) denotes the coefﬁcient function, which is used to quantify the correlations of each sampled node with different neighbor nodes. Instead of applying coefﬁcient function directly on sampled node embeddingˆHand neighbor node embedding˜H, we utilize two learnable weight matrices Wand Wto conduct further linear transformations in which embedding space for both sampled and neighbor nodes can be approximated separately at that layer throughout the training. Towards optimizing two transformation matrices for both sampled and neighbor node embeddings, our model can adaptively uncover better correlations through designated operations (e.g., multiplicative, normalized cosine or distance based operations, etc.) in the transformed embedding space. The Hadamard product operator  (·) is employed to only retain the connected node pairs. Φ (·) is the policy function that conﬁnes a pre-deﬁned budget on the sample size, from which the computation complexity can be bound easily. In this paper, we implement top (K, ·) in row-wise fashion as the policy to only selects neighbors deemed important in this case, and K is the sample budget. Ais the calculated correlation matrix at that layer, and α∈ Ahas also been visually shown in Figure 1(b). Following that, we haveˆh∈ˆHto represent embedding of each sampled node, where i ∈ {1, ..., kVk}. The message aggregation from layer l to l + 1 can be formalized as: where˜h∈˜H, ⊕ denotes the concatenation operator, and σ is the nonlinear activation function (e.g., ReLu). Note we use kVk as the regularizer after aggregating messages from neighbor nodes. Wis a shared weight matrix at that layer and applied on every sampled node before updating its embedding. Graph-level Representation To derive the ﬁnal graph representation from the node embeddings, we design the READOUT operation as follows: Instead of simply taking the mean of all node embeddings, we ﬁrst input the embedding matrix to a shallow multi-layer perception (MLP), the design of this MLP could be found in the experimental setup in Section 4. The row-wise M EAN operation is then applied on the output of the MLP, as shown in Figure 1(c). For notational convenience, we useˆh∈ H to denote the ﬁnal node representations. In order to learn graph representation in the fully supervised setting, we apply a graph-based cross entropy loss onto the graph-level output z. The gradient-based adjustments for all trainable parameters mentioned above achieve two goals: 1) to select the neighbor nodes with the best interests adaptively; 2) to approach the suitable latent embedding space gradually. Lz, y= −y· log z+1 − y· log1 − z where the loss L of graph output zis calculated against corresponding label y, and i means ith logits at that index. Algorithm 1 illustrates the training process of one graph using GPS. There are a few pre-processing to nodes before the training starts, including the generation of sampled nodes and neighbor nodes (lines 1, 3 and 4, respectively). We want to point out that the initial sampled nodes at layer 0 comprise either a subset nodes or all nodes in the graph, depending on the size of the input graph. Line 5 constructs the layer-wise adjacency, and the sampled version of which is then generated at line 6. It can be interpreted that the algorithm only allows the neighbors deemed important to contribute to the update of embeddings of parental nodes (line 7), and the inﬂuence of the sampling parameter K is studied in the next section. Note that the algorithm only passes the sampled nodes to the nexthop layer (line 8). Afterwards, the graph-level representation is composed at line 10, and all trainable weights are updated Input: Graph G = (V, E); label y; sampling hyperparameter K; total message passing layers L Output: Embedding representation of graph z 1: V← root nodes V⊆ V 2: for l = 1,..L do eq.(4) with adjustable parameter K 7:ˆH← Embedding update through eq.(5) 9: end for 10: z← Graph-level representation from eq.(6) 11: Calculate loss from eq.(7), and update all trainable weights. by minimizing the loss at line 11. The use of optimizer is shown in Table 1. the sampling is guided by the designated adaptive sampling policy, enabling a more effective message propagation than GraphSAGE, whose node representations are generated through random sampling of neighborhoods of each node. Other importance-based sampling works adopt a different strategy, where inter-node probabilities need to be calculated and optimized to reduce the variance caused by sampled nodes[Huang et al., 2018; Zeng et al., 2020]. However, this line of work incurs a signiﬁcant computation overhead in AS-GCN[Huang et al., 2018]. The latest work GraphSAINT splits sampled nodes and edges from a bigger graph into subgraphs on which the GCN is applied, causing unstable performance on small molecular graphs (details unveiled in the experiment part). GPS, on the other hand, enjoys a more stable performance and high accuracy at the same time. The performance of the proposed GPS is evaluated on the real-world molecular datasets for graph classiﬁcation tasks (e.g., prediction of toxicity of molecular compounds). Also, we present a detailed analysis of the obtained results, aiming to answer the following key questions: Q1 (Performance Comparison): Does our proposed method outperform the state-of-the-art baseline methods using the real-world molecular graph dataset? Q2 (Sensitivity Analysis): How does the change of sampling parameter K affect the ﬁnal results? Q3 (Ablation Study): To what extent the key component (e.g., correlation coefﬁcient function δ) contribute to the performance of our model? 4.1 Dataset As the majority of the GNN-based embedding works tend to verify their methods on node classiﬁcation tasks, where one large graph is given and smaller graphs are then generated from it through mini-batch splitting[Veliˇckovi´c et al., 2018; Hamilton et al., 2017; Zeng et al., 2020](e.g., Cora, Citeseer, Pubmed etc.). We study the graph-level classiﬁcation task instead, and ﬁve different molecular datasets are selected as the benchmark datasets[Wu et al., 2018]: 1) BBBP records a collections of compounds that whether carry the permeability property to the blood-brain barrier; 2) ClinTox compares FDA approved drugs against terminated drugs due to the toxicity during clinical trials; 3) SIDER involves a list of marketed drugs along with their side effects; 4) BACE collects compounds that acts as the inhibitors of human β-secretase 1; 5) Tox21 is a widely used public compounds dataset, recording the toxicity of each compound. As explained in Section 3, we ﬁrst conduct the preprocessing of the data, which generates the standard input for both our model and other benchmark GNN models (Figure 1(a)), including the initial node feature and adjacency matrix of each graph. Note that GTransformer produces a 8 dimension feature vector for each node in the graph[Rong et al., 2020a]. We follow the inductive data setup and randomly split molecular graphs into 80/10/10 partitions for training, validation and test, where the graphs in test partitions remain unobserved throughout the training process. The overview of the characteristics of different datasetscan be seen in Table 2. 4.2 Experimental Setup Baseline Models To ensure a fair comparison for predictive tasks on graphs, we re-implement other four baseline models (GCN[Kipf and Welling, 2017]and GAT[Veliˇckovi´c et al., 2018]as conventional GNN models, GraphSAGE[Hamilton et al., 2017]and GraphSAINT[Zeng et al., 2020]as sampling-based ones) with the shared model setup using Pytorch, including the same layer depth, readout function for graph-level embedding, loss function, Adam optimizer, epochs, and weight initializer. The shared model setup is summarized in Table 1. Note GraphSAINT achieves stateof-the-art results prior our work. In addition, we implement the normalized cosine coefﬁcient (GPS-C), L2 norm distance coefﬁcient (GPS-S) to compare with the default Hadamard product coefﬁcient (GPS) for the ablation study. Parameter Settings It is worth mentioning that all baseline models are adapted directly from their ofﬁcially released http://moleculenet.ai/datasets-1 https://pytorch.org code, with the minimum changes possible from their original hyperparameter/architecture setup. As listed in Table 1, we apply the same readout function for all models, with the architecture of shallow MLP as [32, 64, 64]. Note the node embedding size is 32 and ﬁnal graph embedding size is 64. We select Adam[Kingma and Ba, 2015]as the optimizer with the learning rate 15e-4. Also, the layer depth is set to be 2 for all models in the experiment, making it consistent with the best reporting performance for the baseline methods[Hamilton et al., 2017; Kipf and Welling, 2017; Veliˇckovi´c et al., 2018]. The sample window of GraphSAGE is set to be 3 by default from the released code, and the number of attention heads is 3 in GAT. While the original GraphSAINT samples sub-GCN graphs from one large graph, we adopt the node sampling from the released source code and choose a smaller node budget and node repeat to meet our experimental scenario (15 and 10, respectively)[Zeng et al., 2020]. All trainable parameters of the neural network are initialized through Xavier[Glorot and Bengio, 2010]. Evaluation Metrics We evaluate the performance of all the models on graph classiﬁcation task through AUC (i.e. Area Under the ROC Curve[Fawcett, 2006]), which is widely adopted to multi-label classiﬁcation scenarios. The multilabel AUC score can be formalized as: where AUC (y) is the area under the curve of ROC for label y, and kyk represents the total number of label y. AUCis the arithmetic average of all labels. 4.3 Evaluation Results As shown in Table 1, we use a shallow MLP (2 linear layers and 1 output layer) as the readout function to generate ﬁnal graph-level output, and report the AUC (i.e., Area Under the ROC Curve) scores of all models on graph prediction tasks across ﬁve different datasets. Performance Comparison (for Q1) We ﬁrst look at the performance of our proposed method comparing with other baseline models. Figure 2 demonstrates the change of AUC scores from the test set along with the increase of training steps. Note that we record test scores of all unseen graphs for every 200 training graphs as one training step, and the training epoch is set to be 10 for all datasets. We select K = 3 for our proposed method owing to the reported results, which will be analyzed in detail in the sensitivity analysis part. Compared with four baseline methods, the proposed GPS generally shows higher scores throughout the training. The best AUC results among all datasets are also recorded and demonstrated in Table 3, in which the highest are marked in bold. Our method outperforms other four baseline models in all datasets in the inductive setting by an average improvement of 4.74%. In particular, GPS improves the latest reporting sampling model GraphSAINT[Zeng et al., 2020]by an average of 6.17% while demonstrating a much more stable performance throughout the training (as indicated in both Fig. 2 and Table 3). Additionally, our method achieves a signiﬁcant improvement comparing to other conventional (GCN, Figure 2: Test AUC score at different steps in 10 epochs period. One training epoch is a pass for all graphs in the training set GAT) and sampling (GraphSAGE) models w.r.t the recorded best results, gaining an average increase by 3.7% on Tox21, 2.8% on SIDER, 1.07% on ClinTox, 3.08% on BBBP, and 8.31% on BACE respectively. The comparison results with GraphSAGE once again suggest the importance of employing a policy-driven sampling strategy when selecting neighbor nodes rather than using the random sampling seeds. We also compare our proposed methods with other benchmarks in terms of training speed. As shown in Table 4, GraphSAGE presents the fastest speed owing to less trainable parameters and random sampling strategy. GPS ranks second, showing a small gap with GraphSAGE while signiﬁcantly improving the AUC score. We also observe splitting a graph into subgraphs in GraphSAINT slows down the training speed, especially for molecular graphs. In general, GPS makes a good balance between training speed and accuracy. Sensitivity Analysis of Sampling Parameter (for Q2) We study the impact of sampling parameter K on the overall performance of GPS and explain why we select K = 3 in the preceding part. As clearly seen in Figure 3, we test different K on all datasets and observe an increasing performance of the model at the beginning. Interestingly, it reaches the best Table 4: CPU execution time per epoch (second) Figure 3: The best AUC score on among different datasets with the score when K = 3 with ﬂuctuation or slight decline afterwards(circled in dashed line). Intuitively, it proves the concept that important neighbor nodes carry most of the topological properties of the graph especially in small molecular graphs, and the traversal to the whole neighborhood may not be needed. This also indicates that the increase of K adds extra noise when updating the node embeddings through message aggregation and thus deteriorate on the unseen graphs in the test set. Ablation Study (for Q3) To answer Q3, we implement the variants of GPS, i.e., GPS-D and GPS-C, by replacing the element-wise dot product to L2 norm distance-based and the normalized cosine-based coefﬁcient, all of which are considered as the common strategy when it comes to measuring the correlation scores between vectors[Luo et al., 2018]. As indicated in Table 3, the default GPS, which uses the elementwise dot product as the coefﬁcient function in eq.(4), achieves a better performance on all dataset compared with varients GPS-D and GPS-C. Speciﬁcally, it improves by an average of 1.03% on Tox21, 1.84% on SIDER, 0.37% on ClinTox, 4.55% on BBBP, and 10.52% on BACE. We also observe that the use of simple dot product-based coefﬁcient function requires less computation times when conducting the matrix operations compared to the other two variants. We present the GPS work in this paper, in which a novel adaptive sampling strategy is designed to generate embeddings effectively for both unseen nodes and graphs. By incorporating the importance-based policy into the sampling process, GPS is able to achieve state-of-the-art performance on graph classiﬁcation in the inductive setting. The experimental results implicitly prove that our proposed method is able to generalize topological properties of the given graph without knowing the entire graph structure upfront, thus making it easily expandable to larger graphs such as social network or citation network. As future work, an interesting direction is to explore the impact of edge properties in the sampling process, especially for heterogeneous graphs with different edge properties.