Crowdsourcing has become an essential choice for producing large-scale training and evaluation datasets for a broad spectrum of machine learning tasks, including computer vision, natural language processing, etc. However, crowdsourcing requires a careful task design that is usually reduced to a classiﬁcation task with a single known objective response [ retrieval and recommender systems training and evaluation, there is no objective response as the obtained rankings are relatively complex to evaluate properly. This challenge is usually addressed by relaxing the requirement of the objective answer and trying to gather the subjective human opinions on how the objects should be ranked [1]. As most humans are non-experts in ranking evaluation, preferences are gathered using pairwise comparisons (side-by-side, or SbS comparisons). In this scenario, the crowdsourcing tasks offered to the workers show a pair of objects with a question of which object is better. However, to train or evaluate the systems described above, it is necessary to obtain the ranking from the resulting comparisons. This process is called the aggregation and is fraught with various difﬁculties such as dealing with noisy or unreliable responses from crowd workers. Scientiﬁc community has developed speciﬁc models for aggregation of pairwise comparisons into ranked lists, including Bradley-Terry [ CrowdBT [ for computing quality criteria, such as normalized discounted cumulative gain or expected reciprocal rate. Unfortunately, most evaluation datasets reported in studies are either small but open [ large but proprietary [ models, forcing the researchers to use non-realistic synthetic models in their experiments. https://github.com/Toloka/IMDB-WIKI-SbS Nikita PavlichenkoDmitry Ustalov pavlichenko@yandex-team.rudustalov@yandex-team.ru Today, comprehensive evaluation of large-scale machine learning models is possible thanks to the open datasets produced using crowdsourcing, such as SQuAD, MS COCO, ImageNet, SuperGLUE, etc. These datasets capture objective responses, assuming the single correct answer, which does not allow to capture the subjective human perception. In turn, pairwise comparison tasks, in which one has to choose between only two options, allow taking peoples’ preferences into account for very challenging artiﬁcial intelligence tasks, such as information retrieval and recommender system evaluation. Unfortunately, the available datasets are either small or proprietary, slowing down progress in gathering better feedback from human users. In this paper, we present IMDB-WIKI-SbS, a new large-scale dataset for evaluating pairwise comparisons.It contains 9,150 images appearing in 250,249 pairs annotated on a crowdsourcing platform. Our dataset has balanced distributions of age and gender using the well-known IMDB-WIKI dataset as ground truth. We describe how our dataset is built and then compare several baseline methods, indicating its suitability for model evaluation. 5], factorBT [4], and others [12]. The resulting lists are then used as ground truth datasets We attempt in this paper to bridge the gap between these two extremes, presenting IMDB-WIKISbS, a new large-scale dataset for evaluation pairwise comparisons, building on the success of a well-known benchmark for computer vision systems IMDB-WIKI [ information offered by IMDB-WIKI as ground truth while providing a balanced distribution of ages and genders of people in photos. Table 1 shows the comparison of our dataset to other datasets used in research papers in terms of size. The rest of the paper is organized as follows. Section 2 explains how IMDB-WIKI-SbS was derived and sampled from the original IMDB-WIKI dataset. Section 3 describes how the sampled pairs of photos were annotated on the Toloka crowdsourcing platform. Section 4 presents the empirical evaluation of three baselines on our new dataset. Section 5 concludes with ﬁnal remarks. This section describes how we chose the photos for our dataset from a popular IMDB-WIKI dataset [10] and produced sample pairs for further crowdsourced annotation. We have derived our dataset from the IMDB-WIKI as follows. As in the original dataset, the odds ratio between males and females is roughly 1.4:1, and 50% of the data represent people aged 28–45, which does not allow for unbiased comparison between age and gender groups provided in the dataset. We decided to narrow our dataset span from 10 to 70 years. We excluded all the entries in which the face bounding box and gender label are missing. Then, we sampled uniformly 75 photos per age and gender. One person can appear only once in every group, but the same person can appear in multiple groups. If a person has multiple photos in the group, we have chosen the one with the highest face detector score provided by the dataset authors. As the result, our ﬁnal dataset contained (60 + 1) × 2 × 75 = 9,150 wrong gender information for some photos. However, we believe the amount of such mistakes does not affect the evaluation, given the size of our dataset. To produce a dataset allowing algorithm benchmarking, we decided to provide a connected graph of pairwise comparisons. To do so, we generated an Erd the NetworkX toolkit [ the distribution of labels would be uniform during crowd annotation. Although pairwise comparisons graphs might be disconnected in real-world scenarios, we believe that it is simpler to remove the available edges from our dataset and then evaluate how well the algorithm recovers and handles the missing ones. We decided to focus on the connected version due to the evaluation of the popular Bradley-Terry model that requires such a graph conﬁguration [2]. https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/ Table 1: Summaries for related datasets and the dataset presented in this paper. This section describes how we set up the quality control and annotated our dataset on the Toloka crowdsourcing platform, https://toloka.ai/. We used the following quality control conﬁguration. We limited the minimum time for task completion to at least four seconds. If the worker skipped more than ten task pages, we blocked them from accessing additional tasks. Our tasks were available to the top-50% workers on Toloka who set English as the spoken language. Additionally, to avoid malicious behavior, we used 200 golden comparisons that contained images of people with more than 20 years gap in age. We mixed these golden comparisons with the regular tasks. Failing to solve them correctly led to blocking access to our tasks. This dataset contains images of people with reliable ground-truth age assigned to every image. The task interface contained two images and two buttons, left and right. We intentionally omitted the “no answer” to avoid ambiguous cases not handled by all the algorithms and prevent abusing this option by malicious workers. This setup allowed us to obtain 250,249 comparisons by 4,091 workers. One pair was annotated by only one unique worker, but a worker could annotate multiple tasks. Each task page contained three regular tasks and one control task in random order. While annotating, we used the cropped face images instead of the complete photos. On average, each task page was done in 23 seconds, and there were about 20 task pages per worker. The whole annotation process took 6 hours. As a result, 125,302 pairs labeled “left” and 124,947 pairs labeled “right”. This section describes how we perform basic evaluation of baseline methods on our dataset using the commonly used normalized discounted cumulative gain (NDCG@k) score [8]. We have evaluated the performance of four baselines: a speciﬁc algorithm for aggregation of pairwise comparisons, an algorithm that utilizes only comparisons graph information, a random baseline, and the reversed ground-truth ranking. We provide a brief description of these methods below. Bradley-Terry. comparisons [ Given two items is. The likelihood of received comparisons according to this probabilistic model is maximized with respect to the scores. We use the implementation of the Bradley-Terry model available in the Crowd-Kit computational quality control toolkit for crowdsourcing [11]. PageRank. We built our directed graph as follows. As nodes, we used all the 9,150 images in our dataset. As arcs, we used all the 250,249 pairs in our dataset. The direction of an arc represents which image in the pair was labeled as older during the annotation. The resulting ranking is obtained by sorting the items by their PageRank scores in descending order. Random. generated random score. We ran this baseline to show the performance gain obtained by specialized methods. Reversed GT. methods offer reasonable ranking order. Bradley-Terry is a probabilistic model for predicting the outcome of pairwise 2]. Under this model, each itemiis associated with some positive real-valued scorep. iandjwith scorespandprespectively, probability ofito be ranked higher thanj PageRank is a classical algorithm for computing node centralities in a directed graph [3]. As a random baseline, we used a ranking obtained by sorting the items by a uniformly As a sanity check, we used a reversed ground truth (GT) dataset to ensure the Table 2: Comparison of pairwise aggregation methods on the whole IMDB-WIKI-SbS dataset. Our evaluation in Table 2 shows that even the simple pairwise comparison aggregation model performs well on our dataset compared to the random baseline while leaving space for the more principled models. Additionally, we evaluated the same baselines on two equally sized parts of our datasets separated by gender and found no speciﬁc difference in algorithm performance. To assess the difﬁculty of each age group, we evaluated the algorithm performance among different age groups in Table 3. Even though each group contained the same number of photos, aggregation algorithms performed worse than the random baseline. We believe there are two reasons for this. First, for workers, between-group comparisons were more straightforward to interpret than the in-group ones, leading to the different grades of difﬁculty to the algorithms and explaining the dramatic difference to the evaluation on the whole dataset. Second, as the comparison graph, in this case, is not connected, the Bradley-Terry model failed to deliver a meaningful ranking. Table 3: Comparison on pairwise aggregation methods on different age groups in IMDB-WIKI-SbS. NDCG@10 is used due to the relatively small number of objects per group. In this work, we have collected the largest available open dataset for crowdsourced pairwise comparisons. The described IMDB-WIKI-SbS dataset has been published at Toloka/IMDB-WIKI-SbS dataset will foster the development of better human-computer systems by allowing careful evaluation of these systems against human judgments and encourage the scientiﬁc community to develop better aggregation methods for crowdsourced pairwise comparisons.