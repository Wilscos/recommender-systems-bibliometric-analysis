E-commerce platforms usually display a mixed list of ads and organic items in feed. One key problem is to allocate the limited slots in the feed to maximize the overall revenue as well as improve user experience, which requires a good model for user preference. Instead of modeling the inuence of individual items on user behaviors, thearrangement signalmodels the inuence of the arrangement of items and may lead to a better allocation strategy. However, most of previous strategies fail to model such a signal and therefore result in suboptimal performance. In addition, the percentage of ads exposed (PAE) is an important indicator in ads allocation. Excessive PAE hurts user experience while too low PAE reduces platform revenue. Therefore, how to constrain the PAE within a certain range while keeping personalized recommendation under the PAE constraint is a challenge. In this paper, we propose Cross Deep Q Network (Cross DQN) to extract the crucial arrangement signal by crossing the embeddings of dierent items and modeling the crossed sequence by multichannel attention. Besides, we propose an auxiliary loss for batchlevel constraint on PAE to tackle the above-mentioned challenge. Our model results in higher revenue and better user experience than state-of-the-art baselines in oine experiments. Moreover, our model demonstrates a signicant improvement in the online A/B test and has been fully deployed on Meituan feed to serve more than 300 millions of customers. • Information systems →Computational advertising ; Online advertising; Ads allocation. Ads Allocation, Deep Reinforcement Learning, Arrangement Signal, Adaptive Ads Exposure ACM Reference Format: Guogang Liao, Ze Wang, Xiaoxu Wu, Xiaowen Shi, Chuheng Zhang, Yongkang Wang, Xingxing Wang, Dong Wang. 2022. Cross DQN: Cross Deep Q Network for Ads Allocation in Feed. In Proceedings of the ACM Web Conference 2022 (WWW ’22), April 25–29, 2022, Virtual Event, Lyon, France. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3485447.3512109 Feed, mixed with organic items and ads, is a popular product on many e-commerce platforms nowadays [7]. Platforms serve users and gain revenue via feed. In general, there are two ways for platforms to get revenue. Firstly, once users consume organic items or ads, the e-commerce platform will gain the platform service fee (hereinafter referred to as fee) according to the orders. Secondly, as an ad is clicked by a user, the platform will charge the corresponding advertiser. For the sake of the platform, displaying more ads is benecial to ads revenue but harmful to fee since ads are less likely engaging than organic items [23]. Usually, the number of ads is limited in feed to ensure good user experience and engagement. Hence, how to allocate limited slots reasonably and eectively to maximize overall revenue has become a very meaningful and challenging problem [11, 16, 22]. The structure of an industrial ads allocation system is shown in Figure 1. Blending Server takes ads sequence and organic items sequence as input and outputs a mixed sequence of the two. For Blending Server, there are two common strategies: xed slots strategy and dynamic slots strategy. Most platforms simply allocate ads to pre-determined slots [10,13]. Such strategies may lead to suboptimal overall performance. Dynamic slots strategy adjusts the number and slots of ads according to the interest of users. For instance, if a user has a higher tendency to consume commercial ads, the platform will allocate more ads at conspicuous slots to maximize possible benets. Except for personalization, dynamic slots strategies have lower ads blindness [21] and better adaptability, signicantly outperforming xed slots strategy and gradually becoming today’s trend. Early dynamic slots strategies use some Figure 1: Structure of an ads allocation system. The process of ads allocation takes place in the Blending Server. Figure 2: While inserting 𝑎𝑑into feed, the CTR of organic items increases while the CTR of 𝑎𝑑decreases. classic algorithms (e.g., Bellman-Ford, unied rank score) to allocate ads slots. Since the feed is presented to the user in a sequence, recent dynamic ads allocation strategies usually model the problem as Markov Decision Process [14] and solve it using reinforcement learning (RL) [4, 22, 25]. However, existing RL-based dynamic slots strategies encounter several major limitations: i) Most approaches ignore the crucial arrangement signalwhich is the inuence of the arrangement of displayed items on user behaviors. For example, as illustrated in Figure 2, once an ad is inserted into feed, the click-through rate (CTR) of surrounding organic items and ads uctuate. This signal receives attention in the scenario of Re-Rank recently [2,5,6,19] but is largely neglected in ads allocation. ii) Most of existing methods lack an ecient balance between the personalization of dierent requests and the constraint on the percentage of ads exposed (PAE) in a period. PAE is the most important constraint in ads allocation, which balances the user experience and platform revenue. Previous methods constrain all the requests or the requests within the same hour [17] with the same target PAE, resulting in a lack of personalization and dierentiation in the allocation of ads between dierent requests. To address the limitations of existing methods, we present a novel framework called Cross Deep Q Network (Cross DQN) based on deep reinforcement learning. Specically, we design two novel units called State and Action Crossing Unit (SACU) and Multi-Channel Attention Unit (MCAU) to explicitly extract the arrangement signal. Besides, we propose an auxiliary loss for batch-level constraint to balance the personalization of dierent requests and the constraint on PAE in a period. The contributions of our work are summarized as follows: • A superior ads allocation strategy. In this paper, we propose a novel RL-based framework named Cross DQNto dynamically adjust the number and the slots of ads in feed, which can eectively extract the arrangement signal and reasonably balance personalization of dierent requests and the constraint on PAE. • Detailed industrial and practical experience. We successfully deploy Cross DQN on the Meituan feed and obtain signicant improvements in both platform revenue and user experience. Traditional strategy for ads allocation in feed is to display ads at xed slots. Recently, dynamic ads allocation strategies gains growing attention. According to whether RL is used, existing dynamic ads allocation strategies can be roughly categorized into two categories: non RL-based and RL-based. Non RL-based methods usually use classical algorithms to allocate ads slots. Koutsopoulos[9]dene ads allocation as a shortestpath problem on a weighted directed acyclic graph where nodes represent ads or slots and edges represent expected revenue. The shortest path can be found by running Bellman-Ford algorithm. Furthermore, Yan et al. [21]takes the impact of the interval between ads into consideration and re-ranks ads and organic items jointly via a uniform ranking formula. RL-based methods model the ads allocation problem as an MDP and solved it with dierent RL techniques. Zhao et al. [25]proposes a two-level RL framework to jointly optimize the recommending and advertising strategies. Zhao et al. [24]proposes a DQN architecture to determine the optimal ads and ads position jointly. Xie et al. [20]proposes a hierarchical RL-based framework to rst decide the channel and then determine the specic item for each slot. In contrast to the previous work, we incorporate the arrangement signal into a RL-based dynamic ads allocation model to improve the performance. In our scenario, we present𝐾slots in one screen and handle the allocation for each screen in the feed of a request sequentially. The ads allocation problem is formulated as a Constrained Markov Decision Process (CMDP) [1] (S,A,𝑟,𝑃,𝛾,C), the elements of which are dened as follows: • State space S. A state𝑠 ∈ Sconsists of the information of candidate items (i.e., the ads sequence and the organic items sequence which are available on current step𝑡), the user (e.g., age, gender and historical behaviors), and the context (e.g., order time). • Action space A. An action𝑎 ∈ Ais the decision whether to display an ad on each slot on the current screen, which is formulated as follows: where𝑥=1 display an ad in the 𝑘-th slot0 otherwise,∀𝑘 ∈ [𝐾]. In our scenario, we do not change the order of ads sequence and organic items sequence in Blending Server. • Reward 𝑟. After the system takes an action in one state, a user browses the mixed list and gives a feedback. The reward is calculated based on the feedback and consists of ads revenue 𝑟, fee 𝑟and user experience 𝑟: • Transition probability 𝑃.𝑃 (𝑠|𝑠, 𝑎)is dened as the state transition probability from𝑠to𝑠after taking the action𝑎, where𝑡is the index for the screen/time step. The action taken in the state aects user behaviors. When the user pulls down, the state𝑠transits to the state of next screen𝑠. The items selected to present by𝑎will be removed from the state on the next step𝑠. If the user no longer pulls down, the transition terminates. • Discount factor 𝛾. The discount factor𝛾 ∈ [0,1]balances the short-term and long-term rewards. • Constraint C. The platform-level constraint is that the absolute dierence between the total PAE in a period and the the target value𝛿should be less than a threshold𝜀to ensure stable ads revenue. The PAE is formulated as follows: where𝑁is the number of requests in a period,Numand Nummean the number of ads and organic items in the𝑖-th request. In this work, we choose one week as the period. Consequently, the platform-level constraint can be formulated as follows: Given the CMDP formulated as above, the objective is to nd an ads allocation policy𝜋:S → Ato maximize the total reward under the platform-level constraint. The structure of popular RL model, Dueling DQN, [18] is shown in Figure 3(a), which receives the state only as the input. Such a structure fails to extract cross information between the action and the state, making it dicult to model the arrangement signal of mixed lists. A common solution is to concatenate state and action directly, but the model remains hard to extract the information between the items in the mixed list designated by the action. To this end, we propose a novel structure Cross DQN (cf. Figure 3(b)). The State and Action Crossing Unit (SACU) is designed to cross the embeddings of the items in a mixed list designated by the action. The Figure 3: Architectures of Dueling DQN and Cross DQN. Multi-Channel Attention Unit (MCAU) is designed to eectively extract arrangement signal from dierent channel combinations. Specically, we show the detailed structure of Cross DQN in Figure 4. The model takes a state (including the organic items/ads sequence, context information, etc.) and the corresponding candidate actions as the input. Then, Item Representation Module (IRM) generates the representations (especially the representations of ads and organic items). Next, Sequential Decision Module (SDM) generates Q-values of dierent actions with the help of SACU, MCAU and an auxiliary loss for batch-level constraint. In SACU, the state embeddings are intersected according to the action to form a unied matrix representation. In MCAU, the crossing matrix generated from SACU are split into dierent channels to calculate the multichannel attention weight. Finally, SDM chooses the action with the largest Q-value. We will introduce them in detail in the following subsections. Item Representation Module (IRM) generates the state embedding from the raw state. To eciently process the information from different sources, IRM generates two sequences of mixed embeddings: one for ads and one for organic items. The embedding for each item encodes not only the information of the item itself but also the information of the user prole, the context, and the interaction with historical user behaviors. First, we use embedding layers to extract the embeddings from raw inputs. We denote the embeddings for ads, organic items, historical behaviors of the user, the user prole, the context as{e}, {e},{e},e, anderespectively, where the subscript𝑖denotes the index within the sequence and𝑁,𝑁, and𝑁are the number of ads, organic items, and historical behaviors. Then, we use a target attention unit [15] to encodes the interaction between the historical behaviors of the user and the corresponding item, which is similar to Zhou et al. [26]: Afterwards, we append the embeddings of the user prole and the context to the embedding of each item: where||denotes concatenation. Notice that there are some strong features for ads and organic items in our scenario (e.g., discount, delivery fee, delivery time), which are concatenated with the embedding of each item and input into SDM. For ease of notation, we can also write the embeddings for ads and organic items in matrix form, each row of which represents one item in the sequence, i.e., Figure 4: Network architecture of Cross DQN. Item Representation Mo dule (IRM) generates the state embedding based on the raw state. Sequential De cision Module (SDM) generates Q-values of dierent actions with the help of State and Action Crossing Unit (SACU), Multi-Channel Attention Unit (MCAU) and auxiliary loss for batch-level constraint. Involving several attention units, IRM may be time-consuming upon deployment. However, IRM is an independent module within Cross DQN so that we can invoke IRM in parallel to other modules preceding Cross DQN. See more details in Section 4.8. To evaluate the Q-value of a certain state-action pair, we need an ecient representation of the mixed list designated by the corresponding action. State and Action Crossing Unit (SACU) helps us to construct a sequence of embeddings corresponding to the mixed list from the state embedding. First, given an action, we generate the corresponding action oset matrices for ads and organic items:M∈ {0,1}and M∈ {0,1}, where the(𝑖, 𝑗)-th element represents whether the𝑗-th ad/organic item is presented on the𝑖-th slot. Recall that𝐾 is the number of slots in one screen. For example, given the action 𝑎 = (0, 1, 0, 0, 1) with 𝐾 = 5, the action oset matrix Mis Then, we can calculate the cross matrixM, which is the embedding of the mixed list corresponding to the given action, using the action oset matrices. With SACU, we generate the embedding for the mixed list which enables us to eciently extract the arrangement signal in the next module. The user may focus on one or more aspects (e.g., discount, delivery fee, delivery time) of the mixed sequence at the same time. Accordingly, we propose MCAU to simultaneously model the user’s attention to dierent aspects of the mixed sequence. The cross matrixM∈ Rgenerated by SACU contains 𝑁dierent channels. Each channel represents an information dimension in the latent space and can be used to model one aspect of the mixed sequence. Meanwhile, the user may pay attention to more than one aspect of the mixed sequence at the same time. So the sequence information of two or more channels need to be combined for modeling. Next we will detail that how the sequence information of multiple channels is combined and modeled. For𝑁channels, we formulate the number of channel combinations as 𝑁, which is calculated as follows: We formalize the mask matrix for the𝑖-th combination asM. For example, the mask matrix for combination of the rst channel and the last channel is Nextly, signal matrix calculated by cross matrix and mask matrix are input into corresponding self-attention network [15] to model the attention across the𝐾items and generate a latent vector, as follows: Latent vectors output by dierent self-attention network are concatenated together to represent the arrangement signal extracted from dierent channels, as follows: With the help of SACU and MCAU, Sequential Decision Module (SDM) takes the embeddings generated by IRM and candidate actions as the input, and outputs Q-values corresponding to dierent actions. Given a set of𝑁candidate actions{𝑎}, SACU generates a cross matrixMfor each action and MCAU generates corresponding arrangement signal representation for each action. Subsequently, the outputs of the V network and the A network [18] can be calculated as follows: where pool indicates average pooling over dierent rows (i.e., different items in the ads/organic items sequence). Finally, SDM outputs the Q-value𝑄 (𝑠, 𝑎)corresponding to the 𝑖-th candidate action on the current screen as follows: Recall that our objective is to maximize cumulative reward under the constraint on the average ads exposure. The key for a successful strategy is to satisfy the constraint while maintaining a dierentiated recommendation for dierent users/scenarios. For example, if the user is prone to be annoyed by ads, we should expose less ads to the user, and vice versa. Dierent auxiliary losses to constrain the percentage of ads exposure (PAE) can result in dierent level of differentiation. A common solution is to use a request-level constraint, i.e., constraining the PAE of each request to be close to the PAE target𝛿. Such a solution may result in poor dierentiation since the PAE of each request is constrained to the same target𝛿regardless of the context. To allow for dierentiation, Wang et al. [17]propose to use an hour-level constraint that allows for using dierent PAE targets in dierent hours. However, the level of dierentiation is still limited within an hour. To this end, we propose a batch-level constraint to constrain the average PAE of the requests in a batch instead of constraining the PAE of each request. We denote the PAE associated with the action𝑎asPAE(𝑎). For example, the PAE of𝑎 = (0,1,0,0,1)is 0.4. Given a batch of transitions 𝐵, our batch-level constraint can be written as: 𝐿(𝐵) =𝛿 −1|𝐵|PAE(arg max𝑄 (𝑠, 𝑎)) However, the argmax function is not dierentiable. Therefore, we use a soft version of argmax instead, i.e., we use PAE(arg max𝑄 (𝑠, 𝑎)) ≈1𝑍exp𝛽𝑄 (𝑠, 𝑎)PAE(𝑎 where𝑍 =Íexp[𝛽𝑄 (𝑠, 𝑎)]is the normalization factor and𝛽 is the temperature coecient. Unlike previous request-level or hour-level constraints that limit the PAE for each request, we only limit the average PAE estimated using randomly sampled batches. Such a weaker form of constraint encourages the model to choose the action with a PAE that is deviated from 𝛿 but may better adapt for the current context. We show the process of oine training in Algorithm 1. We train Cross DQN based on an oine dataset𝐷generated by an online exploratory policy𝜋. For each iteration, we sample a batch of transitions𝐵from the oine dataset and update the model using gradient back-propagation w.r.t. the loss: where𝐿is the same loss function as the loss in DQN [12],𝐿 is the auxiliary loss for the constraint, and𝛼is the coecient to balance the two losses. Specically, 𝐿(𝐵) =1|𝐵|𝑟 + 𝛾 max𝑄 (𝑠, 𝑎) − 𝑄 (𝑠, 𝑎). (21) Oine data𝐷 = {(𝑠, 𝑎, 𝑟, 𝑠)}(generated by an online exploratory policy 𝜋) We show the process of online serving in Algorithm 2. In the online serving system, Cross DQN selects the action with the highest reward based on current state and converts the action to ads slots set for the output. When the user pulls down, the model receives the state for the next screen, and then makes a decision based on the information on the next screen. In our scenario, current state will transit to the next state or terminate depending on whether the user pulls down. However, the next possible state corresponding to a given action is deterministic if the interaction does not terminate. Based on this observation, we can cache the decisions for multiple screens and transmit to Figure 5: Model de composition for online service. the client at once to reduce the time cost for the communication between the server and the client. Model Decomposition. Cross DQN will be called𝑇times for one cache, which is time consuming for industrial scenarios where latency is a major concern. Fortunately, the outputs of IRM can be reused across dierent calls of Cross DQN in one cache, which saves up to about 80% computation time. Since the generation of item representations does not rely on the previous modules (such as ranking and ads bidding), we calculate the representation of the items parallel to the previous modules, which further reduces the latency. As shown in Figure 5, the Cross DQN is decomposed into IRM and SDM for deployment. The two parts are trained end-to-end but deployed on dierent services for real-time prediction. The IRM is calculated parallel to Ad Ranking and Organic Ranking systems. Hence, it is latency-free for real-time inference of SDM. Parameter Sharing. Both of IRM and SDM use parameter sharing (cf. Figure 4) across dierent ads/organic items. As for IRM, we can calculate the representations for all recalled items at the same time without ranking information through parameter sharing and parallel computing. Meanwhile, in SDM, we use parameter sharing across dierent actions to guarantee the consistency of the reward evaluation of the actions and ensure that the batch-level constraint is eective. In addition, parameter sharing can reduce the scale of parameters, accelerate model training and reduce memory usage. We will evaluate Cross DQN model through oine and online experiments in this section. In oine experiments, we will compare Cross DQN with existing baselines and analyze the role of dierent designs in Cross DQN. In online experiments, we will compare Cross DQN with the previous strategy deployed on the Meituan platform using an online A/B test. 5.1.1 Dataset. We collect the dataset by running an exploratory policy on the Meituan platform during March 2021. We present the detailed statistics of the dataset in Table 1. Notice that each request contains several transitions. The features for the ads/organic items include the identity, the category, the comment score, etc. The features for the user prole include the identity, the gender, etc. 5.1.2 Evaluation Metrics. We evaluate the model with revenue indicators and experience indicators. As for revenue indicators, we use ads revenue and service fee in a period to measure platform revenue. Specically, the ads revenue is gained from advertisers calculated using Generalized Second Price (GSP) [3] and chargedÍ per click. The total ads revenue is calculated as𝑅=𝑟. The service fee is charged from merchants’ orders according to a certainÍ percentage. and the total service fee is calculated as𝑅=𝑟. In our platform, user experience is measured by whether the user demand (e.g., nding a satisfying product) can be fullled. As for experience indicators, we use the average conversion rate and average experience score to measure user experience. The conversionÍ rate calculated as𝑟=CTR × CVRis the ratio of the number of orders placed by the user to the number of his/her requests. The experience score𝑟dened in Section 3 reects the degree of satisfaction of the user demand. 5.1.3 Hyperparameters. We implement Cross DQN with TensorFlow and apply a gird search for the hyperparameters. The hidden layer sizes of the IRM are(128,64,32,8,2)and the hidden layer sizes of the SDM are(16,8,1). The learning rate is 10, the optimizer is Adam [8] and the batch size is 8, 192. In this section, we train Cross DQN with oine data and evaluate the performance using an oine estimator. Through extended engineering, the oine estimator models the user preference and aligns well with the online service. We conduct experiments to answer the following two questions: i) How does Cross DQN perform compared with other baselines? ii) How do dierent designs (e.g., SACU, MCAU) and hyperparameter settings (e.g.,𝛼,𝛽) aect the performance of Cross DQN? 5.2.1 Baselines. We compare Cross DQN with the following ve representative methods: • Fixed. This method displays ads in xed slots, such as the slot indexed by 3, 6, 9, · · · . • GEA[21]. GEA is a non RL-based dynamic ads slots strategy. It takes the impact of ads intervals into consideration and ranks the ads and organic items jointly with a rank score RS = (CTR × charge + GMV × takerate) exp(𝛽𝑑), where charge is the fee paid by advertisers, takerate is the take rate (i.e., the fee charged on each transaction by the platform), and 𝑑 is the interval between two ads. • CTLRL[17]. Constrained Two-Level Reinforcement Learning (CTLRL) uses a two-level RL structure to allocate ads. The upper level RL model decomposes the platform-level constraint into hour-level constraints, and the lower level RL model sets the hour-level constraint as the request-level constraint. • HRL-Rec[20]. HRL-Rec is an RL-based dynamic ads slots strategy. It divides the integrated recommendation into two levels of tasks and solves using hierarchical reinforcement learning. Specically, the model rst decides the channel Table 2: The result of Revenue Indicators and Experience Indicators. Each experiment are presented in the form of mean ± standard deviation. The improvement means the improvements of Cross DQN across the best baselines. (i.e., select an organic item or an ad) and then determine the specic item for each slot. • DEAR[24]. DEAR is also an RL-based dynamic ads slots strategy. It designs a deep Q-network architecture to determine three related tasks jointly, i.e., i) whether to insert an ad to the recommendation list, and if yes, ii) the optimal ad and iii) the optimal location to insert. 5.2.2 Performance Comparison. We present the experiment results under the same PAE level in Table 2 and have the following observations: i) Compared with all these baselines, Cross DQN achieves strongly competitive performance on both the revenuerelated metrics and experience-related metrics. Specically, Cross DQN improves over the best baseline w.r.t.𝑅,𝑅,𝑅and𝑅 by 3.09%, 2.05%, 0.83% and 1.58% separately. ii) Cross DQN outperforms the xed slots strategy. A reasonable explanation is that the ads positions calculated by Cross DQN are more personalized, which leads to an increase in revenue as well as an improvement of user experience. iii) Cross DQN outperforms GEA, which indicates that an RL-based method may perform better than a rule-based method. iv) Cross DQN also performs better than CTLRL possible due to the fact that the PAE of dierent requests of Cross DQN within the same hour are more personalized. v) Compared with the state-of-the-art RL-based methods, i.e., HRL-Rec and DEAR, the superior performance of Cross DQN justies the explicit modeling of the arrangement signal. 5.2.3 Ablation Study. To verify the impact of dierent designs (SACU, MCAU, batch-level constraint), we study three ablated variants of Cross DQN which have dierent components in SDM. DQN. Notice that without the help of the auxiliary loss, we can adjust the coecients in the reward function to realize the same PAE as the PAE of other baselines. •Cross DQN (-aux-mcau) additionally blocks the MCAU and uses one self-attention unit instead on top of the previous ablated version. •Cross DQN (-aux-mcau-sacu) concatenate the embeddings of the action and the state directly without SACU. The results shown in Table 2 reveal the following ndings: i) The performance gap between Cross DQN (-aux-mcau-sacu) and Cross DQN (-aux-mcau) indicates the eectiveness of SACU. By explicitly crossing the embeddings of the states and the actions, SACU can eectively generate cross matrix representation for subsequent extraction of arrangement signal, therefore improving the overall metrics. ii) The MCAU is an additional process after the crossover to strengthen the mutual interaction. The performance of Cross DQN (-aux) is superior to Cross DQN (-aux-mcau), which veries the eectiveness of extracting arrangement signal of different channel combinations. iii) Cross DQN outperforms Cross DQN (-aux), resulting from the fact that the batch-level constraint brings a certain revenue increase and makes the PAE in a period more stable. 5.2.4 Hyp erparameter Analysis. We analyze the sensitivity of these three hyperparameters:𝜂,𝛼and𝛽.𝜂is the weight for the user experience in the reward function (cf. Eq. (2)).𝛽is the temperature parameter that controls the degree of the approximation in Eq. (19). 𝛼is the hyperparameter which balances the main loss and auxiliary loss (cf. Eq (20)). Hyperparameter 𝜂.The experimental results of dierent values of𝜂are presented in Figure 6a. As𝜂increases,𝑅increases but𝑅 decreases. A reasonable explanation is that the system of dynamic ads allocation tends to insert fewer ads when𝜂becomes larger, which has a benecial impact on user experience and fee. Hyperparameter 𝛼.As shown in Figure 6b, we nd that the auxiliary loss for batch-level constraint has greater inuence on return. When𝛼increases, the standard deviation of reward decreases. This phenomenon shows that the PAE and revenue are more stable under batch-level constraint. It is worth noticing that the mean of reward increases when𝛼changes from 0 to 1. One possible explanation is that the ads allocation under a certain batch-level constraint of PAE will be more reasonable, which ensures the quality of display results and improves the revenue and user experience. However, if 𝛼is too large, it will deviate from the learning goal, resulting in a decline in reward. Hyperparameter 𝛽.The right curve in Figure 6 reveals that the mean of reward increases and the standard deviation of reward decreases as𝛽increases within a certain range. This phenomenon demonstrates that accurate calculation of PAE results in stable and high reward. On the contrary, the reward may decrease when𝛽 exceeds a certain threshold, suggesting the necessity to carefully tune this parameter in practice. We compare Cross DQN with xed ads positions and both strategies are deployed on the Meituan platform through online A/B test. We keep total PAE the same for all methods for a fair comparison. As a result, we nd that𝑅,𝑅and𝑅increase by 12.9%, 10.2% and 9.1%, which demonstrates that our Cross DQN not only signicantly increases the platform revenue, but also improves user experience. It is worth noting that this increase values are 11.5%, 10.7% and 10.0% in oine experiments. One possible reason for this dierence in absolute value is the dierences in data distribution. In this paper, we propose Cross DQN to optimize ads allocation in feed. In Cross DQN, we design State and Action Cross Unit and Multi-Channel Attention Unit to explicitly extract the arrangement signal that is the inuence of the arrangement of items in mixed list on user behaviors. In addition, we introduce an auxiliary loss for batch-level constraint to achieve the personalization for dierent requests as well as the platform-level constraint. Practically, both oine experiments and online A/B test have demonstrated the superior performance and eciency of our solution. In our scenario, user experience is also an important objective for the long-term growth of the platform since the improvement of user experience directly increases the retention rate and enhances the reputation of the platform. In the future, it is benecial to optimize for more user experience metrics and pay more attention to the modeling of long-term benets. In addition, it is worth noting that our method follows the oine reinforcement learning paradigm. Compared with online reinforcement learning, oine reinforcement learning faces additional challenges (such as the distribution shift problem). The impact of these challenges to the ads allocation problem is also a potential research direction in the future.