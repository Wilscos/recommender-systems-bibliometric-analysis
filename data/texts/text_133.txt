Graph neural networks have triggered a resurgence of graph-based text classiﬁcation. We show that already a simple MLP baseline achieves comparable performance on benchmark datasets, questioning the importance of synthetic graph structures. When considering an inductive scenario, i. e., when adding new documents to a corpus, a simple MLP even outperforms the recent graph-based models TextGCN and HeteGCN and is comparable with HyperGAT. We further ﬁne-tune DistilBERT and ﬁnd that it outperforms all state-ofthe-art models. We suggest that future studies use at least an MLP baseline to contextualize the results. We provide recommendations for the design and training of such a baseline. Text classiﬁcation is an active area of research as the amount of new methods and recent surveys show (Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019). In this work, we refer to text classiﬁcation as the topical categorization of text. Note that also other tasks such as question answering (Rajpurkar et al., 2016) or natural language inferencing (Wang et al., 2019) can be casted as text classiﬁcation on a technical level (Devlin et al., 2019). In those cases, the positional information of the sequence is more important than it is when the task is pure topical text classiﬁcation. What is interesting to observe is that among the various methods compared for text classiﬁcation, the good old multi-layer pereceptron (MLP) is rarely among them. This is surprising as MLPbased models were shown to be good performers for classiﬁcation tasks (Shen et al., 2018; Mai et al., 2018; Galke et al., 2017). MLPs are conceptually simple and have only few hyperparameters (number of layers, hidden dimension), and can be combined with any input representation such as a bag-of-words, optionally with TF-IDF weighting, and/or pretrained word embeddings. It appears that the MLP model has been forgotten as baseline in the literature. However, considering strong baselines is an important means to argue about true scientiﬁc advancement (Shen et al., 2018; Dacrema et al., 2019). We review the key research in text classiﬁcation and identify the top performing models. We extract the scores reported for these model on established benchmark datasets. We show the lack of using an MLP as strong baseline and run our own experiments with different variants of a simple MLP. Finally, we ﬁne-tune a pretrained language model DistilBERT (Sanh et al., 2019), a size-reduced version of BERT (Devlin et al., 2019), on the text classiﬁcation datasets. Our results show that a SimpleMLP with one hidden layer is close or even outperforms recent graphbased approaches (Yao et al., 2019; Liu et al., 2020; Ragesh et al., 2021). We argue that a one-hiddenlayer MLP satisﬁes the universal approximation theorem (Cybenko, 1989): A single hidden layer with nonlinear activation is sufﬁcient to approximate any compact function to an arbitrary degree of accuracy (depending on its width). A ﬁne-tuned DistilBERT sets a new state of the art. We conjecture that there is a forgotten merit in using MLPs for text classiﬁcation as a simple and very strong model and that it should be used in future studies. In fact, other ﬁelds have reported a resurgence of MLPs. In example, an MLP baseline outperforms various other Deep Learning models for business prediction (Venugopal et al., 2021). In computer vision, Tolstikhin et al. (2021) and MelasKyriazi (2021) propose pure MLP models and question the necessity of self-attention in Vision Transformers (Dosovitskiy et al., 2021). Liu et al. (2021) show similar results in natural language processing, while acknowledging that a small attention module is necessary for some tasks. This shows the importance of MLP baselines and we hope with this work to contribute to their resurgence. We discuss related works on text classiﬁcation to identify the best performing approaches. We base the identiﬁcation of the models in our comparison on the recent surveys, which cover the range from shallow to deep classiﬁcation models (Li et al., 2020; Zhou et al., 2020; Kowsari et al., 2019; Kadhim, 2019). We note that the recent surveys from 2020 and 2019 include both classical and Deep Learning models, but none considered a simple 1layer MLP. A notable exception is the inclusion of DAN (Iyyer et al., 2015), a deep MLP model withnhidden layers, in (Li et al., 2020). We complement our search by checking results and papers on paperswithcode.com. We focus our analysis on models showing strong performance on benchmark datasets (see Table 1). We identify the best performing models per different approach. Furthermore, we explicitly screen for further literature in the key NLP and AI venues. For all models, we have veriﬁed that the same train-test split is used and check whether modiﬁed versions of the datasets have been used (e. g., less classes). Embedding-based modelsClassic machine learning models are extensively discussed in two surveys (Kowsari et al., 2019; Kadhim, 2019) and other comparison studies (Galke et al., 2017). Iyyer et al. (2015) have proposed deep averaging networks (DAN) combining word embeddings and deep feedward networks. Their results suggest to use pretrained embeddings such as GloVe (Pennington et al., 2014) over a randomly initialized neural bag of-words (Kalchbrenner et al., 2014) as input. In fastText (Joulin et al., 2017) a linear layer on top of pretrained embeddings is used for classiﬁcation, which is often considered as baseline. Furthermore, Shen et al. (2018) explore further embedding pooling variants ﬁnd that simple word embedding models (SWEM) can rival RNNand CNN-based approaches. We consider all three models in our comparison and show that a single wide hidden layer is stronger than using pretrained embeddings or more layers. Note that those approaches that rely on a logistic regression on-top of a word embedding, e. g., fastText, share a similar architecture as an MLP with one hidden layer. However, the standard training protocol involves pretraining the word embedding on large amounts of unlabeled text. In our experiments, we show that this is not necessary and may be even harmful for (topical) text classiﬁcation. Overall, we identify logistic regression and MLP as top performer of classical models to include in this paper. Recurrent neural networks (RNN)are a natural choice for any NLP task and have been extensively investigated, too. However, it turned out to be challenging to ﬁnd numbers reported in the literature that can be used as reference. The bidirectional LSTM with two-dimensional max pooling BLSTM-2DCNN (Zhou et al., 2016) has been applied on a stripped-down to 4 classes version of the 20ng dataset. Thus, the high score of96.5reported for 4ng cannot be compared with papers applied on the full 20ng dataset. Also TextRCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset. The results of TextRCNN is identical with BLSTM-2DCNN. For the MR dataset, BLSTM2DCNN provides no information on the speciﬁc splitting of the dataset. RNN-Capsule (Wang et al., 2018) is a contemporary model for sentiment analysis, with an accuracy of 83.8 for the MR dataset. Graph-based text classiﬁcationhas a long history in NLP. It appears also to be the currently most active area, perhaps due to the recent interest in graph-neural networks (GNN) (Hamilton, 2020). An early work is the term co-occurrence graph of the KeyGraph algorithm (Ohsawa et al., 1998). Cooccurence graphs have also been used for automatic keyword extraction such as in RAKE (Rose et al., 2010). Modern approaches exploit this idea in combination with GNNs. Those include TextGCN (Yao et al., 2019), TensorGCN (Liu et al., 2020), HeteGCN (Ragesh et al., 2021), and HyperGAT (Ding et al., 2020). In TextGCN, the authors set up a graph based on word-word connections given by window-based pointwise mutual information and word-document TF-IDF scores. HeteGCN split the adjacency matrix into its word-document and word-word submatrices and fuse the different layers’ representations when required. TensorGCN uses multiple ways of converting text data into graph data includic a semantic graph created with an LSTM, a syntactic graph created by dependency parsing, and a sequential graph based on word cooccurrence. Finally, HyperGAT extended the idea of text-induced graphs for text classiﬁcation to hypergraphs. The model uses graph attention and two kinds of hyperedges. Sequential hyperedges represent the relation between sentences and their words. Semantic hyperedges for word-word connections are derived from topic models (Blei et al., 2003). In TextGCN’s original transductive formulation, the entire graph including the test set needs to be known for training. This may be prohibitive in practical applications as each batch of new documents would require retraining the model. When these methods are adapted for inductive learning, where the test set is unseen, they achieve notably lower scores (Ragesh et al., 2021). GNNs for text classiﬁcation use corpus statistics, e. g., pointwise mutual information (PMI), to connect related words in a graph (Yao et al., 2019). When these were omitted, the GNNs would collapse to bag-of-words MLPs. Thus, GNNs have access to more information than MLPs. GloVe (Pennington et al., 2014) also captures PMI corpus statistics, which is why we include an MLP on GloVe input representations in our experiments. Hierarchical methodsexploit a hierarchical taxonomy of the classes for text classiﬁcation. HRDGCNN (Peng et al., 2018) follows this idea and ﬁrst converts the text to a word co-occurence graph on which hierarchically regularized convolution operations are applied. The model was applied on two popular benchmark datasets for multi-label text classiﬁcation with a Micro-F1 score of0.76for RCV1 and and0.65for NYT. One year later, Xiao et al. (2019) proposed the hierarchical text classiﬁcation model PCEM, which uses path information from the taxonomy in the learning algorithm. Experiments on RCV1 showed a Micro-F1 score of 77.83, slighthly higher than HR-DGCNN (Peng et al., 2018). A direct comparison of PCEM with HR-DGCNN was not performed or reported. Multi-label text classiﬁcation often comes naturally with hierarchical taxonomies. However, we focus our study on single-label datasets. PCEM had been applied on the single-label 20NG data, where it showed a Micro-F1 score of70.73. Notably, the authors focus on a weakly-labeled classiﬁcation task with only 1% of the labels and exploit the path information in the taxonomy. This makes it difﬁcult to compare PCEM with our results (see Table 2). Generally, multi-label classiﬁcation adds another level of complexity for comparing the model performance, where F1-scores instead of accuracy are reported. This raises further challenges, which we leave as future work. Transformer-based ModelsSince our aim is to argue for universally while at the same time seek lightweight models, we look into Transformers that have been distilled into smaller-sized variants. There are several candidates of smaller-sized versions of BERT. We excluded TinyBert (Jiao et al., 2020) since it requires a full BERT model as teacher for ﬁne-tuning. MobileBERT (Sun et al., 2020) would be suitable for our goals, but it relies on a special BERT-large model with invertedbottleneck modiﬁcations. DistilBERT (Sanh et al., 2019) is a distilled version of BERT (Devlin et al., 2019) with 40% reduced parameters while retaining 97% of BERT’s language understanding capabilities. We use DistilBERT because its inference times are 60% faster and it is more likely to be reusable by labs with limited resources. Furthermore, DistilBERT (Sanh et al., 2019) can be directly ﬁne-tuned to downstream tasks while showing state-of-the-art performance among sizereduced BERT variants. Thus, we use the DistilBERT in our experiments. We describe our SimpleMLP model and provide recommendations for using it as baseline in future work. TokenizationParallel to developments in largescale language models, also tokenization has evolved in recent years. We borrow the tokenization strategy from BERT (Devlin et al., 2019) along with its uncased vocabulary. The tokenizer relies primarily on WordPiece (Wu et al., 2016) for a high coverage while maintaining a small vocabulary. Input representationFor text classiﬁcation, the content of the input words is more important than the word order (Conneau et al., 2018). Therefore, we use length-normalized bag-of-words inputs. We further experiment with TF-IDF weighting, normalized to unit L2-norm. Both are viable options. In contrast to conventional wisdom (Iyyer et al., 2015), we ﬁnd that pretrained embeddings, e. g., GloVe, often have a detrimental effect when compared to using one wide hidden layer instead. Depth vs widthIn text classiﬁcation, width seems more important than depth. We recommend to use a single hidden layer, i. e., one input-tohidden and one hidden-to-output layer, with1, 024 hidden units and ReLU activation. While this might be overparameterized for single-label text classiﬁcation tasks with few classes, we rely on recent ﬁndings that suggest that overparameterization leads to better generalization (Neyshabur et al., 2018; Nakkiran et al., 2020). We further motivate the choice of using wide layers by our own prior work on multi-label text classiﬁcation (Galke et al., 2017), in which we have shown that MLP outperforms all tested classical baselines such as SVMs, k-Nearest Neighbors, and logistic regression. In follow-up work (Mai et al., 2018), we found that also CNN and LSTM could not substantially improve over the wide MLP. Having a fully-connected layer on-top of a bagof-words leads to a high number of learnable parameters. Still, we can implement this ﬁrst inputto-hidden layer efﬁciently by using an embedding layer followed by aggregation, which avoids large matrix multiplications. For demonstration purposes, we also experiment with more hidden layers (SimpleMLP-2), as suggested by Iyyer et al. (2015), but we do not observe any improvement when the single hidden layer is sufﬁciently wide. Optimization and RegularizationWe seek to ﬁnd an optimization strategy that does not require dataset-speciﬁc hyperparameter tuning. This comprises optimizing cross-entropy with Adam (Kingma and Ba, 2015) and default learning rate10, a linearly decaying learning rate schedule and training for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufﬁcient stochasticity. For regularization during this prolonged training, we suggest to use a high dropout ratio of0.5. Regarding initialization, we rely on framework defaults, i. e.,N (0, 1)for the initial embedding layerpp and random uniformU(−d,d)for subsequent layers’ weight and bias parameters. DatasetsWe use the same datasets and train-test split as in TextGCN (Yao et al., 2019). Those datasets are: •Twenty Newsgroups (20ng)(bydate version) with all 20 classes, •Movie Review (mr)(Pang and Lee, 2005) with the split of (Tang et al., 2015), •R8 and R52, which are subsets of the Reuters 21578 news dataset with 8 and 52 classes, respectively, •and Ohsumed, a corpus from the MEDLINE database excluding those that belong to multiple classes. We summarize the basic characteristics of the datasets in Table 1. ProcedureWe have extracted accuracy scores from the literature according to our systematic selection from Section 2. Without any hyperparameter tuning, we run our SimpleMLP as described in Section 3. We ﬁne-tune DistilBERT for 10 epochs with a linearly decaying learning rate of5 · 10 and an effective batch size of 128, while truncating inputs to 512 tokens. We repeat all experiments 5 times and report mean and standard deviation. Table 2 shows the accuracy scores for the text classiﬁcation datasets. All graph-based models in the transductive setting show similar accuracy scores (maximum difference is 2 points). As expected, the scores decrease in the inductive setting up to a point where they are matched or even outperformed by our SimpleMLP baseline. The strong performance of SimpleMLP rivals all techniques reported in the literature, in particular the the recently published graph-inducing methods. MLP only falls behind HyperGAT, which relies on topic models to set up the graph. Another observation is that1hidden layer (but wide) is sufﬁcient for the tasks, as the scores for MLP variants with 2hidden layers are lower. We further observe that both pure and TF-IDF weighted input representations lead to better results than approaches that exploit pretrained word embeddings such as DAN, fastText, and SWEM. With its immense pretraining, DistilBERT yields the overall highest scores. Table 2: Accuracy on text classiﬁcation datasets. SD in parentheses. References indicate source of numbers. DistilBERT outperforms HyperGAT by 7 points on the MR dataset while being on-par on the others. Intuitively, we explain the strong performance of MLPs that for text classiﬁcation a bag-of-words model is oftentimes sufﬁcient to grasp the topic of a text, although word order is discarded. As such, our SimpleMLP can effectively learn the function from features to class labels without any hyperaparameter tuning or early stopping. Potentially, the scores could be even increased by using a grid search over hyperparameters. However, the goal of this study is to suggest a simple but effective baseline that works well on different datasets, even with only choosing default hyperparameters. Finally, we observe that a ﬁne-tuned DistilBERT even sets a new state-of-the-art. But still in practice, such a model has to be handled, and a from-scratch trained model like SimpleMLP may be favored in contexts where an existing model cannot be used due to legal or other reasons. Overall, the rationale of our study is to encourage a quick use and take up of a simple (MLP) baseline, which—as discussed in this work–is almost always omitted. We expect that similar observations would be made on other text classiﬁcation datasets. We acknowledging that all our current datasets are limited to English language. It is, however, notable that methods that discard word order work well for (topical) text classiﬁcation. Despite the strong results for SimpleMLP, it is certainly not a silver bullet. There are also tasks where the natural structure of the graph data provides more information than the mere text, e. g., citations networks or connections in social graphs. In such cases the performance of graph neural networks is the state of the art (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) and are superior to MLPs that use only the graph’s vertex features and not the graph structure (Shchur et al., 2018). In contrast, the surplus value of synthetic graph structures like word co-occurrence graphs computed over a text corpus is questionable. Our results in Table 2 show that SimpleMLP challenges or outperforms all recent graph-based methods when applied to unseen documents (inductive setting), except for the expensive LDA-enriched HyperGAT. We argue that a simple multi-layer perceptron enhanced with all of today’s best practices should be considered as a strong baseline for text classiﬁcation tasks. In fact, the experiments show that our SimpleMLP is oftentimes on-par or even better than recently proposed models that synthesize a graph structure from the text. A future direction of this work would deeper analyze more challenging multi-label text classiﬁcation tasks. The code is available on GitHuB athttps://github. com/lgalke/text-clf-baselines.