On multimedia content sharing platforms (e.g. Pinterest, Instagram, and TikTok) [1], users recently face the trouble of locating their interesting items from the overloaded information. Therefore, multimedia recommender systems have been designed as the core components of these service providers, which aims to discover the candidate items for users according to their tastes. In order to suggest users a list of items meeting their individual interests, one of the key factors of the multimedia recommendation is to model the user preferences towards the content information. Present articles [2, 3, 4] typically incorporate pre-existing features of items into the recommendation framework, especially the collaborative ﬁltering (CF), so as to proﬁle the users based on historical interactions and item contents. Some representative methods, like VBPR [2] and ACF [3], tend to extract features of items via deep neural networks and then integrates them with the collaborative embeddings to learn the user preference. Despite their effectiveness, we argue that such a paradigm is insufﬁcient to proﬁle user preferences. One major reason is that these deeplearning-based feature extractors are tailored for computer vision [5, 6, 7] and natural language processing [8, 9], rather than the personalized recommendation. Hence, simply using such pre-existing features easily leads to suboptimal representations of user preference. To solve this problem, one solution is to model the user intents to guide the user and item representation learning, which could distill the informative signal affecting the users’ choices. In particular, the user intents are used to describe the motivations why people prefer some items. Based on the distribution over them, the item could be re-expressed according to users’ motivation and the user preference reﬂects the motivation when she/he makes decisions. For instance, Liu et al. [10] devised a co-attention model to reﬁne the items’ features with the learned users’ representations; Lei et al. [11] proposed the duel-net deep network to project the visual and user information into a latent semantic space to mimic the intents. However, such methods are limited to ﬁlter out the features no one concerned, instead of discovering the users’ motivations. Although some models [12, 13] utilize the social information or user demographics to remedy this drawback, the gap between these external information and user intents still exists. Therefore, it is crucial to explicitly model the user intents and represent the users and items with the distributions on the learned intents. However, it is non-trivial due to the following challenges: involves the information both from the users’ behaviors Fig. 1: Illustration of the users’ multi-level intents on video content. and item content. Speciﬁcally, it is intuitively plausible that users’ behaviors towards items reﬂect their intents; meanwhile, the content information tends to be the source of the user intents. Considering these two types of information are heterogeneous, how to effectively organize them into a uniﬁed manner and distill the informative signal on user intents is the primary challenge in our task. is not isolated in this structure. As shown in Figure 1, uprefers the actor and category; whereas upays more attention to the visual quality and storylines of videos. Whereinto, the visual quality and actor are shown by frames, while the storyline and category are summarized with the whole video. It conveys the fact that user intents can be divided into different levels from the ﬁne-grained (e.g. preferences for the frame) to the coarse-grained (e.g. interests for the whole video). Moreover, the coarser-grained ones can be summarized from the ﬁne-grained ones. Hence, how to model the hierarchical structure corresponding to users’ multi-level intents is another unavoidable challenge we are facing. Inspired by the recent progress in graph convolutional networks (GCNs), we propose to employ the information aggregation mechanism to model a hierarchical user intent graph against the above challenges. Towards this end, we ﬁrst construct a co-interacted item graph, where the edge corresponds to the item pair consumed by the same users, in order to integrate the information of item contents with that of user behaviors. Upon such a cointeracted graph, we then develop a Hierarchical User Intent Graph Network (HUIGN), which learns the multilevel user intents’ representations as well as the hierarchical structure. Speciﬁcally, we design two types of information aggregation: 1) intra-level aggregation which captures the local structure information (i.e. user co-interacted behaviors) for each node and distills the user intents via incorporating it with content information; and 2) inter-level aggregation which establishes some supernodes in the higher level and gathers information of nodes to summarize these supernodes as coarser-grained user intents. Iteratively performing these two aggregations, we explicitly learn the representation of multi-level user intents and the hierarchical structure from ﬁne-grained intents to coarsegrained ones. Finally, we characterize the user and item as a distribution over the discovered intents to predict their interactions. To evaluate the proposed model, we conduct experiments on three publicly accessible datasets designed for the micro-video personalized recommendation. The results show that our proposed model achieves state-of-theart performance. Besides, with the visualizations of items’ representations, our model provides the answer to what the user intents are. In a nutshell, we summarize the main contributions as: intents learning in the multimedia recommendation and propose to explicitly model the multi-level user intents to optimize the user and item representations. learn the user intents from the ﬁne-grained to the coarsegrained. It incorporates the users’ behavior with items’ content information through the intra- and inter- level graph convolutional operations. demonstrate the effectiveness of our proposed method. In addition, we provide the semantic representations of user intents through displaying the items’ characters based on the user intents. As a side contribution, we have released the data and codes to facilitate other researchers. In this section, we mainly review the work related to our research, including the multimedia recommendation and graph convolutional network. 2.1 Multimedia Recommendation In the multimedia recommendation, items’ content information is incorporated into the CF-based schema to generate a high-quality recommender system [14, 15, 16, 17]. The fashions are based on the idea that the user preference on item contents can be modeled by their interaction historical records. For this purpose, Chen et al. [3] explored the blurs of the item- and component-level feedback to the user preference proﬁling and proposed an attentive collaborative ﬁltering (ACF) module. Leveraging the proposed model, they learned the user preference from two levels of item features. However, due to neglecting users’ effect during the feature extraction, the item features are unsuitable for user preference modeling. Wei et al. [4] suggested that the users’ tastes were aware of the different modalities’ content information and presented a novel model, dubbed Multimodal Graph Convolution Network. By using the graph convolutional operations, the method captures the modelspeciﬁc user preference and distills the item representations simultaneously. Although the learned user representation can be treated as user preference, they prefer to reﬂect the users’ speciﬁc characters on the item’s ﬁne-grained features instead of the user intents. Due to the limitation, some researchers work on reﬁning the item representation with the user characters. By introducing some user demographics from social platforms, Fig. 2: Schematic illustration of our proposed model. It contains intra-level aggregation, cross-level aggregation, and interaction prediction operations. In the different modalities, the item is represented by its distributions over the learned user intents, which are remarked with orange, blue, and green dashed. Cui et al. [12] designed a regularized dual-factor regression model to learn common attribute representations for users and items. With the social attributes, they integrated user characters into the item representations. Further, Fan et al. [13] replaced the demographics with users’ social network information and proposed a novel graph neural network framework for social recommendations, in which the social relation between users is combined with useritem interactions for features modeling. But, either the user proﬁles or the social information of users, is probably inaccessible, even irrelevant to the user preference. The studies tend to learn the representations of user intents according to the supervisory signal. However, they ignore the complicate relation between the user intents, which facilitates the intents modeling. In this work, we consider the correlation between the ﬁne-grained and coarse-grained intents as well as that between intents in the same level. As such, we organize the user intents associated with these two kinds of correlation as a hierarchical graph and utilize the graph convolutional operations to learn the representations of multi-level user intents. 2.2 GCNs based Recommendation Empowered by the success of GCNs in the node representing, it is widely used by several tasks with nonEuclidean data, like computer vision [18, 19], natural language processing [20, 21], and recommender system [22, 23, 24]. The core of the GCNs is collecting the signals of the node’s neighbors and encoding the local structure information into the node representation, which enhances its representativeness. For the personalized recommendation, Wang et al. [25] modeled the high-order connectivity in the user-item graph and explicitly injected the corresponding collaborative ﬁltering signals into the user and item representation. Besides encoding the local structural information into the representation, GCNs operations can be used to learn hierarchical representations of graphs. For instance, Ying et al. [26] presented a differentiable graph pooling model to generate the hierarchical representations of graphs. Towards the goal, the module recursively leverages GCNs operations to learn a matrix, which is used to project the nodes into a coarser space. Similarly, Yu et al. [27] designed a general Layout-Graph Reasoning (LGR) layer to mine contextual graph semantic information from speciﬁc nodes to abstraction nodes. Although we follow the same idea to construct a hierarchical graph, we design two kinds of operations: (1) intra-level aggregation that distills the informative signal from item features to model the ﬁne-grained intents; and (2) inter-level aggregation which explicitly represents the supernodes corresponding to coarse-grained intents in the higher level by summarizing the low-level nodes according to their afﬁnities. 3.1 Preliminary Given a set U of N users, a set V of M items, and their interaction matrix R ∈ R, we use Rto represent the interaction between user u∈ U and item v∈ V. Thereinto, R= 1 denotes that uhas interacted with v; otherwise, R= 0. Moreover, the multimedia content information of each item is provided, involving the visual, acoustic, and textual modalities. And, we use annotations v, a, and t to indicate them, respectively. For notational convenience, without particular clariﬁcation, we discard these modality indicators to elaborate on each component in the single modality and do the same operations on other modalities. As mentioned before, our research objective is to learn the multi-level user intents and accordingly optimize the representations of users and items. Following the assumption that users with similar historical behaviors prefer the similar items [28], we propose to exploit the correlation between items co-interacted by the same users to distill their ﬁne-grained intents towards the items. Formally, it could be formulated as, where xand zdenote the representation vectors of item vand the distilled information. In addition, xand N represent the item which is co-interacted with i and the number of the co-interacted items of item i. Further, to learn the representation of each coarser-grained user intent, we iteratively aggregate the ﬁner-grained ones according to their afﬁnities to the coarser-grained. For this purpose, we construct a co-interacted item graph, G = {X, A}, to organize the users’ behavior and items’ content information together. Whereinto, x∈ X is initialized with the extracted features in different modalities (e.g., visual, acoustic, and textual features). A ∈ Rreﬂects the connections between the nodes according to their co-interacted records.The presence of the connection between each node and its neighbor signiﬁes that they are historically interacted by several user in common, like the videos co-watched by same audiences and products copurchased by same customers. Speciﬁcally, we collect the item-item pairs according to their historically interactions and treat them as the neighbors. The adjacent matrix A is formally deﬁned as where N (v) represents the set of neighbors of v. 3.2 Model Framework In this section, we detail our proposed framework. As shown in Figure 2, the framework contains three components, including the intra-level aggregation, interlevel aggregation, and interaction prediction. By iteratively performing the intra-level and inter-level aggregation operations, we capture information on the user intent from the co-interacted item graph and explicitly model their hierarchical structure from the ﬁne-grained to the coarse-grained. We then represent the items based on them and learn the users’ characters to predict the interactions between users and items. 3.2.1 Intra-level Aggregation Different from the prior works which assume the items are independent with each other, we explicitly leverage the correlation between items to distill the informative signal from features. Thus, we reorganize the items as a cointeracted graph and cast their correlations to the nodes’ local structure in this graph. Taking the advantage of the message aggregation mechanism in GCNs [29, 30], the local structure information (i.e. user behaviors) of each node is encoded and used to reﬁne its features (i.e content information), which distills the intents of users who have clicked or watched it. Therefore, each node could be represented with the vector conveying the user intents towards the item. Accordingly, we reformulate Eq.1 as the intra-level aggregation function at the l-th layer and provide the matrix form as, where f (·) means the intra-level aggregation function. X is set as X at initial iteration and Adenotes the adjacent matrix of co-interacted graph A. With the aggregation operations, we learn the topological structure of each node’s neighborhood as well as the distribution of node features in the neighborhood [31]. The distribution of the neighborhood features can provide a strong signal for distilling the information on the user intents [32]. More speciﬁcally, in our proposed framework, the aggregation function could be ﬂexibly implemented using different methods, such as GraphSAGE [31] and GCN [33]. Here, we utilize a light variant of GCNs, which removes nonlinearities and collapses weight in the aggregation phase, formally where D is the degree matrix of adjacent matrix A. 3.2.2 Inter-level Aggregation Intuitively, the coarse-grained user intents could be summarized from the ﬁne-grained ones. Therefore, we develop a inter-level aggregation operation to explicitly model the relationship between the low-level (i.e. ﬁnegrained) and the high-level (i.e. coarse-grained) user intents. In particular, we establish several supernodes, which can be regarded as coarser-grained user intents, to construct (l+ 1)-th level graph. Here, we deﬁne and randomly initialize the supernodes as, X= [ x, · · · , x, · · · , x where xand Kdenote the representation of the kth supernode and the number of the supernodes in (l+1)-th layer, respectively. In order to discover the relationship of intent pairs across the levels, we perform the inner product for each node and supernode pair to measure its afﬁnity. It is formulated as: where zis the vector representing the i-th node in lowerlevel graph and eis the score of afﬁnity between the i-th node and k-th supernode. Following this, the weights of each node assigned to supernodes are calculated by normalizing the scores, where γis the assignment weight of the i-th node towards k-th established supernode. Through such an assignment operation, our model constitutes the relationship across the lower level and higher level graphs. Beyond the relationship between the ﬁner-grained and coarser-grained intent pair, the adjacent matrix of the highlevel graph can be computed with the low-level nodes’ adjacent matrix and assignment matrix. It is formulated as, where Γ∈ Rdenotes the matrix of assignment weights. Whereinto, its i-th row γis the assignment weight vector of i-th node over the supernodes. Considering that using graphs has the limitation of providing exclusive sets in the space partitioning [34], we adopt a softassign strategy to alleviate the unnecessary overlapping. Thus, we introduce a cross-entropy loss to make each node’s assignment vector close to the one-hot vector, which facilitates to disentangle the user intents in the same level. It is deﬁned as, Moreover, we introduce an independence loss to keep user intents independent, so as to enhance the robustness of the proposed model, formally, where I∈ Ris the identity matrix in the l-th level. 3.2.3 Interaction Prediction By iteratively conducting L intra-level and inter-level aggregation operations, our model explicitly models the user intents as well as their hierarchical structure. We then calculate the distributions over the user intents at each level for each item, as where ˆxrepresents the item according to the user intents at the l-th layer. Considering the vectors are based on the user intents in different levels, we employ the layer-aggregation mechanism [25, 35] to integrate these vectors as the item’s representation, formally, where the symbol || means the concatenation operation. Then, we deﬁne and randomly initialize the trainable representations of users, formally, u= [ u, . . . , u, · · · , u, · · · , u where ucould be used to measure the user’s characteristics on the corresponding intent. These representation vectors are learned by the back-propagation operation during the training phase. Following the idea that users have different preferences in different modalities [4], we combine the multi-modal representations in different modalities as a whole vector, formally, where uand vdenote the user’s and item’s multi-modal representations, respectively. 3.3 Optimization To learn the parameter of the proposed model, we concatenate the ID embedding vectors and representation vectors for the users and items, as whereˆuandˆvdenote the ID embedding vectors of the user and item, respectively. Then, Bayesian Personalized Ranking (BPR) [28] is adopted to perform the pair-wise ranking for the recommendation. As such, we construct a triplet of one user u, one observed item v, and one unobserved item v, formally as, where T is a triplet set for training. The BPR loss function can be deﬁned as, where φ(·) is the sigmoid function. independence loss, we reach the objective function as follows, L = λL+ λL+ L+ λ||θ|| where λ, λ, λand θ represent the regularization weights and the parameters of the model, respectively. 3.4 Discussions 3.4.1 Model Size. Although L layers are stacked to model the multilevel graph structure, there are few additional parameters introduced into our framework. Speciﬁcally, in each aggregation layer, we simplify the aggregating operations, in which the nonlinear function and weight matrices are discarded. Therefore, it does not introduce any external parameter for computing. In the inter-level aggregation layer, to represent the user intents in various layers, we establish several supernodes and represent them with the vectors. In particular, we initialize Kvectors of D dimensions in the l-th layer, where D and Kare much less than the number of items. In addition, considering that L is usually a number smaller than 5, the cost of these external parameters is negligible. To summarize, our algorithm uses very few additional model parameters to learn the multilevel user intents and predict the interactions between users and items. 3.4.2 Complexity Analysis. According to the architecture of our proposed framework, we elaborate on each component to analyze the time cost. In particular, beneﬁting from the light variant of GCN, the l-th intra-level aggregation layer has computational complexity O(K× K× D) without any other cost, such as the cost from the element-wise nonlinear operation and matrix multiplication. In terms of the l-th inter-level aggregation layer, its computational complexity is O(K×K×D). Even though there are multiple layers in our framework, the computational complexity closes to O(M × M × D), sinceP Kis much smaller than M. In addition, only the inner product is conducted in the prediction layer, for which the time cost of the whole training epoch is O(|T | × D). Whereinto, |T | is the number of triplets in the training set. Therefore, the overall training complexity of our proposed model is closed to O(M × M × D + |T | × D), while the complexity of MMGCN is O(|T | × D + (M + N ) × (M + N) × D). TABLE 1: Summary of the datasets. The dimensions of visual, acoustic, and textual modalities are denoted by V, A, and T, respectively. To evaluate the proposed model, we conducted experiments on the three public datasets to answer the following research questions: with state-of-the-art multimedia recommendation models and other user intent modeling based methods? intents, number of the modalities, cross-entropy loss and independence loss) affect the performance of our model? user intents? Before answering the above three questions, we ﬁrst describe the datasets, evaluation metrics, baselines, and parameter settings in the experiments. 4.1 Experiments Settings 4.1.1 Dataset To evaluate our model, extensive experiments are conducted on three publicly accessible datasets, namely Movielens, Tiktok, and Kwai, which are designed for the micro-video personalized recommendation. The statistics of datasets are summarized in Table 1. Movielens datasethas been extended by collecting the videos’ trailers, titles, and descriptions [4]. In order to learn the visual and acoustic features, the keyframes and audio track are extracted from the trailers. With the pre-trained models (e.g. ResNet [5], VGGish [36], and Sentance2Vector [37]) , the visual, acoustic, and textual features are extracted from the frames, audio tracks, and descriptions, respectively. popular micro-video sharing platform. It consists of users, micro-videos, and their historical interactions. Besides, the multi-modal features are also extracted from the microvideos, involving visual, acoustic, and textual modalities. released a large-scale micro-video dataset for user behavior prediction. Besides the users and micro-videos information, the dataset contains the users behaviors towards the micro-video (i.e. clicks, likes, and follows) associated with the timestamps. Within this dataset, we collected some interaction records during a certain period for our experiments. Different from the above datasets, the audio and textural information is missing in Kwai. For each dataset, we randomly split the historical interaction records into the training set, validation set, and testing set with the ratio 8 : 1 : 1. Whereinto, the validation set and testing set are respectively used to tune the hyper-parameters and evaluate the performance in the experiments. With regard to the training set, we performed negative sampling to create the training triples and constructed the co-interacted item graph for our proposed model. Speciﬁcally, we captured the video pairs which were co-watched by at least 5 different users. We then treated obtained videos as the nodes of the graph and connected them corresponding to the co-watched video pairs. 4.1.2 Evaluation Metrics For each user in the validation set and the testing one, we regarded the micro-videos which the user had no interaction with as the negative samples. Using the trained model, we scored the interactions between the user and micro-video pairs and ranked them in the descending order. In addition to the precision@K (P@K for short) and recall@K (R@K for short), we also adopted Normalized Discounted Cumulative Gain (NDCG@K for short) to evaluate the effectiveness of top-K recommendation. By default, we set K = 10 and reported the average values of the above metrics for all users during the testing phase. 4.1.3 Baselines To evaluate our proposed model, we compared it with several state-of-the-art baselines. We brieﬂy divided these baselines into three groups: 1) deep-learning-based methods (i.e., VBPR, DUIF, CB2CF), 2) user-intent-agnostic methods (i.e., NGCF, MMGCN), and 3) user-intent-based methods (i.e., DisenGCN, MacridVAE). recommendation. It incorporates the content information into the collaborative ﬁltering framework. To ensure fairness, we fused the multi-modal features as the side information and fed them into the model, which could infer user preference towards the learned features. method learns the user reference to the item’s features, which are concatenated and reﬁned by the trainable deep learning model. to represent the rich content information. And, a mean squared error (MSE) serves as a bridge from items’ feature representations into their collaborative embeddings. method propagates the users and items representations to model the high-order connectivity in the graph. It leverages the collaborative ﬁltering signals to explicitly encode user information into item representations. based framework designed for the multimedia recommendation, in which the model-speciﬁc user preference can be learned. Performing the graph convolutional operations on the user-item graph in each modality, it can reﬁne the items’ representations with the learned users’ model-speciﬁc preferences. which the nodes’ representations are disentangled into independent factors. To model the user intents hidden in the representations of users and items, we extended it into the personalized recommendation task. As such, we constructed the user-item graph and disentangled the nodes’ representations into several independent channels, which can be viewed as the representations of user intents. Based on this model, the user intents are captured and incorporated into the item representations. intents into the macro- and micro- level. Thus, some concepts are learned to represent the macro-level intents, each dimension of which is viewed as the users’ microlevel intent. To model the items with learned user intents, it forces each item associated with a unique concept and represents it with the corresponding one-hot vector. 4.1.4 Parameter Settings We implemented our model with the help of Pytorchand torch-geometric package. More speciﬁcally, we initialized the model parameter with the Xavier [42] initializer and took the Adam [43] as the optimizer. And, to ensure the fair comparison, we ﬁxed the dimension of the embedding vector to 64 for all models. In terms of the hyperparameters, we used the grid search: the learning rate is tuned in {0.0001, 0.001, 0.01, 0.1, 1} and regularization weight is searched in {0.00001, 0.0001, 0.001, 0.01, 0.1}. Besides, we employed the early stopping strategy, which stops the training if recall@10 on the validation data does not increase for 20 successive epochs. For the baselines, we did the same options and followed the designs in their articles to achieve the best performance. 4.2 Performance Comparison (RQ1) To evaluate our proposed model, we report the empirical results of all methods and the improvements, which are calculated between our proposed method and the strongest baselines highlighted with underline, in Table 2. Analyzing this table from top to bottom, we observed: especially DUIF purely based on the features, have poor performance w.r.t. precision, recall, and NDCG. It demonstrates that the features extracted by the pre-trained model are suboptimal in the multimedia recommendation. Although VBPR and CB2CF achieve the comparable results on Kwai, we suggest that it may beneﬁt from the dense interactions per user. into item representations outperform the user-agnostic based model. From this comparison, it indicates that the user characteristics enhance the item representations. With users’ preferences, the item representations are probably reﬁned to reﬂect the features users concerned. modeling based approaches, including DisenGCN, MarcridVAE, and our proposed model, achieve better results. It demonstrates that representing users and items TABLE 3: Performance of Proposed Model w.r.t. the structure of user intents over three datasets. (Each constant in the ﬁrst column denotes the number of user intents in each layer, and the number of constants is the number of layers.) with user intents boosts the personal recommendation. This is because the user intents reﬂect the users’ motivations and tastes. Whereas, user preference learned by NGCF and MMGCN is used to ﬁlter out the nonessential features in the recommendation. performance of DisenGCN and MarcridVAE. We believe that the gap is caused by the different levels of user intents. Compared with the single-level user intents, the two-level intents learned by MarcridVAE could be treated as an extension, which further disentangles the user intents into the speciﬁc ones and general ones. outperforms the state-of-the-art baselines. In terms of the NDCG@10, it improves over the strongest baselines by 9.35%, 12.10%, and 10.7%. We suggest that the improvement mainly comes from the multi-level user intents as well as the user and item representations based on them. Different from DisenGCN, our model constructs the hierarchical structure to model the multilevel user intents and explicitly learns the user intents representation. Besides, our model uses the continuous vector to characterize both users and items, which alleviates the restriction of MacridVAE on the item representation. Moreover, observing the results of topK recommendation lists where K is in range of {1, 5, 10}, they show that our proposed model is robust to the various recommendation scenes and achieves the state-ofthe-art performance on three datasets. 4.3 Ablation Study (RQ2) 4.3.1 Effects of User intents’ Structure In this work, we focus on modeling user intents as well as their hierarchical structure to optimize the user and item representations. To investigate the effect of structure, we conducted experiments on different user intents structures. In particular, we searched the number of layers in the range of {1, 2, 3, 4}. Also, we considered the comparison between the structures, which contain the same layers but different numbers of user intents in each layer. From the summaries in Table 3, we capture the following observations: and the number of user intents (i.e. supernodes) in each layer affect the performance of our proposed model. For the range from one to three, increasing the layer could boost the performance on the three datasets in most cases. Although the absolute improvements are not signiﬁcant, we ﬁnd that the three-layer variants TABLE 4: Performance of HUIGN with different modalities on Movielens and Tiktok datasets. The visual, acoustic, textual, and multiple denote the visual, acoustic, textual, and multiple modality representations, respectively. surpass the single-layer ones by 2.5% and 7.14% w.r.t. recall@10 on Tiktok and Kwai datasets with the negligible memory and time complexity according to our analysis in Section 3.4. Besides, we construct the hierarchical structure to correspond to the multi-level user intents, whose effectiveness is illustrated in Section 4.4. It is consistent with our suggestion regarding the effect of multi-level user intents in personalized recommendation. the user intents consisted of four levels achieves the suboptimal results in terms of the three metrics. We believe that there are two main reasons. One is that the four-layer structure we designed may disagree with the real-world user intents when people watch the video; and the other is that such many levels of user intents could result in overﬁtting during the training phase. layers, we ﬁnd that the results of the user intents with {32, 8, 4} structure are better than that of the other one. Although more parameters are introduced into the model, they do not improve the performance. Jointly considering the case of four-layer structure, it indicates that our model does not beneﬁt from the graph convolutional operations on the deeper or/and wider graph. single-level variant achieves the comparable performance comparing with the strongest baselines. We attribute the improvement to re-express the item with the learned user intents. It also veriﬁes that our proposed method could capture the informative signal by incorporating the item content with the user historical behaviors. 4.3.2 Effects of Modalities In the multimedia recommendation, multi-modal contents are the information sources of the user intents. As such, it is necessary to conduct experiments to investigate these critical variables on Movielens and Tiktok, which consists Fig. 3: Performance in terms of Precision@10, Recall@10 and NDCG@10 w.r.t. different co-interacted frequencies on Movielens and Kwai. (The histograms indicate the numbers of edges in different frequencies.) of the visual, acoustic, and textural modalities features. In Table 4, we exhibit the results in different modalities in terms of three metrics to compare with the results of multiple modalities. From this table, we have the following ﬁndings: best performance compared to models that use only the acoustic or textual information. This observation is consistent with the ﬁnding in [4]. It reveals that the most effective content information comes from the visual modality in micro-video recommendation. Besides the fact that the visual contents convey more information than the audio and textual description, we believe that the user pays more attention to the visual cues when she/he browsers the micro-video. In particular, on Movielens, the results in the visual modality exceed the others by a signiﬁcant margin. We attribute the improvements to the high-dimension feature vectors, which facilitates the user intent modeling. modality features outperforms the one with multimodal features. Such a phenomenon may be caused by our simplistic approach in combining the multi-modal representations; we leave the investigation of relations among multi-modal user intents for the future work. Even so, compared with the baselines, our proposed model also achieves the best performance, which demonstrates the effectiveness of representations based on user intents. 4.3.3 Effects of Cross-entropy and Independence loss In our model, we introduced the cross-entropy loss and independence loss. To demonstrate the effectiveness of these loss functions (i.e. Eq. 9 and Eq. 10), we performed the experiments on different co-interacted items graphs. In particular, according to the different co-interacted frequencies, we built several graphs with different numbers of edges. On Movielens and Kwai, we constructed ﬁve graphs where the co-interacted frequencies are larger than 3, 4, 5, 6, and 7, respectively. The results in terms of three matrices on Movienlens and Kwai are illustrated in Figure 3. Whereinto, HUIGN-I, HUIGN-C, and HUIGN-C-I denote the variants without independence loss, cross-entropy loss, and both of them, respectively. Besides, in the ﬁgure, the histograms represent the numbers of edges in different graphs. From the ﬁgure, we observed that: to ﬁnd that their curves w.r.t. metrics in the ﬁgures follow similar trends in different cases. Except the cross-entropy loss prevents overﬁtting in the training phase, we believe that the loss beneﬁts the user intents representations. With this loss function, not only the features reﬂecting the primary user intents are enhanced but also the others are faded during the assignment, which reﬁnes the information used to learn the user intents. Thus, the signals of nodes are integrated into the supernodes which are closest to the intents of users who consumed them. variants without the independence loss are susceptible to the number of edges in the item graph. As shown in the ﬁgures, the curves representing the HUIGN-I are more smooth than those of other variants. Hence, we suggest that the independence loss makes the method more robust. The attribute comes from that the independence loss could be used to disentangle the user intents at the Fig. 4: Visualization of items’ representations using the tSNE dimensionality reduction algorithm in 3D. Whereinto, X, Y, and Z annotate the three axes in the 3-dimensional coordinate, respectively; red and green represent the mapping operations. same level, which prevents each intent from the noise caused by other intents. which shows the performance of HUIGN is better than the others’. Combined with the above observations, it demonstrates that the advantages of these two loss functions are mutually strengthened. Speciﬁcally, with the independence loss, the users’ intents can be disentangled into several independent ones, each of which reﬂects one factor of users’ motivations. Based on these intents, each node’s assignment vector are easily closing to the one-hot vector, which enhances the user intents representations and beneﬁts the independence. 4.4 Visualization (RQ3) In this section, we aim to provide the semantic description for the user intents learned in our proposed model. For this purpose, we used the t-Distributed Stochastic Neighbor Embedding (t-SNE) in 3-dimension to visualize the user intents and introduced some descriptions for them. In particular, we performed the t-SNE algorithm on items’ characters in the textual modality and leverage the videos’ descriptions to offer the semantic information, as illustrated in Figure 4. And, we have the following ﬁndings: we ﬁnd that most items are clustered into several regions. The observation is consistent with our idea that representing the items according to their characters on several user intents. Further, when we respectively project items to three axes in the coordinate, we could ﬁnd that they are projected into several separate regions in each axis. More speciﬁcally, the distribution on the Y-axis is concentrated in 4 discrete regions and the distribution on the X-axis can be grouped into 8 regions, which follows our designs of multi-level user intents. Although, on the Z-axis, the points do not obviously fall into 32 regions like our setting, we suggest that there is no clear margin between the user intents at the low level. Therefore, we represent each cluster with the items belonging to it. we displayed the videos’ descriptions and highlighted the keywords in the ﬁgure to semantically exhibit three distinct clusters. From each clusters’ keywords, we could infer the main themes of items in this cluster. Taking Cluster1 as an example, we observe that the most frequent words are ‘comedy‘ and ‘love‘. It reveals that some users prefer these videos for the comedic elements and love stories in videos. And, for the other displayed clusters, users’ interests are towards ‘music‘ and ‘documentary‘. By jointly analyzing with the above ﬁnding, we are able to provide the semantic representation for each user intent with the keywords in videos’ descriptions. In this work, we proposed to model user intents as well as their structure for user and item representation learning in the multimedia recommender system. For this purpose, we developed a new framework based on graph convolutional networks, which explicitly models a hierarchical graph to represent the multi-level user intents. By performing the information aggregation operations on the constructed cointeracted item graph, the model learns the user intent implied in the historical interaction records and builds the relationship between the ﬁner-grained and coarser-grained user intents. Ultimately, the user and item characteristics based on the user intents provide us with the explainable and high-quality recommendations. To the best of my knowledge, this is the ﬁrst attempt to explicitly model the user intent without any external information, such as user social information and demographics, in the recommender system. In future, there are still many extensions that we need to continue to explore. For instance, we would further investigate the relationship of user intents among the multiple modalities. Although the user preference is dependent on different modalities, we believe the consistency probably emerges at the general level. Besides, we extend our work by introducing external users’ social signals and items’ knowledge information, which aims to learn an explainable connection among the user intents, social relations, and item knowledge.