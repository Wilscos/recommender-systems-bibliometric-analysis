Recommender systems, a pivotal tool to alleviate the information overload problem, aim to predict user’s preferred items from millions of candidates by analyzing observed user-item relations. As for tackling the sparsity and cold start problems encountered by recommender systems, uncovering hidden (indirect) user-item relations by employing side information and knowledge to enrich observed information for the recommendation has been proven promising recently; and its performance is largely determined by the scalability of recommendation models in the face of the high complexity and large scale of side information and knowledge. Making great strides towards eﬃciently utilizing complex and large-scale data, research into graph embedding techniques is a major topic. Equipping recommender systems with graph embedding techniques contributes to outperforming the conventional recommendation implementing directly based on graph topology analysis and has been widely studied these years. This article systematically retrospects graph embedding-based recommendation from embedding techniques for bipartite graphs, general graphs, and knowledge graphs, and proposes a general design pipeline of that. In addition, comparing several representative graph embedding-based recommendation models with the most common-used conventional recommendation models, on simulations, manifests that the conventional models overall outperform the graph embedding-based ones in predicting implicit user-item interactions, revealing the relative weakness of graph embedding-based recommendation in these tasks. To foster future research, this article proposes constructive suggestions on making a trade-oﬀ between graph embedding-based recommendation and the conventional recommendation in diﬀerent tasks as well as some open questions. Keywords: Recommender Systems; Graph Embedding; Machine Learning; Knowledge Graphs; Graph Neural Networks 2.2 Graph embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2.1 Deﬁnitions and concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2.2 Recommendation based on graph embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.3 Optimization algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3 A general design pipeline of graph embedding-based recommendation . . . . . . . . . . . . . . . . . . 14 1. Introduction the traﬃc ﬂow big data helps to quantify the potential infectious crowds during the pandemic [3], the scientiﬁc research big data can facilitate academic-industry collaboration [4], and the multimedia social big data usually entertains consumers [5]. But meanwhile, the high-volume, high-velocity, and high-variety, also called the three “V” features [6], of big data bring problems. Information overload [7, 8] is a case in point, referring to the excess of big data available to a person aiming to make a decision, say, which articles are relevant to a researcher’s focus, which products meet a consumer’s demand, and which movies pique an audience’s interest; thus discounts the information retrieval [9] eﬃciency. Counterbalancing these pros and cons of big data to maximize its beneﬁts requires the development of big data mining techniques [10], among which recommender systems [11–13] have turned out to be a pivotal tool to alleviate the information overload problem, aiming to predict a user’s (e.g., researcher, 3.1 Recommendation with static user-item interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.1.1 Models based on matrix factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.1.2 Models based on Bayesian analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.1.3 Models based on deep learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.1.4 Other models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.2 Recommendation with temporal user-item interactions . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.2.1 Models based on matrix factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.2.2 Models based on Markov processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.1 Three categories of techniques for general graph embedding . . . . . . . . . . . . . . . . . . . . . . . 26 4.1.1 Techniques based on translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 4.1.2 Techniques based on meta path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.1.3 Techniques based on deep learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.2 Recommendation involving side information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 4.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.1 Three challenges of knowledge graph embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.2 Recommendation involving knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6.1 Experiment setups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.1.1 Data sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.1.2 Evaluation metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.1.3 Evaluated models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 6.2 Results and analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 6.2.1 Predicting explicit user-item interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 6.2.2 Predicting implicit user-item interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Does big data [1, 2] beneﬁt people’s lives? On its face, the question seems absurd. It is true that, for example, consumer, and audience) preferred items (e.g., articles, products, and movies) from millions of candidates. Apart from this, recommender systems have seen commercial practices ranging from startup-investors matching [14] to energy eﬃciency in buildings [15]. [18–21] encountered by recommendation models, the core component of recommender systems. The rationale for recommendation models lies in the accurate inference for user’s preferences for items, the prerequisite for well recommendation performance, by analyzing observed user-item relations, among which user-item interactions (Sec. 2.1.1 gives details) are primary resources. However, user-item interactions are usually sparse as a result of only a few of the total number of items that were interacted by a user, called the sparsity problem. When coming to a new user, that no interaction between the user and items yet has been observed leads to the cold start problem, and the same is true of a new item analogically. Consequently, inadequate user-item interactions out of the sparsity and cold start problems weaken the accuracy of inference for user preference by recommendation models; thus defend recommendation performance. As for tackling the sparsity and cold start problems, employing side information [22, 23] and knowledge [24–26] (Sec. 2.1.1 gives details) as a supplement to user-item interactions in order to uncover hidden (indirect) user-item relations enriching observed information for the recommendation has been proven promising recently. performance, the discussion about whether graph embedding-based recommendation (Sec. 2.2.1 gives details) outperforms the conventional recommendation implementing based on graph topology analysis (Sec. 2.1.4 gives details) is an ongoing controversy. With regard to the scalability [16, 27] performing the recommendation per second for millions of users and items when data is highly complex and large-scale as a result of the three “V” features of side information and knowledge inherited from big data, graph embedding-based recommendation outperforms conventional recommendation determined by their diﬀerent rationales: after organizing information into graph representations (Sec. 2.1.2 gives details), conventional recommendation runs by analyzing the graph topological characteristics such as users’ co-interactions with common items [28] or global topological diﬀusion [29, 30]. In contrast, graph embedding-based recommendation implements by using the nodal embedding vectors preserving graph topological features once learned from the graph representations by embedding techniques [31] (Secs. 4.1 and 5.1 give retrospects). When employing side information and knowledge in diﬀerent recommender systems, graph embedding-based recommendation can directly reuse the learned nodal embedding vectors rather than repeating the analysis of graph topological characteristics as conventional recommendation does; thus substantially improves the scalability of recommendation models. Besides, the storability of embedding vectors makes them support downstream machine learning tasks [32] requiring feature vectors of data instances as inputs, such as classiﬁcation [33–39], link prediction [40–43], and clustering [44]; thus enables graph embedding-based recommendation to outperform conventional recommendation in terms of model extensibility. to a user, graph embedding-based recommendation substantially underperforms conventional recommendation as a result of its general adoption of machine learning methodology [46], almost a black box, whose idea lies on the input-output data ﬁtting for underlying pattern discovery by numerical or analytic optimization methods [47], whereas conventional recommendation can directly realize the explainability through resolving the graph topological characteristics pertaining to a user-item node pair. But some recent studies argued that by employing knowledge in the recommendation [45, 48–50] (Sec. 5.2 gives details), by neural network interpretability [51], as well as by causal learning (causal inference) [52–58] to reason and understand user preference, the explainability of graph embedding-based recommendation can also be indirectly realized. In addition, controversies over graph embeddingbased recommendation and conventional recommendation are also embodied in recommendation accuracy. Although by employing side information and knowledge, graph embedding-based recommendation can achieve distinctive improvement in recommendation accuracy beyond conventional recommendation [59–62], while it seemly still reveals the relative weakness in some recommendation tasks for predicting implicit user-item interactions compared with conventional recommendation, proved in Sec. 6 on simulations. Similar results were unraveled by Dacrema et al. [63] too. recommendation will lead to longstanding discussions on these controversies in the future, involving expanded perspectives from accuracy, scalability, extensibility, and explainability, as well as participated by interdisciplinary researchers ranging from mathematicians to data scientists. Developing both graph embedding-based recommendation and conventional recommendation is not contradictory, for the methods of analyzing graph topological characteristics behind conventional recommendation can inspire graph embedding-based recommendation in the Developing recommender systems requires surmounting the sparsity problem [16, 17] and cold start problem Concerning the ability in eﬃciently employing side information and knowledge to promote recommendation Nevertheless, as for model explainability (or interpretability) [45]: why did models return such recommendations The current lack of uniﬁed evaluation criterion on graph embedding-based recommendation and conventional utilization of such as subgraphs [64], motifs [65–67], and neighborhood [68–70] to promote embedding explainability [39] and recommendation performance. Meanwhile, graph embedding-based recommendation has pioneered novel recommendation scenarios including conversational recommender system (CRS) [71] and news recommendation [72], providing more promising application prospects for conventional recommendation. It seems that developing both of them to complement each other could improve recommender systems larger than only focusing on one side. embedding-based recommendation [22, 25, 26, 73–78] generally lack a systematic structure and an in-depth description, defending the comprehensive communications with interdisciplinary researchers and the researchers of conventional recommendation. To bridge this gap, this article builds an all-around perspective on recommender systems involving both graph embedding-based and conventional methods, and proposes a general design pipeline of graph embedding-based recommendation; then systematically retrospects graph embedding-based recommendation from embedding techniques for bipartite graphs, general graphs, and knowledge graphs. In addition, this article also compares the strengths and weaknesses of representative graph embedding-based recommendation models with those of the most common-used conventional recommendation models, on simulations, in diﬀerent tasks, revealing that conventional recommendation models can outperform graph embedding-based recommendation models in predicting implicit user-item interactions. By analyzing these experimental results, this article provides constructive suggestions on making a trade-oﬀ between graph embedding-based recommendation and conventional recommendation, as well as proposes some open questions for future research. ing an all-around perspective on recommender systems and a general design pipeline of graph embedding-based recommendation. Secs. 3, 4, and 5 retrospect embedding techniques for bipartite graphs, general graphs, and knowledge graphs, respectively, as well as retrospect the graph embedding-based recommendation models, correspondingly. Tabs. 3 and A2 provide an overview of these models. Sec. 6 displays simulation results of representative graph embedding-based recommendation models and the most common-used conventional recommendation models in diﬀerent tasks and data scales, and proposes constructive trade-oﬀ suggestions and some open questions after analyzing the results. Finally, Sec. 7 puts forward prospects on graph embedding-based recommendation, ranging from current challenges to potential solutions. 2. Deﬁnitions of subjects and problems recommendation. To invite more participation of researchers, particularly that from interdisciplinary ﬁelds, into the discussion of these controversies, Sec. 2.1 devotes to an all-around perspective of recommender systems for newcomers, as well as illustrates the rationale behind conventional recommendation. Sec. 2.2 illustrates graph embedding techniques and the rationale for the recommendation based on these techniques, preparing for Secs. 3, 4, and 5. Sec. 2.3 proposes a general design pipeline of graph embedding-based recommendation. At the end of this section, the notations used in this article are uniﬁed in Tab. 4. 2.1. Recommender systems mation collected from recommender systems, aiming to predict unobserved user-item interactions. This section divides the observed information into three categories: user-item interactions, side information, and knowledge, according to their relative complexity. Before being employed in the recommendation, the three categories of information are usually organized to graph representations: bipartite graphs, general graphs, and knowledge graphs, correspondingly, as the bases of measuring the k-order proximity between user-item node (or vertex) pairs and also the foundation for predicting unobserved user-item interactions. To clarify the above process, Sec. 2.1.4 takes two common-used conventional recommendation models as examples for illustration. 2.1.1. Information event that a 24-year-old male student named Tom watched Iron Man on Netﬂix on January 28, 2021, and rated this movie with ﬁve points as an example. This event, also called observed information, can be structured as {Tom, male, 24, student, watched, Iron Man, 5, 2021-1-28, Netﬂix} for storage, in which user-item interactions, i.e., {Tom, watched, Iron Man, 5, 2021-1-28}, side information, i.e., {Tom, male, 24, student}, and knowledge, i.e., {Iron Man, Netﬂix} are involved. Tab. 1 brieﬂy compares the three categories of information. Unlike the maturity of reviews on conventional recommendation [11–13], newly published reviews on graph The rest of this article is organized as follows. Sec. 2 covers basic deﬁnitions of subjects and problems, build- Sec. 1 introduces some major controversies between graph embedding-based recommendation and conventional In general, the target of recommendation is to infer user’s preferences for items by employing observed infor- In engineering, observed information in recommender systems is usually stored in a tuple form. Considering the Table 1: A brief comparison between user-item interactions, side information, and knowledge. Employing side information and knowledge as a supplement to user-item interactions can promote recommendation accuracy [59–62], providing richer user-item relations to promote the accuracy of inference for user preference and can substantially alleviates the sparsity and cold start problems. and implicit ones divided according to whether the interactions explicitly carry the user’s aﬀection degree on items or not. Formally, explicit user-item interactions can be deﬁned as the user’s ratings on items, which quantify the user’s aﬀection degree on items based on the assumption that one rates higher on the items he prefers than those he shows indiﬀerence. Under this deﬁnition, user’s rating biases termed user biases [79–81] can be dug out from explicit user-item interactions; for example, the rating biases between two users that one is used to rate at least three points on items to which he is even indiﬀerent, while the other is extremely strict: no more than three points for any favorite item. Meanwhile, user biases can lead to item biases [79, 82]; for example, the biases of averaged ratings given by tolerant users and critical users on the same item. In this regard, taking into account for both user biases and item biases can remove a distorted view of user preference and item popularity [79, 80], promoting a high-quality recommendation. Despite the above advantages, explicit user-item interactions still have the following disadvantages in practice: (1) Accessible explicit user-item interactions are usually too sparse to assure suﬃcient information resources for the recommendation as a result of a user’s general habit of browsing, clicking, or watching but rarely rating on items when surﬁng online. (2) Explicit user-item interactions are even inaccessible in some recommendation scenarios, restricted by user privacy protection [83, 84]. Alternatively, implicit useritem interactions, formally deﬁned as a binary state using 1 to indicate the existence of the interaction (such as click, browse, or watch) between a user and an item, and 0 otherwise, occur more frequently and are easier to be accessed than explicit interactions due to the no requirement for user’s additional operations like giving ratings for implicit ones. However, implicit user-item interactions can’t directly carry user’s preferences for items; thus brings the so-called one-class problem [85], which has been tried to surmount by such as converting implicit user-item interactions to explicit ones [86, 87]. user’s preferences for items inferred from such interactions change over time in a long-term or short-term form and are generally divided to user’s long-term preferences and short-term preferences, respectively. Many factors can lead to the changes of user’s long-term preferences for items over time, such as changes in user’s personal hobbies like that one may prefer comedy movies in his childhood while ﬁnding science ﬁction movies more interesting after entering college, or special events like seasonal changes and holidays, or even the change of one’s family status. At the same time, user’s short-term preferences for items are usually aﬀected by one’s latest interactions with items. For example, one’s interest in comedy movies may decline after watching comedy movies excessively in a short period. Capturing both user’s long-term and short-term preferences for items can promote recommendation performance [88–90]. For that purpose, the sequential recommendation [91] aiming to mine underlying patterns in the changes of user’s short-term preferences based on deep learning [92] has become a tendency in recent years, as well as other common-used methods including matrix factorization and Markov processes (Sec. 3.2 gives details). Since not all existing recommendation models take into account for the changes of user’s preferences, this article deﬁnes terms temporal user-item interactions and static user-item interactions to distinguish the recommendation models considering preference changes or not, respectively. uncover abundant hidden (indirect) user-item relations to alleviate the cold start and sparsity problems in the recommendation. Back to the ﬁrst example in this section, the side information {Tom, male, 24, student} recording Tom’s gender, age, and occupation, i.e., the properties of Tom, can uncover (or saying establish) the indirect relations between Iron Man and the audiences who are close to Tom in personal properties. In practice, side information usually refers to user’s social information [93] and locations [94], or item’s proﬁles [95], labels [96, 97], and textual User-item interactions, the primary resources for the recommendation, generally contain two aspects: explicit In practice, user-item interactions are occurring continuously instead of only at a speciﬁc time or period; hence Side information [22, 23], generally deﬁned as the information of the properties of users and items, can content [98], to name a few. For instance, microblogging [99] is a primary resource for user’s social information, involving user’s tweets, relationships such as following or friends, and individual proﬁles. Microblogging information has two salient advantages: abundant and almost real-time [100]. In detail, abundant microblogging information carrying various relationships between users as well as their preferences expressed directly, which is far more diverse beyond user-item interactions, can promote the accuracy of inference for user preference by, for example, enhancing with one’s friends’ preferences as a supplement. Meanwhile, microblogging information is almost real-time due to user’s tweeting habits: one tends to share his daily feelings in microblogging every day, providing more opportunities to capture the user’s latest preferences than from user-item interactions, which do not necessarily occur every day. objects related to objective facts in the world. Among them, subjects and objects are termed as entities, the abstract or concrete things of ﬁction or reality with special types and attributes. The connections between entities, i.e., predicates, are termed as relations. Formally, knowledge can be deﬁned as the collection of entities with diﬀerent types and attributes and relations. Take the ﬁrst example in this section one step further, suppose the knowledge about the movie Iron Man excerpted from Wikipedia displays that Iron Man is a 2008 American superhero ﬁlm based on the Marvel Comics character of the same name. Produced by Marvel Studios and distributed by Paramount Pictures, it is the ﬁrst ﬁlm in the Marvel Cinematic Universe (MCU). Then, the movie Iron Man belonging to the class of American superhero ﬁlm can be identiﬁed as an entity with the attribute of the ﬁrst ﬁlm in the MCU. Moreover, the relation produced connecting entities Marvel Studios and Iron Man and the relation distributed connecting the entities Paramount Pictures and Iron Man can also be identiﬁed. Based on the knowledge represented by these entities and relations, hidden relations between Tom and other movies are established like that Tom may be inclined to the productions by Paramount Pictures, uncovering more about Tom’s preferences. In practice, category information organized by tree logic on the e-commerce platforms is also a common-used knowledge, enabling a better understanding of user’s multi-level preferences for items. regarded, this article still sustains an opposite argument: there are apparent distinctions between side information and knowledge, not satisfying the premise of unifying them into one category. First, the resources of side information and knowledge are diﬀerent. Side information, such as user’s social and location information, is usually actively requested from personal privacy information. As for item proﬁles, the descriptions of such as products from merchants and product usage experiences from customers are also requested. In contrast, since always existing in the real world, knowledge can be naturally perceived and be stored in such as texts on the Web. Back to the ﬁrst example in this section, the knowledge {Iron Man, Netﬂix}, an existing fact in the real world, records that Iron Man is produced by Netﬂix. Second, the complexity of side information and knowledge is distinctive, for most of the side information is about user’s and item’s properties, while knowledge is more versatile and it describes almost everything like multi-modal information [101] in the real world and at the same time grows rapidly, making itself far more complex in semantics and multiplicity compared with side information. Third, knowledge is re-usable. It is a prominent advantage that side information incapacitates due to the changes of user and item properties over time. All in all, the three distinctions between side information and knowledge make the techniques employing knowledge in the recommendation more challenging than those for side information as a result of the large-scale, multiplicity, and evolution features of knowledge beyond side information (Sec. 5.1 gives details). 2.1.2. Graph representations information, and knowledge according to their distinctive complexity. Before being employed in the recommendation, the three categories of information should be organized into machine-readable graph representations: bipartite graphs, general graphs, and knowledge graphs, correspondingly, for data mining. Tab. 2 brieﬂy compares the three graph representations. To unify the notations, this article uses G = (E, R, E, R) to denote a graph representation, where E is the node set, R is the edge set, E denotes the set of diﬀerent node types, and R denotes the set of diﬀerent edge types. Knowledge [24–26] is generally recorded in logically organized language containing subjects, predicates, and A subcategory of side information that being a supplement to user-item interactions as knowledge is generally Sec. 2.1.1 divides observed information for the recommendation into three categories: user-item interactions, side Table 2: A brief comparison between bipartite graphs, general graphs, and knowledge graphs. |E| and |R| are the number of diﬀerent node types and edge types, respectively. edges (i.e., |E| = 2 and |R| = 1) where edges exist only between nodes with diﬀerent types, is a common-used representation for user-item interactions. Speciﬁcally, a bipartite graph represents users and items with two types of nodes, adding edges between the user-item node pairs corresponding to the implicit user-item interactions in recommender systems; the edges can further be weighted by ratings if the interactions are explicit. Meanwhile, by representing users and items with the rows and columns of a matrix where its elements record the user-item interactions (i.e., 0/1 or ratings), a bipartite graph can also be directly converted to a matrix, which enables recommendation models to be built on algebra techniques, such as matrix factorization (Secs. 3.1.1 and 3.2.1 give details). Fig. 1 gives toy examples to illustrate how to represent explicit user-item interactions with a bipartite graph and a matrix, respectively. Figure 1: Representing user-item interactions with a bipartite graph and a matrix, respectively. In (a), the weighted bipartite graph represents the explicit user-item interactions from a movie recommender system, where, for example, the edge weighted 5 between Alice and Avengers 4: the ﬁnal battle represents the interaction {Alice, Avengers 4: the ﬁnal battle, 5}, meaning that the user Alice watched the movie Avengers 4 and rated it ﬁve points. The bipartite graph in (a) can further be converted into a weighted matrix as shown in (b), where 0 represents the absence of an edge corresponding to the unobserved user-item interaction. where |E| > 1 and/or |R| > 1. Under the deﬁnition, a bipartite graph can be seen as a subcategory of general graphs. Compared with bipartite graphs being constrained to the situation of two node types where no edge exists between nodes with the same type as a result of their grid structural features, general graphs are more ﬂexible in representing side information that could be both homogeneous and heterogeneous, such as using a homogeneous graph to represent user’s social information containing one node type (i.e., user) and one edge type (i.e., friend relationship), as well as using a heterogeneous graph to represent the enriched user’s social information appending the attributes of users and items, in which case a bipartite graph is incompatible to accommodate such diverse attribute nodes with diﬀerent types. Furthermore, by connecting these attribute nodes with user-item node pairs, general graphs can also be integrated with bipartite graphs, providing a method of the utilization of side information in the recommendation. Bipartite graphs, formally deﬁned as G= (E, R, E, R) containing two types of nodes and one type of To represent side information, general graphs are deﬁned in this article to unify homogeneous graphs = (E, R) [102] where |E| = 1 and |R| = 1 and heterogeneous graphs G= (E, R, E, R) [103, 104] Knowledge graphs [24] can convert knowledge into a machine-readable form by ﬁrstly organizing the entities (i.e., subjects and objects) and relations (i.e., predicates) extracted from knowledge into triplets (subject, predicate, object). Speciﬁcally, let E denote the set of subjects and objects, let R denote the set of predicates, knowledge can be represented with triplets S = (h, r, t), where h ∈ E is the head entity (i.e., subject) and t ∈ E is the tail entity (i.e., object), r ∈ R is a directed edge from h to t. Following the process illustrated in Fig. 2, a knowledge graph can be directly constructed with these triplets, and it can also be integrated with general graphs or bipartite graphs. Since the boundless scale and complex semantics of knowledge, a knowledge graph G contains extremely diverse node types and edge types such that |E| → ∞ and |R| → ∞, regarded as the most complex instance of general graphs. In order to suﬃciently and eﬃciently capture and preserve the complexity of knowledge graphs, research into multi-viewed graphs and multi-layered graphs as novel graph representations has been a tendency recently (Sec. 5.1 gives details). [106], and Alibaba [107], requires automatic knowledge harvesting [108–110] techniques. Publicly available data of knowledge graphs, including YAGO [111], WikiTaxonomy [112], DBpedia [113], Wikidata [114], and WebOfConcepts [115], to name a few, are promising resources for recommender systems. Figure 2: Representing knowledge with a knowledge graph. In (a), the subjects, predicates, and objects contained in the proﬁles as knowledge about three classic movies excerpted from Wikipedia are ﬁrstly identiﬁed, and then are organized into triples (subject, predicate, object). These triplets are used to construct the knowledge graph in (b), where the colors of pink, yellow, blue, orange, and green represent the node types of movie, movie production company, actor, actor’s birthplace, and character, respectively. In practice, constructing knowledge graphs for commercial applications, such as those in IBM [105], Amazon 2.1.3. K-order proximity of the proximity [116, 117] between user-item node pairs will perform, regarded as the implementation of the recommendation from the perspective of link prediction [41, 42], for the proximity can quantify the similarity of unconnected user-item node pairs corresponding to the likelihood of occurrence of unobserved user-item interactions in recommender systems. Intuitively, the greater proximity of an unconnected user-item node pair usually indicates a higher likelihood of occurrence of the corresponding unobserved user-item interaction, where the likelihood can be seen as a reﬂection of the user’s aﬀection degree on the item. by a random walker traveling through the graph. Speciﬁcally, the ﬁrst-order proximity is the local pairwise proximity of a node pair v Fig. 1(a) as an example, the ﬁrst-order proximity of the node pair Tom-Flipped is 2, and that of Tom-Avengers 4: the ﬁnal battle is zero due to the nonexistence of an edge between them. However, edges in a graph representation are usually in a small proportion as a result of the sparsity problem in recommender systems, in which case the ﬁrst-order proximity is insuﬃcient in some situations like that, for example, the proximity between two unconnected but intrinsically similar nodes will be measured as zero by the ﬁrst-order proximity. For example, the ﬁrst-order proximity of Tom-Bob in Fig. 1(a) is zero, while they may have similar movie preferences intuitively inferred from that they co-rated the movie Gone with the Wind with the same points of ﬁve. To make up for the ﬂaw of the ﬁrst-order proximity, the second-order proximity of a node pair v proximity between two neighborhoods of nodes v ﬁrst-order proximity between v Pearson coeﬃcient (PC) [119], and Jaccard index [120]. Apparently, the nonexistence of a node connected with both v with the Wind shown in Fig. 1(a). Analogously, the higher-order proximity between node pairs can also be measured, which has become an important research focus recently (Sec. 4.1.2 gives details). systems by employing side information and knowledge could be more comprehensible. Take Fig. 1(a) for example, given a new coming node representing the movie Avengers 2: age of ultron. At the beginning, it will be isolated in the bipartite, in which case the measurement of the proximity is infeasible due to the nonexistence of direct or indirect relation between the new node and the others, incapacitating the prediction of future interactions between Avengers 2: age of ultron and the users for the recommendation. Consequently, it leads to the cold start problem. By employing the knowledge represented with the knowledge graph in Fig. 2(b), which uncovers the relation between Avengers 2: age of ultron and Avengers 4: the ﬁnal battle, the indirect relations between the coldstart node and the user nodes can be established across the movie node Avengers 4: the ﬁnal battle, enabling the proximity measurement between them. Meanwhile, the rich relations as a supplement provided by knowledge and side information can make a graph denser by establishing more connections in the graph, substantially alleviating the sparse problem. Besides, the established edges can also help promote the measurement accuracy of user-item node proximity combined with those in more diverse k-orders. as the ratings on items given by users in matrix factorization-based method (Sec. 3.1.1 gives details), the Pearson coeﬃcient (PC) [119] between nodes in k-nearest neighbors-based method (Sec. 2.1.4 gives details), the allocated resources on items diﬀused from users in diﬀusion-based method (Sec. 2.1.4 gives details), and the probability of the occurrence of user-item interactions in the deep learning-based model (Sec. 3.1.3 gives details). On the other hand, the proximity can even be meaningless in such as translation-based method (Sec. 4.1.1 gives details). For the sake of generality, this article further deﬁnes the proximity as a metric quantifying the relative magnitude of the similarity between nodal pairs. Note that the proximity sometimes is also named similarity. 2.1.4. Methods and conventional models likelihood of occurrence of unobserved user-item interactions, collaborative ﬁltering [28, 121–124], diﬀusionbased [14, 30, 125, 126], and content-based [127–129] are three prevalent recommendation methods. One by one, given an unconnected user-item node pair, by analyzing the proximity between the item node and the user node’s neighbors having co-connected item nodes with it, the collaborative ﬁltering method measures the proximity of the user-item node pair based on the assumption that the user’s preferences for items might be relevant to those After organizing observed information for the recommendation into graph representations, the measurement In general, the proximity of a user-item node pair is the rank of the pair’s occurrence in a sequence generated is measured based on the proximity between Sand Sthat can be calculated by such as cosine index [118], and vindicates a zero second-order proximity of v− v, such as that of Avengers 4: the ﬁnal battle-Gone Under the deﬁnition of proximity, the mechanism of tackling the cold start and sparsity problems in recommender In practice, in diﬀerent recommendation methods the proximity usually has diﬀerent meanings in value, such In technique, for measuring the proximity of user-item node pairs on graph representations and predicting the of his neighbors in the real world. The diﬀusion-based method initiated a strategy for applying physic diﬀusion processes, such as heat spreading [29] and mass diﬀusion [30], to the recommendation. The content-based method aims to build user’s proﬁles, which will be matched with item’s attributes and descriptions for implementing the recommendation. common-used conventional recommendation models adopting the collaborative ﬁltering method and diﬀusion-based method, respectively. Illustrations of these models can clarify the rationale for analysis on graph topological characteristics behind the conventional recommendation. At the same time, the two models are also used as experimental benchmarks in Sec. 6. measures the second-order proximity between node Tom and other user nodes by Pearson coeﬃcient (PC) [119], which is a common-used metric as where R(i given by user i. Following that, the ﬁrst-order proximity between node Tom and node The Truman Show, i.e., the predicted rating ˆr where N (∗) contains the neighbors within the second-order hops from node Tom, and α = normalization factor. Speciﬁcally, in Fig. 1(a), Alice and Tom both rated Flipped, as well as Bob and Tom both rated Gone with the Wind. Calculated by Pearson coeﬃcient in Eq. (2.1), the second-order proximity between node Tom and node Alice is −0.9795, and that between node Tom and node Bob is 0.8593. Following the observed relations that Alice rated The Truman Show with 4 points and Bob rated The Truman Show with 1 point, the unobserved rating ˆr Eq. (2.2). from the perspective of mass diﬀusion, explores global topology diﬀusion implemented on implicit user-item interactions. As shown in Fig. 3, for generating recommendations to Tom, ProbS ﬁrstly allocates a unit of resources for all the movie nodes connected with node Tom, respectively. Then, in the second step, the resources on each movie node are equally distributed and diﬀused along edges from the movie node to its connected user nodes, where the cumulated resources received by node Alice and node Bob actually represent their proximity with node Tom. In the third step, the resources diﬀused to each user node are equally distributed and diﬀused back to the connected movie nodes, eventually being used to represent the proximity between node Tom and its connected and unconnected movie nodes. Based on the results, Tom’s preference for The Truman Show beyond that for Avengers 4: the ﬁnal battle can be predicted. Note that the second and third steps of ProbS are a two-step diﬀusion process, which can be iterated to multiple rounds for better recommendation accuracy. User-based collaborative ﬁltering (UBCF) [28] and probabilistic spreading algorithm (ProbS) [30] are two Take the bipartite graph in Fig. 1(a) as an example, for generating recommendations to Tom, UBCF ﬁrstly )∩ R(i)denotes the set of items rated by both users iand i, and rdenotes the averaged rating Unlike UBCF analyzing the graph topological characteristics of users’ co-interacted interactions on items, ProbS, Figure 3: Schematics of ProbS. The ﬁrst step is the allocation of resources. The second and third steps are a two-step resource diﬀusion process for an iteration. 2.2. Graph embedding diﬀerent recommendations, graph embedding-based recommendation implements on directly reusing nodal embedding vectors once learned from graph representations to represent user and item nodes. For that purpose, embedding techniques for bipartite graphs, general graphs, and knowledge graphs, among which most are based on machine learning methodology performed by optimization algorithms, have been widely studied in decades, meeting more challenges in capturing and preserving possible much of the original topological features of graph representations with increasing complexity. 2.2.1. Deﬁnitions and concepts methods for downstream tasks, including node classiﬁcation [33, 34], graph classiﬁcation [35–39], link prediction [40– 43], clustering [44] and stuﬀ. Speciﬁcally, as for building the feature vectors of data instances as inputs required by machine learning methods [133], typical representation learning techniques, such as the artiﬁcially given methods [134] implementing based on hand-engineering with expert knowledge and the bag-of-words methods [135], are usually constrained to Euclidean data. However, as illustrated in Secs. 2.1.1 and 2.1.2, most of the information in recommender systems have a graph representation, which is non-Euclidean with complex and diverse hidden relation (or connectivity) patterns. For that gap, graph embedding has recently become a general solution to convert non-Euclidean graph representations into a low-dimensional Euclidean space, using embedding vectors, also called embeddings, of nodes, edges, subgraphs, or whole graphs to preserve and reconstruct intrinsic topological features of graph representations. and k (k  n) is the dimension of the Euclidean space. Through the mapping, the embedding, a k-dimensional vector, of an arbitrary node v between two nodes v two embeddings to R by such as the dot-product [136]. Intuitively, a greater F(Φ(v a larger possibility of the existence of an edge (v adding possible edges between node pairs. A well-performed graph embedding is supposed to ﬁnd out a mapping Φ : G → R possibly by operating F(Φ(v Diﬀerent from the conventional recommendation repeating the analysis on graph topological characteristics for Graph embedding [31, 130–132] bridges the gap of the utilization of non-Euclidean data in machine learning Formally, as for nodes, graph embedding is deﬁned as a mathematical process ﬁnding out a mapping Φ : G → that projects a graph representation G to a Euclidean space R, where n is the size of G (i.e., G has n nodes) [133] performed by optimization algorithms (Sec. 2.2.3 gives details). That is, given a graph representation G with a node-set V , after constructing training samples (S {(v proximity set P from a deﬁned hypothesis space Φ [133] containing all possible mappings, learn a candidate Φ the averaged train error between predicted proximity F(Φ training samples. Then, on test samples, evaluating the graph embedding performance by getting the averaged test error between F(Φ the learning process on training samples will continue performed by optimization algorithms, searching for the optimal Φ lies in the (high-order) input-output data ﬁtting, contributing to an accurate mining for complex patterns hidden in graph representations. Unless otherwise speciﬁed, the graph embedding techniques retrospected in this article are all based on machine learning methodology. as embeddings makes the consumption of it more reusable and convenient in applications such as completion of knowledge graph to predict missing relations, question answering, query expansion, and recommender systems [78]. 2.2.2. Recommendation based on graph embedding based recommendation can learn the embeddings of users and items and use them to measure the user-item proximity. Methodologically, similar to the process of learning the mapping Φ : G → R techniques, in general, graph embedding-based recommendation constructs hypothesis spaces U and V for users and items, respectively. U and V are used to project the user nodes and item nodes in G to a common Euclidean space. To learn them, building reasonable objective functions measuring the averaged error between the predicted proximity F(U(v E[L(F(U(v step in graph embedding-based recommendation. After learning the user and item embeddings, for unobserved user-item interactions, their probability of existence can be quantiﬁed with the corresponding proximity of useritem node pairs, in which case the recommendation for a user can be realized by sorting the probability of the existence of the user’s unobserved interactions with items in descending order and selecting the top ones as the ﬁnal recommendations. involving users and items are reusable and possibly optimal to the preservation and reconstruction of the original patterns in the information; thus can substantially carry the properties of users and items as well as their indirect hidden relations, in which case the embeddings as a supplement to those learned from user-item interactions enables the alleviation of the cold start problem by building relations between ever non-interacted user-item pairs and the sparsity problem by uncovering more hidden relations between sparsely connected user-item pairs in the recommendation. in Tab. 3 by diﬀerent categories. In technique, the process of ﬁnding out Φ : G → Ris mostly based on machine learning methodology , v)|v, v∈ V } randomly sampled from G in a speciﬁed proportion and the corresponding observed , P) satisfying S∩S= ∅, graph embedding techniques based on machine learning methodology ﬁrstly, In this way, in practice converting the information in graph representations into numerical representations By implementing graph embedding on graph representations involving user and item nodes, graph embedding- From a practical perspective, the user and item embeddings learned from the side information and knowledge The recommendation models based on graph embedding techniques retrospected in this article are summarized Table 3: A comparison of graph embedding-based recommendation in diﬀerent categories. Recent focuses are concentrating on the developments of the pros and the solutions to the cons of methods. 2.2.3. Optimization algorithms mization algorithms [47, 137, 138], which perform the learning process of searching for the optimal U the eﬀectiveness and eﬃciency of optimization algorithms determine the performance of graph embedding-based recommendation. gradient descent (SGD) [139] and its parallel version ASGD [140], the two most popular ones out of their simplicity and eﬃciency, as well as other representative latest advances like Mini-bath Adagrad [141], nmAPG [142], Adam [143], and ADMM [144]. Objective functions, once formulated, can be solved as optimization problems by numerical or analytical optifrom the deﬁned hypothesis spaces in order to satisfy the extremum of objective functions. In this way, Brieﬂy, common-used optimization algorithms in graph embedding-based recommendation include the stochastic 2.3. A general design pipeline of graph embedding-based recommendation equivalent to perform as where Φ : G → R vector space, H is a hypothesis space, F : (·, ·) → R is for proximity measurement, L is the loss function quantifying prediction accuracy, and E is the expectation of overall loss named objective function. p proximity between nodes i and j, and Θ is the set of hyper-parameters. Designing an eﬀective and eﬃcient Eq. 2.3 contributes to returning accurate recommendations in practice. For that, this section proposes a general design pipeline of graph embedding-based recommendation. mental data sets, real recommender systems, Internet of things, or other commercial data products, among which the public experimental data sets seem to be the priority of designing and evaluating recommendation models because of its low cost and convenient access. Then, converting the information into graph representations is the second step. In this step, constructing appropriate graph representations ranging from unweighted and weighted to indirect and direct ones in speciﬁc forms such as general graphs, hypergraphs, multi-viewed or multi-layered graphs and stuﬀ in order to maximally capture and preserve the original patterns in the information promotes the recommendation performance because the recommendation directly implements based on graph representations. After that, the third step is to build proximity measurements measuring the proximity between node pairs in graph representations. Methodologically, the 1-order proximity (i.e., the direct proximity) recorded in the information from a speciﬁc recommendation scenario can point out the possible forms of proximity measurements in this scenario, helping build higher-order ones to further uncover indirect and more accurate proximity between node pairs. Meanwhile, building accurate proximity measurements also largely improves the eﬃciency of Eq. 2.3 as function F. space H should be ﬁrstly constructed for searching the optimal parameters of map Φ in Eq. 2.3. Before that, selecting proper recommendation methods and graph embedding techniques and at the same time seeking inspiration in the normal modeling architectures of hypothesis space adopted by these methods and techniques is a trick. In this regard, this article summarizes some common-used ones in Tabs. 5, 6, 7, 8, 9, 10, and 11 oriented to diﬀerent techniques and recommendation models. After constructing the hypothesis space, node embeddings can be obtained through map Φ. These embeddings are used to predict the proximity between an arbitrary node pair i − j through measurement F. Loss function L is designed to measure the loss between the predicted proximity ˆp observed (i.e., true) proximity p function E, an expectation function, quantiﬁes the model’s accuracy. Searching the optimal map Φ in the hypothesis space H is performed by optimization algorithms. In fact, designing proper loss function, objective function, and optimization algorithms is not diﬃcult because mature theories and applications related to the three aspects already exist in the machine learning ﬁeld. Hence, what really matters in step 4 seemly is that how to construct an eﬀective and eﬃcient hypothesis space. model performance is up to its ceiling while still not meets expectations, going back to adjust the hypothesis space could help make breakthroughs; or going back from step 4 to step 3 to rebuild proximity measurements, which at the same time may also guide to reconstruct graph representations in form. Constructing reasonable graph representations well capturing and preserving original patterns in the information is essential. For that, analyzing the topology characteristics of constructed graph representations and going back from step 2 to step 1 to reﬁne the preprocessing of information for better denoising performance is a strategy. Besides, going back from step 4 to step 3 to try more suitable graph representations carrying the same information while being more compatible with the input and implementation mechanism of recommendation models is another promising strategy. aiming to design recommendation models serving speciﬁc tasks related to collected data (i.e., information). This way can sort of guide the design of recommendation models by such as mining hidden patterns from data and involving them in modeling for higher accuracy; however, it can also restrict model generalization for other tasks. In fact, this pipeline can also start from step 4 and be seen as generalization-oriented, aiming to design versatile recommendation models ﬁtting to diverse tasks with diﬀerent input data and proximity measurements, in which Overall, under the framework of machine learning methodology, graph embedding-based recommendation is As shown in Fig. 4, the ﬁrst step is to collect information for the recommendation from such as public experi- When coming to design graph embedding-based recommendation models (the fourth step in Fig. 4), a hypothesis In turn, the four steps are recurrent as an iterative revising and updating process. As shown in Fig. 4, when This design pipeline on its face is data-oriented (or task-oriented) preferred by most computer science researchers, case physicists and mathematicians may prefer. There is no priority between the data-oriented and generalizationoriented strategies; they are two completely diﬀerent perspectives for designing models. To simultaneously realize both, designing recommendation models based on multi-task learning [51, 145] seemly is a novel and promising direction. At the end of this section, the notations widely used in this article are uniﬁed in Tab. 4. knowledge graphs, as well as the corresponding recommendation models based on these techniques, respectively. 3. Bipartite graph embedding for the recommendation are employed to uncover more hidden ones. In this case, models based on bipartite graphs for the recommendation are of top priority in research, considered as the basis of the expansion of those for general graphs and knowledge graphs. According to the taxonomy of user-item interactions in Sec.2.1.1, Secs. 3.1 and 3.2 retrospect recommendation models based on bipartite graph embedding techniques for static user-item interactions and temporal user-item interactions, respectively. 3.1. Recommendation with static user-item interactions tions can be divided into three categories: those based on matrix factorization, Bayesian analysis, and deep learning methods. From an overview, the matrix factorization method, which is the pioneer of bipartite graph embedding, provides well extensibility. As a probabilistic version of the matrix factorization method, by setting regularization terms of models through prior knowledge such as the fact that the error follows a Gaussian distribution, the Bayesian analysis method to some extend alleviates the non-convex optimization issue of the matrix factorization method caused by the sparsity problem. With the ability in eﬃciently learning and preserving non-linear patterns in data, the deep learning method has been dominating recent research. 3.1.1. Models based on matrix factorization models. In short, SVD decomposes a matrix A matrices and Σ is a diagonal matrix composed of A’s singular values; doing matrix product on U, V , and Σ can reconstruct A. Following this idea, through SVD, a user-item rating matrix is supposed to be represented with such decomposed elements, based on which the embeddings of users and items can be obtained and the user-item rating matrix can be reconstructed by doing some operations. in textual information retrieval. In detail, based on some numbered documents and terms appearing in at least two documents, LSA constructs a term-document matrix A where its element a i’s appearance in a document j. Through truncated SVD [148], A is decomposed by A ≈ on which the embedding of term i is represented by the i-th row of the matrix U the j-th row of the matrix V user’s query q (a set of words), LSA calculates the embedding of q by ˆq = q query’s proximity with all documents by doing operations on their embeddings. hundreds of millions, LSA becomes unfeasible in decomposing such an extremely huge user-item interaction matrix R as a result of the high complexity of SVD and the sparsity of U, V , which leads to an NP-hard problem [149]. To break the limitation, Simon Funk, on his blog, proposed FunkSVD inheriting LSA’s idea and implementing based on optimization algorithms for large-scale matrix factorization for the recommendation (Tab. 5 gives details). From an inverse view, FunkSVD does not directly decompose R by R = U ΣV represented with two matrices U, V , the embeddings of users and items. After initializing U, V , FunkSVD continues to search their values by optimization algorithms for the optimal U, V satisfying UV as possible. information such as user biases and item biases into models for better recommendation accuracy. From then on, FunkSVD inspired a variety of subsequent variants. For instance, BiasSVD [79] accommodates user bias and item bias into FunkSVD by deﬁning the bias in ratings as a term of b (Tab. 5 gives details), which can be linearly appended to the formula of FunkSVD. Based on BiasSVD, SVD++ [79] further accommodates user’s implicit interactions by modeling each user’s preferences as a term (Tab. 5 gives details) appended to U analysis for mining more auxiliary information related to the recommendation seems to be a point of breakthrough. For instance, Hu et al. [154] discovered that a positive correlation existed between the ratings on an individual The following Secs. 3, 4, and 5 retrospect embedding techniques for bipartite graphs, general graphs, and The recommendation runs by analyzing observed user-item interactions, of which side information and knowledge In general, recommendation models based on bipartite graph embedding techniques for static user-item interac- Singular value decomposition (SVD) [146] is the rationale behind matrix factorization-based recommendation For that purpose, latent semantic analysis (LSA) [147] was recognized as a pioneer of the application of SVD Feasible as LSA in theory, when coming to recommender systems where the number of users and items are usually One of the most salient advantages of FunkSVD is its strong extensibility, enabling it to accommodate auxiliary Table 5: Examples of modeling matrix factorization-based recommendation. (1) In FunkSVD, λ(kU the regularization term for preventing from overﬁtting [150] by penalizing the magnitudes of parameters. (2) In BiasSVD, µ is the average of R, and b for his implicitly interacted items, where y social neighbors of user i and item j, respectively. s NCRPD-MF, v review words v of geographical neighborhood and category; and z represents both popularity and geographical distance. (6) In FM, in order to predict a user’s rating ˆr(x) on an item, a feature vector x consisting of the features of the user and the item such as their one-hot encoding and the user’s ratings on other items and stuﬀ is constructed. w strength of the i-th variable, respectively; and v business given by customers and its geographical neighbors regardless of their business type, in which case there might be a potential inﬂuence of market environment in an individual business. Based on the discovery, Hu et al. proposed NCRPD-MF in order to accommodate this correlation relationship as auxiliary information into BiasSVD (Tab. 5 gives details). based recommendation models (KNN). For instance, Slim [155] implements factorization on a user-item matrix to learn the item-item proximity matrix for KNN (Tab. 6 gives details). The 3-tier SVD++ model [156] proposed by Koren et al. accommodates the item-item proximity information calculated by KNN into SVD++. In addition, such combinations can also decrease the dependence of users’ co-interactions with items for calculating the item-item similarity matrix S in KNN. For instance, applying matrix factorization, FISM [157] learns two embedding matrices P and Q preserving the relational patterns between item pairs and reconstructs S with P and Q, in which case only user-item interactions are required without further describing the co-interactions involved. embedding elements, which is not interpretable due to the general meaninglessness of negative values in practical applications. In this regard, non-negative matrix factorization [158–164] for the recommendation seems to be a promising solution. The other one is the violation of the triangle inequality principle [165, 166] in FunkSVD and its variants because they usually measure the user-item proximity by the dot-product in the Hilbert space, hindering the preservation of ﬁnd-grained user preference. To deal with that, metric learning [167, 168] provides a new theory for the recommendation in measuring the user-item proximity while satisfying the triangle inequality principle. The strong extensibility of FunkSVD and its variants also explores their combinations with k-nearest neighborhood- However, there are two ﬂaws of FunkSVD and its variants. The ﬁrst one is the possible negativity of the learned Some attempts have been made [169–173] in order to factorize a user-item rating matrix R in the metric space; for instance, Zhang et al. [170] proposed a method to convert R into a distance matrix. Furthermore, the metric space is unnecessary to be Euclidean, for HyperML [174] can also factorize R in the hyperbolic space. factorization. Diﬀerently, Lee et al. [175] proposed an assumption that a user-item matrix R is partially observed; so it should be considered as a low-rank matrix while restricted in the vicinity of certain row-column combinations. Aharon et al. [176] overturned the conventional assumption that a transform matrix should always be known and ﬁxed. Halko et al. [177] proposed a randomization assumption that achieves a fast matrix factorization on large-scale data. It is optimistic that the breakthroughs in the research into matrix factorization-based recommendation are most likely to derive from the applications of these new fundamental mathematical theories in the recommendation. Table 6: Examples of combining matrix factorization-based models with neighborhood-based collaborative ﬁltering methods. (1) In FISM, ˆr between the items rated by the user i with respect to their similarity to the item k. (2) In SVD with prior, E measures the error of such as squared loss, absolute loss, and generalized Kullback-Liebler divergence, R(U, V ) is a regularization term, and α is the coeﬃcient to balance the eﬀects from unobserved ratings. 3.1.2. Models based on Bayesian analysis systems brings the non-convex optimization problem [178] in factorizing such a huge and sparse user-item rating matrix for the matrix factorization method, in which case the recommendation accuracy is sensitive to model hyperparameters. In the worst case, setting inappropriate model hyper-parameters might severely interfere with learning convergence. For automatic hyper-parameter adjustment [179, 180], the Bayesian analysis method, to some extent, can guide the setting of such as regularization terms of matrix factorization-based recommendation models through prior knowledge. factorization (PMF) [181], a probabilistic version of FunkSVD (Tab. 7 gives details). In detail, it hypothesizes that the error r embedding V distribution is equivalent to where λ form while the hyper-parameter λ of FunkSVD has clearer setting rules in PMF. Under the rules, however, PMF may still overﬁt due to the inappropriate setting of hyper-parameters σ, σ PMF that U and V are independent, Bayesian PMF (BPMF) [183] hypothesizes that the distributions of U, V are non-Gaussian and that λ The above recommendation models are mostly built on a global low-rank mathematical assumption of matrix In practice, the giant amount of users and items while very sparse interactions between them in recommender The rationale behind Bayesian analysis-based recommendation can be well unraveled by probabilistic matrix =σσ, λ=σσ. Eq. (3.1), the objective function of PMF, is almost the same as that of FunkSVD in ﬁeld [184], Mrf-MF [185] hypothesizes that the prior distributions of U, V are related to each user’s neighborhood (Tab. 7 gives details). In addition, Poisson factorization [186], Bernoulli-Poisson factorization [187], and OrdNMF [163] can apply the Bayesian analysis method in the recommendation based on ordinal data [188]. above models, for the recommendation based on implicit user-item interactions, the Bayesian analysis method also supports a pair-wise ranking strategy sorting by the relative value of a user’s preferences for item pairs, providing new methods of modeling objective functions. BPR-OPT [189] was a pioneer of the pair-wise ranking strategy. It hypothesizes that a user prefers his interacted items more than non-interacted ones (Tab. 7 gives details). Methodologically, for each user i, BPR-OPT builds a training data D abbreviated as > maximizing the log of the posterior distribution is equivalent to where λ factorization-based method by, for instance, setting ˆx based on explicit user-item interactions, can also implement that based on implicit ones. Table 7: Examples of modeling Bayesian analysis-based recommendation. (1) In PMF, I which equals to 1 if user i rated item j and 0 otherwise. (2) In BPMF, W is the Wishart distribution related to the degrees of freedom ν the user i removed, among i, j movie j by r and 0 otherwise. automatic machine learning [179, 180, 191], which is a recent hot focus in the recommendation. Besides, the prior knowledge in the Bayesian analysis method also provides a causal inference approach [53, 56] to understand user behaviors in the recommendation. 3.1.3. Models based on deep learning two methods are often based on shallow learning, only capturing the linear patterns hidden in user-item interactions Apart from the point-wise ranking strategy sorting by the relative value of ratings adopted by most of the represents regularization parameters. Such a pair-wise ranking strategy can be combined with the matrix with ˆx− ˆx[189]. In this way, the matrix factorization method, usually oriented to the recommendation = I × σ. The same is true of items. (4) In BPR-OPT, ˆxis a function of vector Θ, capturing the relationships and j. (5) In RBM,˜R∈ Ris an observed binary indicator matrix of user i with˜R= 1 if the user rated The Bayesian analysis method is a powerful tool for hyper-parameter adjustment, exerting its great value in Despite general acknowledgement of the matrix factorization and Bayesian analysis methods’ eﬀectiveness, the in the recommendation. From a mathematical point of view, represent the features of user i and item j with vectors v f : R analysis methods is generally linear, which is insuﬃcient in ﬁtting non-linear relations between (v For non-linear mapping, recent years have witnessed a boom in applying the deep learning method [192] in the recommendation to build deep learning models. are ﬁrstly learned through pre-training, used to construct the feature vector of a user involving other information of the user’s historical activities like his watched videos and searched tokens as well as the user’s characteristics such as his geographical location, age, and gender; then, the user’s feature vector is the input of a deep neural network with multiple layers, used to learn the user’s embedding; ﬁnally, unobserved implicit user-item interactions can be predicted based on user embeddings. Salient advantages of Youtube Net include its fast parallel computing [194] and non-linear mapping learning beneﬁts from a deep learning framework. Besides, the high capacity and diversity of input data of Youtube Net also widen its applications. neural collaborative ﬁltering (NCF) [195] learns a user’s feature vector based on his implicit interactions with items through pre-training and combines it with the user’s characteristics, the same is true of constructing item feature vectors. For the prediction of an implicit user-item interaction, NCF concatenates the corresponding feature vectors of the user and item as the input of a deep neural network containing a generalized matrix factorization (GMF) layer and multiple MLP layers both jointed to a NeuMF layer, which comes out the predicted proximity between the user-item pair (Fig. 6(a) gives details). For more information input, ConvMF [196] used a convolution neural network (CNN) [197] to enhance PMF [181] for representations of documents. To alleviate the overﬁtting problem encountered by deep learning models, Cheng et al. [134] proposed to combine deep learning with wide learning [198]. Figure 5: Schematics of Youtube Net. The generation component for recommendations of Youtube Net, performed by minimizing the cross-entropy loss with a descent on the output from sampled softmax. deep learning method also supports a more robust measurement for the user-item proximity that helps uncover the complex non-linear relations between user-item pairs by using their embeddings. For instance, NCF uses a deep neural network to learn the non-linear relations between the embeddings of a user-item pair with respect to their proximity. Deep matrix factorization (DMF) [199] further extended this idea to the prediction of ratings based on a user-item matrix Y combining both the implicit and explicit user-item interactions (Fig. 6(b) gives details). In this regard, causal learning [52, 54, 55, 57, 58] seems to be a potential solution, among which the restricted Boltzmann machine (RBM) [190] derives the rationale for causal learning in the recommendation. As shown in Fig. 7, for a user, RBM takes each element in the user’s feature vector as a unit independently, building the so-called causal relations from the units to the user’s interacted items encoded by one-hot and modeling the causality between the user’s features and his behavior (i.e., his interactions with items). With the learned relational weights, RBM ∈ Rand v∈ R, respectively; then, the recommendation can be described as learning a mapping × R→ R such that f(v, v) = ˆr≈ r. The mapping f learned by the matrix factorization and Bayesian Youtube Net [193] was a pioneer. Its schematics are shown in Fig. 5: the embeddings of all videos on Youtube Under the general framework of Youtube Net, subsequent realizations have been proposed recently. For instance, Compared with linear operators such as the dot product generally adopted by shallow learning models, the However, the lack of explainability of deep learning-based recommendation models is a long-standing problem. Figure 6: Schematics of NCF and DMF. (1) In (a), NCF ﬁrstly learns four mappings to project user i’s one-hot code to U neural networks to learn the user and item embeddings. Finally, the learned embeddings are used to predict the proximity by is mapped to Y is performed by minimizing the objective function L = − can sort of give practical meaning to how features inﬂuence user behavior. Factorization machine (FM) [153] is another pioneer, which builds the causal relations between each pair of elements in a user’s feature vector, named feature interactions, by integrating support vector machine (SVM) and SVD (Tab. 5 gives details). Although RBM and FM are shallow models, their rationales for learning the mutual causality of richer relations built on sparse user-item interactions motivated a variety of deep causal learning-based recommendation models including, for example, DeepFM [200], xDeepFM [201], deep Boltzmann machine [202, 203], and stuﬀ. , Uand item j’s one-hot code to V, V, respectively. Then, cross-combinations on them are performed by = U Vand φ= a(W(a(...a(W(UV)+ b))...)) + b), respectively, used as the input of = σ(h(φφ)). (2) In (b), for a low-dimensional user embedding U, the corresponding row in matrix Y of the user Figure 7: Schematics of RMB. Based on the framework of a neural network, RBM is a two-layer model, where the left layer represents the binary hidden features with K units (i.e., the K elements in the user feature vector) of a user and the right one represents the one-hot encoding of items; relations are build between the user and his interacted items, which can be rated by the scores from 1 to L. 3.1.4. Other models esis that unobserved user-item interactions exist because of a user’s limited view of items in recommender systems, meaning that the non-interacted items haven’t ever been exposed to the user and thereby unobserved user-item interactions are valueless for the recommendation. Devoogth et al. [205] argued that the non-interacted items might not always be beyond a user’s view but could be eschewed by the user as a result of their properties meeting the user’s dislikes. This is the so-called not missing at random assumption [206, 207]. In that case, the user exposure assumption neglects a user’s negative preferences, which in fact provide valuable information for such as constructing negative samples [208] for the recommendation. Building on the not missing at random assumption, Devoogth et al. [205] appended a term α that an item is eschewed by a user, where ˆr addition, Liang et al. [204] used a matrix constructed based on the Bernoulli condition to represent the probability of an item’s exposure to a user. user-item interactions because implicit ones only record user’s interactions with items without revealing overt preferences. This is the so-called positive-unlabeled problem [86, 209, 210]. Second, the general strategy adopted by recommendation models built on the not missing at random assumption representing a user’s negative preferences with equally allocated weights could eliminate the deviations in a user’s negative preferences for diﬀerent items in reality. In respect to the two ﬂaws, for instance, Saito et al. [211] used a bias to distinguish user’s aﬀection degrees toward diﬀerent items. He et al. [212] proposed a negative weighting allocation strategy considering the popularity of a user’s non-interacted items, in detail, by replacing the second term of the objective function of SVD with prior shown in Tab. 6 with caused by a true negative preferences. niques, the cross-domain recommendation [213–215] seems to be a promising direction oriented to the alleviation of cold start and sparsity problems by relating two or more recommender systems. In detail, the cross-domain [213] refers to two types of domains: one is the target domain, in which the recommendation implements and the other is the source domain consisting of other recommender systems providing additional observed interactions corresponding to the users and items in the target domain. With the interactions, more hidden user-item relations in the target domain can be uncovered. This is sort of similar to the utilization of side information and knowledge in the recommendation but distinguished in rationales that the cross-domain recommendation can take direct observed user-item interactions as supplements. For illustration, suppose a user having zero or very few interactions with Until recently, most of the recommendation models supported the user exposure assumption [204] in its hypoth- However, there are still two ﬂaws in the not missing at random assumption. First, it is not ﬁt to implicit As for the recent applications of bipartite graph embedding-based recommendation based on the above techitems in the target domain, leading to the cold start and sparsity problems for the recommendation. Meanwhile, in source domains if the interactions between the user and some items corresponding to the ones in the target domain exist, the recommendation in the target domain can still implement based on the user’s preferences analyzed from the source domains or by uncovering user-item relations in the target domain corresponding to those from the source domains. For that purpose, transfer learning [216–220] is an eﬃcient method to bridge the two domains. In technique, for instance, EMCDR [213] learns a one-to-one mapping to transfer user embeddings from the source domains to the target domain by relating the common users and items in two domains, in which case the embedding of a cold-start user in the target domain can be learned. Besides, DDTCDR [221], based on the assumption that a user’s preferences are almost consistent in diﬀerent recommender systems, further allows the exchange of information between two domains by dual transfer learning [222–224]. dation models, especially for complex ones on large-scale data, is essential, attracting more attention of mathematicians. For that, speeding up the learning (i.e., convergence) process of recommendation models is a frontier focus. For instance, He et al. [212] proposed an element-wise ALS (eALS) to reduce the time complexity of ALS to linearity. Boyd et al. [225] applied ADMM [144] to speed up the optimization in SLIM by switching the constraints and regulation teems in objective functions. GFNLF [226] achieves a faster convergence of non-negative matrix factorization (NMF) via adopting α − β−divergence in objective functions and incorporating a generalized momentum method into its convergence process. 3.2. Recommendation with temporal user-item interactions interactions are occurring over time by existing users’ new interactions with more items and entrances of new users and items. (2) User’s long-term and short-term preferences for items are usually changing. By dynamically updating the embeddings of users and items, the real-time recommendation [227–229], a new framework for the recommendation based on temporal interactions, realizes the learning and preservation of user’s behavioral changes, contributing to the recommendation accuracy. method [230, 231], which enables the matrix factorization-based recommendation models to accommodate new user-item interactions in a low computing complexity, and the Markov processes method, which can model the changes of user’s short-term preferences for items. 3.2.1. Models based on matrix factorization ommendation models and repeatedly learn the embeddings of users and items based on the whole interactions combining the existing and new coming ones. Apparently, it consumes extremely high in computing resources and becomes infeasible in large-scale data. Alternatively, the online learning (or online recommendation) method [230, 231] can directly update the embeddings of users and items by using only the new coming interactions. functions or optimization frameworks can be ﬂexibly extended with appended terms for the accommodation of new coming interactions. For instance, as shown in Tab. 8, the objective function of SL with prior is separated into n blocks, where each of them measures the changes of user’s embeddings. SGD-PMF and DA-PMF [232] use appended terms accommodating a new coming interaction (u in updating embeddings U preferences by modeling the changes of the user and item embeddings in functions with time as an independent variable. For instance, TimeSVD++ [233], as an extension of SVD++, models the b b(t), b instance-decay [235]. Focus more on theory, research into optimization algorithms determining the learning eﬃciency of recommen- Temporal factors in recommender systems mainly derive from the following situations: (1) New user-item Commonly used methods of real-time recommendation include the online learning (or online recommendation) A simple strategy for accommodating new user-item interactions into the recommendation is to reload rec- In general, online learning implements based on the framework of matrix factorization method, whose objective In addition, the framework of the matrix factorization method is convenient in capturing user’s long-term (t) and U(t) involving time. Other methods include by such as moving time-window [234] and setting Table 8: Examples of modeling temporal matrix factorization-based recommendation. (1) In SL with prior, rate during updating iterations. (3) In DA-PMF, Y 3.2.2. Models based on Markov processes on analyzing users’ sequential activities [91]. Its idea generally lies in the learning of an overall-shared transition matrix [238] modeling the latest user-item interactions, where the elements represent the transition probability between item pairs, that is, the probability that a user will interact with an item after having already interacted with others. With the transition matrix, unobserved interactions can be predicted by ranking the items that mostly meet the transition probability of a user’s interacted items. During this process, learning an accurate transition matrix is crucial for Markov processes-based recommendation, in which case considering environmental factors [239] in modeling the transition probability seems to contribute a lot. Table 9: Examples of modeling Markov processes-based recommendation. (1) In Fossil, the model is reduced to the ﬁrst-order when L = 1. (2) In MFMP, it has X in Eq. (3.3) are deﬁned as ρ erences. To overcome the ﬂaw, combining the Markov processes method with the matrix factorization method that can capture long-term preferences is a promising direction. For instance, FPMC [238] builds several transition matrices corresponding to each user by the Markov processes method and uses them to aggregate into a tensor, where the missing values represent unobserved user-item interactions. After that, FPMC uses the Tucker decomposition (TD) method [241] to factorize the tensor like the matrix factorization method does in order to obtain the user and item embeddings. Besides, Fossil [240] fuses user’s long-term preferences learned by the matrix factorization method and short-term preferences captured by a high-order Markov chain to the hybrid preferences. Wu et al. [242] applied embedding techniques to build transition matrices enabling the transition probability to involve the user’s long-term preferences. Most of these works fuse user’s long-term and short-term preferences linearly but in- =PVVis a k ×k matrix,which is independent from i. (2) In SGD-PMF, η is the step size controlling the convergenceP As for capturing the changes of user’s short-term preferences, the Markov processes method implements based However, Markov processes-based recommendation models cannot capture the changes of user’s long-term prefevitably lose the higher-order patterns hidden in user-item interactions. In this regard, Wang et al. [243] proposed a two-layer structure constructed with diﬀerent aggregation operations. Bayesian analysis method for automatic hyper-parameters adjustment. Take MFMP [244] as an instance, which is the probabilistic version of TimeSVD++ [233]. It hypothesizes that the changes of U Gaussian Hidden Markov processes rule; so maximizing the posterior distribution can be performed by 3.3. Summary are concisely organized in Fig. 8, among which it is overt that the matrix factorization method is the most extensible one succeeding various subsequent variants. However, the current focus has substituted the deep learning method for the matrix factorization method, for the advantage in non-linear pattern discovery and parallel computing of the former is meeting the technical requirements of increasing complexity and scale of data for the recommendation. As for the Bayesian analysis method, its breakthrough runs through the entire timeline, indicating its indispensable contributes to the other methods for automatic hyper-parameter adjustment. On the other hand, the Markov method does not seem to be common in use. In addition, Markov processes-based recommendation models can also be enhanced, to some extent, by the The key developments of bipartite graph embedding-based recommendation models retrospected in this section Figure 8: Timeline of key developments in bipartite graph embedding for the recommendation. 4. General graph embedding for the recommendation hidden (indirect) user-item relations contributes to the recommendation accuracy and can also alleviate the cold start and sparsity problems. However, side information is usually represented with general graphs that are beyond bipartite graphs in complexity, limiting the matrix factorization method as well as others oriented to bipartite graph embedding in the practice of the recommendation involving side information. To make up for the ﬂaw, researchers have tried to extend the matrix factorization method for general graph embedding, like collective matrix factorization [245, 246] and spectral methods [247–250]. These new methods are eﬀective but still cannot perform eﬃciently enough on large-scale data as a result of their high computing complexity. graph embedding, named general graph embedding techniques, have been proposed. With these eﬃcient techniques, it is no longer diﬃcult to employ side information in the recommendation and has been attracting more research attention recently. Sec. 4.1 divides the general graph embedding techniques into three categories and Sec. 4.2 retrospects their utilization in the recommendation from two diﬀerent perspectives: technique-oriented and scenariooriented. 4.1. Three categories of techniques for general graph embedding path, and deep learning. From an overview, in a graph, translation-based techniques build on algebraic theory and can ﬂexibly and suﬃciently preserve local topological features. Meta path-based techniques implement by the random walking across nodes and can further capture global topological features. For capturing and preserving non-linear topological features, deep learning techniques are the priority. The following gives more details of them. 4.1.1. Techniques based on translation a relation r from the head node (or entity) h to the tail node t, whose embeddings are denoted by h, r, and t, respectively. when (h, r, t) existing, h + r ≈ t indicates that t is one of the nearest neighbors of h + r, and otherwise h + r should be far away from t. However, in practice, a node in general graphs could play multiple roles in diﬀerent communities like that, for example, a head node h could have positive impacts on other tail nodes in a community but may also have negative impacts in another community, in which case h should be distinguishably denoted by h and h meaning of such diﬀerent roles. In fact, a node’s diﬀerent roles can be revealed by its diverse relations with other nodes, that is, the positive relations of a node with other nodes could reveal its positive role in the community and vice versa. Based on this idea, TransH [252] maps each pair of (h, t) to multiple relation-speciﬁc hyper-planes multiple roles. For example, a triplet (location, contains, location) may be interpreted by multiple semantics such as country-contains-city, country-contains-university, or something; thereby using only a relation embedding r is insuﬃcient in distinguishing such abundant relational semantics. In this regard, TransR [68] maps nodes and relations separately into a node space and multiple diﬀerent relation spaces corresponding to the diverse relations between head-tail pairs, respectively. CTransR [68] clusters the related node pairs (h, t) with similar semantics into multiple groups under speciﬁc relation r. In these ways, TransR, and CTransR can preserve multiplex semantics of nodes and relations in general graphs beyond TransE. Tab. 10 gives details of these techniques. To possibly capture more of it, TransD [253] further takes into account the multiple types of nodes and relations. Ji et al. [69] argued that the relations might belong to diﬀerent graph patterns like that some relations link a large number of node pairs while others may not, and might also have uneven balances that the node pairs linked by relations could diﬀer in quantity. With the Chinese restaurant process (CRP) [254], TransG [255] clusters the semantic components π based ranking criterion like As illustrated in Sec. 2, employing side information such as the properties of users and items in uncovering So far, to uplift the model scalability on employing general graphs, feasible and eﬃcient techniques based on Techniques of general graph embedding can be divided into three categories: those based on translation, meta The triplets (h, r, t) illustrated in Sec. 2.1.2 can also represent the side information in general graphs, describing From the perspective of algebraic theory, TransE [251] takes r as a translation from h to t in the metric space; , respectively. Representing hand hwith a common embedding h as TransE does will lose their practical distinguishing the diverse relations between nodes. Analogously, a relation in general graphs could also play In addition, in general graphs, the multiplicity of nodes and relations is far beyond their multiple semantics. An eﬃcient objective function for the above translation-based techniques is performed by minimizing a marginwhere [x] such as those shown in Tab. 10. S = {(h, r, t)|h, t ∈ E} is the golden triplets set or named training samples, and largely inﬂuences the training performance, usually constructed manually by such as replacing either the head node or tail node by random (but usually not both at the same time). based techniques somewhat loses the global topological features in a graph. To extend these techniques, deepening the proximity order for transformation seems to be an eﬀective strategy. For instance, RPE [256] builds a path space to preserve relational semantics contained in a path connecting non-adjacent nodal pairs with higher-order proximity. However, computing complexity is always the ceiling. For higher eﬃciency in preserving the global topological features of a graph, meta path-based techniques illustrated in the next section are prevalent alternatives. Table 10: Examples of modeling translation-based techniques (1) In TransH, for each node pair (h, t), the embeddings of nodes h and t after being mapped to a hyper-plane w embedding of their speciﬁc relation on w tail pair with a relation r into relation-speciﬁc spaces. (3) In TransD, h tail node, respectively, and r matrices. (4) In TranSparse, θ measures the sparse degree of a matrix, recording the fraction of zero elements over the total number of elements. For each relation r, a sparse transfer matrix M hyper-parameter, denotes the minimum sparse degree, N 4.1.2. Techniques based on meta path cessing (NLP): given N words chosen from a corpus containing hundreds of millions of words; then use them to try to make up a legal sentence like w the sentence are hypothesized to have similar meanings; so their representations, something like embeddings, should be similar. In technique, by taking legal sentences as training samples, NPL [258] learns the representation of a given word related to its context (Fig. 9(a) gives details). Reversely, Skip-gram [259] aims to learn the representations of words in a given context around its central word, surprisingly enabling the learned word representations to preserve linguistic regularities and patterns (Fig. 9(b) gives details). Compared with NPL, Skip-gram is more available for large-scale data. Furthermore, by using hierarchical softmax [260] to identify phrases, Word2vec [261] extends Skip-gram from a word-based model to a phrase-based one. Tab. 11 gives details of these models. similarly, given N nodes chosen from a graph by random walking starting at an arbitrary node; then use them to build a sequence. After implementing random walking many times, several sequences are obtained. The nodes = {(h, r, t)|h∈ E} ∪ {(h, r, t)|t∈ E} is the negative triplets set or named negative samples, whose quality On the other hand, the method of algebraic transformation on local topology adopted by the above translationdenotes the maximum number in N. Meta path-based techniques were inspired from the idea of word representation [257] in natural language pro- Taking graph as an analogy of corpus, the idea of word representation can be applied in graph embedding: Figure 9: Schematics of NPL and Skip-gram. In (a), a legal sentence w where w x = (C(w P (w In (b), reversely, Skip-gram takes w appearing in a common sequence with higher frequency are hypothesized to have higher proximity measured by their embeddings. The eﬀectiveness of this idea was ﬁrstly proven by DeepWalk [262, 263]. In technique, Deepwalk adopts the depth-ﬁrst searching strategy for building random walks W v, where W “sentence”. For each node v by v hierarchical softmax [260] (Tab. 11 gives details). DeepWalk performs well in preserving the high-order proximity between nodes. As for the ﬁrst-order and second-order proximity between nodes, LINE [264] adopts the breadth-ﬁrst searching strategy for building random walks, somewhat can be seen as “WideWalk” (Tab. 11 gives details). of learned embeddings, facing more challenges than word representation whose legitimacy of constructed sentences can be guaranteed by linguistic knowledge while the legitimacy of random walks still lacks a recognized test standard. For that, subsequent variants of DeepWalk and LINE have tried to design more intelligent random walking rules. For instance, Node2vec [70] deﬁnes a more ﬂexible notion of node’s neighbors, which allows random walks to identify special nodes that are within a common community or play similar roles in a graph. In technique, Node2vec designs a biased random walking rule guided by a return parameter p and an in-out parameter q as Where v normalization constant, and ω uniﬁes DeepWalk, LINE, and Node2vec into a matrix factorization framework, providing a deeper perspective of these techniques in algebra for designing new random walking rules. and types. However, the above techniques are basically oriented to homogeneous graphs. In order to capture and preserve the multiplicity of heterogeneous graphs, which account for the largest proportion of general graphs, for instance, PTE [266] extends LINE to be available on bipartite graphs (Tab. 11 gives details). In addition, by records the index of words in a corpus. NPL takes the sentence as a training sample and transforms it into = α|w, ..., w) = softmax(b + Wx + U tanh(d + Hx)), which represents the probability of w’s index to be α. ∈ W[i − w : i + w], where the window size is 2ω + 1. Training is performed by maximizing P (v|v) by The suﬃciency of random walks in the preservation of graph topological features directly determines the accuracy In practice, as illustrated in Sec. 4.1.1, nodes and relations in general graphs usually have multiple semantics Table 11: Examples of modeling meta path-based techniques. (1) In Skip-gram, w and output embedding, respectively; N is the number of words in a corpus and L is the number of words in a sentence (i.e., a training sample). c is the size of a context. (2) In Word2vec, N distribution. (3) In LINE, the embedding of v is denoted by v when treating v as a vertex while by v (4) In PTE, nodes in a bipartite graph are divided into two sets V set V decomposing a bipartite graph into two homogeneous graphs, BiNE [267] designs a random walking rule of “rich nodes are getting richer” to satisfy the power-law distribution phenomenon. Recently, using manually designed meta paths [268, 269] by expert knowledge to rule random walking on heterogeneous graphs has been a promising solution, in which case covering as much abundant and reasonable semantics in meta paths as possible is its primary goal. HIN2Vec [270] is a case in point. By capturing multiple relation types between nodes, HIN2Vec jointly learns the embeddings of nodes based on training samples hu, v, r, L(u, v, r)i indicating that a relation r exists between u and v when hu, v, ri = 1 and vice verse. Similarly, Metapath2vec/ Metapath2vec++ [271] captures the multiple types of both nodes and relations in meta paths by deﬁning the translation probability between nodes of random walking as Where v [272–274] were proposed to capture and preserve the multiple semantics of relations between nodes. 4.1.3. Techniques based on deep learning embedding; by the same token, it is also true of general graph embedding. AutoEncoder [275] provides an unsupervised learning framework that is diﬀerent from those in Sec. 3.1.3 based on supervised learning. In technique, AutoEncoder usually consists of two components: the Encoder and the Decoder, as shown in Fig. 10(a), where the Encoder learns the embedding of input X and stores it in the hidden layer Z; then, by using the embedding, the Decoder aims to reconstruct X as Y possibly approximating to X, minimizing the reconstruction error between X and Y . Such a ﬂexible unsupervised learning framework provided by AutoEncoder motivated numerous variants. is walked from node vin set Vis denoted by P (v|v). Sec. 3.1.3 illustrates the ability of deep learning methods in preserving non-linear features in bipartite graph For instance, Vincent et al. [276] further enhanced the robustness of the learned embedding in Z by corrupting the input X to et al. [277] proposed a multi-layer version of AutoEncoder named Deep AutoEncoder. More variants of AutoEncoder include SCAE [278], generalized AutoEncoder (GAE) [279], variational AutoEncoders (VAEs) [280], and deep hierarchical variational AutoEncoder (Nvae) [281], to name a few. Figure 10: Schematics of Autoencoder and SDAE. In (a), the Encoder is ﬂexibly constructed according to diﬀerent tasks, such as by an LSTM [282] in the task for machine translation (MT) and by a CNN [283] in the task for computer vision (CV). Since determining the quality of embeddings in Z, construction of the Encoder should be carefully implemented, at the same time somewhat inﬂuencing the lower bound of reconstruction error between X and Y . In (b), the strategies for corrupting X to noise (SP), among which SP is only for binary data. [284], a semi-supervised deep model, for example; it realizes the preservation of the second-order proximity between node pairs by reconstructing their common neighborhoods with two deep AutoEncoders sharing parameters. Meanwhile, SDNE also preserves the ﬁrst-order proximity between nodes by using a Laplacian Eigenmaps-based supervised component. so are its variants. When the size of input X is far longer than that of Z, the transformation from X to Z across the Encoder could overly lose essential information. For example, as shown in Fig. 11(a), when the input sequence lose essential correlation information involved in the original sequence. To deal with the issue, by means of the The framework of AutoEncoder and its variants can be applied to general graph embedding. Consider SDNE However, in the framework of AutoEncoder, the size of hidden layer Z (i.e., the embedding size) is ﬁxed, and , x, ..., x) of Seq2seq [285] is large in size, compressing z, z, ..., zinto the embedding ﬁxed in size could attention mechanism [286–289], the attention weights a importance. In other words, Seq2seq with attention mechanism aims to distinguish the diﬀerent importance of z and extract the most representative ones, which are preserved in the embedding. This way enables the embedding to preserve possibly primary information with a ﬁxed and limited length. In technique, the attention weights can be calculated by an attention function mapping a query Q and a set of key-value pairs K, V to an output (Fig. 11(b) gives details). Kim et al. [290] further proposed a structured attention network (SAN) taking into account the structured dependency of the attention layer. Transformer [291] substitutes a self-attention component learning the attention weights for the RNN structure in the Encoder (Fig. 11(c) gives details) and can support parallel computing. The attention and self-attention mechanisms can also be applied to the AutoEncoder-based frameworks for general graph embedding. For instance, GATs [292] constructs a masked self-attention block layer for graph convolution (Fig. 11(d) gives details). 4.2. Recommendation involving side information technique-oriented and scenario-oriented. Firstly, in technique, extracting properties of users and items from general graphs and employing them into the recommendation can perform by the three categories of techniques retrospected in Sec. 4.1 as follows. dation by taking a user as a translation linking the pairs of his sequentially interacted items, which are adjacent, during a certain period; that is, for a user embedding holds between the embeddings of the user’s previous interacted item acted item take diﬀerent relation types as the same one). In practice, the next interacted items are usually diverse; thus taking into account the abundant item types which distinguish diﬀerent relation types between item pairs is necessary. For the transition oriented to 1-to-n relations, by building multiple semantic-speciﬁc transition matrices related to diﬀerent diverse items, CTransRec [294] can further distinguish and preserve relation types. building a user-item proximity matrix where elements are measured based on random walks; with the matrix as input, the matrix factorization methods can perform. Take HIN [295] for instance. In a general graph, HIN hypothesizes that a user’s preferences for items randomly walk along designed meta paths within a boundary centered on this user. The accumulation of preferences reached to an item quantiﬁes its proximity with the user. Intuitively, the higher accumulation of an item results from its more indirect relations with the user; so indicates the higher possibility of the existence of the hidden user-item interaction. After implementing such diﬀusion from each user, a global user-item proximity matrix can be built. In contrast, in the diﬀusion along diﬀerent meta paths, FMG [296] builds multiple local user-item proximity matrices corresponding to each of the meta paths and learns multiple distributed embeddings for each user-item pair. These embeddings are the input of FM [153] enabling FMG to preserve the feature interactions between inter-meta paths. In the practice of diﬀerent recommendation scenarios, designing suitable meta paths to rule random walking directly determines the recommendation accuracy of these models. At this point, for instance, by removing the nodes except those representing users and items in meta paths, Shi et al. [297] designed a graph schema to discriminate node types when random walking. However, it requires much expert knowledge in designing meta paths as most meta path-based models do. For an automatic random walking strategy without hand-craft design, RippleNet [298] stimulates the diﬀusion of one’s preferences from his historical interacted items as centers to trigger multiple “ripples”, whose hierarchical propagation traces automatically generate multiple meta paths in a general graph. In addition, by using an LSTM layer [282] to identify the holistic semantic of meta paths, KPRN [299] can realize reasoning on meta paths. Chen et al. [300] further took into account the temporal factors in general graphs and proposed a novel temporal meta path guided explainable recommendation (TMER). motivated a variety of recommendation models. For instance, by using the explicit interactions with items (i.e., ratings) of a user while the unobserved ones correspond to a zero value, AutoRec [301] builds a rating vector of the dimension equaling to the number of all items; this rating vector is the input of AutoEncoder coming out the reconstructed output where completes the missing (i.e., zero) ratings. Besides, based on implicit user-item interactions, Wu et al. [302] applied SDAE to the recommendation by assuming that the corrupted layer of SDAE represents a user’s preferences because of the possible incompleteness of the user’s observed implicit interactions. This section retrospects recommendation models involving side information from two diﬀerent perspectives: Based on translation-based techniques, for instance, TransRec [293] applied TransE in the sequential recommennext. item. However, TransRec inherits the defect of TransE that only be suited for 1-to-1 relations (i.e., As for meta path-based techniques, a common-used strategy for applying them in the recommendation is by When coming to apply deep learning-based techniques in the recommendation, the framework of AutoEncoder Figure 11: Schematics of Seq2seq, attention mechanism, self-attention, and GATs. In (a), Seq2seq, as an Encoder, learns the embedding Z with a ﬁxed-length compressed from zQ (b), attention weight is a ∼ p(a|K, Q), where K represents the input X and Q is the result of the last iteration in the Decoder. Diﬀerent from Seq2seq where the output Z is directly compressed from z of vectors z Z = softmax( (d), for a node i, GATs takes its neighborhood h is a is some neighborhood of node i, and || is the concatenation operator. Finally, h Liang et al. [303] ﬁrstly applied variational AutoEncoders (VAEs) [280] to the recommendation. In addition, CVAE [304] is a Bayesian model based on VAEs, which can simultaneously employ user’s ratings and item’s proﬁles in the recommendation. As for the application of the attention and self-attention mechanism in the recommendation, for instance, AFM [305] uses the attention mechanism to weigh the feature interactions in FM [153]. Xu et al. [306] proposed a multi-layer long- and short-term self-attention network (LSSA) for the sequential recommendation. employing side information have also made successful attempts in the industry. For instance, by taking the records of user’s session-based online activities in Taobao as side information, Alibaba [307] constructed a weighted and directed network, which is comprised of user-item interactions in a continuous period [308] and can be employed in extracting hidden consumption habits of users to promote recommendation accuracy. In addition, SHINE [309] p(y|Z, y, ..., y), where X = (x, .., x) is the input, y, ..., yis the output, and T is usually not equal to T. In and Vby matrix W, Wand W, respectively, as model parameters. Based on them, z, ..., zis obtained by =, where W ∈ Ris a shared weight matrix, a ∈ Ris a weight vector, N It is positive that the above models based on general graph embedding that enhances the recommendation by simultaneously takes the semantic networks, social networks, and proﬁle networks of users as side information, using them to predict the sign of a sentiment link (i.e., user’s attitudes towards an item) without analyzing textual information like user’s comments on items. user’s social information [310] and location information [311] as side information in the recommendation, in which case recent years have pioneered the location-based social recommendation [312] thriving for a long period. The reason is that user’s online information is usually collected from online social platforms allowing users to express their needs, desires, and attitudes towards items and events by such as tweeting or posting. So, such information can reﬂect and record user’s preferences for items more timely, suﬃciently, and explicitly compared with user-item interactions [100]. Besides, the online information across multiple social platforms can be connected by users. For example, users are encouraged to use their social platform accounts (like Twitter or Weibo) to log on to Amazon or Taobao. This way enables user’s comments on products, a type of social information revealing one’s hobbies or demands shared on social platforms, to be employed in e-commerce recommender systems. with the collection of user’s properties from microblogging, METIS [100] aims to learn an item’s demographics described by the properties of users who have interacted with it. For such purpose, METIS uses classiﬁcation algorithms to detect user’s purchase intents and at the same time extracts item’s demographic attributes from microblogging information in real-time, based on which the recommendation can perform by matching users and items. Take MART [313] for another instance. It ﬁrstly employs online social information to build social networks, a type of general graphs; then relates them with recommender systems by users as bridges. When coming to a coldstart user, MART retrieves the user’s social information providing valuable information such as his attributes like gender, age, and career in these social networks, aiming to discover his preferences for items. The rationale behind MART lies in a learned transformation mapping user’s property information in social networks to the corresponding embeddings in recommender systems. Apart from user’s property information, employing more valuable ones in social networks in the recommendation is promising. One instance is the social ties [314], a concept to distinguish strong and weak ties between objects. With respect to it, Granovetter et al. [315] exempliﬁed the more crucial role of weak ties in social networks compared with strong ones because the weak ties largely determine the connection patterns of clustered components in social networks. Wang et al. [316] discovered that by distinguishing social ties in social networks the recommendation accuracy can be improved. Based on the discovery, they proposed TBPR, a recommendation model that can measure the tie strength between node pairs by Jaccard’s coeﬃcient and classify them into strong and weak ones, which are used to distinguish a user’s interacted items into ﬁve types quantifying the user’s preferences for diﬀerent of them. The distinguished preferences are used as the input of the BPR framework [189] performing the pair-wise ranking strategy for training. et al. [317] discovered that more than 80% of a user’s visited new places are located within a 10km vicinity of the user’s lastest visited places, unraveling the inﬂuence of one’s location information on his decisions of future visits. Furthermore, user’s location information can be further used as a supplement to online social information. The resources of user’s location information are diverse, among which user’s spatial trajectory information is a general one collected from user’s mobile techniques, like GPS [318], WiFi [319], ad-hoc networks [320], with permission. Besides, user’s check-in information of visits is also a source. In contrast with spatial trajectory information, it requires lower privacy rights because it can be directly collected from user’s visit records; however, it cannot be updated in real-time as the spatial trajectory information does. In fact, a user’s location information is usually related to his online social information, for one would like to share proﬁles of his visited places, such as comments, experiences, photos, moods, to name a few, on social platforms. These proﬁles can to some extent express the motivation of one’s visits to some places [321] and uncover his preferences for them. In this regard, recent research named the location-based social recommendation has explored to enhance the recommendation accuracy by employing user’s location information as a supplement to online social information. records in linking users and locations, seen as two types of nodes, and constructs a bipartite graph, in which under the framework of the UBCF method the proximity between user-location pairs is measured by analyzing user’s covisits information. Besides, by using PageRank with BCA [322], LFBCA also enhances such proximity measurement by employing user’s social relationships in building relations between user nodes in the bipartite graph. However, there are two problems. First, ignoring the timeliness of user’s location information could cause embarrassment, such as recommending a place where far away from the current location of users. To deal with it, users can be further subdivided into in-town and out-town ones [323]. Second, user’s location information is usually sparse because of the On the other hand, from the scenario-oriented perspective, the rest of this section retrospects the utilization of For the utilization of online social information in the recommendation, many models were proposed. For instance, In addition, employing user’s location information in the recommendation is also a focus. For instance, Wang Models of location-based social recommendation are diverse. For instance, LFBCA [317] employs user’s check-in user’s limited visits to new places due to economic or time cost constraints. For that, mining more side information of places, like their proﬁles given by users and the preferences from local people [324], as supplement is helpful. Among them, place proﬁles are particularly valuable because they reveal sentimental attributes, like user’s positive or negative attitudes towards places as well as the changes of attitudes, which can be used to infer whether a place meets a user’s sentimental preferences and needs. For instance, sentimental attributes can be used to measure the proximity between user-location or location-location pairs [321], in which case the techniques of textual sentiment analysis [325, 326] in NLP play a crucial role in extracting these sentimental attributes. In recent years, taking into account temporal factors in the location-based social recommendation is setting oﬀ a new tendency [327, 328]. 4.3. Summary of the Fig.) and recommendation models based on these techniques (i.e., the outer part of the Fig.), respectively. Apparently, all the three categories of techniques can be applied in the recommendation, in which case those based on meta path and deep learning are the broadest ones. Relevant research in recent years has concentrated on taking into account temporal factors when embedding general graphs and also on preserving the multiplicity of heterogeneous graphs. than one graph is a direct strategy for performing general graph embedding techniques on the recommendation. However, this strategy could dilute the signiﬁcance of user nodes and item nodes resulted from the diversity of node types in general graphs. It seems that using the attention and self-attention mechanism to highlight the two most salient node types (i.e., user and item) of the recommendation is a promising focus. Fig. 12 concisely organizes the key developments of general graph embedding techniques (i.e., the inner part Integrating general graphs with user-item bipartite graphs by linking their co-existing nodes appearing in more Figure 12: Timeline of key developments in general graph embedding for the recommendation. 5. Knowledge graph embedding for the recommendation complexity of knowledge graphs is generally embodied in the extreme abundance of node and edge types, making knowledge graph embedding confront the challenges of large-scale, multiplicity, and evolution. On the other hand, such complexity also contributes to far more improvements of recommendation accuracy by employing knowledge graphs in the recommendation compared with by using general graphs due to the far more diversity of information carried by knowledge graphs. the eﬃciency of those techniques is constrained when performing on large-scale knowledge graphs. Furthermore, general graph embedding techniques are generally insuﬃcient in capturing and preserving the high multiplicity of knowledge graphs. three challenges of knowledge graph embedding and retrospects advanced techniques for overcoming these challenges. Sec. 5.2 retrospects the applications of knowledge graph embedding techniques in the recommendation from two perspectives: embedding-based and path-based, as well as highlights two promising research directions. The large-scale challenge means that knowledge graphs are usually too enormous to be even completely observed, making general graph embedding techniques have diﬃculty in working with an acceptable computational complexity. The multiplicity of knowledge graphs refers to the following three main situations: (1) multi-typed nodes and singletyped edges, (2) single-typed nodes and multi-typed edges, (3) nodes and edges are both multi-typed. Capturing and preserving such multiplicity of knowledge graphs contributes to suﬃciently utilizing the valuable information in knowledge graphs. However, for that purpose, general graph embedding techniques have inborn ﬂaws, including the lack of versatile expert knowledge in designing meta paths for such complex knowledge graphs and the general inability of translation-based techniques in capturing higher-order topological features. In addition, the evolution of knowledge graphs brought from the constant and rapid growth of knowledge challenges the update of the embeddings once learned. for parallel computing and fast response. The rationale behind GNNs is the label propagation (or message passing) [332, 333] built on the assumption that each node and its features can be reconstructed with the help of its neighborhood in a graph. To achieve that, each node in GNNs consists of two components: aggregator and updater. The aggregator aggregates the features from a node’s neighborhood to build a context embedding. Then, the updater uses the context embedding as well as other input information to generate the embedding of the node. Furthermore, stacking K diﬀerent nodes adjacent to each other or repeating the propagation K times on the same node can expand the receptive ﬁeld of GNNs to K-hop graph neighborhoods. Under the framework of GNNs, transductive learning [334] and inductive learning [335, 336] are two primary strategies for label propagation, where the former aims to infer the labels of unlabelled nodes based on labeled ones and the latter aims to learn a global function for labeling. In detail, for example, the inductive learning strategy takes each node in a graph as both the receiver and disseminator of information from and to its neighborhood within speciﬁed hops, in which case the features of nodes are updated during such a continuous information exchanging process, a parallel computing process of label propagation that reﬁnes the embeddings of nodes until global convergence. Moreover, spectral methods [265, 337] achieve fast response by extracting a graph’s critical components based on spectral sparsiﬁcation theory [338]. Generative graph methods [286, 339, 340] can further speed up the learning process on large-scale graphs by generating new relationships linking hidden semantic properties in knowledge graphs, such as analogical properties [339] and circular correlation [340], to equip knowledge graphs with enriched topological properties. dependence of knowledge graphs via the label propagation between nodes, for their rationale for composing the extracted multi-scale localized spatial features to construct highly expressive embeddings like convolutional neural network (CNN) [341] dose while being suitable for non-Euclidean data structure such as graphs. In addition, in terms of graph representations on which embedding techniques run, multi-viewed graphs [342, 343] provide a clearer representation to model heterogeneous graphs containing single-type nodes and multi-type edges (Fig. 13(a) gives examples). Multi-viewed graph embedding aims to integrate the information in diﬀerent views together (termed as collaboration [344]) and at the same time to capture the distinctive properties carried in each view (termed as preservation [344]). In this way, the learned embeddings can preserve both the local features of nodes in diﬀerent As illustrated in Sec. 2.1.2, knowledge graphs can be seen as the most complex case of general graphs. The Eﬀective as general graph embedding techniques are in employing knowledge graphs in the recommendation, In this regard, more advanced techniques for knowledge graph embedding are needed. Sec. 5.1 illustrates the In general, knowledge graph embedding meets three challenges [329]: large-scale, multiplicity, and evolution. In the face of the large-scale challenge, graph neural networks (GNNs) [25, 330, 331] provide eﬃcient frameworks Regarding preserving the multiplicity of knowledge graphs, GNNs can capture both feature and topological homogeneous communities (i.e., views) as well as their global features. In technique, multi-viewed clustering [345– 347] and multi-viewed matrix factorization [217, 348] were two attempts while lacking suﬃcient collaboration as a consequence of their simple and independent concatenations of the learned embeddings from diﬀerent views. In fact, accomplishing both the collaboration and preservation contributes to a high performance of multi-viewed graph embedding. For such purposes, more eﬃcient methods have been proposed recently. For instance, by building random walk pairs across diﬀerent views, Mvn2Vec [344] can capture the global structure of a graph. In addition, for each node i, MNE [349] learns an overall-shared embedding h in a speciﬁc view v. After that, MNE combines h collaboration for node i, where X view v. Among these works, apparently, distributing proper importance weights ω [350], in which case the attention mechanism seems to be promising assistance. Figure 13: Examples of multi-viewed graphs and multi-layered graphs. In (a), the multi-viewed graph represents a heterogeneous graph containing single-typed nodes colored in black and three types of edges colored in red, blue, and purple, respectively, where each view represents a homogeneous component connected by edges in the same type. In (b), the multi-layered graph represents a heterogeneous graph containing two types of nodes and three types of edges by clustering the nodes into two layers colored in red and blue, respectively, where each layer contains nodes in the same type and the interconnections between layers are colored in purple. types, multi-layered graphs [351–353] provide another representation to model them. Multi-layered graphs have diverse forms in broad applications ranging from multi-scale graph embedding [354, 355] to cross-domain scenarios like critical infrastructure systems [356] and collaboration platforms [357]. Among them, the simplest one is the coupled heterogeneous graph containing two types of nodes and three types of edges (Fig. 13(b) gives examples). For example, when it comes to a knowledge graph about actors and movies, the two layers in Fig. 13(b) consist of an actor community representing cooperations between actors and a movie community representing category relations between movies, respectively. The edges between the two layers can represent the participation relationships between actors and movies. In technique, a variety of translation-based methods have been proposed recently for coupled heterogeneous graph embedding. For instance, as a node-oriented model, EOE [358] maps the embeddings of nodes in diﬀerent layers to each other through an overall-shared harmonious matrix. However, such a nodeoriented strategy could neglect the inﬂuences of inter-connection edges between diﬀerent layers. To deal with that, an edge-oriented strategy [272, 359] is proposed to take into account the inter-connection edges. When coming to a multi-layered graph with enormous layers, applying meta path-based methods can achieve better eﬃciency. For instance, by building ruled random walks across diﬀerent layers, PMNE [360] can capture the global topological features of a coupled heterogeneous graph. Furthermore, follow the previous example of an actor-movie knowledge graph, if the attributes of actors and movies need to be represented with nodes in multi-types in layers, a more complex multi-layered graph that can distinguish diﬀerent types of nodes and edges is required, as shown in Fig. 14 for example. For that, promising models include GATNE-T/GATNE-I [361], DMNE [362], to name a few. A As for the other situation of multiplicity where heterogeneous graphs contain nodes and edges both in multicommon idea of these works is to apply fast learning algorithms to capture the heterogeneous information in each layer quickly and use an attention mechanism to learn their diﬀerent importance. a research direction in recent years. Beneﬁt from the structure and the label propagation (or message passing) mechanism, GNNs can deal with dynamic graphs [331]. Other attempts include ctRBM [363], M HTNE [365], and TGAT [366], to name a few. See [367] for an in-depth review. Figure 14: An example of complex multi-layered graphs. In the multi-layered cross-domain heterogeneous graph, intra-connections in each layer are one-typed that belong to a particular domain while the nodes are heterogeneous. 5.2. Recommendation involving knowledge in recommender systems to eﬃciently employ knowledge in the recommendation. as embeddings, which can be employed in the recommendation by such as the cross-domain recommendation methods and transfer learning retrospected in Sec. 3.1.4. Based on this strategy, a lot of works were carried out. For instance, kaHFM [368] learns item’s embeddings of semantic features from a textual knowledge graph as a supplement to the factorization machine (FM) [153]. By extracting the structure, textual, and visual features of items from three diﬀerent knowledge graphs, CKE [369] constructs a synthetic embedding for each item as supplementary information. In general, since the eﬃciency of GNNs in preserving both entity features and relations of knowledge graphs, applying them to the embedding-based strategy is a common-used method [333, 370]. Among these works, for a user, how to accurately measure the high-order proximity between his interacted items and those far away named “remote” items in knowledge graphs is an open and important focus because the “remote” items are candidates of diverse recommendations to the user. For such purposes, for instance, KGCN [333] deepens the range of neighborhoods from one hop to multiple hops away, aiming to measure the proximity between nodes with a longer distance. Meanwhile, discriminating the unequal inﬂuences of a central node’s diﬀerent neighbors is another focus. Based on GATs [292], KGAT [370] builds an attention-based aggregation learning diﬀerent contribution weights from neighbors. Similarly, Hyper-Know [371] realizes that by building aggregations using hyperbolic attention with Einstein midpoint in hyperbolic space. In the training of these models, unobserved edges are usually taken as negative samples [87, 212]. However, Togashi et al. [372] argued that such a strategy for negative sampling could aggravate the cold start problem as a consequence of handing new items as negative samples, leading to biased and sub-optimal results toward already popularized items. To deal with the problem, Togashi et al. proposed KGPL applying pseudo-labeling [373] to distinguish possible weak-positive pairs in unobserved edges. Besides, more subsequent works [374, 375] have been devoted to this focus in recent years. tures of users and items from knowledge graphs as embeddings, aims to achieve explainable recommendation [45, 48–50] via extracting user-item relations from knowledge graphs through random walking across multiple types of nodes and edges guided by designed meta paths, helping understand user’s preferences for items by Capturing and preserving the dynamics of knowledge graphs in the face of the evolution challenge has become Embedding-based and path-based are two typical strategies for equipping the techniques retrospected in Sec. 5.1 The embedding-based strategy is to learn the supplemental features of users and items from knowledge graphs The path-based strategy, which is diﬀerent from the embedding-based strategy learning supplementary feafollowing the extracted user-item relations along multi-hop meta paths [299]. Intuitively, take Fig. 2(b) for example, Avengers 2: age of ultron seems to meet Alice’s preferences for movies by following the relations: (Alice, Watched, Avengers4)∧(Avengers4, ProduceBy, TWDC)∧(TWDC, Produce, Avengers2) and (Alice, Watched, Avengers4)∧(Avengers4, StarBy, Robert)∧(Robert, star, Avengers 2), which shows Avengers 2 is produced by the same company as that of Avengers 4 and also has a common actor in Avengers 4 that Alice watched in the past. It’s overt that the path-based strategy enables the recommendation to equip with reasoning and explainability abilities. In technique, the path-based strategy can conduct random walking to generate random walks on knowledge graphs where are integrated with user nodes and item nodes from bipartite graphs, in which case three issues include: (1) How to design reasonable meta paths to rule random walking? (2) How to measure user-item proximity in a random walk? (3) How to distinguish the diﬀerent importance of multiple random walks in order to highlight the most valuable ones? In these issues, for instance, KPRN [299] learns the embeddings of nodes in each random walk. These embeddings can preserve the hidden sequential patterns between nodes and are used to learn a random walk’s embedding representing its holistic semantics through a long short-term memory (LSTM) [282]. Using these embeddings of random walks can measure the proximity of user-item pairs. Furthermore, by designing a weighted pooling layer, KPRN can also distinguish the diﬀerent importance of multiple random walks between a user-item pair. However, the pooling layer implements based on enumeration of all possible random walks between a useritem pair, requiring a high computational complexity particularly on knowledge graphs with an enormous scale. Alternatively, Xian et al. [376] designed a soft reward strategy that can quickly highlight the valuable random walks between a user-item pair. Zhu et al. [73] further took into account user’s history click sequences for weight random walks. Recently, Chen et al. [300] argued that ignoring temporal factors in user-item interactions could result in less convincing and inaccurate recommendation explainability. For that, Chen et al. proposed TMER to construct item-item instance paths between consecutive items for the sequential recommendation. One is the conversational recommender system (CRS) [71, 377–379], which can chat with users, understand their expressions, and reason their preferences for items by using such as semantic knowledge and domain knowledge. The motivation of CRS lies in the substantial solution to the issue of user privacy infringement encountered by conventional recommendation systems, like those involving side information collected from user’s social and location information which is generally protected as privacy. Alternatively, CRS directly asks its needed information from users for the recommendation. In general, CRS consists of two components [380]: one is the dialog component, which is used to interpret human language into proper machine-readable forms and generate responses to users through a multi-round natural language conversational system [381, 382]. The other is the recommendation component reasoning user’s preferences by analyzing the content of user-machine conversations, based on which return recommendations to users. With the two components, CRS implements based on three stages: initiation, conversation, and display [383]. In detail, the dialog component initiates a conversation with users by raising a suitable question. Then, the conversation starts and continues to collect needed information from users through multi-round questions and answers. By analyzing the collected information, the recommendation component generates possible candidates and picks out the top-N ones as recommendations to users. The three stages repeatedly implement till the returned recommendations satisfy user preferences. In addition, since CRS implements reasoning by using knowledge graphs, the explainability of user’s preferences for items can also be realized. For example, suppose a user expressed his aﬀection on Avengers 2: age of ultron and Avengers 4: the ﬁnal battle in conversations, by using the knowledge graph in Fig. 2(b), CRS can infer that the user may also like other movies produced by TWDC or stared by Robert. In technique, CRS can be seen as a content-based recommendation method, building user’s proﬁles in the dialog component and matching them with item’s attributes (like reviews [380]) in the recommendation component. The item-oriented method [383–388] for CRS is the most common-used one, uncovering hidden relations between items by employing knowledge graphs. Furthermore, Zhou et al. [389] discovered that the word-level enrichment in conversations reveals user’s personal habits in word usage and is valuable to understand user’s preferences. In light of that, Zhou et al. proposed a word-oriented method to assist the item-oriented models in learning user embeddings. Recently, research into eﬃciently guiding users from non-recommendation scenarios to the desired one via proactive conversations has become a crucial direction of CRS [380, 390–392]. ommender systems [393–396]. Because of their textual recommendation scenarios [397], there raises three more challenges for news recommender systems compared with those based on non-textual scenarios: ﬁrst, the news is distinctly time-sensitive. In this case, the relevance to the reality makes the news fade away rapidly over time and the out-of-date news is replaced by new ones. Second, user’s interests in the news are topic-sensitive, diversiﬁed, and changeable related to current social focuses. Third, news language is generally comprised of professional knowledge In practice, there are two promising research directions of knowledge graph embedding-based recommendation. The other promising direction of knowledge graph embedding-based recommendation in practice is news recand common sense organized through logic, requiring sophisticated techniques combined with embedding and NLP to understand, reason, and preserve them. Equipping content-based recommendation methods with knowledge as external resources enable the learning and preservation of relatedness between news words as well as their latent knowledge-level connections. For instance, after identifying word entities in the news, DKN [397] employs an enormous knowledge graph to build relations between these word entities and constructs a small domain knowledge graph for each piece of news. Based on this knowledge graph, DKN learns the embeddings of news pieces by using a convolutional neural network (CNNs) [398]. 5.3. Summary of the Fig.) and recommendation models based on these techniques (i.e., the outer part of the Fig.), respectively. Compared with those shown in Figs. 8 and 12, the developments of knowledge graph embedding techniques started relatively recently, and the breakthroughs are spurring the recommendation involving knowledge forwards the next stage of eﬃciently dealing with multiplex and evolving large-scale data. These novel techniques and methods seem to be also ﬁt right to the recommendation based on general graph embedding. 6. Performance evaluation dation models and the most common-used conventional recommendation models, comparing their pros and cons on six tasks related to diﬀerent recommendation scenarios, data scales, and data sparsity and making a trade-oﬀ between them. Sec. 6.1 gives the experiment setups, including data sets, evaluation metrics, and evaluation models. Sec. 6.2 presents experimental results and analysis. Fig. 15 concisely organizes the key developments of knowledge graph embedding techniques (i.e., the inner part Figure 15: Timeline of key developments in knowledge graph embedding for the recommendation. This section displays the results of simulation experiments on representative graph embedding-based recommen- 6.1. Experiment setups models as most of the existing works and some program libraries [399, 400] did, there still lacks enough comparisons between both of them under a uniﬁed task framework, obscuring research into the analysis of their respective strengths to complement each other. For that gap, this section designs simple recommendation tasks for predicting implicit and explicit user-item interactions and selects normal metrics for evaluations on several graph embeddingbased and conventional recommendation models. 6.1.1. Data sets experiments are described in Tab. 12. Among them, MovieLens 100K and MovieLens 1M are two data sets from a popular online movie recommender system named MovieLens allowing users to explicitly show their preferences for watched movies by rating in order to get new movie recommendations. The two data sets, as a control experimental group, both record user’s ratings on movies while being diﬀerent from scales and sparsity, used to distinguish the performance of recommendation models in the same recommendation scenario but with diﬀerent data scales and sparsity. In addition, another data set is Jester 617K, which is from a popular joke review platform allowing users to explicitly show their preferences for jokes, recording user’s ratings on jokes. Table 12: Descriptions of data sets. Density, deﬁned as the ratio of the observed number of interactions to the maximum possible number of interactions between all users and items, is used to quantify data sparsity. Source attaches the hyperlinks of the three public data sets used in simulation experiments. the rating on item j given by user i in MovieLens 100K is constrained to r MovieLens 1M it is r unity, r the ratings no less than 3 into value 1 while the rest of the ratings less than 3 into value 0, the corresponding implicit user-item interactions can be constructed. In this way, six diﬀerent recommendation tasks are designed for evaluations on recommendation models. into a training set (80% of the data set) and a test set (20% of the data set). Following that, the recommendation implements based on the observed interactions in the training set, generating a recommendation list for each user; then, the hit rate of the recommendation list can be calculated by checking how many items recommended to each user meet the observed interacted ones according to the test set. However, such a split strategy inevitably yields deviations of the recommendation accuracy based on diﬀerent realizations (i.e., diﬀerent randomly split training-test sets). To assure the reliability of results, 20 randomly generated realizations are used to implement each of the six recommendation tasks repeatedly and independently in order to obtain the average and standard deviation of results based on the 20 realizations. 6.1.2. Evaluation metrics root mean squared error (RMSE) [296, 403] for evaluations on recommendation accuracy in predicting explicit useritem interactions, and Precision [317], Recall [317, 324], and Rank [404] for those in predicting implicit user-item interactions. that appear both in the training sets and test sets remain while the ones not satisfying the condition as well as the corresponding interactions between them are removed. Denote the observed rating on item j by user i in the test Comprehensive evaluations on conventional recommendation models or graph embedding-based recommendation The three data sets with diﬀerent recommendation scenarios, data scales, and sparsity used in simulation The ratings, as explicit user-item interactions, recorded in the three data sets are slightly diﬀerent in value: To perform the evaluations, each of the data sets organized into tuples (user, item, rating/0/1) is randomly split The evaluation metrics [401, 402] used in simulation experiments include mean absolute error (MAE) [403] and Speciﬁcally, denote the training sets and test sets as Tand T, respectively, in which only the users and items set as r Intuitively, MAE and RMSE quantify the overall deviation between r better recommendation accuracy. might be most likely to be interested as O set as S For simulation experiments in this section, L = 50. Intuitively, Precision and Recall quantify the hit rate of a recommendation list (i.e., in a recommendation list the proportion of items meeting a user’s interacted items within the test set) in two diﬀerent methods. A greater Precision or Recall indicates a better recommendation accuracy. each of his interacted item j ∈ S item j in O A greater overall Rank indicates a better recommendation accuracy. 6.1.3. Evaluated models variants in each category are partially selected as evaluated models in simulation experiments. As benchmarks, some of the most commonly used conventional recommendation models are selected. Tab. 13 gives an overview of them. Table 13: Evaluation models. Corresponding to the data sets, recommendation tasks in simulation experiments are divided into two aspects: predicting explicit and implicit user-item interactions. recommendation models: FunkSVD [79], probabilistic matrix factorization (PMF) [181], and AutoRec [301] are selected from Secs. 3.1.1, 3.1.2, and 4.1.3, respectively. Besides, the factorization machine (FM) [153] selected from Sec.3.1.1 is a representative work tackling the sparsity problem. Meanwhile, as benchmarks, three conventional recommendation models including Overall Average, user-based k-nearest neighbor collaborative ﬁltering (UserKNN) [405], and item-based k-nearest neighbor collaborative ﬁltering (ItemKNN) [28] are selected, among which Overall Average [13] takes a user’s averaged rating on items in the training set as the predicted rating on his non-interacted items; UserKNN is illustrated in Sec. 2.1.4 and ItemKNN is an item-oriented version of UserKNN; HybridKNN is a combination of UserKNN and ItemKNN. The recommendation accuracy of these models in predicting explicit user-item interactions is evaluated by MAE and RMSE. and the predicted one as ˆr. Then, MAE and RMSE are deﬁned as For a user i, denote the generated recommendation list containing the top L predicted items in which the user . Suppose there are M users in the test set; then Precision and Recall are deﬁned as Rank is a more ﬁne-grain metric to evaluate the recommendation accuracy. For a user i, denote the Rank of From Secs. 3, 4, and 5, the representative graph embedding-based recommendation models pioneering subsequent In detail, for recommendation tasks for predicting explicit user-item interactions, three graph embedding-based mendation models are selected from Sec. 3.1.3, including generalized matrix factorization (GMF) [195], multi-layer perceptron (MLP) [195, 406], and neural collaborative ﬁltering (NCF) [195]. Besides, another is TransE [251] selected from Sec. 4.1.1, used to explore the performance of the application of models oriented to general graphs on bipartite graphs. Meanwhile, six conventional recommendation models, including UserKNN [405], ItemKNN [28], hybrid k-nearest neighbor collaborative ﬁltering (HybridKNN) [407], heat spreading algorithm (HeatS) [29], probabilistic spreading algorithm (ProbS) [30], and hybrid spreading (HybridS) [408], are selected as benchmarks, among which HeatS, ProbS, and HybridS are models established on theories of physical dynamics, and HybridS combines the advantages of ProbS oriented to high recommendation accuracy and HeatS oriented to high recommendation diversity [409]. The recommendation accuracy of these models in predicting implicit user-item interactions is evaluated by Precision, Recall, and Rank. 6.2. Results and analysis and implicit user-item interactions are shown in Tabs. 14 and 15, respectively. The hyper-parameter settings of evaluated models are given in Tab. A1 in Appendix A. 6.2.1. Predicting explicit user-item interactions conventional models in recommendation accuracy, in which case this advantage of graph embedding-based models becomes more salient with the increase of data scales while data sparsity seems not to be a decisive factor. However, graph embedding-based models are overall less stable than conventional models. and MovieLens 1M, which are sorted by data scales in ascending order, are 4.33% lower, 6.23% lower, and 15.76% lower than that of conventional models, respectively; at the same time, in terms of the averaged RMSE, it is 2.76% higher, 5.05% higher, and 6.74% lower than that of conventional models, respectively. Apparently, with the increase of data scales, the averaged MAE and RMSE of graph embedding-based models are getting overall lower values than those of conventional models. It seems that the machine learning methodology adopted by graph embeddingbased models exerts in this case, beneﬁting from the better data ﬁtting performance based on larger data volume. However, concerning data sparsity, the results in Tab. 14 seem to show nothing of its role because the increase of density from MovieLens 100K to MovieLens 1M brings overall greater advantages in MAE and RMSE of graph embedding-based models beyond conventional models, but that is opposite as the density continues to increase from MovieLens 1M to Jester 617K. These results are insuﬃcient to prove the existence of an optimal data sparsity making the overall recommendation accuracy of graph embedding-based models better than that of conventional models. On the other hand, Tab. 14 shows that graph embedding-based models have higher standard deviations of the averaged MAE and averaged RMSE than those of conventional models on the three data sets, manifesting the overall lower stability of graph embedding-based models compared with conventional models. Table 14: Results on explicit user-item interactions. All results are averaged over 20 realizations, and the value in brackets is the standard deviation. To make the comparison clearer, the comparatively lower averaged MAE and averaged RMSE on each data set are in bold. Note that n.a. indicates that FunkSVD failed on Jester617K as a consequence of the gradient disappearance and explosion [410]. on the three data sets. It seems that the rationale for modeling all interactions between variables behind FM helps As for recommendation tasks for predicting implicit user-item interactions, three graph embedding-based recom- On the basis of the experiment setups, the simulation results of recommendation models in predicting explicit In predicting explicit user-item interactions (i.e., ratings), graph embedding-based models overall outperform As shown in Tab. 14, the averaged MAE of graph embedding-based models on MovieLens 100K, Jester 617K, In addition, it can be observed in Tab. 14 that FM outperforms all other models in recommendation accuracy uncover richer patterns hidden in sparse data for a more accurate recommendation, particularly on MovieLens 100K with the lowest data density. This observation encourages the research into models like FM that are based on small and sparse data sets but can achieve high recommendation accuracy. Sort of similar to few-shot learning [411, 412], it might be a potential direction of future breakthrough. 6.2.2. Predicting implicit user-item interactions conventional models in recommendation accuracy on the three data sets, in which case recommendation scenario seems to be a decisive factor. Meanwhile, the lower stability of graph embedding-based models than conventional models also embodies in these tasks. Table 15: Results on implicit user-item interactions. The length of the recommendation list is set at 50 (indicated by @50). All results are averaged over 20 realizations, and the value in brackets is the standard deviation. To make the comparison clearer, the comparatively higher averaged Precision, averaged Recall, and averaged Rank on each data set are in bold. Note that, for conventional models, HeatS oriented to recommendation diversity is not taken into account for the averaged results of recommendation accuracy. Rank@50 that graph embedding-based models lower than conventional models are 48.84%, 49.70%, and 60% on MovieLens 100K, respectively; and those on MovieLens 1M are 49.02%, 49.46%, and 80.85%, respectively; and on In predicting implicit user-item interactions (i.e., 0/1), graph embedding-based models overall underperform As shown in Tab. 15, the relative percentages on the averaged Precision@50, averaged Recall@50, and averaged Jester 614K they are 2.56%, 3.11%, and 21.70%, respectively. From these results, it seems that data scale and sparsity are not decisive factors in recommendation accuracy. In detail, increasing data scale no longer brings consistent improvement in recommendation accuracy: from Jester 617K to MovieLens 1M, the averaged Precision, averaged Recall, and averaged Rank of graph embedding-based models are lower than those of conventional models in a wider gap, while the case from MovieLens 100K to Jester 617K is the opposite. Meanwhile, it can be observed that decreasing data sparsity can shorten the accuracy gap between graph embedding-based and conventional models by comparing their averaged results on MovieLens 1M and Jester 617K, while this case does not hold revealed on MovieLens 100K and MovieLens 1M. The decisive factor inﬂuencing recommendation accuracy most likely lies in the recommendation scenario showing a distinctive diﬀerence in the relative number of users and items, for that of Jester 617K, a joke recommendation scenario dominated by the number of users, is distinguished from the cases on MovieLens 100K and 1M about a movie recommendation scenario. When concerning the stability of models, apparently, conventional recommendation models outperform graph embedding-based models in terms of the standard deviations of their averaged results on all three data sets. 6.3. Summary making a trade-oﬀ between graph embedding-based and conventional recommendation in diﬀerent recommendation tasks and some open questions for future research. for recommendation accuracy, graph embedding-based recommendation is a prior choice in predicting explicit useritem interactions, especially when the data scale is large. Meanwhile, in predicting implicit user-item interactions, conventional recommendation maintains a priority to graph embedding-based recommendation without considering the utilization of side information and knowledge. (2) When concerning stability, the conventional recommendation is always a prior choice beneﬁt from its less adjustment for hyper-parameters compared with graph embedding-based recommendation. (3) In addition, recommendation eﬃciency is also an essential factor to be taken into account especially for practical applications. In this regard, graph embedding-based recommendation is always considered as a priority because of its rationale for reusing the embeddings once learned for the recommendation. interactions; and they intrinsically develop from small to large in data scale and from sparse to dense in data sparsity, especially since the utilization of side information and knowledge to enrich user-item relations. Correspondingly, the trade-oﬀ between graph embedding-based and conventional recommendation usually varies in these diﬀerent development stages of recommender systems. In the early stages, the conventional recommendation can perform well on a small data scale, and at the same time its better explainability compared with graph embedding-based recommendation provides clearer views on user behaviors, which is the direct feedback for guiding a more adaptive model conﬁguration. With the increase of data scale when advancing into the following stages of recommender systems, graph embedding-based recommendation should gradually dominate the model conﬁguration for better recommendation eﬃciency and accuracy. In the long run, a versatile conﬁguration mixing conventional and graph embedding-based recommendation models in order to complement each other with their respective advantages is an optimal strategy for improving recommender systems. performance? If it does, how? (2) How to select appropriate models for a speciﬁc recommendation scenario? (3) When designing a recommendation model, is it better to adopt a task-oriented strategy or a generalization-oriented strategy? 7. Discussions and outlook of graph embedding-based recommendation. In summary, as retrospected in this article, bipartite graph embeddingbased recommendation provides ﬂexible frameworks with high extensibility and can capture the dynamics in useritem interactions while usually being incompatible for eﬃciently accommodating side information and knowledge. General graph embedding-based recommendation ﬁlls this gap. Knowledge graph embedding-based recommendation further enhances the abilities in capturing and preserving the multiplicity and evolution of large-scale data. However, among them, ﬂaws and challenges also exist as illustrated in this article. For that, complementing them to each other with their respective unique advantages by such as combining embedding-based methods with path-based methods or extending shallow models to deep learning versions like deep matrix factorization [413] is seemly promising. In conclusion, based on the above simulation results, this section proposes some constructive suggestions on The graph embedding-based and conventional recommendation perform distinctively for diﬀerent tasks. (1) As In practice, recommender systems usually involve both the tasks for predicting explicit and implicit user-item However, there still remain few open questions: (1) Does data sparsity inﬂuence a model’s recommendation The rapid development of computing resources and machine learning methodology contributes to the prevalence To the end, this section puts forward some open questions of graph embedding-based recommendation as well as potential solutions as follows. based recommendation? In making great strides in recommender systems from conventional to graph embeddingbased recommendation, are we really achieve much progress on its performance? This debate has been raised more than once in recent years, where Dacrema et al. [63] threw cold water on the recent DNN-based recommendation and proved that conventional recommendation models can outperform graph embedding-based ones in tasks for predicting implicit user-item interactions. This conclusion is also veriﬁed and extended by the simulation results in Sec. 6. In terms of that, graph topology analysis behind conventional recommendation seemly still plays an essential role. Indeed, recent works combining graph topology analysis on, for example, subgraphs, motifs, and neighborhoods with graph embedding-based recommendation promoted the performance of such as random walking. For future research, analyzing higher-order graph topology [414–417], namely network cycles, cliques, and cavities, is seemly valuable, for such topology is able to uncover higher-order relations between nodes in graph representations beyond pair-wise relations as most current methods constrained to and reveal higher-order dependencies between nodes. For its combinations with graph embedding-based recommendation, it is positive that utilizing higher-order graph topology to guide random walking, to reﬁne or reconstruct graph representations for more suﬃcient expression beyond the conventional three categories in Sec. 2.1.2 (like what hypergraphs [416, 418] do), and to guide label propagation (or message passing) in GNNs seemly are three promising breakthroughs. mendation drives the evolution of recommendation models from machine perceptual learning based on data ﬁtting to machine cognitive learning for reasoning, long regarded as one of the points in fully understanding user’s preferences for items and assuring high model reliability aﬀecting user’s trust in recommendations and helping uncover the black-box from which the mechanism of graph embedding-based recommendation can be reﬂected and improved. For such purposes, future research directions seemly include knowledge-based explainable recommendation [45, 48–50], causal learning (causal inference) [52–58], and neural network interpretability [51]. explicit interactions are one primary source for the recommendation, and side information like social networks is usually a key point in promoting recommendation performance. However, users might be loath to share with recommender systems their explicit interactions with items directly carrying preferences or their social networks, preventing their privacy data from leak and infringement by the public. For this reason, in the future, how to perceive user preferences from the frequency (e.g., click or play counts and viewing time) of implicit user-item interactions would be an essential focus. On the other hand, instead of escaping from utilizing such privacy-sensitive information, federated learning [51, 419, 420] is a promising strategy, which employs user’s private data in distributed training for model parameters on user’s local devices, and then uploads these learned parameters to the terminal server almost without carrying only privacy data. Since user’s local devices could be poor in computing, eﬃcient embedding techniques for large-scale data are necessary. Furthermore, once the explainable recommendation is realized, the elements of embeddings will be given practical meanings, which might be used to reason user’s privacy information such as their preferences for items or anything like that. In this case, how to ensure transmission conﬁdentiality seemly is a necessary issue on federated learning. plines? Eﬀectively and eﬃciently involving rich information in the recommendation requires suﬃcient identiﬁcation and preservation of hidden patterns and multiplicity in the information by, for example, identifying diﬀerent tie strengths in social networks and distinguishing possible abundant semantics from knowledge, in which case techniques of such as community detection and language representation and reasoning play essential roles. In this consideration, social sciences and natural language processing (NLP) seemly are two ﬁelds most likely overlapping with research into graph embedding-based recommendation, particularly since conversational recommender systems (CRS) has become a promising solution to user privacy protection and explainable recommendation, where the techniques of CRS in semantic sentiment analysis and dialogue system are largely based on NLP. demand to quickly access their preferred information from millions of candidates and commercials aim to eﬃciently reap proﬁts from their products through Precise Marketing. The rise of graph embedding-based recommendation is just starting, and challenges are facing, as well as various promising focuses are inspired. Expanding the research into graph embedding-based recommendation from the current focus on recommendation accuracy to improving the diversity and novelty of recommendation would broaden its potential applications. Opening up more applications of graph embedding-based recommendation in real life will give full play to its social and commercial values. To promote model accuracy, how can analysis on graph topology contribute to graph embedding- How to realize the explainability of graph embedding-based recommendation? Explainable recom- What are promising strategies for protecting user privacy in the recommendation? In practice, What would be the overlaps between graph embedding-based recommendation and other disci- In the future, recommender systems will be a perpetual theme in the information age, as long as users have the Acknowledgements as Xu Na for her suggestions on ﬁgures and tables, and Hao Wang for his suggestions on the abstract. The author acknowledges Linyuan L¨u for her valuable discussions and Shuqi Xu for her helpful remarks, as well