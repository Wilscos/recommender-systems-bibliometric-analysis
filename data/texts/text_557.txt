Knowledge graph is generally incorporated into recommender systems to improve overall performance. Due to the generalization and scale of the knowledge graph, most knowledge relationships are not helpful for a target user-item prediction. To exploit the knowledge graph to capture target-specic knowledge relationships in recommender systems, we need to distill the knowledge graph to reserve the useful information and rene the knowledge to capture the users’ preferences. To address the issues, we propose Knowledge-aware Conditional Attention Networks (KCAN), which is an end-to-end model to incorporate knowledge graph into a recommender system. Specically, we use a knowledge-aware attention propagation manner to obtain the node representation rst, which captures the global semantic similarity on the user-item network and the knowledge graph. Then given a target, i.e., a useritem pair, we automatically distill the knowledge graph into the target-specic subgraph based on the knowledge-aware attention. Afterward, by applying a conditional attention aggregation on the subgraph, we rene the knowledge graph to obtain target-specic node representations. Therefore, we can gain both representability and personalization to achieve overall performance. Experimental results on real-world datasets demonstrate the eectiveness of our framework over the state-of-the-art algorithms. • Information systems → Recommender systems. Network Representation Learning; Graph Convolutional Network; Knowledge Graph; Conditional Attention ACM Reference Format: Ke Tu, Peng Cui, Daixin Wang, Zhiqiang Zhang, Jun Zhou, Yuan Qi, and Wenwu Zhu. 2021. Conditional Graph Attention Networks for Distilling and Rening Knowledge Graphs in Recommendation. In Proceedings of the 30th ACM Int’l Conf. on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482331 Nowadays, recommender systems [12,15,47] are widely used in various Internet applications, for example, search engines [2], video websites [7], and E-commerce [31]. The recommender system aims to nd proper items for target users to meet their personalized interests based on the user-item historical network. However, the common critical problem of recommender systems is data sparsity, i.e., the user behaviors or user-item interactions are very limited comparing with the volume of items. Besides, it is hard to recommend items for a new arrival user [32]. To address the limitations, the researchers have proposed to incorporate side information into the recommender systems, such as attributes [18], contexts [20], images [28]. Among the various types of side information, knowledge graphs [9, 44] usually contain more abundant information about the characteristics and connections of nodes. Dierent from the normal network, the knowledge graph is composed of a set of triplets, i.e., <head entity, relations, tail entity>. It can describe not only node attributes, such as< Movie,Genre,Actioner >in Figure 1, but also node relationships, such as< Actor,Friend,Director>in Figure 1. Recently, numerous massive knowledge graphs, such as Wordnet [26], Freebase [3] and DBpedia [1], have been published. These knowledge graphs which describe the facts and common sense can be partly aligned to the nodes of most network applications and be regarded as their side information. They can benet the recommender systems by introducing relatedness among entities, enriching the entity information, and producing the explainability. However, due to the generalization and scale of the knowledge graph, most knowledge relationships are not helpful for a user-item prediction. To exploit knowledge graph to capture target-specic knowledge relationships in recommender systems [29], we need to address the following new requirements: Figure 1: Illustration of Knowledge-aware recommender system. (1) Knowledge Graph Distillation: Knowledge graphs are massive and comprehensive for containing more information. For a specic item recommendation, most knowledge relationships maybe not helpful. Thus learning semantic relationships on the full knowledge graph for a given task is very time-consuming and noisy. Distilling the knowledge graph which gives a small sub-structure related to the target task from the full massive knowledge graph is necessary. It is worth noting that the knowledge graph distillation describes the process of transforming the full knowledge graph into a small concentrated one to capture users’ preference accurately, rather than mimicking a pre-trained, larger teacher model by a small student model like knowledge distillation [16]. (2) Knowledge Graph Renement: Personalized recommendation mines user’s interests from the past purchasing behaviors. In personalized recommendation with knowledge graph, attention mechanisms are usually used to measure user’s preference [43,45]. However, they give the same weights of knowledge edges for dierent target users by edge attentions. For example, in Figure 1, we aim to recommend for targets usersUserandUser. The weights of edge< Movie, genre, Comedy >are only based on the nodesMovieandComedy, and are independent ofUser andUser. ButUserandUsermay have dierent preferences on the genre of the movie. That is to say, the weights of edge <Movie2,genre,Comedy> may be dierent forUser andUser. Thus, for a given target, we should rene the knowledge graph to give dierent weights for all the knowledge relationships instead of its neighbors. Therefore, we believe that a good knowledge-aware network learning method should distill and rene the knowledge graphs. Early knowledge graph-aware algorithms are embedding-based models [5,45]. They learn entity and relation representations rst by knowledge graph embedding algorithms [6,48] and then incorporate the latent embeddings into the recommender system. The direct way to exploit the knowledge graph in the embedding space fails to solve the distillation and renement issues and thus harm the performance. The path-based methods [17,47,49] explore dierent meta-paths from knowledge graphs to build relationships of two objects, such asUser−→ Movie−→Actioner−→ Movie. They distill the knowledge graphs into multiple meta-paths. However, these methods heavily depend on the hand-crafted design of the meta-path. Besides, the multiple paths can not handle the diversity of the users’ preference on the knowledge relationships. Recently, some graph convolutional network-based methods [43,45] are proposed to propagate information on knowledge graphs. They usually use an attention mechanism [39] to produce weights of dierent neighbors for knowledge graph renement. However, the attentions are only based on the two nodes in the same edges and are independent with the target nodes. To overcome the Knowledge Graph Distillation and the Knowledge Graph Renement issue, we propose a novel model named Knowledge-aware Conditional Attention Networks (KCAN). First, we propose a knowledge-aware graph convolutional network to propagate embedding on the knowledge graph by knowledge-aware attention to capture the global similarity of entities like KGAT [45]. After that, a subgraph sampling strategy is designed to sample target-specic subgraphs based on the attention weights to distill the knowledge graph. Also, the proposed Local Conditional Subgraph Attention Network (LCSAN) propagates personalized information on the sampled subgraph based on local conditional attention for rening the knowledge graph. In conclusion, the proposed KCAN can eectively distill and rene the knowledge graph at the same time. It is worthwhile to highlight the following contributions of this paper: •We highlight the importance of distilling and rening knowledge graphs in recommender systems and propose to use a conditional attention mechanism on target-specic subgraphs to capture user preference. •We propose a novel framework named Knowledge-aware Conditional Attention Networks (KCAN) to incorporate the knowledge graph into the recommendation. In particular, the Knowledge-aware Graph Convolutional Network (KAGCN) layer and target-specic subgraph sampling are designed with the Knowledge Graph Distillation issue in mind. Moreover, the Knowledge Graph Renement issue is addressed by the Local Conditional Subgraph Attention Network (LCSAN) layer. •Extensive experiments on real-world scenarios are conducted to demonstrate the eectiveness of our framework over several state-of-the-art methods. The rest of the paper is structured as follows. We rst give a brief review of the related works in section 2. Then we formally dene the solved problems and introduce the details of our proposed model in section 3. In section 4, we report the experimental results. Finally, we give a conclusion in section 5. Knowledge Graph Embedding (KGE) [44] aims to learn latent representations for all components of the knowledge graph including Figure 2: The framework of the proposed KGAN. The framework is composed of four modules: Knowledge Graph Embedding layer, Knowledge Graph Distillation module (KAGCN layer and Target-sepcic Sampling), Knowledge Graph Renement module (LCSAN layer) and Multi-layer Perceptron (MLP) two-tower prediction layer. entities and relations. Then the low-rank embeddings which preserve the inherent structure and knowledge graph can be used in the downstream tasks such as knowledge graph completion and recommendation. The KGE methods can be roughly divided into two classes: translation distance based models [4,24] and semantic matching based models [27,38,51]. The translation distance based models exploit distance-based scoring functions. The existing relationships in the knowledge graph will have higher scores. The scoring functions usually measure the distance of two entities under the space of the relation. For example, TransE [4] assumes ℎ𝑒𝑎𝑑 + 𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛 = 𝑡𝑎𝑖𝑙and use||ℎ𝑒𝑎𝑑 + 𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛 − 𝑡𝑎𝑖𝑙 ||as scoring function on the(ℎ𝑒𝑎𝑑, 𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛, 𝑡𝑎𝑖𝑙)relationships. TransR [24] rst projects the entity representationsℎ𝑒𝑎𝑑and𝑡𝑎𝑖𝑙into the space specic to relation𝑟and measures the distance between entities ℎ𝑒𝑎𝑑and𝑡𝑎𝑖𝑙under that space. The semantic matching models exploit similarity scoring functions. For example, RESCAL [27] uses a bilinear function to learn the similarity of entities. For the recommendation task, the main issue is how to introduce the representations of entities and relations into the recommendation system for enriching the item information. The graph convolutional network (GCN) [22,54] has been proposed to process network data in an end-to-end manner. Earlier works dene the graph convolutional operations in the spectral domain. Bruna et al. [8] are inspired by the graph signal process [35] and dene the convolution in the Fourier spectral domain. Kipf et al. [22] propose to use a rst-order approximation to deal with the complexity issue. Recently, lots of non-spectral GCN, which directly dene convolution on the graph, have been proposed in the spatial domain. GraphSage [13] samples a xed-size neighborhood of each node and then aggregates over it. GAT [40] introduces a self-attention strategy to specify dierent weights to dierent nodes in a neighborhood. However, these works are designed for homogeneous graphs instead of knowledge graphs. Our method is conceptually inspired by GCN. Similarly, Schlichtkrull et al. [33] and Wang et al. [43] also propose to apply GCN and GAT into the knowledge graph. They rst applies knowledge graph embedding [44] methods to obtain representation for entities. Then they propagate the representation over the user-item bipartite graph and knowledge graph to collect the high-order information. Besides, KGCN-LS [42] extends the previous knowledge based GCN method and adds a label smoothness mechanism to propagates the user interaction labels on the knowledge graph. However, the major dierence between our work and these methods is that our model consider the knowledge graph distillation and the knowledge graph renement whiling learning representations. In general, existing knowledge graph-aware algorithms [6,29] can be roughly categorized into two types: (1) Embedding-based methods [50,53]. These methods learn entity and relation representations rst by knowledge graph embedding algorithms and then incorporate the latent embeddings into the original network. CKE [53] combines collaborative ltering with knowledge graph embedding. Cao et al. [5] use the knowledge graph to augment the modeling of user-item interaction while completing the missing facts in the knowledge graph based on the enhanced user-item modeling at the same time. Nevertheless, these methods only use knowledge graph embedding as regularization and lose the rich topology structure of the knowledge graph. Additionally, Wang et al. [45] use graph convolutional network to explicitly model the high-order relations by recursive propagation in the knowledge graph. But all these methods usually have no constraint between the target users and entities and thus hurt the modeling of user preferences. (2) Path-based methods. These methods [17,46,47] regard the knowledge graph as a heterogeneous information network [52] and explore dierent paths from knowledge graphs and measure node relationships by meta-path-based similarity [37]. Besides, some methods leverage the meta-path based random walk to learn better entities representation, such as HIN2Vec [11] and metapath2vec [10]. Dening eective meta-paths usually requires domain knowledge, and it can not generalize to a new dataset. Recently, some works [36,49] propose to apply reinforcement learning to choose meta-paths while training automatically. However, characterizing the user-item similarity by separate paths may lead to information loss. Our works propose to use target-specic subgraphs to distill the knowledge graph. Xiao et al. [34] also use subgraphs to distill the knowledge graph. But it pre-computes all user-item subgraphs by the knowledge graph and just learns on these local subgraphs. In such a way, the semantic similarity between entities on the whole knowledge graph is missing. Our model can not only preserve the global semantic similarity but also dynamically generates target-specic subgraphs to distill and rene knowledge graph for inferring local user preference accurately. In this section, we will introduce the proposed Knowledge-aware Conditional Attention Networks (KCAN). The framework is shown in Figure 2. The model is composed of four modules: (1) Knowledge Graph Embedding layer. It learns representations for each entity and relation by a traditional knowledge graph embedding method TransH. (2) Knowledge Graph Distillation, which propagates embedding with a knowledge-aware attention mechanism and uses a target-sampling strategy to distill the knowledge graph. (3) Local Conditional Subgraph Attention Network (LCSAN), which propagates personalized information on the subgraph based on local conditional attention to rene the knowledge graph. (4) Multi-layer Perceptron (MLP) two-tower prediction layer, which combines the embeddings from the above two layers and predicts the nal results with a non-linear layer. Let𝐺 = (𝑉, 𝐸)denotes a network, where𝑉is the set of nodes and𝐸 ⊆ 𝑉 × 𝑉is the set of edges. For a node𝑣 ∈ 𝑉,N (𝑣) = {𝑢| (𝑣, 𝑢) ∈ 𝐸}is the set of its neighbors. In our setting, we have an external knowledge graphG = {(ℎ, 𝑟, 𝑡) |ℎ ∈ E, 𝑟 ∈ R, 𝑡 ∈ E}, whereℎ,𝑟and𝑡denotes the head entity, relation and tail entity of a knowledge graph triple.EandRare the set of entities and relations of knowledge graphG. Actually, our model can be used in any graph tasks with a side knowledge graph. The target setsTrely on the goal of the graph task. For example, in node classication, the target set is a single nodeT = {𝑣 }. For recommendation task, given a bipartite user-item graph, the goal is to predict the existence or similarity𝑦of a targeted user-item pairT = {𝑢, 𝑣 }. For drug reactions prediction task, the target set is all the drugs which leads reaction togetherT = {𝑑, .., 𝑑}. In our paper, we only discuss recommendation task with bipartite user-item graph𝐺and an external knowledge graphG. A node in network𝐺may also be an entity in knowledge graphG. In general, we use entity to refer all nodes in both𝐺andG. For an entity𝑣, the neighborhood of𝑣 in knowledge graph is denoted asN (𝑣) = {(𝑟, 𝑣)|(𝑣, 𝑟, 𝑣) ∈ G} and we mark the K-hop neighborhood of entity𝑣as𝑆(𝑣). We mark the embedding of entityℎin𝑘-th layer ase∈ Rwith 𝑘 = 0, 1, 2, ..., 𝐾. For simplify, we mark eas e. To distill the whole knowledge graph, we must recognize the importance of the knowledge relationships and eliminate the useless relationships. For this purpose, we rst vectorize the entities by knowledge graph embedding. 3.2.1 Knowledge Graph Embedding. Knowledge graph embedding [6, 19,48] is an eective way to learn entity and relation representations while preserving the topology structure. Since we regard the click relation in the recommendation as a type of knowledge relationships and a user may click many items, so there are a lot of one-to-many mappings in the knowledge graph. Here we use TransH [48] which can solve the one-to-many and many-to-one issues. For a given triple(ℎ, 𝑟, 𝑡), its score or distance is dened as follows: jections of entities embeddingeandeon the relation-specic hyperplanew, anddis the relation-specic translation vector; ∥ · ∥is the𝐿1-norm. The lower score of𝑓(ℎ, 𝑡)indicates the triplet is more likely to be true and vice versa. By projecting to the relationspecic hyperplane, TransH enables dierent roles of an entity in dierent triplets. The loss of knowledge graph embedding is Bayesian personalized ranking loss [30], which aims to maximize the margin between the positive samples and the negative samples: L=− ln 𝜎 (𝑓(ℎ, 𝑡) − 𝑓 where𝜎is the sigmoid function,𝑡is uniformly sampled by replacing𝑡with another entity randomly. To increase the representation ability, we also train the model on both the knowledge graphGand the user-item network𝐺with this loss function. For this purpose, we treat all edges in network𝐺with the same type𝑟. Then the network𝐺can be also seen as a knowledge graph with one and the same relation type. 3.2.2 Knowledge-Aware Graph Covolutional Network. However, the knowledge graph embedding only focuses on the separate knowledge triples, thus it fails to capture the high-order similarity between entities. Inspired by KGAT [45] and KGNN [23], we present to capture the high-order similarity of entities by aggregating the knowledge embedding with graph convolutional network. To distinguish the inuence of dierent types of relations in the knowledge graph, we propose to use a knowledge-aware attention mechanism over the propagation: where𝜋(𝑣, 𝑡)is relation-specic attention coecient. The attention measures the importance of the entity𝑣to𝑡under the relation 𝑟-specic space. When the entity𝑡is closer to entity𝑣under the relation𝑟-specic space, the more information should be propagated. Motivated by this, we dene knowledge-aware attention as follows: wherecos(x, y) =is the cosine similarity. In this way, it will propagate more information for closer entities. Finally, we combine the entity representation eand the neighborhood representation𝑒and update the entities representation ase. For simplicity, we concatenate two representations and add a nonlinear transformation like GraphSAGE aggregator [13]: where||is the concatenation operation, and LeakyReLU is the activation function.W, bare the trainable weights. By this knowledge-aware propagation mechanism, we can preserve the global similarity between entities after several iterations. 3.2.3 Target-specific Sampling. For a given user-item target(𝑢, 𝑣), the attentions of KAGCN rely on the knowledge edges and do not concern with the targets. To capture the local inuence of the targets, we exploit subgraphs instead of the whole enormous knowledge graph. Compared to the meta-path or random-walk based model which uses path to capture the locality, the subgraph can contain diversity. A target set is marked asT = {𝑣, ..., 𝑣} which𝑘 =2 andTis a user-item pair. Due to the locality of the users’ preference, we set the receptive eld of each node𝑣as their K-hop neighborhood 𝑆(𝑣). In a real-world knowledge graph, the number of neighbors may vary signicantly over all entities, and some entities may have a huge number of neighbors. To distill the knowledge graph and reduce the training time, we sample a xed-size set of neighbors for each entity instead of the full neighborhood. We set the sampled size as𝑀. The traditional way is to sample neighbors uniformly like GraphSAGE [13]. However, the knowledge graph is usually noisy and full of target-independent information. Additionally, considering that we have obtained the global attention𝜋(𝑣, 𝑡), which measures the similarity of entities, we sample neighbors with their attention score𝜋(𝑣, 𝑡)as the sampled probability. After getting the sampled𝐾-hop neighborhood asˆ𝑆(𝑣)for each node𝑣, we can obtain the receptive eld of the target set by merging them: The knowledge graph distillation module distills the knowledge graph to a target-specic subgraph as the receptive eld of the target set. In this part, we re-weight the knowledge to rene the knowledge graph. For that, we propagate entities’ information on the distilled target-specic subgraph. To capture the preference of the target to the knowledge relationships, we propose to use a conditional attention mechanism over propagation to rene the subgraph as follows: where𝛼 (𝑣, 𝑟, 𝑡 |T )is the conditional attention which relies on target T, andN(𝑣)is the neighbors of the entity𝑣in the receptive eld of the target setˆ𝑆(T ). To better rene the subgraph based on the target, the conditional attention should contain two aspects: (1) the importance𝛼(𝑣, 𝑟, 𝑡) of the knowledge relationship(𝑣, 𝑟, 𝑡). This part is independent of the target, and it measures the importance of knowledge relationship itself upon the task. If the knowledge relationship is a noise edge in the task, we should reduce its inuence. In the KAGCN part of the section 3.2.2, we had measured it by the knowledge-aware attentionˆ𝜋(𝑣, 𝑡). So we can just set𝛼=ˆ𝜋(𝑣, 𝑡)for simplicity. (2) the importance𝛼(𝑡 |T )of the entity to the target set. This term measures the local preference of the target to the tail entity. The entities which are more similar to the target should be more important for the target. To make the target set and the entity comparable, we calculate the representation of the target set as the concatenation of all contained entities, i.e.,e= ∥e. Subsequently, we can measure the importance of the entity to the target set by their representations: whereais a weight vector.WandWare the linear transformation matrices of targets and entities respectively. Combining the two importance scores, we get the conditional attention: 𝛼 (𝑣, 𝑟, 𝑡 |T ) = somax(LeakyReLU(𝛼(𝑣, 𝑟, 𝑡) ∗ 𝛼(𝑡 |T ))). (10) The larger attention indicates the more important of the knowledge related to the target. By the conditional attention, the local knowledge graph is rened into a weighted graph varying with the target. Finally, similar to KAGCN, the target-specic entities representations can be obtained as follows: e= AGG(e, e Furthermore, in order to capture high-order preference, we further stack more LCSAN layers. Especially, since the subgraphs are composed of𝐾-hop composed neighborhoods, we stack the LCSAN with𝐾times to make sure that the information of each node can be propagated over all the subgraph. In general, the𝑖-th LAGCN is as follows: where 𝑖 = 2, ...𝐾. We set 𝐾 = 2 in all our experiments. After conducting the LAGCN, we obtain the target-specic representationse. To increase the representability of the embeddings, we concatenate it with the output representationseof KAGCN ase= [e∥e]. In such way, we can preserve both global similarity and local preference eectively. To automatically balance the two parts, we feed the concatenated representations einto a multi-layer perceptron (MLP) layers to obtain the nal output representations: whereWandbare learnable weights. For recommendation with user-item(𝑢, 𝑖)prediction, the predicted score is their inner product of user and item representations with the given targetT= {𝑢, 𝑖}: Since we only observe the positive interactions in the recommender systems, we randomly sample some unobserved relations as the negative samples. To make the learned similarities of the positive interactions are larger than the negative samples, we optimize the model with the Bayesian personalized ranking loss [30]: L=− ln 𝜎 (𝑝 (𝑢, 𝑗 |T) − 𝑝 (𝑢, 𝑖 |T where𝐸is the observed edges between user𝑢and item𝑖and𝐸 is the uniformly sampled unobserved edges. Objective Function. Combing the loss in Equation 2 and 16, we get the total objective function as follows: whereΘ = {ˆE, W, b, W, W, W, b|∀𝑖 ∈ {1,2, .., 𝐾 + 1}, ∀𝑗 ∈ {1,2, ..., 𝐾 }}is the set of all parameters, andˆEis the embeddings of all entities and relations.𝜆is the weight of regularization. We optimizeLandLalternatively and Adam [21] is used to optimize these parameters. The detailed training algorithm is shown in Algorithm 1. The learning rate for Adam is initially set to 0.025 at the beginning of the training, and the total epoch number is set as 200. Training Complexity. The time cost of our proposed KCAN mainly comes from two part, KAGCN and LCSAN. The time complexity of KCAN is𝑂 ((|𝐸| + |𝐸|) ∗ 𝐹)where|𝐸|is the number of edges in network𝐺,|𝐸|is the number of triples in knowledge graph Gand𝐹is the dimension of the knowledge graph embedding.Í For LCSAN, the time complexity is𝑂 (|𝐸| ∗ 𝑀 ∗𝐹𝐹), where𝑀is the xed-size number of neighbors while sampling subgraphs, and𝐹is embedding size of𝑖-th layer. We usually set𝑆 as 20. So the total complexity of KCAN is𝑂 ((|𝐸|+|𝐸|) ∗𝐹+|𝐸|∗Í 𝑆 ∗𝐹𝐹). Note that the𝑀and𝐹are small user-specied constants, the proposed KCAN is linear to the scale of network and knowledge graph. In this section, we evaluate our method on three real-world datasets to prove its ecacy. We aim to answer the following research questions: • RQ1: What do the knowledge attention and the conditional attention in the proposed model learn? • RQ2: Does our proposed KCAN outperform the state-of-theart knowledge-aware methods? • RQ3: How do our proposed KAGCN, LCSAN layers aect the performance of KCAN respectively? • RQ4: How do dierent choices of hyper-parameters aect the performance of KCAN? Algorithm 1Training Algorithm for Knowledge-aware Conditional Attention Networks (KCAN) Require:User-item network𝐺; Knowledge graphG. The congurationΘ = {ˆE, W, b, W, W, W, b|∀𝑖 ∈ {1,2, .., 𝐾 + 1}, ∀𝑗 ∈ {1, 2, ..., 𝐾 }}. Compute the loss of knowledge graph embedding in Equation 2 and update knowledge graph embedding𝑒∈ Θ by Adam. Compute the knowledge-aware attentionˆ𝜋(𝑣, 𝑡)in Equation 4. Based on Equation 5, propagate over the knowledge graph and the network by the knowledge-aware attentionˆ𝜋(𝑣, 𝑡) to obtain embedding 𝑒. Based on𝑒and𝑒, compute the predicted score 𝑝 (𝑢, 𝑖|T ) by Equation 15. Compute the target loss in Equation 16 and update the conguration Θ by Adam. In order to comprehensively evaluate the eectiveness of our proposed method KCAN, we use three public benchmark datasets: MovieLens, Last-FM and Yelp. • MovieLensis a widely used benchmark dataset in movie recommendations. It contains the explicit ratings (ranging from 1-5) on the MovieLens website. We transform the rating into implicit feedback where each entry is marked with 1 indicating that the user has rated the item over a threshold score (4 for this dataset) and otherwise 0. • Last-FMis a music listening dataset collected from Last.fm online music systems. The timestamp of the dataset is from Jan, 2015 to June, 2015. To ensure the quality of the dataset, we use the 10-core setting like [45], i.e., retaining users and items with at least ten interactions. • Yelpis obtained from the 2018 edition of Yelp challenge. The items are local businesses like restaurants and bars. Similarly, we also use 10-core setting on this dataset. Besides, we also need to construct a corresponding knowledge graph from a massive knowledge graph for each dataset. Actually, it is a rule-base rough knowledge graph distill process. Microsoft Satoriis used to build a knowledge graph for MovieLens, we rst select a subset of triplets whose relation name contains "movie" and the condence level is greater than 0.9 from the whole knowledge graph. We match the entities with the items by their title names. Similarly, the corresponding knowledge graph is built from Freebase for Last-FM. For Yelp2018, we use the local business information network, for example, category, location, and attribute, as the knowledge graph. The detailed statistics of the user-item networks and the knowledge graphs are summarized in Table 1. We compare our proposed KCAN with four representative types of baselines, including regularization-based (CKE [53]), factorizationbased (NMF [15]), path-based (RippleNet [41]) and GCN-based (KGAT [45]). • CKE[53] combines collaborative ltering with the structural knowledge content, the textual content and visual content in an unied framework. In this paper, we implement CKE by combining collaborative ltering and the structural knowledge content. It uses knowledge graph information as regularization to ne tune the collaborative ltering. • NMF[15] is a novel factorization machine model for prediction under sparse settings. It deepens factorization machine under the neural network framework for learning higherorder and non-linear feature interactions. • RippleNet[41] combines regularization-based and pathbased methods.It stimulates the propagation of user preferences over the set of knowledge entities. • KGAT[45] introduces graph attention into recommendation and explicitly models the high-order connectivities in knowledge graph in an end-to-end fashion. We uniformly set the embedding size as 16 for all methods. The hidden size of our method and KGAT is set as a tower structure with 16, 8, 8. For our model and RippleNet, the number of hops is set as 2. The dropout rates of NFM, KGAT, and our model are set as 0.1. The xed-size number of neighbors while sampling subgraphs is set as 20 for our model. We do grid search from {0.01,0.025,0.05,0.1}to tune the learning rate parameter and from {10,10,10,10,10}for the weights of the𝐿normalization. All the experiments are conducted on Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz with GeForce GTX Titan X GPU. In this subsection, we show what the knowledge attention and the conditional attention in the proposed model learn on the MovieLens dataset in Figure 3. Figure 3 (a) shows the global knowledge attention in the Equation 4. The attention is only related to knowledge relationships which is the same as the KGAT. From this way, we can infer user preferences. The larger attention means the more important edge. As we can see, the path𝑢−→ 𝑖−→ 𝑖has the highest attention score between nodes𝑢and𝑖. We can explain that the item𝑖is recommend to user𝑢because the user𝑢clicked item𝑖which is similar to item 𝑖. Besides, the edge 𝑢−→ 𝑖with small weights 0.001 has less information. We can lter it out to distill the knowledge graph. The Figure 3 (b), (c) show the local attention (Equation 10) with targets(𝑢, 𝑖),(𝑢, 𝑖)respectively. The red nodes are the nodes which needs to be predicted. We rene the target-specic subgraph with dierent weights by the local attention. We can see that there are dierent attentions for the same edge𝑢−→ 𝑖 in the two subgraphs. The edge𝑢−→ 𝑖is more important for the prediction of(𝑢, 𝑖)than the prediction of(𝑢, 𝑖). All the baselines can not learn this information which is important for predicting user-item pair. We evaluate our method in two recommendation scenarios. (1) TopK recommendation. We use the leave-one-out strategy, which has been widely used in previous works [15,30] to evaluate the recommendation performance. For a user, we randomly sample 100 items that are not interacted by the user, ranking the test item among the 100 items. The performance is judged by Hit Ratio@K (Hit@K) and Normalized Discounted Cumulative Gain@K (NDCG@K) [14]. If the true test item ranks in the top𝐾lists, the Hit@K will be one otherwise zero. Compared with Hit@K, NDCG@K pays more attention to the ranking order. The more front position of the true item will have a larger NDCG@K. Then we compute the two metrics for each user and obtain the average score at𝐾 =10. (2) Click-through rate (CTR) prediction. We predict the score of each user-item pair, including positive items and randomly sampled negative items, by the trained model. The evaluation metric in CTR prediction is set as the area under the curve(AUC). The AUC is equivalent to the probability of positive samples are ranked higher than negative samples [25]. The performance comparison results of the top-K recommendation and CTR prediction are presented in Table 2 and 3, respectively. The observations are illustrated as follows: •Our proposed KCAN achieves signicant improvements over the baselines on all the datasets in both top-K recommendation and CTR prediction. It demonstrates the eectiveness of our proposed method KCAN. Moreover, KCAN outperforms 𝑟0.044𝑟0.043𝑟0.001𝑟0.022 𝑟0.044𝑟0.013𝑟0.085𝑟0.062𝑟0.051 Figure 3: (a) Example from MovieLens dataset.The knowledge attention in the whole knowledge graph and the user-item graph. (b) The conditional attention on the target (𝑢, 𝑖)-specic subgraph. (c) The conditional attention on the target (𝑢 specic subgraph. The red nodes are the nodes which needs to be predicted. Table 2: Hit@10 and NDCG@10 in top-K recommendation. Figure 4: Left: AUC w.r.t. the log value of the weights of 𝐿2 normalization log(𝜆). Right: AUC w.r.t. the xed-size number of neighbors 𝑀 in LCSAN. the KGAT in all experiments, and it indicates the eectiveness of conditional attention for rening the knowledge graph to capture the local user preference. •CKE is the worst one in most cases. It demonstrates that directly using the knowledge graph as a regularization can 𝑟0.011𝑟0.031 𝑟0.021𝑟0.050𝑟0.050 not make full use of the knowledge graph. Besides, the NFM outperforms CKE because it preserves the second-order similarity implicitly by the input of the cross feature. •KGAT achieves better performance than CKE, NFM, Ripple in Last-FM and Yelp datasets. It veries the eectiveness of propagating information in the knowledge graph to preserve global similarity. •In MovieLens dataset, CKE, NFM, KGAT have poorer performance than the performance in the other two datasets. The reason is that MovieLens dataset has very few users and items, and a large number of knowledge graphs, it is more dicult to exploit knowledge graph eectively in this dataset. However, Ripple achieves a good performance in this dataset. Ripple uses paths from an item in a user’s history to a candidate item, and the path-based methods can capture the local preference. It indicates the importance of preserving local preference instead of the whole knowledge graph, especially in a massive knowledge graph. Moreover, the improvements of our method KCAN over Ripple may prove that the way we distill the knowledge graph is better than than the way using multiple paths. In this part, we evaluate how dierent parts of KCAN aect the performance. To study their respective eects, we compare our KCAN with three variants: (1)KCAN. It removes the LCSAN layer from KCAN. (2)KCAN. It removes the KAGCN layers and uses the knowledge graph embedding as the input of LCSAN. (3)KCAN. It removes both the LCSAN and KAGCN. The result is shown in Table 4. TheKCANperforms the worst among the three variants, it demonstrates the two layers, KAGCN and LCSAN, are benecial for knowledge-aware recommendations. This conclusion has been double veried since the three variants are worse than the KCAN. Besides, the KCANoutperforms theKCAN, which demonstrates the necessity of distilling and rening the knowledge graphs. Table 4: Eects of KAGCN, LCSAN on top-K recommendation. Figure 5: The loss w.r.t the number of epoches. In the section, we evaluate the scalability and how dierent settings of hyper-parameters aect the performance of KCAN. Especially, we evaluate the eect of the weights of𝐿2 normalization𝜆and the xed-size number of neighbors𝑀in LCSAN. Besides, we also measure the convergence of the alternative optimization in this part. For brevity, we only report the AUC results with Last-FM datasets, and similar trends can be observed on the other datasets. 4.5.1 Weight of𝐿2 normalization𝜆. We show how the weights of 𝐿2 normalization𝜆aect the performance in Figure 4 Left. The𝜆 varies from{10,10,10,10,10}. When𝜆increases from 10to 10, the performance is improved, demonstrating that the 𝐿2 normalization can avoid overtting to some extent. When𝜆 increases from 10to 10, the performance becomes poorer. It makes sense since the normalization is much larger than the losses which need to be optimized in this situation. 4.5.2 The fixed-size number of neighbors𝑀. In the target-specic sampling step, we sample a xed-size set of neighbors to distill the knowledge graph and reduce the training time. We vary the xed-size number of neighbors𝑀from{5,10,20,30}. The result is shown in Figure 4 Right. We can see that the performance raises rstly when the xed-size number of neighbors𝑀increases. This is reasonable because a larger𝑀can embody more information in the subgraphs. After𝑀larger than 20, the curve is relatively stable. It demonstrates that our algorithm is not very sensitive to 𝑀. Besides, we can nd that a small𝑀, such as 20, can also achieve a relatively good result. 4.5.3 The convergence of the alternative optimization. In the KCAN model, an alternative optimization is used to optimize the loss of knowledge graph embeddingLand the loss of target prediction L, respectively. In this part, we measures the convergence from the experiment by plotting the loss curve. The number of epoches varies from 1 to 200. The result is shown in Figure 5. We can see that the two curves descends very quickly, indicating the eciency of the framework. About 100 epoches, the two loss are relatively stable, demonstrating our KCAN can converge fast. Besides, we can nd that the loss curve ofLhas very tiny shakes. It is reasonable because the process of sampling target-specic subgraph will introduce sampling bias. In this paper, we investigate the problem of incorporating the knowledge graph into the recommender system. To address the knowledge graph distillation issue and knowledge graph renement issue, we propose a novel knowledge-aware graph convolutional network model named Knowledge-aware Conditional Attention Networks (KCAN). The framework consists of two main modules, the Knowledge Graph Distillation module, and the Knowledge Graph Renement module. We propagates embedding with knowledgeaware attention in a recursive way to capture the global similarity of entities. After that, a subgraph sampling strategy is designed to distill the knowledge graph based on the attention weight of the KAGCN. Also, the LCSAN propagates personalized information on the sampled subgraph based on local conditional attention for rening the knowledge graph. Benetting from the two layers, the proposed KCAN can eectively preserve topology similarity and distill and rene the knowledge graph at the same time. Extensive experiments on three real-world scenarios are conducted to demonstrate the eectiveness of our framework over several state-of-the-art methods. In the future, we aim to further reduce the time complexity of KCAN. Another interesting direction is to make the recommendation more explainable. This work was supported in part by CCF-Ant Group Research Fund.