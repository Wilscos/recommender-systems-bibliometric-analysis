Abstract. This paper discusses a data-driven, empirically-based framework to make algorithmic decisions or recommendations without expert knowledge. We improve the performance of two algorithmic case studies: the selection of a pivot rule for the Simplex method and the selection of an all-pair shortest paths algorithm. We train machine learning methods to select the optimal algorithm for given data without human expert opinion. We use two types of techniques, neural networks and boosted decision trees. We concluded, based on our experiments, that: 1) Our selection framework recommends various pivot rules that improve overall total performance over just using a ﬁxed default pivot rule. Over many years experts identiﬁed steepestedge pivot rule as a favorite pivot rule. Our data analysis corroborates that the number of iterations by steepest-edge is no more than 4 percent more than the optimal selection which corroborates human expert knowledge, but this time the knowledge was obtained using machine learning. Here our recommendation system is best when using gradient boosted trees. 2) For the all-pairs shortest path problem, the models trained made a large improvement and our selection is on average .07 percent away from the optimal choice. The conclusions do not seem to be aﬀected by the machine learning method we used. We tried to make a parallel analysis of both algorithmic problems, but it is clear that there are intrinsic diﬀerences. For example, in the all-pairs shortest path problem the graph density is a reasonable predictor, but there is no analogous single parameter for decisions in the Simplex method. Keywords: Machine learning, empirical performance of algorithms, algorithm analysis, simplex method, all-pairs shortest paths, neural networks, decision trees What is the best way to select an algorithm? Two diﬀerent algorithms for the same computational task have diﬀerence performances: one algorithm is better on some inputs, but worse on the others. Over the years there have been various theoretical frameworks answering this question. Worst-case analysis aims to ﬁnd the extreme instances that strain the performance the most. Average-case analysis on the other hand assumes that input instances come from a ﬁxed probability distribution, thus we can talk about average running time or average complexity. More recently, the Smooth analysis is a hybrid of the worst-case and average-case analysis of algorithms where one measures Supported by NSF grants DMS-1818969 and HDR TRIPODS UC Davis Computer Science. UC Davis Mathematics. UC Davis Mathematics. the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. The performance of many algorithms varies dramatically on the types of input one provides, thus the theoretical evaluations often say nothing useful for the non-expert user. How is a non-expert user supposed to make the right algorithmic choices when a large number of choices are possible? How can someone make reasonable consistent choices of parameters for tuning complicated algorithms? Here we propose an ML-based methodology to make those choices in the absence of an expert or to corroborate what an expert can suggest. Consider for example the very famous Simplex method of G. Dantzig [17]. This is a well-studied algorithm, researchers have found the worst-case behavior of Simplex algorithm is exponential, for most known deterministic pivot rules [2, 4,26,28,32, 37, 46] and randomized pivot rules [30, 34]. On the other hand, under a speciﬁc probability distribution for input instances, the average running time of Simplex algorithm is polynomial in terms of the input size [14]. Similarly, the smooth analysis shows that the Simplex method is eﬃcient [16]. Despite the theoretical success, neither of the three theoretical evaluations matches the empirical performance of the Simplex method, which is known to be very fast in practice. Today the Simplex method has been investigated and improved enormously from its original version [12]. It is known that the running time or number of iterations for the Simplex method depends not just on the input data, but how we tune the algorithm itself. E.g., what choice of pivot rule shall we make? This is a question that has been answered by experts by ﬁxing a default pivot rule, which often performs well, but may not be always the optimal choice (this choice is often steepest edge pivot rule). As a proof of concept we demonstrate how machine learning can recover the hard-won wisdom of experts. The purpose of our paper is to discuss a pragmatic framework for empirical algorithm selection tuning and comparison. In the present article we demonstrate a machine learning-based selection and tuning of algorithms. Our framework is data-driven, empirically-based, and can help non-experts make reasonable consistent algorithmic decisions without prior knowledge of the algorithms. Users of algorithmic methods often have no knowledge of the worst-case examples, nor can they assume to know the exact distribution of their data. Users only have access to data sets. The simple principle we propose here is that, if one has suﬃciently many data instances, one can create a practical machine learning recommendation system to eﬃciently automate the selection of algorithms or their parameter conﬁgurations for concrete data sets, with the intention to speed up computation. We picked two case studies to illustrate the framework, but it would apply almost in the same way to other algorithms where the input is based on matrices. Algorithm selection has seen a strong surge in both practical and theoretical research and we only touch the fraction of the literature that we know deals with algorithm similar to our case studies (for much more we recommend [6, 29, 40, 42, 53] and the many references therein). Several authors have been directly concerned with algorithm selection and tuning for discrete algorithmic problems (see e.g., [3, 5, 36] and the many references therein). The papers [9, 52] are great surveys of uses of learning in combinatorial optimization. In [10] the authors redeﬁne mixed integer convex optimization problems as a multi-class classiﬁcation problem where the machine learning predictor gives insights on the optimal solution. Dai et al. [35] develop a method to learn heuristics over graph problems. Several authors have proposed ways to use machine learning to select the best branching rules (see [1,36]). Machine learning methods have also been useful in aiding the selection of reformulations and decompositions for mixed-integer optimization [13,39]. Some libraries organize data for various NP-hard tasks (where the aim is to predict how long an algorithm will take to solve concrete instances of NP-complete problems, or to choose best approximation schemes tailored by instances) [11, 38, 48]. In fact the approach we present here is a simpliﬁcation of the empirical hardness model to predict the running time of algorithms applied to improve logic satisﬁability (SAT) solvers [20, 41]. There are also now a number of well-established software implementations for algorithm tuning (see [21, 23] and the many references therein). In this work we present two case studies of ML-algorithm selection, where we see the behavior is clearly dependent on the right choice of algorithm or algorithm version: First, the Simplex method. It is widely used in solving linear programming (LP) problems. Geometrically, Simplex algorithm starts on a vertex of the feasible region (which is a polytope), and generates a path via improving edges until optimum is reached. A pivot rule helps to decide which improving edge to pick if there are multiple choices. In this case, we are interested in applying diﬀerent machine learning models to study and improve the choice among ﬁve pivoting rules for the Simplex algorithms on linear programming based on features of diﬀerent LP instances. Second, we do algorithm selection on the problem of computing the shortest paths between all pairs of vertices on a graph. The All-Pairs Shortest Path (APSP) algorithms that we consider for the comparison are All-Pairs Dijkstra [18]; Floyd-Warshall [24], which iteratively improves the lengths of shortest paths using dynamic programming; and ﬁnally an algorithm proposed by Peng et al. [50], which is a dynamic programming improvement of All-Pairs Dijkstra to skip extraneous computations. We demonstrate that the total performance of algorithms, when guided by Machine Learning (ML) decision-making, is clearly faster than using a single static choice for these algorithms. We implemented two ML methodologies, boosted decision trees and neural networks. We tested two diﬀerent schemes of predicting the fastest algorithm: direct classiﬁcation and run time prediction. In direct classiﬁcation a machine learning method is trained to predict which algorithm will run the fastest. In the run time prediction setting, a machine learning method is trained to estimate how long an algorithm will run on a particular instance, then we pick the algorithm that is expected to run the quickest. In addition, we tested diﬀerent data representations and features. We discuss the details in each of the two situations. Next we present the details and in the end we discuss conclusions. We begin with some standard deﬁnitions related to linear programming and the Simplex algorithm. This introduction is meant to be brief, and we refer to textbooks (see [17, 51]) for more extensive background knowledge. For the remainder of this section we let A ∈ R given. Deﬁnition 1. A linear program in standard form is the optimization problem of maximizing c subject to Ax = b and x ≥ 0. Deﬁnition 2. We say that B ⊆ [n] with |B| = m is a basis if and only if the columns of A linearly independent, or equivalently A basis B if Ax = b, x≥ 0 and for all j 6∈ B: x= 0. Deﬁnition 3. The vector of reduced costs for a basis B is deﬁned as We say j ∈ [n] is an improving pivot with respect to B if and only if z> 0. With the deﬁnition of an improving pivot, the Simplex method can be summarized as a process of starting with a feasible basis B and updating with improving pivots until no such improving pivots exist. Now we present three basic pivot rules that our experiments consider: 1. Dantzig: this rule was suggested by Dantzig [17]. In every iteration Dantzig’s rule picks the non-basic variable with the largest positive reduced cost to be the entering variable. 2. Greatest Improvement: this rule picks the improving pivot that results in the largest increment of the objective function. 3. Steepest edge: this rule performs the improving pivot with the largest rate of increment of objective function per distance traveled along the improving edge. In the following example we brieﬂy explain how diﬀerent pivot rules choose diﬀerent pivots using tableaux. We can see that x, x, xhave negative coeﬃcients and are the potential entering variables. For Dantzig’s rule, we pick x. For greatest improvement, we pick xsince the increment of objective function by each variable is x: 12.5, x:, x: 12. And for steepest edge, we pick xsince the rate of each variable is x:, x:, x:. We study the pivoting strategies for primal Simplex algorithm implemented in DOcplex [49]. These include Dantzig’s rule, hybrid (DOcplex’s default), greatest improvement, steepest edge and devex. Hybrid is a pivot rule DOcplex implemented as default, which uses Dantzig’s rule in the earlier iterations when there are a lot of choices of improving pivots and switch to steepest edge later. Devex is an approximate version of steepest edge developed by P. Harris [31]. DOcplex also implemented a steepest edge with slack initial norms, which is slightly cheaper in computation. But in our testing, it usually is not better than steepest edge. As a consequence it was not included in the algorithm portfolio. The existing libraries (MIPLIB 2017 [45], NETLIB [19] etc.) of linear programming or integer programming are too small for our training purpose. Hence we generated our own data for training and testing. We adapted the algorithms introduced by Bowly et al [15]. Their method involves generating constraint matrix A, and a solution pair (α, β). They used A, α, β to generate the ﬁnal linear problem maximizing cx subject to Ax ≤ b. For simplicity, we replaced the generation of variable constraint graph by generating Erd˝os-R´enyi (ER) random graphs. For training and validation set, we generated 24634 instances of linear programming problems with number of constraints ranging from 120 to 200 and number of variables ranging from 50 to 100. For testing, we generate 7279 more instances. Note that these linear programs will most likely be characterized as “easy” problems by MIPLIB 2017. For the ER random graphs, the parameter p was drawn from U{0.2, 0.8}. For other hyperparameters in generating the LP instances, we draw the coeﬃcient mean µ uniform distribution U{1, 10}, primal versus slack basis γ from U{0.2, 0.8}, fractional primal λ from N (0, 1) and Beta fraction a = 0.5. After generating the LP instances, we solve our LP problems using primal Simplex solver in DOcplex with default initialization. We store the number of iterations for each instance using diﬀerent pivot rules. Note that the LP instances we generate may have degeneracy, and empirically there is a high likelihood of degeneracy where the constraint matrix is low-density. We have two diﬀerent ways of choosing features for the linear programming instances. The ﬁrst method we use is a bag-of-features, where we add features based on heuristics from previous studies on the Simplex method. Apart from m, n the number of constraints and the number of variable, we add three sets of features: variable constraint graph features, coeﬃcient values, and normalized coeﬃcients. Variable constraint graph features include the minimum, maximum, mean, and standard deviation of the degree sequences of variable nodes and constraint nodes. Coeﬃcient values include the statistics of the coeﬃcient matrix A, the constraint vector b, and the objective function c (i.e. he minimum, maximum, mean, standard deviation, norm of the vector, and the smallest non-zero absolute value). Finally, normalized coeﬃcients are the statistics of row and column normalized coeﬃcients ({ The other way we have implemented features related to the coeﬃcient matrix A, is the Truncated Singular Value Decomposition (SVD), which is a method of dimension reduction [43]. The truncated SVD of a matrix A ∈ R where U ∈ R Multiplying U by Σ allows for the computation of an m × k matrix. Applying this procedure again to (U Σ) choose k = 20 in this experiment for the best performance. We still include the features of statistics of the constraint vector b and objective function c. We train four models to choose which pivoting strategies will perform the best on each LP instance. Two models use the bag of features that we choose for LP problems, and the other two use the SVD to replace the features of the coeﬃcient matrix A. Boosted Trees We train two boosted trees to predict the best pivot rule for each LP instance. The ﬁrst one is an empirical hardness model, that is, for all ﬁve pivot rules, we use regression on the features we selected to predict number of iterations that the solver will take using certain pivot rule. The second model is a boosted tree classiﬁer using truncated SVD as features. will then compute a k × k matrix with similar features to the original matrix A. We Bag-of-features boosted trees The ﬁrst model is an empirical hardness model, where we use gradient boosted trees to do regression and predict the number of iterations each pivot rule would cost. Table 1 shows the hyperparameters for diﬀerent pivot rules. This model results in a 67.78% accuracy on the test set with 178.5934 iterations on average. Figure 1 shows the gain of features for boosted tree regressors. We can see that apart from number of constraints and number of variables, some of the common features that are important are: maximum number of constraint degree, max and mean of variable degree, min and mean of coeﬃcient matrix A, min, mean, norm and standard deviation of objective function c etc. One could take the subset of important features to train smaller models, which makes the training much faster, but the accuracy will drop to 66.44% with 178.7804 iterations on average. Boosted tree classiﬁer The other boosted tree uses the truncated SVD with k = 20 as part of the features while keeping the features of constraint vector and objective function. This random forest contains 102 trees with minimum child weight of 5, maximum depth of 5, learning rate of 0.1, subsample and column subsample by tree ratio of 0.8. This model results in a 67.15% accuracy on the test set with 179.0714 iterations on average. The feature importance is shown in Figure 2. As we can see, number of variables and constraints (the ﬁrst and second feature), as well as features related to constraint vector and objective function are of great importance. Meanwhile, the diagonal entries of the SVD matrix have a relatively high importance. Neural Networks We train two models to classify which pivoting strategies will perform the best on each LP instance. The ﬁrst model uses the bag of features that we choose for LP problems, and the second model uses the truncated SVD matrix to replace the features of the coeﬃcient matrix Bag-of-features Neural Network We ﬁrst train a neural network using features of LP instances we pre-selected. The architecture of the network consists of four hidden layers of ReLU activation function with 64 neurons. Each hidden layer has a dropout of 0.1. The output layer contains ﬁve neurons with the softmax activation function. We train the neural network to minimize the categorical cross-entropy loss with the RMSProp optimizer with a learning rate of 0.01 and momentum of 0.2. We train with a batch size of 64 for 50 epochs. This model results in a 62.2% accuracy on the test set with 179.279 iterations on average. Figure 6 (in Appendix) plots the accuracy and loss during each epoch. Truncated SVD Neural Network We then train a neural network using truncated SVD matrices as features for coeﬃcient matrix A while keeping the features of constraint vector and objective function. The architecture consists of four layers of 512 hidden units with ReLU activation function. The output layer contains 5 neurons with the softmax activation function. We train the neural network to minimize the categorical cross-entropy loss with the ADAM optimizer with a learning rate of 0.001. We train with a batch size of 64 for 100 epochs. This model results in a 72.78% accuracy on the test set with 179.18 iterations on average. Figure 7 (in Appendix) plots the accuracy and loss during each epoch. Here we summarize the performance of our models. Table 2 shows the average number of iterations (from the most to the fewest) if we use certain pivot rule or follow our models to solve the LP instances in the test set. It also demonstrates the prediction accuracy of our models. Table 3 shows the instance-wise comparison between our model recommendations with the most popular steepest edge pivot rule. We can see that the best performance of our four models is 69.06% of the number of iterations steepest edge will take. And they vary on the worst case behavior, with our gradient boosted tree regressor being the most consistent: their worst case will only cost 174.19% of what steepest edge will perform. We run the Wilcoxon Signed Rank test on each of the test instances between our models and steepest edge pivoting strategy. We can see in Table 3 that except for SVD20 NN model, the other three models have signiﬁcant improvement compared to using steepest edge pivoting strategy on all test instances. Table 3. Comparison between our models and steepest edge pivot rule on test set per instance. The All-Pairs Shortest Path (APSP) algorithms that we consider for the portfolio are: 1. All-Pairs Dijkstra’s Algorithm [18]: This algorithm simply applies the standard Dijkstra’s algorithm for every possible starting node to calculate every possible pair of shortest paths. 2. Floyd-Warshall [24]: This algorithm is a dynamic program that stores all the lengths of shortest paths between every pair of nodes. It begins by initializing the path lengths to the weight of the path connecting every node, or ∞ if no edge exists. Then it loops through all possible pairs of nodes n times to incrementally update the shortest path between them by checking all intermediary nodes. 3. Peng [50]: Applies All-Pairs Dijkstra’s using dynamic programming to store solutions in order to reduce redundant calculations. In addition, the nodes in the graph are sorted in decreasing order by their degrees in order to maximize the number of calculations that are skipped. The standard Dijkstra’s algorithm may be faster if the overhead cost of sorting the nodes is too high on a particular graph. Two other commonly used APSP algorithms were also considered initially: All-Pairs BellmanFord [8, 25] and Johnson’s algorithm [33]. These algorithms have the beneﬁt of being applicable to graphs with negative edge weights, making them more versatile. However, in our testing we noted that these algorithms were always the slowest for all test cases, so these algorithms were not included in the algorithm portfolio. Graphs for the training and test sets were generated randomly using four methods: Erd˝os-R´enyi (ER) random graphs [22], Barab´asi-Albert (BA) random graphs [7], Watts-Strogatz (WS) small world random graphs [47], and ﬁnally geometric random graphs. Graphs were generated with nodes between 20 and 1250. After a graph is generated, every edge within it is given a random integer weight between 1 and 100. The parameters for the generation of the graphs were chosen in order to produce a variety in the densities of the graphs while also trying to ensure the graphs are connected. The training set consists of 2309 graphs and the test set contains 1125 graphs with nodes between 20 and 500. For ER graphs, the parameter p was set to be a random number between a transition where the graph will likely be connected. For BA graphs, the parameter m is an integer chosen uniformly between 5 and n − 1. In the WS model, the mean degree K was picked randomly between ln n and n − 1, and the parameter β was chosen randomly between 0 and 1. The geometric graphs were constructed by ﬁrst generating n points within the unit cube. If two points are within a distance of  of each other, an edge is added between them. The value of  was chosen to be between After generating each graph, we run all three algorithms on them and record how long each algorithm takes to run, along with the algorithm that runs the fastest. In the classiﬁcation setting, the label for each sample is the algorithm that performed best, and in the regression setting the label is the runtime for the respective algorithm. 10% of the training data is set aside for validation. Assuming 100% accuracy in predicting the fastest algorithm, the total runtime of the best algorithm on each graph of the test set is 4356 seconds. In addition, we test the neural network on a real-world Facebook social network, provided by Stanford [44]. It contains 4039 nodes and 88,234 edges, and has a topology that is not very well represented by the training set alone. Figure 3 displays each graph from the test set plotted by its density vs its number of nodes, labeled by the algorithm that runs fastest on it. The ﬁgure shows that Dijkstra only runs fastest on graphs with a low number of nodes and low density, Floyd-Warshall tends to run fastest on graphs with high density, and Peng’s algorithm is fastest on most lower density graphs. Fig. 3. Graphs from the test set plotted as density vs number of nodes and labeled with the fastest algorithm We represent graphs for training the models in two ways. The ﬁrst method we use is the Truncated Singular Value Decomposition (SVD) of the adjacency matrix as described earlier in the pivot rule selection. The second representation is a sampling of the degree sequence of the graph. In the Truncated SVD representation, we use k = 20 as the parameter. We lose information on the number of nodes and edges of the graph, so we add 1/n and the density as features. It has been shown that the density of a graph is an important feature to determine when a shortest path algorithm is faster [27]. Given a degree sequence of a graph, and some parameter q, we wish to reduce the degree sequence down to q elements. We take elements with indices bci, for i = 0, 1, . . . , q − 1. This representation does not maintain the size of the graph and the values are not normalized. So we add 1/n as a feature and divide every element of the sequence by n. Peng’s algorithm is optimized for graphs with few high-degree nodes and many low-degree nodes, so this representation could capture the information necessary to distinguish when an algorithm will be faster. Boosted Trees We train two diﬀerent boosted trees to predict the fastest APSP algorithm on a given graph. The ﬁrst model uses the truncated SVD of the adjacency matrix as its features. The second model uses a sample of the degree sequence of the graph. Hyperparameters were tuned via grid search on the parameters of the representation, the maximum depth of the trees, minimum child weight, learning rate, and subsample rate. Truncated SVD Boosted Tree We begin by training a boosted tree random forest with truncated SVD parameter k = 5. The forest consists of 64 boosted trees, with maximum depth 6, minimum child weight of 1, and learning rate 0.1. This random forest results in an accuracy of 93.6% and the total time taken on all the graphs using its predictions is 4359 seconds. Figure 4 plots the gain for each feature. We see that the most important feature is the graph density (second feature), with the diagonal elements of the matrix having relatively high importance. Fig. 4. The gain for the SVD boosted tree with k = 5. First two features are number of nodes followed by graph density. The remaining are elements of truncated SVD matrix. Degree Sequence Boosted Tree Another boosted tree is trained using the sampled degree sequence representation. This random forest contains of 32 trees with a depth of 8, minimum child weight of 1, and learning rate 0.1. The parameter q is set to 50. This forest achieves 93.4% accuracy with a total time taken of 4359 seconds. The total gain for the features is shown in Figure 5. The number of nodes (ﬁrst feature) has high importance, as well as the degrees of the nodes about 3/4 the way through the degree sequence. Fig. 5. The gain for the degree sequence boosted tree with q = 50. First feature is the number of nodes, the remaining are the elements of the reduced degree sequence. Neural Networks In total we train three models to classify which algorithm will perform the best on each instance. The ﬁrst model is based on a collection of neural networks that predicts the running time of each algorithm. The second uses the truncated SVD as its representation, and the ﬁnal model uses the degree sequence for its representation. Runtime Prediction Model The ﬁrst model is based on runtime prediction. For each of the three algorithms, a neural network is trained to predict its runtime on a given instance, then the algorithm with the fastest predicted runtime is chosen as the label for the classiﬁcation. The networks have four hidden layers, the ﬁrst layer uses the ELU activation function, and the rest use the ReLU activation function. The ﬁrst layer consists of 512 neurons, and the other three have 256 neurons each. Dropout with p = 0.25 is added to all the layers. The networks are trained to minimize the mean squared error. The classiﬁcation accuracy for these networks is 75.4%. The total runtime of all the graphs in the test set using the algorithms predicted by this classiﬁer is 4594 seconds. Figure 8 (in Appendix) plots the accuracy and loss during each epoch. Truncated SVD Neural Network The next model trained is a neural network to classify which algorithm will run fastest on each graph using the truncated SVD representation with k = 20. The architecture of the network consists of ﬁve hidden dense layers with 128 neurons each. Dropout with p = 0.5 is added to each layer to prevent overﬁtting. The ﬁrst hidden layer has the ELU activation function, and the other four use the sigmoid activation. The output layer contains three neurons with the softmax activation function. We train the neural network to minimize the categorical cross-entropy loss with the ADAM optimizer with a learning rate of 0.001. We train with a batch size of 64 for 300 epochs. Figure 9 (in Appendix) plots the loss and accuracy of the model at every epoch. This model achieves an accuracy of 93.7% and using its predictions on the test set causes a total running time of 4360 seconds on the test set. Degree Sequence Neural Network The ﬁnal model we trained is one that uses the sampled degree sequence with q = 50 as the representation. The architecture for this neural network is two hidden layers with 128 neurons in the ﬁrst layer, and 64 neurons in the second layer. The ﬁrst hidden layer uses the exponential linear activation function as before, and the other one uses the sigmoid activation function. Every layer has dropout added with p = 0.5. The neural network is trained to minimize the categorical cross-entropy loss using the ADAM optimizer with a learning rate of 0.001. The network is trained for 300 epochs with a batch size of 64. The accuracy and loss for this model is plotted in Figure 10 (attached in Appendix). The network has a test accuracy of 93.3% and the total time taken on the test set is 4364 seconds. To compare models and determine which one performs best, we run the Wilcoxon Signed-Rank test on each of the 30 instances of each model. First, comparing the SVD NN model to the Degree Sequence NN model, we ﬁnd that the degree sequence neural network has a higher accuracy of 93.0% compared to the SVD’s accuracy of 92.1%. With a p value of p = 0.002, we conclude that the results are not due to chance. The SVD model takes 4365s to run all test cases while the degree sequence models takes 4366s. Running the Wilcoxon test returns p = 0.2, and so we determine that they perform similarly even though the degree sequence model has a higher accuracy. We now compare the SVD NN model to the SVD Boosted Tree model. The tree model has an accuracy of 93.1%. The Wilcoxon test returns p = 0.00002, so we can conclude that there is a signiﬁcant diﬀerence in the accuracies of these models. A Real-World Graph We tested the SVD classiﬁcation neural network on the Facebook social network [44] to verify if the output is correct and to test the generalization of the neural network. When the algorithms are applied to the graph, Dijkstra’s algorithm ran in 532s, Peng’s algorithm took 40s, and Floyd-Warshall ran in 12,670s. So Peng’s algorithm was considerably faster on this graph than the other algorithms. Inputting this graph into the neural network, the outputted probability vector is (0.0000164, 0.999, 0.0000148). The ﬁrst coordinate represents the probability that Dijkstra’s algorithm is fastest, the second coordinate corresponds to Peng’s algorithm, and the third coordinate is for Floyd-Warshall. So the neural network is very conﬁdent that Peng will run the fastest on the graph, which is supported by the actual runtime of only 40s. Table 6 summarizes the results of all the classiﬁers. We note that all the ML models largely improves on the performance over just using one algorithm, and they perform similarly. Table 7 summarizes the largest time saved from correct classiﬁcations and largest time lost from incorrect classiﬁcations for each model by instance. We can see that every model was able to correctly classify the test instance that had the largest impact on overall time saved. Although the neural networks had the highest test accuracy, they were slightly slower compared to the boosted trees due to the fact that they misclassiﬁed the more important test cases; boosted trees were able to correctly classify the more important test cases and had overall better performance. In this paper, we show one can rely on ML-methods to predict the performance of diﬀerent algorithms in diﬀerent input instances. We can then make recommendations and decide the best algorithm to use in a particular situation. For the diﬀerent pivoting strategies for the Simplex algorithm, we ﬁnd that gradient boosting decision trees work the best in predicting the correct number of iterations. Our ML-method corroborates what human experts have recovered from their experience that most frequently the steepest-edge pivot rule is a great choice. Tuning hyperparameters helps to improve the performance of the models, but it is feature engineering that actually improves the models by a huge amount. Throughout the process we learn that certain features, such as variable constraint graph degrees, and coeﬃcients in A and c, are more important than other features that people empirically believe (row and column normalized features). Truncated Singular Value Decomposition gives us a Table 4. Summary of p-values for accuracy from Wilcoxon-Signed Rank Test. Table 5. Summary of p-values for total time from Wilcoxon-Signed Rank Test. Table 6. Summary of total time and accuracy of eachtime for each model by instance model convenient way to encode the matrix A into features. This improves the prediction accuracy of our models, but might not necessarily enhance the performance in number of iterations. All of our four models are able to outperform the popular steepest edge pivoting rule by a small edge, and their performance on the test set are pretty close. Although there is a gap between our model and the theoretical optimum, our experiments show that machine learning can help to improve the choice of pivoting strategy. With a proper way to encode linear programs of diﬀerent dimensions, we might be able to improve the performance of the Simplex algorithm further. For the problem of computing all-pairs shortest paths we found applying ML techniques to perform algorithm selection vastly improves on the overall performance over selecting an individual algorithm. We found that the method for classiﬁcation did not greatly aﬀect the performance, neural networks and boosted trees both had very similar performance. We discovered that the density of a graph and the degrees of the nodes are the most important features in selecting algorithms for APSP. Based on Figure 3 it seems as if it might be possible to select Peng’s algorithm for graphs with density less than 0.5, and Floyd-Warshall otherwise. But if we use this rule as a classiﬁer, we get a test accuracy of only 78.6% and a total time taken on the test set of 4508 seconds. This heuristic largely under-performs our ML models, showing that ML can be used to discover deeper, useful patterns in the data to improve results. We have presented a very simple machine learning data-driven approach for empirical algorithm selection or parameter tuning that is widely applicable. Given data and a collection of algorithms or parameters from which to choose, our empirical algorithm selection and tuning approach can be utilized to obtain automatic recommendations by almost anyone. We must of course discuss the beneﬁts and shortcomings of our empirical algorithm selection. Our approach does not formally prove our selection is optimal for all input instances, but instead only with respect to available data. Although our methods apply to any algorithmic problem with matrices as input we lack a theoretical recipe to choose the features used for training, and will likely change depending on the algorithm. We cannot answer questions such as “Is there a canonical best ML method?” or “What is the optimal neural network architecture (number of layers, activation functions, etc) for a particular algorithm selection problem?”. But this is not a limitation of our approach, rather it is a drawback of the entire theory of machine learning. On the other hand, there are multiple advantages to using our approach. Foremost, it is very basic and simple but improves computation. A user does not require expert-level knowledge of algorithms to make reasonable decisions. Our approach is a pragmatic way to justify algorithmic choices based on available data, and we also provide some consistency and rigor for evaluating algorithms’ performance. Moreover, human experts tend to narrow algorithmic choices to one popular default setup which leads to a one-size-ﬁts-all situation. Our approach allows variability in the choice of algorithm or parameters depending on the concrete instance and, most importantly, results in a clear improvement of running time or computational cost.