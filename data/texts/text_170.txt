Department of Computer Science and Engineering, University of *Corresponding author(s). E-mail(s): apolyzou@ﬁu.edu, Contributing authors: kalan028@umn.edu; karypis@umn.edu; Course selection is challenging for students in higher educational institutions. Existing course recommendation systems make relevant suggestions to the students and help them in exploring the available courses. The recommended courses can inﬂuence students’ choice of degree program, future employment, and even their socioeconomic status. This paper focuses on identifying and alleviating biases that might be present in a course recommender system. We strive to promote balanced opportunities with our suggestions to all groups of students. At the same time, we need to make recommendations of good quality to all protected groups. We formulate our approach as a multi-objective optimization problem and study the trade-oﬀs between equal opportunity and quality. We evaluate our methods using both real-world and synthetic datasets. The results indicate that we can considerably improve fairness regarding equality of opportunity, but we will introduce some quality loss. Out of the four methods we tested, GHC-Inc and GHC-Tabu are the best performing ones with diﬀerent advantageous characteristics. Higher education is valuable but it involves a signiﬁcant ﬁnancial cost that increases every year [1]. It is important for institutions to provide value to their students, so they often employ machine learning supporting tools. Many schools provide course recommendation systems (CRS) to facilitate course selection [2–8]. Existing CRSs empower learners to explore the curriculum, make informed decisions and plans while scaling advice to large cohorts [9– 11]. They help students to choose relevant elective courses in their curriculum according to diﬀerent criteria, e.g., their individual performance, preferences, interests, and needs. Such systems inﬂuence students’ choices, degree plan, and ultimately, their career paths. highly biased, resulting in outcomes that replicate existing biases [12]. Such biases can be harmful to a model, leading to discrimination against certain groups of users [13]. Users form diﬀerent protected groups based on their protected attributes, which include gender, age, race, color, or disability. Educational data is not an exception; the simplest example is the gender bias present in historical data related to student enrollment and performance. Women have been historically underrepresented in science, technology, engineering, and mathematics, while education, health, and welfare are their most common ﬁelds of study [14]. Higher education could play a key role in improving gender equality. As course recommendation approaches get embedded in operational systems that drive decision-making, it is important to ensure that they do not discriminate against any group of users. CRSs need to be useful and beneﬁcial to all students regardless of their protected attributes. under the spectrum of multiple stakeholders [15–17]. They study the beneﬁt trade-oﬀ among the system, the vendors (i.e., the items), and the users. Other researchers consider fairness on the item side. They are interested in the diversity of items in the recommendation list of each user and attempt to impose equal exposure of diﬀerent groups of items [18–22]. Such interpretations of fairness are not suﬃcient to ensure the equal treatment of the students in a CRS as they do not consider the existence of diﬀerent groups of users. cording to this ideal, every student should have equal educational opportunities irrespective of race, gender, socioeconomic class, sexuality, or religion. To this end, we propose FaiREO, a new type of fairness for course recommendation systems. We assume that course recommendation involves a notion of opportunity. That is, by recommending a course, a recommender system provides to a student the opportunity to review the course’s contents and consider taking it, something that they might not have done otherwise. This course could open a new path for them to explore and lead them to a job with better beneﬁts. FaiREO promotes that each student group receives equally high-quality recommendations and equal opportunities to consider a particular course. It Machine learning models are built based on data, but often, this data is A body of work in recommender systems considers the case of fairness Our motivation is driven by the equality of educational opportunity [23]. Acalso alleviates the feedback loop bias occurring when users consume biased recommendations and generate biased data that are later used to generate new (biased) recommendations. across the diﬀerent student groups. We introduce four greedy hill-climbing algorithms, GHC(Gc), GHC(NoNe), GHC-Inc, and GHC-Tabu, that work in two phases. First, they make an initial assignment of recommended courses to users, and then, they reﬁne this initial solution in order to improve the overall fairness, according to a multi-objective function. This function captures and balances two fairness-related, but often conﬂicting goals: equality in recommendation quality and equality of opportunity oﬀered to students across diﬀerent protected groups. captures the equality of opportunity, in course recommendation (CR), lem. The experimental evaluation with synthetic and real data from the University of Minnesota shows the behavior and eﬀectiveness of the proposed algorithms. We can mitigate or even eliminate unfair recommendations w.r.t. opportunity. In the process, we may introduce unfairness in terms of unbalance in the quality of recommendations across the student groups. tion of fairness and our problem statement. Sect. 3 reviews the existing work in fairness on recommender systems, as well as other problems that could be related to our problem. In Sect. 4, we formulate our objective functions and present our developed algorithms. Sect. 5 details all the information regarding our experimental setup, and Sect. 6 presents and analyzes our evaluation results. Finally, Sect. 7 summarizes our ﬁndings and concludes our paper. In this work, we make the following assumptions: FaiREO operationalizes this by recommending each course at fair rates This paper’s contributions include: (1) a new deﬁnition of fairness in recommender systems, FaiREO, that (2) a multi-objective optimization problem formulation to consider FaiREO (3) a set of steepest-ascent hill climbing algorithms to solve this problem, (4) a methodology for generating synthetic datasets suitable for this prob- The rest of the paper is organized as follows: Sect. 2 presents our deﬁni- (Assumption 1) The student body has at least one protected attribute based on which we can form protected groups of students (student groups). When there are more than two protected attributes that we need to consider, we create a protected group for each combination of values that they take. (Assumption 2) We have access to a method that computes the recommendation scores for the available courses a student might take next semester. The scores accurately capture how well a course matches the tion. Capital calligraphic letters will be used for sets. Lower bold case letters will indicate vectors, e.g., o, and their elements will be denoted by regular lower case letters, e.g., o and their indexed elements will be denoted by regular lower case letters, e.g., y. We use a superscript in parenthesis to refer to the students to whom we recommended the course with the corresponding index. For example, n the number of students that were recommended course j. Table 1 deﬁnes and presents the symbols we use. Course selection is often aﬀected by existing biases and stereotypes, as well as other people’s actions and opinions. As a result, course enrollment data, which student’s academic level, background, and knowledge. Courses that the student has already taken receive zero recommendation scores. We will refer to the recommendation solution that suggests the k highest scored courses for each student as the HSC solution. (Assumption 3) Course recommendations are fair when they are distributed proportionally to the protected groups according to their population. Alternatively, the system administrators may have insight into the desired distribution of recommendations that we consider to be fair. In any case, the fair recommendation distribution we need to achieve for each course is described by the fair distribution matrix, X. Notation. For the rest of the paper, we will adopt the following notais the input data of a CRS, exhibits historical, stereotype, and social biases [13]. A CRS may propagate these biases to the recommendation output. For example, such a system would rarely recommend coding classes to female students in a computer science department as computer programming is stereotypically considered a male-dominated area. Such a programming class could have provided Anna with the experience needed for a software engineering job. A fair CRS would ensure that students in all protected groups are oﬀered the same opportunities; a course’s recommendations are distributed proportionally to the protected groups. initial goal in a CRS: support students by oﬀering them recommendations of high quality. While these two aspects add to the value of a recommendation system, they can be in conﬂict. We assume that the HSC solution oﬀers the highest quality output, but there are no guarantees for the equality of the opportunities it oﬀers. On the other hand, if we modify the recommendation lists to satisfy equality of opportunity, some recommendations will be of lower quality. As a result, there is a need to balance these two goals. We need to ensure that the equality of opportunity does not come at the expense of the equality in oﬀering good recommendations to the protected groups. ness, referred to as fairness of equality of opportunity (FaiREO), which is deﬁned as follows. Deﬁnition Let S be a population of students, that can be divided based on the value of one or more protected features into g n, . . . , n equality of opportunity, FaiREO, when: 1. Each student group p gets a share of each course’s recommendations relative to 2. All student groups equally receive high quality recommendations with respect student groups matches the determined fair distribution of the recommendations to the student groups. In Eq. 2, we ask that the quality of a solution Our interest is on equal opportunity, but we still need to consider our Motivated by the above discussion, we introduce a new type of group fair- , respectively. A course recommendation system satisﬁes fairness for its corresponding fair ratio, x. Let nbe the number of students in group Sto whom we recommend course j. Recommendations w.r.t. course j oﬀer equal opportunities when: to the courses’ recommendation scores, i.e., where Ris a set of recommended courses for student i, Rthe set of courses recommended based on the HSC solution, ydenotes the score of student i in course j, and Sdenotes the students in the protected group p. Eq. 1 ensures that the distribution of a course’s recommendations to the R(measured by the sum of the recommendation scores of the courses recommended) is similar to the quality of the HSC solution for each student group S. We assume that there is no underlying reason why no courses would be a good match for students in a speciﬁc student group. As a result, the quality of HSC solution will be similar for all the student groups. If the Eq. 2 is true, the solution R The FaiREO deﬁnition in Sect. 2.2 can accommodate many diﬀerent scenarios depending on what we consider the fair distribution of the courses, X, to be. In every case, each row of X will sum up to one, i.e., In this section we explain how our problem relates to similar problems with respect to fairness. We have identiﬁed three main classes of problems closely related to our problem and fairness issues, and we present brieﬂy representative work in the following subsections. As a side note, this paper refers to user group fairness for course recommendation, which is a diﬀerent problem from group recommendation, where you aim to recommend the same set of items to every user in a group. In the problem we examine, we oﬀer individual and personalized course recommendations to the users (in our case, the students) Population-driven distribution. As an initial starting point or in absence of insights about the desirable distribution of the course recommendations to student groups, we could set That would set the fair distribution to match the underlying distribution of the population to the student groups. In this case, all the rows of the X matrix will be the same. Coarse-grained distribution. Instead of using the population distribution to deﬁne x, the department can decide what is the desirable distribution for all the courses. For example, if equally recommending a course to the student groups is not realistic, the department can assign an arbitrary fair recommendation ratio for all the courses in a department. This approach can potentially be less strict but more likely to be achieved in reality. It could assist the achievement of the department’s (i.e., system’s) goals regarding the diversity in course registration. Fine-grained distribution. In its most general form, we have the ability to assign diﬀerent fair distributions for each course. While this formulation allow us to deﬁne diﬀerent fair distributions for the courses, it would require detailed insights and considerable ﬁne-tuning on the administrator’s side. which need to be of as high quality as possible, while they are fairly distributed across the diﬀerent student protected groups. Fairness in recommender systems is relatively new and each work presents its own point of view on the subject. A body of work studies fairness in ranking lists [18–22, 24, 25], where the goal is to provide diverse representation of the items, i.e., equal exposure of diﬀerent groups of items. A ranking is considered to be unfair when speciﬁc protected groups of items are under-ranked and as a result they receive lower visibility in the system. This corresponds to fairness with respect to items. Beutel et al., [19] also account for user engagement. They measure the diﬀerences in accuracy across the groups of items based on pairwise comparisons. According to their deﬁnition of pairwise fairness, assuming that two items have received the same user engagement, then both protected groups should have the same likelihood of a clicked item being ranked above another relevant unclicked item. Deldjoo et al. [26] proposed a generalized cross entropy measure of fairness that was based on a fair distribution of a model’s performance over items or users. Yao et al. [27] propose fairness metrics so that the error is fairly distributed across users. The work in [18] studies fairness in search engines of people, such as job recruiting, companionship, or friendship search. In such cases, an outcome is unfair if members of one protected group are systematically under-ranked than those of another protected group. The recommended candidates are determined by a ranking algorithm. The proposed method to remove the bias is a post-processing process. All the above works are diﬀerent from ours as we do not account for diversity in the recommendation lists. In the course recommendation domain, we recommend a limited number of courses. Item diversity does not guarantee group fairness with respect to equality of opportunity for the student groups. equality of learning opportunities in content recommendation. Diﬀerent desirable properties of the recommended items and their measures are proposed, and the goal is for every list of recommendations to satisfy them above some threshold. While this approach is based on the same principle, their ﬁnal outcome is diﬀerent, as they focus on individual fairness. Our goal in this paper is to recommend each course fairly across the user protected groups. We aim at ensuring user-side fairness in a CRS and we consider protected groups of users/students, and not items. recommendations: the system, the suppliers/vendors/providers and the users/consumers [15–17, 29]. While these works study the trade-oﬀ between the diﬀerent stakeholders’ beneﬁts, we are interested in user-side fairness only. We consider ways to improve fairness while harming as little as possible the relevance of recommended items, both of which are beneﬁts for a single stakeholder that compete with each other. From these approaches, the most relevant notion is C-fairness [29]. It considers the disparate impact of the recommendation on The most relevant work is that of Marras et al. [28], which is also based on Another body of work studies fairness across multiple stakeholders in protected groups of consumers. The proposed method, a modiﬁcation of the Sparse Linear Method (SLIM), does not directly balance the recommendation lists. Rather, it balances the neighborhoods based on which the suggestions are generated for all the users. While their evaluation metric, equity score, captures a similar notion as the ﬁrst part of our FaiREO deﬁnition, it can be computed only per item group (or item/course in our case) and only for two protected groups. As a result, it cannot evaluate the overall fairness of a recommendation solution. the input data which the recommender system ampliﬁes [22, 30]. Tsintzou et al., [31] proposed a metric called bias disparity to measure the diﬀerence between the bias towards diﬀerent movie genres in user proﬁles (input) and in resulted recommendations (output). A similar work proposed a group-based metric to compare the preference ratio in the input and output data (recommendation lists) and quantify the degree to which recommendation algorithms may propagate any biases [32]. More recently, researchers have also studied representations that do not expose sensitive feature information in the user modeling process [33]. In the Fair Resource Allocation or Fair Division problem, we want to fairly divide a resource or goods to agents with diﬀerent preferences over the resource [34]. In the course recommendation context, we could consider the courses as the resources, the students as the agents and the recommendation scores as the expressed preferences of the agents towards the goods. Under this setting, group fairness has been studied in the form of envy-freeness [34–36]. In an assignment, a group is treated fairly when each agent has no envy for the goods assigned to other agents; everyone gets what they value the most. In our case, this is already achieved by the HSC solution which is an envy-free solution. In order to ensure group fairness, we want to reﬁne this initial solution and allocate/recommend diﬀerent courses equally across the protected groups. which the members of each group are allocated the same set of resources, which does not apply in our case of protected groups, as each student of one group can receive diﬀerent recommendations from the others in the same group. Aleksandrov et al., [39] assume that each group has an aggregate preference for a speciﬁc bundle of goods of another group and they consider arithmetic-mean group preferences; a feature that does not apply in the present work. According to the Course Allocation problem [40], we have a set of students with preferences to courses, a set of courses with preferences to students (priority orderings over the students from the course administrator), and each course has a speciﬁc predeﬁned capacity; the goal is to allocate students to seats of courses. Apart from the algorithmic fairness, issues may also arise from biases in In certain works [37, 38], the notion of group fairness deals with settings in Course Allocation is an instance of the combinatorial assignment problem if we consider no preferences on the courses side (one-sided preferences) [41]. In this domain, a highly unfair outcome could lead to some students assigned to their most preferable courses and some other students assigned to their least ones or even to zero courses [41]. Diebold et al., [40] compared two stable matching algorithms to a ﬁrst-come-ﬁrst-serve approach, a mechanism used in many institutions. A matching is considered stable when there is no student-course pair, such that both prefer one another to their current assignment [42]. In a more recent work, Diebold et al., [42] evaluated multiple matching mechanisms with real data in the context of course allocation with indiﬀerences-ties in school preferences. This notion of fairness corresponds to individual and not group fairness. Additionally, in the Fair Course Recommendation problem, there are no restrictions (such as the capacity of a course) other than trying to maintain the highest possible quality of the recommendation. Fair course recommendation according to FaiREO is a multi-objective optimization problem, that simultaneously tries to satisfy both conditions of equal opportunity and quality, as described in Section 2.2. We deﬁne two diﬀerent objective functions (O and Q) to capture each condition, and then we linearly combine them into our overall objective function. We quantify the ﬁrst condition of fairness by using the mismatch between the quantities of Eq. 1, i.e., the distance of course j from the fair ratio that corresponds to protected group p, x recommendations that introduce unfairness in protected group p as: where n is the number of students, m is the number of courses, n of students belonging in group p, k is the number of courses we recommend to the student, n j, and n course j. The term of the absolute diﬀerence captures how far away we are from balancing the opportunities oﬀered in group p regarding course j. The term in the parenthesis corresponds to the number of students that introduce this unbalance in the recommendations of j to group p. Note that the overall sum is normalized with the number of recommendations generated for group p (k courses for every one of the n of the group size. The opportunity objective function targets to minimize the is the number of students from group p to whom we recommend unfairness existing in the recommended lists of courses: The overall opportunity objective is measured by the l-norm of the vector o. To quantify the quality objective, we use the recommendation scores of the courses. We measure the quality of any assignment of courses to students by the summation of the recommendation scores of the suggested courses. We will capture how diﬀerent is the quality of a solution compared to the solution that recommends the highest scored courses to the students (HSC solution). We formulate the fraction of quality loss for each group p as: where R courses recommended based on the HSC solution. y score of student i in course j, and S The summation in the numerator is the diﬀerence in quality between the two solutions. We normalize it to make it invariant of the size of the protected groups and their quality. The quality objective function that minimizes the quality loss is: The overall quality objective is measured by the l-norm of the vector q. There is a trade-oﬀ between the two objectives, as optimizing for the opportunity objective will replace the highest-scored courses with others that have the same or lower scores. This may result in recommendations with lower-scored courses than the HSC solution, which will incur quality loss. The combined objective function is: The parameter α ∈ [0, 1] weighs the importance of each objective. Note that when α takes a marginal value (0 or 1), all the weight is placed in one objective (Q or O). Thus, in these cases, it is very likely that unfairness will be introduced from the other unpenalized objective (O or Q, respectively). We can use any l-norm greater than 1, which penalize high values, to aggregate the vectors o and q for all student groups. The fair course recommendation is a multi-objective, combinatorial optimization problem described by Eq. 8, and it involves a discrete but large is a set of recommended courses for student i, and Rthe set of conﬁguration space. That space cannot be exhaustively searched, as there are students, courses, and recommended courses per student, respectively. technique, with a greedy strategy for performing local search. It includes two phases: 1) the assignment of an initial solution, and 2) the reﬁnement of this solution in order to reach a solution that better minimizes the objective function, V . The reﬁnement consists of a series of moves that the algorithm makes towards a fairer solution. At every step, it performs a single change in one student’s recommendation list by replacing a single course. Iteratively, it considers a neighborhood of solutions that it can reach by making a single move from the current solution, and greedily selects the move that minimizes V . The algorithm terminates when it cannot ﬁnd a single move that improves V . It will reach one local minimum out of many that might exist in such a combinatorial optimization problem. Additional details about these steps are provided in the subsequent sections. We ﬁrst need to decide which will be the initial solution for our reﬁnement algorithm. A common practice is to start from a good solution and try to improve it. There is one solution that minimizes the quality objective; that is the HSC solution. On the other hand, there are many solutions that minimize the opportunity objective without considering the value of recommending a particular course to a student. Since we have access to the recommendation scores of a CRS model (assumption 2, Sect. 2.1), we can use HSC solution as the initial assignment. By design, HSC achieves Q = 0, which is the global minimum w.r.t. the Q objective. We start from the HSC assignment and reﬁne it to support the notion of FaiREO fairness. A fundamental element of search methods is the type of moves allowed to transition from a feasible solution to another one. Given a solution, we remove a course from the recommendation list of a single student, and replace it with another course. This move can be fully described by a triplet (i, j where j ommendation list of student i, respectively. A move is positive when it results in a solution with lower objective function V , and negative, otherwise. Assuming that we recommend courses to students based on the solution R need to specify the neighborhood of solutions that the algorithm will evaluate in order to make a move that improves the combined objective function, V , the most. The corresponding set of candidate moves is denoted by M study two diﬀerent ways to deﬁne them by specifying the allowed values for possible combinations to examine, where n, m, and k are the number of Our approach described in Alg. 1 uses the steepest ascent hill climbing and jare the courses we remove from, and introduce to the rec- Algorithm 1 Greedy Hill Climbing (GHC) Require: R (Recommended courses for every student.) Require: α (Weight of the opportunity objective, needed to compute the V (i, j Alg. 1, but examine diﬀerent set of moves (i, j student and recommended course in R currently recommended to the student i as candidate courses, i.e., j to complete the set of moves in M from the existing solution. This is a full-blown search that will consider changing all student-course pairs (i, j method, GHC(NoNe), there is essentially no neighborhood speciﬁed. If the objective.) GHC(None) or GHC(GC).) ← {(i, j, j solutions. ← ∅, V← ∅  Visited course and student groups while not ﬁnding an improved solution. ← Objective value of solution R. in the visited list. ), which result in two algorithms. Both of them follow the steps of the The simplest solution is to examine all possible one-step-away solutions algorithm examines all moves in M proves the V objective, it terminates. In this case, the Algorithm 1 will reach lines 19–20, and V tively. As a result, in the next iteration, the while loop in line 5 will be false and the algorithm will terminate. searching all the space every time. Rather, we examine a smaller set of moves, hoping that the next best move will belong there. In the GHC(Gc) method, we consider moves altering only the recommendations of students in a speciﬁc (target) protected group T , i ∈ S We choose the target protected group T to be the one that exhibits the highest opportunity unfairness, i.e., the most severe unbalance in recommendations: where V that do not result in positive move. The target course t is the one that is over-recommended the most among the students of group T , i.e., where V positive move. o value. We select (i, j the course t have the most room for improvement during reﬁnement. If the algorithm cannot ﬁnd a better solution, it adds t to the set of visited courses V(Alg. 1, line 20) and ﬁnds the next target course to search. Once there are no courses left to consider as target courses for T , we empty V (Alg. 1, line 20), and explore the next target student group. The algorithm terminates when we have visited all the student groups and courses but cannot ﬁnd a positive move. In terms of computational complexity, with GHC(NoNe), we need to examine the whole search space every time we make a move, which includes nk(m − k) solutions. This reﬂects the fact that we need to consider changing each recommendation (nk) with every course not currently recommended to the student (m − k). To perform one move with GHC(Gc), we need to ﬁnd the target course t and protected group T , which entails examining n in the worst case. Then, we need to evaluate the moves within the speciﬁed neighborhood which involves changing each recommendation of course t in students of the group T if t ∈ R total, the complexity of the GHC(Gc) algorithm is n We also use some heuristics for deﬁning a neighborhood in order to avoid is the set of student groups we have already visited and considered is the set of courses we have already visited that do not result in Figure 1: A diagram of the GHC-Inc method, which uses the GHC(Gc) algorithm as its components. Both GHC-Inc and GHC(Gc) receive as input a value of α, which weighs the importance between the opportunity and quality objectives, and a recommendation solution from a fairness-unaware model, R. Their output is an updated recommendation solution, R atively uses GHC(Gc), where it starts from an initial value α increases its value by α next model of GHC(Gc) as its initial recommendation solution. We also propose another algorithm to optimize the overall objective function, based on GHC(Gc). In the GHC-Inc algorithm, instead of optimizing for the given parameter α, we start optimizing the objective function with a small value of α parameter α value of α parameter α of the opportunity objective, in order to take careful steps in the beginning that do not introduce a high quality loss. Our goal is to reach a more balanced assignment with a lower Q objective. So far, the discussed algorithms stop exploring the solution space when they have reached a local minimum, i.e., there is no single move that would improve the current solution. However, in such a huge solution space, this might not be the global minimum. To further explore the search space after this point, we incorporate the idea of Tabu search in the GHC(Gc) algorithm. Whenever there are no improving moves and GHC(Gc) would stop, the GHC-Tabu algorithm performs the move that degrades the objective function the least. We hope that by taking a negative move, we will get into a diﬀerent neighborhood of solutions that will drive us to a better local minimum. In order for the algorithm to terminate, we control the number of negative moves that we allow it to make. on the next step, and return to the local minimum already visited. We use the tabu list, a short-term memory list structure with the ﬁrst-in-ﬁrst-out property, to store every move we made in order not to reverse it. A parameter controls the tabu list size. We store the pair of student-course (i, j We also need to ensure that the algorithm will not make the reverse move just updated and that we do not allow to take back. However, reversing a move can sometimes lead to a better solution. We introduce an aspiration criterion which allows us to make moves forbidden by the tabu list if they lead us to a solution with lower objective function than the lowest objective achieved so far. In Sect.6, we will present the experimental results when we use as fair distribution the population-based distribution. In this case, the fair ratio x each course j is expressed by the Eq. 3. We generated synthetic datasets to evaluate our approaches since we do not have data regarding the students’ protected attributes. The kind of data that that we need to generate are: 1. the student-course recommendation score matrix Y ∈ R j for student i estimated by any CRS, and 2. the partitioning of students into protected groups, S of real-world datasets. Let us assume a matrix Y obtained from a CRS and the corresponding solution when we recommend the highest scored courses for each student. The fairness of this solution depends on the existence of courses whose recommendation scores tend to be higher for a speciﬁc group of students. In that case, these courses will be good candidates and systematically suggested more times to one particular group than the rest. We model these factors into our dataset generator, by introducing the notion of course buckets, which correspond to a partition of the set of courses C. The number of course buckets is controlled by the parameter g diﬀerent average recommendation score across the protected groups. matrix M ∈ R scores of students in group p for courses in bucket q. We ﬁll the ﬁrst row of M, standard deviation d of the initial vector M scored courses, and the recommendation quality across student groups will be similar. Once we have generated matrix M, we can ﬁnally ﬁll the matrix Y by sampling the recommendation scores for students in group p and courses in bucket q from a normal distribution N (µ to generate a dataset based on this process, we need to specify the following parameters: number of course buckets g ent diﬃculty levels. This allows us to evaluate how our algorithm will operate We want to create synthetic datasets whose characteristics align with these We model the relation between student groups and course buckets via a , by sampling a normal distribution N(µ, d) with mean value µ, and , and the standard deviation dfor the generation of Y. This dataset generator is parameterized in order to create datasets of diﬀerunder diﬀerent settings. The diﬃculty of a dataset is controlled by how close to each other are the mean values in M further away the means are, the further away the scores of diﬀerent protected groups for a course bucket will be (and the less likely it will be to recommend courses from this bucket to all student groups). It aﬀects how many high scores exist for every student in the resulting dataset. If there are many courses with high scores for a student, then it will be more likely to ﬁnd a move that improves the opportunity objective without introducing a high quality loss. 0.3. We generated datasets of three diﬃculty levels: easy (Uni), medium (Gauss(1, 0.1)), and hard (Gauss(1, 0.3)). For the datasets Gauss(1, 0.1) and Gauss(1, 0.3), we set µ In Gauss(1.0, 0.1), the means generated in M will be closer to each other compared to Gauss(1.0, 0.3) datasets. When the means are spread out in a wider range in the matrix M, the recommendation scores generated based on that matrix will have diﬀerent statistical characteristics. The Gauss(1.0, 0.3) datasets will be harder datasets to handle, and we expect to incur a higher quality objective value. The easiest datasets are Uni, where all recommendation scores y there are not courses with high scores by design. In total, we create six families of datasets; for three diﬃculty levels, and for two or four protected groups. For every family of datasets, we create ﬁve versions of them, by using diﬀerent seeds to generate the matrices M and Y. In this way, we get to examine how sensitive are our models to input data with similar characteristics. Figure 2 shows the heatmap of the recommendation scores for each student (y-axis) and course (x-axis) in example datasets of each family. Figure 2: Heat maps of all the students’ recommendation scores for all courses in each synthetic dataset. We set the parameters as follows: n = 600, m = 60, g= {2, 4}, g= 4, d= Table 2: Statistics of the ComptSci datasets regarding the total number of students and their distribution over the three student groups we considered. We collected data from the Computer Science and Engineering department in the University of Minnesota. The data include the grades of undergraduate students and span a period of 10 years, until the fall semester of 2015. We only considered full-time students that actually graduated with a bachelor’s degree. We used the last three semesters to test a recommendation system [4] which was built using the rest of the data. We keep the recommendation scores of the students in the test set, and use them as the matrix Y. We treat every semester as a diﬀerent dataset: fall 2014, spring 2015 and fall 2015, with 188, 170, 112 students and 59, 54, 55 courses, respectively. We will be referring to these datasets as the ComptSci datasets. consider for our experimental evaluation, so we used other student-related information (entry registration status and the number of credits transferred) to simulate the socioeconomic status of the students. This led to three protected groups: high school students with less than 15 credits transferred (HS), high school students with more than 15 credits transferred (HSAP), and those coming from other institutions/colleges (NAS). High school students can take Advanced Placement (AP) courses and transfer the credits earned to their undergraduate program. Minorities and low-income students are underrepresented in AP classes, and a low percentage of them actually take and pass the AP exams [43, 44]. Regarding NAS students, we do not have information about the institution where they transferred from. However, reports statistically show that almost half of NAS students come from 2-year colleges [45] which are considered a major access point to 4-year institutions for minority and low-income students [46]. The fraction of students in our datasets belonging in the HS, HSAP, and NAS protected groups is shown in Table 2. Regarding the norm base, l, we use the L the highest elements of the vectors o and q. L the student group with the highest objective values to get as low as possible. This will limit the worst case scenario for the protected groups. α that controls the trade oﬀ between opportunity and quality loss objectives. The available data did not include any protected attributes that we could We need to specify the number of courses to recommend k, and the value of We set k = 5, and α = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. If α = 0, we get the initial, fairness-unaware recommendation, HSC solution. If α = 1, we would get a recommendation solution that does not consider at all the recommendation scores, and it could end up being worse than random assignment in terms of quality. Tabu, we set the number of negative moves to 150 and the tabu list size to 50. The main experimental results are presented in Figures 3 – 6. In this section, any numbers related to the opportunity, quality, or overall objective values have been multiplied by 100 so that the quantities correspond to percentages. jectives for the synthetic and ComptSci datasets. The x axis corresponds to the highest percentage of quality loss that a student group may have. The y axis corresponds to the highest percentage of unfair recommendations w.r.t. opportunity for a student group. distributed across the student groups for diﬀerent values of α. Each subﬁgure corresponds to one of the discussed methods, GHC(NoNe), GHC(Gc), GHCInc, or GHC-Tabu. The positive side of the y-axis is the percentage of unfair recommendations w.r.t. opportunity of a student group, while the negative side shows the percentage of quality loss that a student group has. Figures 4 and 5 correspond to the synthetic datasets Gauss(1,0.3) with two and four protected groups, respectively. Figure 6 refers to the ComptSci dataset for the Fall ’14 semester. In the following paragraphs, we present the key ﬁndings regarding the performance of the diﬀerent methods we evaluate w.r.t. the opportunity, quality, or overall objective values. GHC-Tabu is an extension of GHC(Gc) which additionally performs negative moves when it reaches a local minimum, to reach a better possible solution. Based on our experimental results, GHC-Tabu performs the same or slightly better than GHC(Gc). That is the reason why we do not include the results of the GHC(Gc) algorithm in Figure 3. We are able to see how and when GHCTabu improves the solution of GHC(Gc) in Figures 4, 5, 6, as they provide a more detailed view of the results. Comparing the (b) and (d) subﬁgures, we cannot notice any particular diﬀerences in the opportunity objective achieved For the algorithm GHC-Inc, we set α= 0.1, α= 0.1. For the GHC- Figure 3 shows the scatter plots between the opportunity and quality ob- Figures 4, 5, and 6 show how the opportunity and quality objectives are GHC(Gc). Figure 3 : Scatter plots of the opportunity and quality objectives for the synthetic and ComptSci datasets with α = {0.1, 0.3, 0.5, 0.7, 0.9}. For each dataset, the scatter plot shows the objective values Q and O achieved for diﬀerent values of α. A line connects points that correspond to consequent values of α. The x axis is the quality degradation percentage (%) of the most degraded group and the y axis is the percentage (%) of unfair recommendations of the most impacted group w.r.t. opportunity. These two quantities are computed as 100 × Q and 100 × O, respectively. The (0, 0) point represents the ideal model that is fair for all student groups in terms of both quality and opportunity. by the two models. However, when α gets higher values, placing more weight on the opportunity objective, we see that the quality objective of the diﬀerent student groups is uneven. Here is where GHC-Tabu helps. It manages to lower the quality objective by improving the objective value of the worst-performing student group. In the case of real datasets, that is harder to accomplish as we see smaller improvements of the GHC-Tabu over the GHC(NoNe). The reason for that is most likely the distribution of the recommendation scores (a) Synthetic datasets. Datasets in a row have increasing diﬃculty. Figure 4: Distribution of the opportunity and quality objective values for different values of α in the diﬃcult synthetic dataset Gauss(1,0.3) with two student groups for the four diﬀerent methods. The values on the y-axis are multiplied by 100 to correspond to percentages. The x-axis represents diﬀerent values of α. The positive side of the y-axis is the percentage of unfair recommendations w.r.t. opportunity of a student group, while the negative side shows the percentage of quality loss incurred for a student group (multiplied by -1). We also include the values for α = 0, which correspond to the initial values of the objectives achieved by the HSC solution, with high opportunity objective but zero quality objective. of a student in the real datasets. In the real data, few courses will have high scores, so it is not as easy to replace them with courses that will balance the opportunity objective, if needed. 6.1.2 The proposed methods successfully improve the fairness For the synthetic datasets with two protected groups, the initial percentage of unfair recommendations w.r.t. the opportunity is on average w.r.t. the opportunity. Figure 5: Distribution of the opportunity and quality objective values for diﬀerent values of α in the diﬃcult synthetic dataset Gauss(1,0.3) with four student groups. The values on the y-axis are multiplied by 100 to correspond to percentages. The x-axis represents diﬀerent values of α. The positive side of the y-axis is the percentage of unfair recommendations w.r.t. opportunity of a student group, while the negative side shows the percentage of quality loss incurred for a student group (multiplied by -1). We also include the values for α = 0, which correspond to the initial values of the objectives achieved by the HSC solution, with high opportunity objective but zero quality objective. 5.2%, 22.0%, 44.1% for the Uni, Gauss(1,0.1) and Gauss(1,0.3), respectively. We manage to eliminate them with 0.1%, 2%, 10% of quality loss, respectively. For four protected groups, we start from 10.0%, 25.1%, 45.0% of unfair recommendations and we manage to eliminate them, while incurring only 0.5%, 2.5%, 10.0% of quality loss, respectively. For the ComptSci datasets, the initial percentage of unfair recommendations w.r.t. the opportunity is 16.5%, 14.9%, 22.1% for the Fall’14, Spring’15, and Fall’15, respectively. We manage to decrease the opportunity objective to 2% or lower, with less than 10% of quality degradation. Figure 6: Distribution of the opportunity and quality objective values for diﬀerent values of α for the ComptSci dataset of Fall ’14. The values on the y-axis are multiplied by 100 to correspond to percentages. The student groups HS, HSAP, and NAS are numbered as 1, 2, and 3 respectively. The x-axis represents diﬀerent values of α. The positive side of the y-axis is the percentage of unfair recommendations w.r.t. opportunity of a student group, while the negative side shows the percentage of quality loss incurred for a student group (multiplied by -1). We also include the values for α = 0, which correspond to the initial values of the objectives achieved by the HSC solution, with high opportunity objective but zero quality objective. When there are two protected groups, for small values of α, all methods manage to achieve O = 0 (ﬁrst row in Figure 3a). However, when the number of protected groups increases (second row of Figure 3a and Figure 3b), this is not always the case. In order to achieve low values of the opportunity objective, we often need to use larger values of α. of protected groups. Figure 7: Percentage of recommendations aﬀected by the algorithms in the case of four protected groups. The green horizontal line represents the % of unfair recommendations of the most impacted group w.r.t. opportunity in the initial HSC solution. 6.1.4 Limiting the neighborhood of local search is beneﬁcial. We compare GHC(NoNe) with GHC(Gc)/GHC-Tabu to evaluate the neighborhood selection presented in Sect. 4.4.3. GHC(NoNe) searches all possible moves to select the best one, while GHC-Tabu’s local search is limited by the target student group and target course. Their signiﬁcant diﬀerence appears in the case of four protected groups (second row in Figure 3a). For example, in the Uni dataset, GHC(NoNe) cannot reach values lower than 2% for the opportunity objective, even for the highest value of α, while GHC-Tabu achieves O = 0. GHC-Tabu manages to better improve the solution w.r.t. the opportunity objective compared to GHC(NoNe). In particular, the easier the dataset, the worse the performance achieved by GHC(NoNe). By starting the local search based on the student group and the course with the highest unfairness w.r.t. opportunity, GHC-Tabu can better identify the changes that needed. On the other hand, GHC(NoNe) gets stuck easier, and reaches a local minimum without correcting enough recommendations. In order to better understand this, we computed the percentage of recommendations changed by the proposed algorithms, as shown in Figure 7. The easier the dataset, the less corrections GHC(NoNe) does. When the dataset is harder, there are more recommendations that need to change, and GHC(NoNe) performs relatively better. In any case, GHC-Tabu manages to correct more recommendations towards a more balanced outcome. trapped at local minima in the case of easy datasets with unnecessary high values for α (last two ﬁgures in the ﬁrst row of Figure 3a). Initially, it freely and carelessly makes moves that introduce a lot of quality loss to improve the opportunity objective which is the most important term because of the high value of α. It reaches a point where the opportunity objective is minimized, but the recommended courses have lower recommendation scores. In contrast to GHC(NoNe), GHC(Gc) searches a smaller space driven by the student groups Additionally, GHC-Tabu (because of GHC(Gc)) is prune on getting and courses with the highest opportunity objective. As a result, it might not be able to replace the recommendations that introduce high quality loss. have similar performance, i.e., they manage to achieve the same values for the Q and O objectives. For example, in the Gauss(1,0.1) dataset for two protected groups, we see that both models achieve O = 0, while introducing similar percentage of quality loss (2%). For smaller values of α though, when we have higher values of the opportunity objective, we see that there is a gap between the O objective achieved by GHC(NoNe) and that by GHC-Tabu (13% vs 7%, respectively). 5, 6, we see that GHC(NoNe) is the worst of the four models across all cases w.r.t. the opportunity objective. Sometimes, it achieves lower percentage of quality loss, however, it has higher unbalance in course recommendations. For example, in Figure 5 with the synthetic data and four protected groups, it achieves lower values for the q higher values of o In Figure 3, GHC-Inc manages to reach the opportunity objective as low (or lower) as the rest of the methods. We can also see in Figures 4, 5, 6 that GHC-Inc performs better than GHC(NoNe), and similarly or better than the GHC-Tabu (and GHC(Gc)) in terms of the opportunity objective. Additionally, another advantage of GHC-Inc is that when it reaches O = 0 for some value of α, it makes no additional moves after that point and it does not degrade the quality objective any further (e.g., Figure 3a, two protected groups, Gauss(1,0.1) and Gauss(1,0.3), and Figures 4c, 5c with the ComptSci dataset). but not equal to zero, it continues to make more moves than needed, while trying to get to a slightly better local minimum. That results in an increased value of the quality objective for minor improvements in the opportunity objective. Especially in Figure 6c, the values of the q for the datasets that GHC-Inc reaches o does the same, for the same value of α, but with lower values of q nity objective without having a substantial quality loss, GHC-Inc is the best method to use. If we are interested in improving fairness w.r.t. opportunity as much as possible, while still maintain fairness w.r.t. recommendation quality, the best performing method is GHC-Tabu. For the remaining datasets with g6= 4 in Figure 3, the proposed methods If we examine the results on the speciﬁc datasets presented in Figures 4, with worse quality objective for high values of alpha. However, if GHC-Inc achieves an opportunity objective that is close to zero, This indicates that when we are interested in just reducing the opportu- Figure 8: Average standard error of the diﬀerent objective values achieved across the diﬀerent families of datasets. In Figures 3 and 7 for the synthetic datasets, the results are averaged over the ﬁve datasets generated with diﬀerent seeds. We compute the standard error (SE) of the objectives for each of those ﬁve and each value of α. Figure 8 shows the averages of these SE over all the six synthetic dataset families (three diﬃculty levels, two and four protected student groups). These give us an indication about the robustness of the methods for datasets with similar characteristics. GHC(NoNe) is very consistent w.r.t. the O objective, where it achieves 0.2 average standard error for most of the values of α, but less consistent regarding the quality objective. GHC-Inc has the opposite behavior, i.e., the lowest SE of the Q and the highest for O. Overall, GHC-Tabu is more consistent as it has similar SE in the two objectives, and the lowest in the combined objective V . Figures 4, 5, and 6 provide us with insights about how the value of α aﬀects the distribution of the objective values across the student protected groups. In particular, the opportunity objective becomes more balanced across the student groups for higher values of α for all methods, as expected. Even for the real dataset in Figure 6, GHC-Tabu achieves o of p (and standard deviation of 0.43%) for α = 0.9, compared to o (and standard deviation of 5.87%) for α = 0, which corresponds to the HSC solution. Even with α = 0.1, GHC-Tabu manages to drop the opportunity objective values per group to o The higher values of α (together with the L performance of the opportunity objective and balance any remaining unfair recommendations to the student groups. groups of the same size, the opportunity objectives are balanced as they are data. For the speciﬁc dataset shown in Figure 4, because there are two protected complimentary, i.e., the courses that are over recommended in the one group are under recommended in the other one, and vice versa. With respect to the opportunity objective, we see some apparent diﬀerence in Figures 4b and 4d. While GHC(NoNe) and GHC-Inc are able to maintain the same q for higher values of α than needed to achieve o for GHC(Gc) and GHC-Tabu. While GHC-Tabu has the lower q α = 0.4 with o this case, we put more weight on the opportunity objective, so the algorithm makes some less careful moves early on that introduce high quality loss which it cannot undo afterwards. As a result, it is trapped in a local minimum, where the quality loss is not fairly distributed in the two groups. This shows that such unnecessary high values of α introduce unfairness with respect to the unbalanced quality objectives. That is the case in other datasets as well, but less noticeable. Course selection plays an important role in students’ progress towards graduation, but also in the career path they will follow afterwards. In this paper, we examined group fairness in the context of course recommendation to ensure that all students are given the same opportunities when using a recommender system. We formulated a multi-objective problem that balances the fairness in opportunity and quality. We developed greedy algorithms that iteratively improve the combined objective function. The results indicate that GHC-Tabu can consistently improve fairness w.r.t. the opportunity with limited quality loss. GHC-Inc is the best method when we only assign a small weight on the opportunity objective as it gradually increases this weight in order to take more careful steps towards a fairer set of recommendations. Acknowledgments. This work was supported in part by NSF (1447788, 1704074, 1757916, 1834251), Army Research Oﬃce (W911NF1810344), Intel Corp, and the Digital Technology Center at the University of Minnesota. Access to research and computing facilities was provided by the Digital Technology Center and the Minnesota Supercomputing Institute.