When high-stakes decisions are automated or informed by data-driven algorithms, decisionsubjects will strategically modify their observable features in a way which they believe will maximize their chances of achieving better outcomes [ subject has a set of actions/interventions available to them. Each of these actions leads to some measurable eﬀect on their observable features that impact their prediction/decision. From the decision-maker’s perspective, some of these actions may be more desirable than others. Consider credit scoring as an example. pay back a loan on time. Financial institutions regularly utilize credit scores to decide whether to oﬀer applicants a mortgage, credit card, auto loan, or other credit products, and determine the condition under which the product is oﬀered (e.g., by setting the interest rate or credit limit). Given their knowledge of credit scoring instruments, applicants regularly attempt to improve their scores to have better chances of approval for their product of interest. For instance, a business applying for a loan may improve their score by paying oﬀ existing debt or cleverly manipulating When subjected to automated decision-making, decision-subjects will strategically modify their observable features in ways they believe will maximize their chances of receiving a desirable outcome. In many situations, the underlying predictive model is deliberately kept secret to avoid gaming and maintain competitive advantage. This opacity forces the decision subjects to rely on incomplete information when making strategic feature modiﬁcations. We capture such settings as a game of Bayesian persuasion, in which the decision-maker sends a signal, e.g., an action recommendation, to a decision subject to incentivize them to take desirable actions. We formulate the decision-maker’s problem of ﬁnding the optimal Bayesian incentive-compatible (BIC) action recommendation policy as an optimization problem and characterize the solution via a linear program. Through this characterization, we observe that while the problem of ﬁnding the optimal BIC recommendation policy can be simpliﬁed dramatically, the computational complexity of solving this linear program is closely tied to (1) the relative size of the decision-subjects’ action space, and (2) the number of features utilized by the underlying predictive model. Finally, we provide bounds on the performance of the optimal BIC recommendation policy and show that it can lead to arbitrarily better outcomes compared to standard baselines. their ﬁnancial records to appear more proﬁtable. While both of these interventions may improve credit score, from the perspective of the ﬁnancial institution oﬀering the loan the former is much more desirable than the latter. But how can the decision-maker incentivize decision-subjects to take such desirable actions? in order to encourage desirable behavior, then fully reveal the model to the public (as is assumed by prior models of strategic learning, e.g., [ full-disclosure (i.e., revealing the exact logic of the predictive model) is infeasible or irresponsible. For instance, credit scoring formulas are closely guarded trade secrets, in part to prevent the risk of default rates surging if applicants learn how to take advantage of them. (as we argued above, credit scoring is one such domain). In such settings, the decision-maker may still be interested in providing some information about the model to decision subjects, for instance, to provide a certain level of transparency and recourse. In particular, the decisionmaker may be legally obliged to (or economically motivated to) guide decision-subjects to take desirable actions that improve their outcomes. One potential solution in such cases is for the decision-maker to recommend desirable actions for the decision-subject to take. Of course, this needs to be done carefully and credibly, otherwise self-interested decision-subjects may not follow the decision-maker’s recommendations. decision rule is not revealed to the decision-subjects. There are several aspects our model aims to capture. First, even though the decision rule they often have prior knowledge about what the predictive model may be. Secondly, when the decision-maker provides recommendations to decision-subjects on which action to take, the recommendations should be compatible with the subjects’ incentives, to ensure they will follow the recommendation. Finally, certain regulations on transparency may require decision-makers to disclose their mechanisms on how they provide recommendations, even before the decision-makers obtain their predictive model or training data. decision-subjects by drawing from the formalism of Bayesian persuasion [ model of persuasion from the information design literature in economics. In general, the Bayesian persuasion setting considers a game between a sender with private information and a receiver. At the beginning of the game, the sender and receiver have a shared prior over some unknown state of nature, which will eventually be revealed to the sender. Using this belief, the sender commits to a signaling policy, a possibly stochastic mapping from states of nature to action recommendations, in order to maximize their expected payoﬀ. Once the state of nature is revealed to the sender, they choose a signal to reveal to the receiver (according to their signaling policy), who then takes a non-contractible action which aﬀects the payoﬀs of both players. In our setting, the shared prior corresponds to the decision-maker and decision-subject’s belief about what the predictive model will be before training. After training, the predictive model is revealed to the decision-maker, who then recommends an action to the decision-subject based on their pre-determined action recommendation policy. Upon receiving this action recommendation, the decision-subject updates their belief about the model, and takes the action which will maximize their utility in expectation. Our contributions recourse pathways under partial transparency as game of Bayesian persuasion between the decision-maker and decision-subjects. Our main technical contributions are as follows: One potential solution is for the decision-maker to ﬁrst carefully design the predictive model Next, let us consider settings where full disclosure about the model is not a viable alternative In this paper, we provide a model to study a strategic learning setting in which the underlying Taken these desiderata together, we model the interaction between the decision-maker and Using tools from Bayesian persuasion, we show that it is possible for the decision-maker the relevant background on Bayesian persuasion. In Section 3, we provide intuition for how formulating the decision maker’s action recommendation problem as a game of Bayesian persuasion can help them incentivize desirable actions with a simple illustrative example. Additionally, we show that using an incentive-compatible action recommendation policy can be arbitrarily better for the decision-maker than revealing the full model to the decision-subjects or revealing nothing. In Section 4, we derive the decision-maker’s optimal action recommendation policy, in Section 5 we provide a simple experimental setup to illustrate the empirical performance of our methods, and we conclude in Section 6. Strategic responses to unknown predictive models. strategic interaction between decision-maker and decision-subjects when the underlying predictive model is not public knowledge. Ghalme et al. when it is public knowledge to the error when decision-subjects must learn a version of it, and label this diﬀerence the “price of opacity”. They show that small errors in decision-subjects’ estimates of the true underlying model may lead to large errors in the performance of the model. The authors argue that their work provides formal incentives for decision-makers to adopt full transparency as a policy. However, even when decision-makers may wish to make their predictive model fully transparent, it is often not feasible due to privacy concerns, legal reasons, etc. of decision-subjects on their ability to improve their observable features in strategic learning settings. Like us, they do not assume the predictive model is fully known to the decision-subjects. Instead, the authors model decision-subjects as trying to infer the underlying predictive model by learning from their social circle of family and friends, which naturally causes diﬀerent groups to form within the population. They show that deploying a predictive model to maximize social welfare may actually cause the quality of some groups to deteriorate, and characterize the disparity in improvements across diﬀerent subgroups. In contrast to this line of work, we study a setting in which the decision-maker provides customized feedback to each decision-subject individually. not revealed and the decision-subjects learn an estimate of the deployed model. While their models circumvent the assumption of full information about the deployed model, they restrict the decision-subjects’ knowledge to be obtained only through past data, and their response does to provide incentive-compatible action recommendations to incentivize rational decision subjects to take desirable actions more often than the two natural alternatives; namely (1) fully revealing the predictive model to the decision subjects, or (2) revealing no information about the model. While the decision-maker is trivially never worse oﬀ in expectation from using an optimal set of incentive-compatible recommendations, we show that situations exist in which they are arbitrarily better oﬀ in expectation than if they had chosen either of the two natural alternatives. Finally, we derive the optimal action recommendation policy for the decision-maker. While the decision-maker’s optimal recommendation policy initially appears incomputable (as it involves optimizing over continuously-many variables), we show that the problem can naturally be cast as a linear program. We ﬁnd that the computational complexity of solving this linear program is closely tied to (1) the relative size of the agent’s action space, and (2) the number of features utilized by the underlying decision rule. The outline of the paper is as follows. In Section 2, we introduce our notation and provide Bechavod et al.[1]study the eﬀects of information discrepancy across diﬀerent sub-populations Both of these works provided strategic learning models in which the predictive model is not correspond to utility maximization. Our work takes inspiration from Bayesian persuasion to model a fully general form of prior knowledge about the predictive model. Persuasion and learning. others’ behavior is known as Bayesian persuasion [ most basic form, Bayesian persuasion can be thought of as a two-player Stackelberg game in which one player (the sender) publicly commits to a signaling policy, conditional on an unknown state of nature. The state of nature is then revealed to the sender, who then sends a signal based on their signaling policy to the other player (the receiver). The receiver uses this signal to form a posterior update over the possible states of nature, and then uses this posterior to take some action which aﬀects the welfare of both players. communities over the last several years. Dughmi computing the optimal signaling policy for several popular models of persuasion. Castiglioni et al. [2]study the problem of learning the receiver’s utilities through repeated interactions. Literature on the incentivized exploration multi-arm bandit problem [ in Bayesian persuasion to incentivize agents to perform bandit exploration. Economics. classical microeconomics [ the moral hazard problem from contract design, in which an employer (the principal) cannot verify the eﬀort an employee (the agent) expends before oﬀering them a contract. Our work is thematically related to Nudge theory [ using indirect suggestions as a way to inﬂuence the behavior and decision-making of groups or individuals. Algorithmic recourse. is closely related to the ﬁeld of algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems [ recourses that are actionable, or realistic, for decision-subjects to take to improve their decision. In contrast, our action recommendations are “actionable” in the sense that they are interventions which promote long-term desirable behaviors while ensuring that the decision-subject is individually no worse oﬀ in expectation. Explainability. Regulation (GDPR) [ for real-world deployment. While this work can be thought of as providing additional transparency into the decision-making process, it does not naturally fall into the existing organizations of explanation methods (e.g., as outlined in [ based on the decision rule. Rather, our goal is to incentivize actionable interventions on the decision-subjects’ observable features which are desirable to the decision-maker, and we leverage persuasion techniques to ensure compliance. Other strategic learning settings. There has been a long line of work in strategic learning that focuses on how strategic decisionsubjects adapt their input to a machine learning algorithm in order to receive a more desirable prediction, although most prior work in this literature assumes that the underlying predictive model or decision rule is fully revealed to the decision-subjects, which is typically not true in reality. Consider a setting in which a decision-maker assigns a predicted label or not someone will repay a loan if given one) to a decision-subject with observable features There has been much interest in persuasion in the computer science and machine learning 22] broadly studies machine learning questions in the presence of strategic decision-subjects. a linear decision rule, i.e., decision-maker. We refer to ( decision-subject is to receive a positive classiﬁcation (e.g., get approved for a loan), regardless of their true label of this, the decision-subject may choose to take some action existing debt) from some set of possible actions with the goal of optimizing the prediction they receive. By taking action incurs some cost more abstract notions of cost such as opportunity cost or the time/eﬀort cost a decision-subject may have to pay in order to change a certain attribute about themselves. Additionally, taking an action changes a decision-subject’s observable feature values from action their disposal in order to improve their outcomes. For convenience, we add taking "no action" (e.g., ∆x(a utility for a positive (negative) classiﬁcation, subject to some cost for taking said action. If the decision-subject had exact knowledge of the decision rule ( could solve an optimization problem to determine the best action to take in order to maximize their utility. However, in many settings it is not realistic for a decision-subject to have perfect knowledge of ( possible ( about the relative importance of each observable feature to the classiﬁer. While we only consider the setting in which all decision-subjects share a common prior settings in which diﬀerent subgroups have diﬀerent priors over the decision rule being used is an interesting direction for future work. With no knowledge of decision-maker would pick an action to their prior, i.e., a For example, a bank may prefer that an applicant pay oﬀ more existing debt than less when applying for a loan. A college may prefer that an applicant study for the SAT instead of taking an SAT prep course. To formalize this notion of action preference, we say that the decisionmaker receives some utility example, The decision-maker has an information advantage over the decision-subject, due to the fact that they know the true value of ( may be able to leverage this information advantage to incentivize the decision-subject to take a more favorable action than the one they would have taken according to their prior. In order for the decision-subject to be incentivized to follow the action recommended by the decision-maker, the recommended action σ ∈ A needs to be Bayesian incentive-compatible. Deﬁnition 2.1 incentive-compatible (BIC) if x, · · · , x)∈ R(e.g., amount of current debt, bank account balance, etc.) using a)∈ R, and ∆x(a) speciﬁes the change in thejth observable feature as the result of taking a. Concretely, we assume that each decision-subject hasmactions{a, a, . . . a} ∈ Aat As a result of taking actiona, a decision-subject receives utilityu(a, θ, b) =sign{(x+ a))θ+b} − c(a). In other words, each decision-subject receives some positive (negative) θ, b) combinations, which can be thought of as “common knowledge” or “intuition” From the decision-maker’s perspective, some actions may be more desirable than others. u(pay oﬀ more debt)> u(pay oﬀ less debt). In the college admissions setting, (study for SAT) > u(take prep course). where the expectation is taken with respect to the decision-subject’s posterior induced by receiving recommendation σ = a. respect to their posterior least as high as the expected utility of taking any other action that an action recommendation policy σ ∈ A recommended by the policy. assumed to be ﬁxed and common knowledge. This is because in order for the decision-subject to perform a Bayesian update based on the observed recommendation, they must know the action recommendation policy. Additionally, the decision-maker must have the power of commitment, i.e., the decision-subject must believe that the decision-maker will select actions according to their revealed recommendation policy. In our setting, this means that the decisionmaker must commit to their action recommendation policy before training their predictive model. This can be seen as a form of transparency, as the decision-maker is publicly committing to how they will use their model before they even train it. For simplicity, we assume that the decision-maker shares the same prior beliefs the observable features before the model is trained. These assumptions are standard in the Bayesian persuasion literature (see, e.g., [ ing policies is extremely challenging. Problem protocol: Bayesian persuasion in the strategic learning setting. As is the case in the traditional Bayesian persuasion literature [ for the decision-maker to design an action recommendation policy such that their expected utility is higher than if they had provided no recommendation. To provide intuition for how leveraging the decision-maker’s information advantage by playing a BIC recommendation policy may lead to higher expected utility for the decision-maker, we study the following example. x(e.g., credit score) and two possible actions: in turn raises their credit score. For the sake of our illustration, we assume credit-worthiness to be a socially desirable trait, and credit scores to be a good measure of credit-worthiness. We assume the decision-maker would like to design a recommendation policy in order to maximize the chance the decision-subject plays action In other words, a recommendationσ=ais BIC if the decision-subject’s expected utility with Finally, we remark that the decision-maker’s action recommendation policy is implicitly 1.Before training, the decision-maker and decision-subject share some priorπ(θ, b) over the true decision rule. 2. After training, the decision rule is revealed to the decision-maker. 3.The decision-maker then uses their recommendation policy and knowledge of the true decision rule to recommend an action for the decision-subject to take. Consider a simple setting under which a single decision-subject has one observable feature (a) = 0) anda= “pay oﬀ existing debt” (i.e., ∆x(a)>0,c(a)>0,u(a) = 1), which receive the loan. In this simple setting, the decision-maker’s decision rule can be characterized by a single parameter and a negative classiﬁcation otherwise. Note that the decision-subject does not know the exact value of b. Instead they have some prior over it, denoted as π(b). decision-subject then takes a possibly diﬀerent action feature from holds for any value of play action action a based on the outcome the decision-subject would receive if they take action for taking the desired action to make a diﬀerence in their classiﬁcation. We refer to this region as region matter what action they take. In this region, any diﬀerence on their classiﬁcation. We refer to this region as region and action Consider the following action recommendation policy. Action recommendation policy S: Case 3: b ∈ H would have taken had they known the true 3, the decision-maker recommends, with probability would not have taken, leveraging the fact that the decision-subject does not know exactly which case they are currently in. If the decision-subject follows the decision-maker’s recommendation from b ∈ H deﬁnition of “small” depends on the shared prior over will be in the decision-subject’s best interest to follow the decision-maker’s recommendation, even though they know that the decision-maker may sometimes recommend taking action not in their best interest to take that action! That is, the decision-maker may recommend that an decision-subject pay oﬀ existing debt with probability in order to secure a loan. We now give a criteria on policy S is BIC. Theorem 3.1. where π(M ) is an instance-speciﬁc value that depends on π(b) and x a] ≥ E[u Based on the initial value of the decision-subject’s observable featurexand the true value , the decision-maker recommends an actionσ ∈ {a, a}for the decision-subject to take. The , we assume the cost of action ais such that c(a) < 2. We observe that in this simple setting, we can bin values ofbinto three diﬀerent “regions”, + ∆x(a) +b <0, the decision-subject will not receive a positive classiﬁcation, even if L. Second, ifx+b ≥0, the decision-subject will receive a positive classiﬁcation no x+ ∆x(a) +b ≥0, the decision-subject will receive a positive classiﬁcation if they take aand a negative classiﬁcation if they take actiona. We refer to this region as regionM. In Case 2, the recommendation policy recommends the action (a) that the decision-subject S, then the decision-maker expected utility will increase from 0 toqif the realizedb ∈ Lor , and will remain the same otherwise. Intuitively, ifqis “small enough” (where the precise (a, b)|σ = a], where the expectation is taken with respect to the decision-subject’s posterior induced by σ = a. Since these conditions are satisﬁed, S is BIC. Proof. Based on the decision-subject’s prior over b, they can calculate L, M, and H will take the form in region classiﬁcation, so they will follow the decision-maker’s recommendation and take action a will take the form The decision-subject’s expected utility of taking actions by σ = a After canceling terms and simplifying, we see that π(L) =π(x+ ∆x(a) +b <0), i.e., the probability the decision-subject is in regionL according to the prior π(M) =π(x+b <0and x+ ∆x(a) +b ≥0), i.e., the probability the decision-subject is in region M according to the prior π(H) =π(x+b ≥0), i.e., the probability the decision-subject is in regionHaccording to the prior Case 1:σ=a. Given the signalσ=a, the decision-subject’s posteriorπ(·|σ=a) over π(L|σ = a) == π(M|σ = a) == 0 π(H|σ = a) == If the decision-subject receives signalσ=a, they know with probability 1 that they are not Mwith probability 1. Therefore, they know that taking actionawill not change their Case 2:σ=a. Given the signalσ=a, the decision-subject’s posterior overL,M, andH π(L|σ = a) === π(M|σ = a) === π(H|σ = a) === are (a, b)|a] = π(H|σ = a) · (1 − 0) + π(M|σ = a) · (−1 − 0) + π(L|σ = a) · (−1 − 0) (a, b)|a] = π(H|σ = a) · (1 − c(a)) + π(M|σ = a) · (1 − c(a)) + π(L|σ = a) · (−1 − c(a)) π(H|σ = a) · (1 − c(a))+π(M|σ = a) · (1 − c(a)) + π(L|σ = a) · (−1 − c(a)) Note that q ≥ 0 always. Finally, in order for q to be a valid probability, we restrict q such that This completes the proof. But how much better can the decision-maker do by using a BIC action recommendation policy, compared to natural alternatives? We answer this question concretely in the following section. As we will see in Section 4, the expected utility of the decision-maker when playing the optimal BIC recommendation policy will be no worse than their expected utility if they had completely revealed the decision rule to the decision-subject, or if they had revealed nothing and let the decision-subject act according to the prior. In this section, we show that the decision-maker’s expected utility when playing the optimal BIC recommendation policy can be arbitrarily higher than their expected utility from completely revealing the decision rule or revealing nothing. In particular, we prove the following theorem. Theorem 3.2. from playing the optimal BIC recommendation policy is arbitrarily higher than the expected decision-maker utility for revealing everything or revealing nothing. Proof. Consider the example in Section 3. Expected utility from revealing nothing. to the prior, they will select action probability 0 otherwise. Plugging in our expressions for that the decision-subject will select action a π(L)(−1−c(a Canceling terms and simplifying, we see that must hold for the decision-subject to select action gives us the condition 2π(M ) − c(a the decision-subject will select action decision-subject would take action that taking action a) is high, and would take action a −q(π(L) + π(H))c(a) + π(M)(2 − c(a)) = −q(1 − π(M ))c(a) + π(M)(2 − c(a)) ≥ 0 Under this setting, the decision-maker will achieve expected utilityπ(M) +q(1− π(M)). Expected utility from revealing everything. to the decision-subject, they will select action since is π(M). Expected utility from BIC policy. from Section 3 sets utility is we see that the decision-maker’s expected utility for the BIC policy is min{ maker’s expected utility will always be 0 from revealing nothing because The decision-maker’s expected utility from playing the BIC policy will be less than everything approaches 0 (the smallest value possible), and the decision-maker’s expected utility from the BIC policy approaches 1 (the highest value possible). This completes the proof. in Table 1. Note that when expected utility is always as least as good as the two natural alternatives of revealing nothing about the model, or revealing everything about the model. Table 1: Decision-maker’s expected utility when (1) revealing nothing about the model, (2) using the BIC recommendation policy, and (3) revealing everything about the model. See Section 3.1 for the full derivations. While our example signaling policy in the previous section leads to higher expected decision-maker utility, there is no reason a priori to believe it is optimal, or that we can expect similar results beyond the one action and one observable feature setting. We now derive the decision-maker’s optimal signaling policy for the general setting with described in Section 2. Under the general setting, the decision-maker’s optimal signaling policy can be described by the following optimization. where we omit the valid probability constraints over the decision-maker wants to design a signaling policy utility, subject to the constraint that the signaling policy is BIC. At ﬁrst glance, the optimization may initially seem hopeless as there are inﬁnitely many values of possible we will show that the decision-maker’s optimal policy can actually be recovered by optimizing over ﬁnitely many variables. over p(σ = a|θ, b), a ∈ A takes the following form u(a) = 1 andu(a) = 0, the decision-maker’s expected utility if they reveal everything min{1· π(M) +q ·(1− π(M)),1}. Substituting in our expression forqand simplifying, Suppose that 2π(M) =c(a)(1− ) andc(a) = 2, for some small >0. The decision- ) =(1− )< , the decision-maker’s expected utility from revealing everything will be . Therefore, asapproaches 0, the decision-maker’s expected utility from revealing The decision-maker’s expected utility as a function of their possible strategies is summarized θ,bcombination) that the decision-maker’s optimal policy must optimize over. However, By rewriting the BIC constraints as integrals overΘand applying Bayes’ rule, our optimization deﬁne below), we can pull diﬀerent regions. Intuitively, a region can be thought of as the set of all ( indistinguishable from a decision-subject’s perspective because they lead to the exact same utility for any possible action the decision-subject could take. Based on this idea, we formally deﬁne a region of Θ as follows. Deﬁnition 4.1 all regions by R. the following form: according to the decision-subject prior. optimize directly over these quantities. The ﬁnal step is to rewrite the objective. For completeness, we include the constraints which make each valid probability distribution. Theorem 4.2 characterized by the following linear program: decision-maker can always recommend the action the decision-subject would play according to the prior. s.t.p(σ = a|θ, b)π(θ, b)(u(a, θ, b) − u(a, θ, b))dΘ ≥ 0, ∀a, a∈ A. Note that ifu(a, θ, b)− u(a, θ, b) is the same for some “region”R ⊆ Θ(which we formally a, θ, b) =u(a, θ, b)− u(a, θ, b),∀a, a∈ A, (θ, b),(θ, b)∈ R. We denote the set of After pulling the decision-subject utility function out of the integral, our optimization takes Now that the decision-subject’s utilityu(·) no longer depends on (θ, b), we can integrate =a|θ, b)π(θ, b) over each regionR. We denotep(R) as the probability that the true (θ, b)∈ R Since it is possible to write the constraints in terms ofp(σ=a|R),∀a ∈ A, R ∈ R, it suﬃces to Note that the feasible region of the optimization problem will always be non-empty, as the We have shown that the problem of determining the decision-maker’s optimal signaling policy can be transformed from an optimization over inﬁnitely many into an optimization over ﬁnitely many regions R ∈ R may require integrating over 2 the worst-case. (Recall that of actions available to the decision-subject.) In this section, we show that much smaller for a special ordering over actions (as shown in Figure 1). This action scheme captures the set of actions that only aﬀect one observable feature and can reasonably be thought of as coming from a ﬁnite set. We believe this action scheme reasonably reﬂects many real-world settings (e.g., consider a setting in which an decision-subject must choose between paying oﬀ some amount of debt and opening a new credit card, when strategically modifying their observable features before applying for a loan). over Θ( characterize the size of can be derived by simply plugging in the decision-subject utility function into Deﬁnition 4.1 and canceling terms. Deﬁnition 4.3 sign{ ∆x(a for any two actions is that a region receive a positive classiﬁcation when ( classiﬁcation for some ( and ( for ( region, which is the union of two convex sets (i.e., the set of ( the decision-subject receiving a positive classiﬁcation and the set of ( results in the decision-subject receiving a positive classiﬁcation). Besides this special case, if by Deﬁnition 4.3. Armed with this alternative deﬁnition of regions over characterize |R| for the setting described in Figure 1. Theorem 4.4. |R| and disposal to improve each observable feature. Proof. action a. In general, the LP which characterizes the optimal signaling policy (Theorem 4.2) optimizes md|R|) variables, wheremis an upper-bound onm,∀i ∈ {1, . . . , d}. In order to ))θ+ b}, ∀a, a∈ A, (θ, b), (θ, b) ∈ R. In other words, (θ, b) and (θ, b) belong to the same region if the diﬀerence in their predictions θ, b) to be in the same region is if taking any action inAresults in a positive classiﬁcation θ, b) and a negative classiﬁcation for (θ, b). We refer to this region as the “all or nothing” ) and (θ, b) produce diﬀerent classiﬁcations for the same action, they are in diﬀerent regions = Ω(m) and|R|=O(m)), wheredis the number of decision-subject observable features, m=maxmis an upper bound on the number of actions the decision-subject has at their In order to characterize the number of regions|R|, we use the notion of a dominated Deﬁnition 4.5 ∆x(a)  ∆x(a that in general if action terms of ∆ i ∈ {1, . . . , d}. Lemma 4.6. Proof. 0. If Therefore, if ( feature, and the decision-subject has at most feature. can essentially be thought of as optimizing over a number of variables that is polynomial in the number of actions intractable. actions associated with a hyperplane in a positive classiﬁcation when taking action a negative classiﬁcation otherwise. Therefore, each region be calculated as π(R) = We provide preliminary experiments for the simple setup described in Section 3 to demonstrate the utility that the decision-maker gains from the optimal BIC action recommendation policy over other baselines. The baseline policies we consider are (1) revealing the full decision rule and (2) revealing no information about b. approve a loan application from an applicant based on credit score threshold classiﬁer. The bank approves the application ( otherwise. Here, we assume the ground-truth threshold value used by the decision-maker to be 670 (i.e. “do nothing” and good measure of credit-worthiness. Finally, we assume the prior to be π(b) ∼ N (µ decision-maker utility compared to the baseline policies. In particular, Figure 2a shows the decision-maker’s expected utility for diﬀerent credit scores two scenarios, each with the standard deviation of the prior distribution of σ= 50 (right) but the same mean of (orange) yields peak decision-maker expected utility around the true For example, in Figure 1, actionais dominated by actiona,∀i,1≤ j < k ≤ m. Note The decision-subject receives a positive classiﬁcation for some (θ, b) if (x+∆x(a))θ+b ≥ adominatesa, then ∆x(a) can be written as ∆x(a) = ∆x(a)+δ(a, a), whereδ(a, a) 0. + ∆x(a))θ + δ(a, a)θ + b ≥ 0 (and a∈ A), since δ(a, a)  0and θ ∈ R. |R|= Θ(m) then follows directly from the fact that each action only aﬀects one observable Under this setting, when the number of observable featuresdis small, the decision-maker Now that the space of regionsRis speciﬁed, it is possible to calculateπ(R) forR ∈ R, given ) without enumerating all possibilities. Recall that a regionRis characterized by the set of A⊆ Athat receive a positive classiﬁcation whenever (θ, b)∈ R. Each actiona ∈ Ais To contextualize this simple synthetic setup, consider a banking institution deciding whether b=−670), which is typically considered as a decent credit score. Recall thata= (a) = 1, u(a) = 0, as, for the sake of our illustration, we assume credit score to be a In Figure 2, we show how the optimal BIC action recommendation policy obtains higher (a) Expected decision-maker utility when varying prior standard deviation, σ Figure 2: (a) decision-maker’s expected utility across diﬀerent decision-subject scores( decision-maker’s total utility computed by summing up expected utility across diﬀerent scores of decision-subjects, for three policies: our BIC action recommendation policy ( revealing the full decision rule ( As the decision-subject’s uncertainty about the true threshold advantage of BIC policy becomes more visible. policy (green) does so around the prior mean true the decision-maker beneﬁts from the BIC action recommendation policy for a broader range of credit scores unlike other baselines. assume a uniform distribution of the decision-subjects’ credit scores in the population and take the sum of expected decision-maker utility values across diﬀerent scores (equivalent to the area under the curve of the decision-maker’s expected utility in Figure 2a). We plot these “total” utility values in Figure 2b, and as expected, the larger of our method compared to the baselines. Intuitively, this is because the more uncertain a decision-subject is about the true decision rule being deployed, the more the decision-maker can leverage this uncertainty to persuade the decision-subject to take desirable actions. utility to decrease, as there is less incentive for the decision-subjects to take the action shown in Figure 3, we indeed observe such a trend as the cost of decreases (right). Nevertheless, our BIC action recommendation policy (blue) yields strictly higher decision-maker utility values compared to the baselines across all conditions. In this work, we propose a way in which a decision-maker can leverage their information advantage over decision-subjects in order to incentivize them to take more desirable actions. In order to do so, we eﬀectively cast the algorithmic recourse problem as a game of Bayesian persuasion. We show that the decision-maker’s optimal signaling policy takes the form of a linear program with Θ( which captures several real-world settings. While solving this LP requires keeping track of a number of variables which may be exponential in the input size, the initial characterization of the optimal action recommendation policy requires optimizing over inﬁnitely many variables. There are several exciting directions for future work, which we outline below. bincreases (i.e., the standard deviation of the prior distribution increases from 10 to 50), To better measure the total amount of decision-maker utility yielded by each policy, we As actionabecomes more cost-prohibitive (or less eﬀective), we expect the decision-maker’s md|R|) variables, and we provide a characterization of|R|for a special action structure Figure 3: Decision-maker’s total utility across diﬀerent cost (left) and ∆ BIC action recommendation policy ( orange), and policy revealing no information ( cost-prohibitive (high cost) or less eﬀective (small ∆ as there is less incentive for the decision-subject to take the action. Nevertheless, BIC action recommendation policy yields strictly higher decision-maker utility compared to the baselines. Characterizing the number of regions for more general action spaces. characterized the number of regions for settings in which the action ordering takes a special structure (namely, each action aﬀects only one observable feature), it may be possible to characterize the number of regions for more complicated settings. In order to do this, new algorithmic techniques may be required. NP-Hardness proof. signaling policy requires optimizing over an exponential number of variables in the worst-case provides strong evidence to suggest that the problem of recovering the decision-maker’s optimal BIC recommendation policy is NP-Hard. This evidence is further supported by hardness results for similar persuasion settings [ a formal reduction to a problem in NP. Public persuasion. by each decision-subject were private. However, if a decision-subject is given access to recommendations for multiple individuals, it may be possible for them to reconstruct the underlying model. While out of the scope of this project, it would be interesting to study models of public persuasion in the Experiments on semi-synthetic datasets. dataset inspired by our example provided in Section 3. It would be interesting to evaluate our methods in a more realistic regime, such as one based on the Taiwan credit dataset [33]. Other simplifying assumptions. insights oﬀered by our analysis. In particular, we consider linear decision rules and assumed all decision-subject parameters (cost function, initial observable features, etc.) were known to the decision-maker. It would be interesting to extend our work to settings with non-linear decision rules, or settings in which not all of the decision-subjects’ parameters are known to the decision-maker.