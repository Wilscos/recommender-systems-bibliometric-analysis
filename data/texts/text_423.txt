Scaling reinforcement learning (RL) to recommender systems (RS) is promising since maximizing the expected cumulative rewards for RL agents meets the objective of RS, i.e., improving customer’s long-term satisfaction. A key approach to this goal is ofﬂine RL, which aims to learn policies from logged data. However, the high-dimensional action space and the non-stationary dynamics in commercial RS intensify distributional shift issues, making it challenging to apply ofﬂine RL methods to RS. To alleviate the action distribution shift problem in extracting RL policy from static trajectories, we propose Value Penalized Q-learning (VPQ), an uncertaintybased ofﬂine RL algorithm. It penalizes the unstable Q-values in the regression target by uncertainty-aware weights, without the need to estimate the behavior policy, suitable for RS with a large number of items. We derive the penalty weights from the variances across an ensemble of Q-functions. To alleviate distributional shift issues at test time, we further introduce the critic framework to integrate the proposed method with classic RS models. Extensive experiments conducted on two realworld datasets show that the proposed method could serve as a gain plugin for existing RS models. Practical recommender systems (RS) are usually trained for estimating a user’s immediate response to a slate of items, without considering the long-term utility. Reinforcement learning (RL) methods are designed to maximize the discounted cumulative rewards over time, which perfectly satisﬁes the original needs of RS. Driven by this motivation, there has been tremendous progress in developing reinforcement learning based recommender systems (RLRS) (Afsar, Crump, and Far 2021). A promising approach for achieving RLRS is learning from the previously collected data because (1) there are lots of logged data available for training RL agents, and (2) it is expensive and risky to train commercial recommendation agents purely with online interactions. However, directly utilizing RL algorithms in the ofﬂine setting often results in poor performance, even for off-policy methods, which can leverage data collected by other policies in principle (Fujimoto, Meger, and Precup 2019; Levine et al. 2020). The main reason may be the overestimation for out-ofdistribution (OOD) queries, induced by distributional shift, both at test and training time (Levine et al. 2020). Unlike the general RL algorithms, ofﬂine RL (batch RL) concerns the problem of training agents from a static dataset, addressing the OOD predictions without access to interacting with the environment. The settings of ofﬂine RL meet the requirements of RLRS, making it achievable to build RLRS agents from logged data. Although it is promising to utilize ofﬂine RL approach in RS, there are still challenges due to the different settings between RL and RS tasks. Firstly, the number of candidate items for RS agents is much larger than the action space in synthetic RL environments. With a larger action space, the number of in-distribution data points is much less than that of OOD queries, which represents as a sparse user-item interaction matrix. Therefore, the RLRS agents are more vulnerable to overestimation errors during training. Moreover, the large action space also intensiﬁes the difﬁculty of estimating an accurate behavior policy for constraining the learned policy’s deviation from the logged data. As a result, solutions from ofﬂine RL domain are probably not suited for RS tasks. Secondly, the non-stationary dynamic of RS also differs from RL environments. In general ofﬂine RL problems, e.g. D4RL (Fu et al. 2020), trajectories are collected by policies with an environment whose dynamics are exactly as same as the test environment. While in recommendation scenarios, the environment dynamics constantly change as the user preferences changing over time. This paper proposes Value Penalized Q-learning (VPQ), an uncertainty-based ofﬂine RL method, to ﬁx the distributional shift issues during training. Then we combine it with different recommendation models, making a trade-off between the robust RS models and the alluring but risky RL methods. Speciﬁcally, we use sequential or session-based recommendation models to represent the sequence of user-item interactions (states). We deﬁne the agent as a functional instance generating recommendations (actions) by the states received from users (environment) so as to maximize their long-term satisfaction, e.g., purchases and clicks. We then take two techniques to address the distributional shift issues for value-based RL methods in recommendation setting. Firstly, we penalize the unstable Q-values in regression target with uncertainty-aware weights, reducing the overestimation induced by the action distribution shift during the learning process. We train an ensemble of Q-functions with Figure 1: RLRS with VPQ. Recommendation models are employed to extract the hidden state for CE head (supervised head trained with cross-entropy loss to perform ranking) and Q head (trained for maximizing the accumulated rewards). We attain the conservative value function via penalizing the unstable targets by uncertainty-aware weights. different random learning rates and set the variance of their predictions as the uncertainty quantiﬁer. Compared with ofﬂine RL methods that subtract uncertainty from predictions, our method is empirically more effective for obtaining the conservative value function in the RS settings. Besides, we employ the critic framework (Xin et al. 2020) to make use of the learned value function while avoiding the performance degradation caused by the distributional shift issues at test time. The architecture of an RS instance integrated with VPQ is shown in Figure 1. At test time, we discard all other components except the CE head. Although it is a trade-off between the RL-based method and the supervised approach, experimental results show that we can still beneﬁt from incorporating VPQ into RS models. We summarize the contributions of this work as follows. (1) We propose an uncertainty-based ofﬂine RL method, VPQ, to address the action distribution shift issue during training by penalizing the unstable predictions with uncertainty quantiﬁcation. (2) We empirically show that, it is more effective to attain the conservative value function by multiplying the unstable predictions with uncertainty-aware weights in the recommendation scenarios. (3) Extensive experiments show that the beneﬁt of integrating with VPQ contains better representation and reinforcing actions with long-term rewards. There are several difﬁculties in building RLRS, such as the state representation (Lei et al. 2020; Liu et al. 2020; Wang et al. 2020b), the slated-based recommendation setting (Sunehag et al. 2015; Swaminathan et al. 2017; Chen et al. 2019; Gong et al. 2019; Ie et al. 2019) and the largescale recommendation problem (Dulac-Arnold et al. 2015; Zhao et al. 2019). However, there has been limited attention paid to the negative impact of ofﬂine setting, i.e., training RLRS agents from static datasets. Here we give a literature review on the general ofﬂine RL methods. Constrain Policy Close to the Behavior Policy. Methods based on different distance metrics are proposed to compel the RL policy close to the behavior policy that produces the dataset (Levine et al. 2020), such as KL divergence (Jaques et al. 2019), Wasserstein distance (Wu, Tucker, and Nachum 2019), MMD (Kumar et al. 2019), Fisher divergence (Kostrikov et al. 2021) and Euclidean squared distance (Fujimoto and Gu 2021). In this approach, GCQ (Wang et al. 2020a) utilizes a neural generator to recover the distribution of dataset, which is similar to BCQ (Fujimoto, Meger, and Precup 2019). These methods may suffer from estimating the dataset trajectories distribution in RS. Estimate Accurate/Conservative Q-values. Algorithms like Ensemble-DQN (Faußer and Schwenker 2015; Osband et al. 2016), QR-DQN (Dabney et al. 2018), and REM (Agarwal, Schuurmans, and Norouzi 2020) in this approach estimate Q-values by predicting the expectation of a distribution or an ensemble. Another instance is CQL (Kumar et al. 2020), which adds two regularization terms to the Q-function update so as to avoid overestimating OOD actions. Despite its good performance in general RL testbeds, CQL may suffer from high dimensional action space (minimization term in CQL) and non-user trajectories in the datasets (maximization term). The proposed VPQ can be categorized into this approach. Besides, a recently proposed method named UWAC (Wu et al. 2021) adds uncertaintyaware weights to the Q-function queries, combining the two main approaches. MDP on Recommendation Problem We represent the recommendation problem as a Markov decision process (MDP) deﬁned by a tuple (S, A, T, d, r, γ): • State space S is a continuous space describing the user’s interaction history. The interaction sequence contains 10 items the user lastly interacted with. We then use the recommendation model to map it into a latent space as the state for Q heads and as the features for CE head. • Action space A is a ﬁnite set of actions of recommendation agents. For a state s, the candidate actions are all items in the dataset. • Initial state distribution dis the distribution that initial state follows. • Transition probability T is a conditional probability with T (s|s, a) describing the distribution over the next state safter taking action aat state s. • Reward function r describes the immediate rewards. For example, agent will get a reward r(s, a) when taking the action aat state s. • Discount factor γ is a scalar that makes trade-off between immediate rewards and long-term rewards. The goal of reinforcement learning is to ﬁnd a policy π(a|s), which deﬁnes a distribution over actions aconditioned on states s, so as to maximize the cumulative reward: where the trajectory distribution pfor a transition probability T , a policy π and an initial state distribution dis: Value-based RL Value-based RL methods, such as DQN (Mnih et al. 2015), usually try to learn a state-action value function Q(s, a) with parameter θ, deﬁned as: An improved policy π(a|s) can be easily obtained by π(a|s) = arg maxQ(s, a). And the parameter θ for the value network is obtained by minimizing the following loss function with a given dataset D: where the item y= r+ γ maxQ(s, a) is the so called target, and Qis the target network with frozen parameters from historical value function Q. A key source of instability of value-based RL methods applying on static datasets is the distributional shift during the training and test phase (Levine et al. 2020). Distributional shift happens when evaluating a trained policy on another different distribution. It can be categorized into two types: state distribution shift and action distribution shift. Here, we explain these issues within the value-based RL methods. State distribution shift affects the performance of RL agents only at test time. The learned policy will always meet unseen states at test time since the dataset usually does not fully cover the state space. The output of the Q-function can be unstable as the loss function in Equation (4) makes no guarantees about the error when encountering OOD state inputs, resulting in performance degradation at test time (Lee et al. 2021). Unfortunately, the narrow distribution of the collected trajectories in RS datasets and the non-stationary environments exacerbate this problem. Action distribution shift affects the agent both at test time and training time. The Q-function is trained on dataset stateaction pairs but is evaluated on all valid actions for policy improvement and real actions generation. At test time, the unstable predictions on OOD actions may result in unreasonable actions as the side effect of the greedy policy. During training, the highest value among noisy predictions in target will be taken as the future reward and then spread to the learned Q-function, leading to a persistent overestimation bias (Jaques et al. 2019; Kumar et al. 2019). In the online setting, active learning around these overestimated state-action pairs would correct them. By contrast, such correction will never happen in the ofﬂine setting since the agents can no longer interact with the environment. Moreover, the large action space in RLRS exacerbates this problem. For example, the general RL testbed Arcade Learning Environment (Bellemare et al. 2013) consists of no more than 18 discrete actions, while the candidate recommending items in the Retailrocket dataset, a real-world commercial dataset, is 70,852 (please refer to Table 1 for the details). To address the distributional shift issues for training agents on ofﬂine RS datasets, we propose VPQ. It learns a conservative value function by penalizing the unstable regression targets with uncertainty-aware weights. After that, we integrate the proposed method with classic RS models to alleviate the distributional shift issues at test time. The training paradigm of the integrated model is shown in Figure 1 and the proposed VPQ is illustrated as Algorithm 1. The main concern of this work is how to use the uncertainty metric to penalize the unstable predictions in the target network, so as to alleviate the impact from the distributional shift in training agents for RLRS. In this part, we ﬁrstly give two different ways of using the uncertainty quantiﬁer, and then we analyse the advantages of our method, ﬁnally we give details about the proposed method. Two Different Ways of Using the Uncertainty Metric. An intuitive approach for uncertainty-based ofﬂine RL is to regress to the following target (Levine et al. 2020; Buckman, Gelada, and Bellemare 2020; Jin, Yang, and Wang 2020): where Unc quantiﬁes the amount of uncertainty for each query. We denote this family of algorithms as p-sub for they remove the uncertainty by subtracting. Our approach to penalizing the unstable prediction is: y = r + γ maxQ(s, a) · W (s, a) where W (s, a) is a uncertainty weight for each prediction. In our practice it is designed as: where λ > 0 is a scaling factor that controls the amount of uncertainty. Unc is deﬁned as the standard deviation across an ensemble of target Q-functions for each state-action pair, in this work. We denote this formulation as p-mul for the stable predictions are calculated by multiplying the unstable values by uncertainty-aware weights. The Advantage of the Proposed Penalty Formulation. We here give an analysis on the two families of penalty formulations in a large action space. With the assumption that unstable predictions on OOD actions for a given state, i.e., Q(s, a), are i.i.d. random variables follow a normal distribution x ∼ N (µ, σ), then the target value can be described as: where n equals to the number of OOD actions, and we denote Qas the maximum value of the n samples. Since the exact calculation of Qcan be very hard, we here use the approximate form from the literature on normal order statistics (Blom 1958; Harter 1961; Royston 1982), i.e., Emaxx≈ µ+σΦ(n − 0.375n − 2 × 0.375 + 1), where Φis the inverse of the standard normal CDF. By using the properties of the expectation and max operator, we have the expected value of the penalized Qwith the p-sub formulation: and the expectation of that with the p-mul form: where Cis a constant number for a given n. There are two beneﬁts of the proposed penalty formulation. Firstly, the penalized Q-values from p-mul is more robust than those from p-sub. For p-sub, a large λ would incur target values less than zero, while a small scaling factor could not reduce the unstable predictions. Unfortunately, Algorithm 1: VPQ: Value Penalized Q-learning Input: K randomly initialized Q-functions with parameters {θ}, a mixture distribution Pfor Random Mixture, a scale factor λ for VPQ and an ofﬂine dataset D Output: {θ} 1: Initialize all trainable parameters. 2: while not done do randomly from D 10: end while for training RLRS agents, Cincreases with an increase in the dimension of action space. To reduce unstable predictions, such a method has to increase the λ in Equation (5), which increases the risk of producing negative Q-values. As we will show in the next part, Q-values that below than zero would harm the performance. By contrast, the penalty from p-mul would always keep the target value greater than zero, even with a large scale factor. Secondly, the proposed p-mul formulation could heavily penalize the unstable and high predictions, while the p-sub formulation mainly penalizes the unstable predictions with small values. To alleviate the distributional shift at training time, it is more crucial to reduce the uncertainty within the large values because of the max operator. Finally, to illustrate the difference between the two forms, we give a toy experiment in Figure 2. Figure 2: Penalize the simulated unstable Q-values in different ways. Heavy black lines are dense points sampled from a Gaussian distribution and sorted by their values. Scaling factor λ controls the intensity of penalization. Details about the Proposed Algorithm. Inspired by the powerful penalty formulation, p-mul, we develop VPQ. For this paper, the uncertainty metric is deﬁned as the standard deviation across an ensemble of Q-functions, i.e., a fully connected layer with K heads. We use the Random Ensemble Mixture (REM) technique (Agarwal, Schuurmans, and Norouzi 2020) to update each head with a random learning rate towards a shared regression target, improving the diversity of the learned Q-functions. The weights for mixing are sampled from a categorical distribution: ﬁrst sample a set of K values i.i.d. from the uniform distribution U(0, 1) and then normalize them. Introducing the uncertainty eliminates the need to estimate the behavior policy for constraint. Besides, as an uncertainty-based ofﬂine RL method, VPQ may enjoy beneﬁts from the progress in uncertainty measurement. Finally, we summarize our method in Algorithm 1. The Critic Framework To alleviate the action and state distribution shift at test time, we introduce the critic framework (Xin et al. 2020), which is inspired by the Actor-Critic framework (Konda 2002). Speciﬁcally, the Q-function learned from the proposed VPQ algorithm is employed to reweight the corresponding crossentropy loss, i.e., At test time, we only use the CE head to generate recommendations. The motivation is that the CE head is naturally free from the action distribution problem since it is optimized for all actions over each dataset state. Although it is a trade-off between the classic recommendation paradigms and the alluring RL methods, the critic framework may improve the performance of RS with stability. In this part we conduct experiments to verify the effectiveness of the proposed VPQ against online and ofﬂine RL methods in the recommendation setting. Experimental Settings Datasets. The datasets used in the experimentsare Retailrocketand Yoochoose. We follow the data processing details in (Xin et al. 2020) and split each into two parts: 80% for training and 20% for testing. Statistics of the ﬁnal datasets are summarized in Table 1. Evaluation Metrics. We use two metrics: HR (hit ratio) for recall and NDCG (normalized discounted cumulative gain) for rank. We compute the ranking performance on clicked and ordered items separately. For example, we deﬁne HR for clicks as: We will release our code soon. https://www.kaggle.com/retailrocket/ecommerce-dataset https://recsys.acm.org/recsys15/challenge/ Table 1: Statistics of datasets used in the experiments. Parameter Settings. Following (Xin et al. 2020), we use RS models to represent the states. The item embedding size and the dimension of latent representations (states) in the RS models are set as 64. For RL methods, the rewards r of clicked and purchased items are set as 0.2, 1.0, respectively. The discount factor γ is 0.5, for the short trajectories in RS task. For VPQ, the scale factor λ is set to 20, and the number of heads K is 15. The optimizer for minimizing Q loss and CE loss is Adam. The learning rate is set to 0.005. The minibatch size is 256, and we evaluate the performance every 2000 batches. Performance Gains from VPQ To verify the effectiveness of VPQ in recommendation task, we integrate it with four classic RS methods: GRU (Hidasi et al. 2016), Caser (Tang and Wang 2018), NextItNet (Yuan et al. 2019) and SASRec (Kang and McAuley 2018). We also compare its performance with the following online RL and ofﬂine RL-based methods: Self-Supervised Q-learning (SQN) (Xin et al. 2020) are RL plugin methods for RS. The main difference between the two methods is SAC learns a Q-function for reweighting the CE loss, while SQN only minimizes the TD error as an auxiliary loss for better representation. • REM An RL method that trains an ensemble of Qfunctions for stable predictions (Agarwal, Schuurmans, and Norouzi 2020). Our implementation uses a shared regression target for its robustness. Comparison with this approach tells us whether the improvement achieved by our method comes from the ensemble framework. • Minus Obtaining a conservative value function with the p-sub formulation in Equation (5). We choose the standard deviation across REM heads as the uncertainty. From the previous analysis, this method should face the dilemma of choosing an appropriate scale factor. • CQL As a state-of-the-art algorithm in general ofﬂine RL, CQL (Kumar et al. 2020) performs well on both discrete and continuous control domains. Our implementation uses the CQL(H) loss as the following: Table 2: Overall performance comparison on two recommendation datasets, averaged over 5 runs. NG is short for NDCG. • UWAC An actor-critic based ofﬂine RL algorithm that adds uncertainty-aware weights on Bellman loss and the actor loss (Wu et al. 2021). We incorporate this method to REM by adding its uncertainty-aware weights to the ﬁnal Q-function estimates, i.e., All the above methods are embedded with the four classic RS models under the same critic framework, except SQN. We select the best hyper-parameter λ for Minus, α for CQL, and β for UWAC by sweeping on each dataset. λ for VPQ is set to 20 for both datasets. Table 2 shows the top-20 performance of prototypical models integrated with or without different online or ofﬂine RL methods. Compared to the corresponding original RS methods, almost all combined methods perform better. The consistent improvement from SQN implies that minimizing the TD error could beneﬁt the representation learning. Improvements from other RL methods is unstable, suggesting that accurate Q-values may improve the performance while an unstable value function could harm the recommendation. Compared to the RL methods, VPQ performs better. CQL performs not so well as the minimization item in its loss function would produce Q-values below zero, which is unreasonable in our setting and may impact its ﬁnal performance. REM behaves similar to SAC, indicating that a simple ensemble method could not address the difﬁcult distributional shift issues. The advantage compared to REM also illustrates that the performance gain from VPQ is not from the ensemble method but from successfully suppressing the unstable Q-values. Besides, the advantage compared to UWAC suggests it is more effective to avoid Q-function explosion by penalizing at bootstrapping time. Compared to the Minus method, VPQ can produce more conservative and stable Q-values for reweighting the CE loss in our observation, while the mean and variance of Qvalues from Minus and REM are higher than that of VPQ. This may suggest that the Minus methods could not reduce the unstable predictions in recommendation scenarios. Figure 3: Comparison of performance when integrating Q head in different ways. Error bars show standard deviation. Ablation Study: Where do the gains come from? We compare four ways of using the learned Q-function: (1) Q, only train the Q head and generate recommendation according to Q-values, (2) CE, only optimize the CE head (the prototypical methods), (3) Aux, use the TD error as auxiliary loss for representation learning (SQN way), (4) Critic, use the learned Q-function as a critic for CE loss (SAC and VPQ way). The base model is NextItNet, and all hyper-parameters are the same as the previous experiments. The results are shown in Figure 3. Q achieves the worst performance on both datasets, although we use the negative feedback (Zhao et al. 2018) technique to stabilize the training process. This veriﬁes the necessity of considering the impact of distributional shift at test time. Aux surpasses CE, suggesting that minimizing the TD error would beneﬁt representation learning. Moreover, by incorporating the learned Q-values with the CE loss, Critic outperforms Aux, achieves the best performance generally. The results above suggest that improvement of incorporating with VPQ comes from better state representation (serving as an auxiliary loss) and exploiting more accurate Qvalues without destroying performance boost from the former (successfully mitigating the distributional shift issues). We conduct experiments to investigate the effect of the scaling factor (λ in VPQ). We give the performance and the values of key indicators of VPQ with different λ in Figure 4. The base model is NextItNet, the dataset is Yoochoose and the key indicators are the mean and standard deviation of the Q-values queried for reweighting. When λ increases from 1 to 20, the recommendation performance improves continually (Figure 4(a)) with the decreasing of the key indicators (Figure 4(b)), suggesting that the scaling factor λ controls the amount of penalization on unstable predictions and thereby affects the performance. When λ is greater than 20, increasing the penalization does not lead to a more conservative Q-function, i.e., the key indicators (almost) stop decrease with the λ increases. Instead, it hurts the performance of predicting items with high rewards (purchased items). Besides, the optimal λ for other three base models and the Retailrocket dataset is also 20. (b) Correlation between the hyper-parameter λ and indicators Figure 4: Sweeping the scaling factor on Yoochoose dataset. We plot the scaling factor λ using a base-e logarithmic scale. Error bars show standard deviation. Scaling RL methods into recommendation tasks is challenging due to the severe distributional shift issues in RS settings. In this paper we proposed Value Penalized Q-learning (VPQ) to address the distributional shift during training. It is an uncertainty-based ofﬂine RL algorithm that avoids estimation of difﬁcult behavior policy or constraints on the learned policy, and is suitable for RS with large item set. In addition, we illustrate its properties and empirically demonstrate its advantage over other ofﬂine RL methods in terms of robustness and effectiveness. The consistent improvement from incorporating with VPQ shows that it could be a gain plugin for recommendation models.