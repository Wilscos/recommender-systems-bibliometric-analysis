Recently, the graph neural network (GNN) has shown great power in matrix completion by formulating a rating matrix as a bipartite graph and then predicting the link between the corresponding user and item nodes. The majority of GNN-based matrix completion methods are based on Graph Autoencoder (GAE), which considers the one-hot index as input, maps a user (or item) index to a learnable embedding, applies a GNN to learn the node-specic representations based on these learnable embeddings and nally aggregates the representations of the target users and its corresponding item nodes to predict missing links. However, without node content (i.e., side information) for training, the user (or item) specic representation can not be learned in the inductive setting, that is, a model trained on one group of users (or items) cannot adapt to new users (or items). To this end, we propose an inductive matrix completion method using GAE (IMC-GAE), which utilizes the GAE to learn both the user-specic (or item-specic) representation for personalized recommendation and local graph patterns for inductive matrix completion. Specically, we design two informative node features and employ a layer-wise node dropout scheme in GAE to learn local graph patterns which can be generalized to unseen data. The main contribution of our paper is the capability to eciently learn local graph patterns in GAE, with good scalability and superior expressiveness compared to previous GNN-based matrix completion methods. Furthermore, extensive experiments demonstrate that our model achieves state-of-the-art performance on several matrix completion benchmarks. Our ocial code is publicly available. • Mathematics of computing → Graph algorithms;• Computing methodologies → Neural networks. matrix completion, graph neural networks, GAE-based model, inductive learning, recommender system ACM Reference Format: Wei Shen, Chuheng Zhang, Yun Tian, Liang Zeng, Xiaonan He, Wanchun Dou, and Xiaolong Xu. 2021. Inductive Matrix Completion Using Graph Autoencoder. In Proceedings of the 30th ACM Int’l Conf. on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3459637.3482266 Matrix completion (MC) [5,11,17] is one of the most important problems in modern recommender systems, using past user-item interactions to predict future user ratings or purchases. Specially, given a partially observed user-item historical rating matrix whose entries represent the ratings of users with items, MC is to predict the missing entries (unobserved or future potential ratings) in the matrix based on the observed ones. The most common paradigm of MC is to factorize the rating matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), and then predict the missing entries based on these latent embeddings. Traditional matrix completion methods [3,5] have achieved great successes in the past. However, these methods mainly learn the latent user (or item) representation yet largely neglect an explicit encoding of the collaborative signal to reveal the behavioral similarity between users [24]. These signals are crucial for predicting the missing rating in the rating matrix, but hard to be exploited, since they are hidden in user-item interactions [8]. Recently, many works [2,18,26,28] have studied using a GNN to distill collaborative signals from the user-item interaction graph. Specially, matrix completion is formulated as link prediction, where the rating matrix is formulated as a bipartite graph, with users (or items) as nodes and observed ratings/interactions as links. The goal of GNN-based matrix completion methods is to predict the potential or missing links connecting any pair of nodes in this graph. Graph Table 1: We compare the GNN-based matrix methods from dierent aspects: 1) whether they learn node-specic representations for p ersonalized recommendation (denoted as Specic), 2) whether they learn local graph patterns (denoted as Local), (3) whether they are ecient matrix completion methods (denoted as Ecient), (4) whether they are inductive matrix completion methods (denoted as Inductive) Autoencoder (GAE) [12] is a popular GNN-based link prediction method, where a GNN is rst applied to the entire network to learn node-specic representations. Then the representations of the target nodes are aggregated to predict the target link. Many GNN-based matrix completion methods directly apply GAE to the rating graph to predict potential ratings such as GC-MC and NMTR [2,6]. By exploiting the structure of the bipartite user-item graph, the node-specic representations learned by GAE, which represents user-specic preferences or item attributes, are more expressive than the patterns learned by the traditional matrix completion methods for personalized recommendation. Despite its eectiveness, there remain two main challenges to apply GAE-based matrix completion to real recommender systems. The rst challenge stems from a key observation from real-world scenarios: There are a large number of users or items in a real recommender system that have few historical ratings. This requires a model to predict potential ratings in a sparse rating matrix. However, GAE-based models usually fail in this situation since there are a few historical ratings in a sparse rating matrix for GAE-based models to train node-specic representations for personalized recommendation [29]. The second challenge is applying the GAE-based models to real recommender systems for the large-scale recommendation. In real recommender systems, new users (or items) are emerging that are not exposed to the model during training. This requires that the model to be inductive, i.e., the model trained on a group of users (or items) can adapt to new groups. However, previous GAE-based models are all transductive models so that the learned node representations cannot be generalized to users (or items) unseen during training [28]. The following question arises: Can we have a GAE-based model that can not only guarantee good performance on a sparse rating matrix but also enable inductive learning? In fact, using GAE to simultaneously satisfy the two requirements for matrix completion is a non-trivial challenge when high-quality user (or item) features are unavailable. The one-hot node indices (together with learnable node-specic embeddings) in the GAE-based model give a maximum capacity for learning distinct user preferences (or item attributes) from historical ratings. On the other side, learning distinct user preferences (or item attributes) in GAE also requires adequate rating samples from the rating matrix. Accordingly, without adequate rating samples in a sparse rating matrix, it is hard for GAE to obtain satisfactory performance. Moreover, for unseen nodes from a new rating matrix, GAE lacks the representations of them, and therefore cannot predict the potential ratings in a new rating matrix, which makes inductive learning impossible. To overcome these two challenges, Zhang and Chen[28]propose an inductive matrix completion based on GNN (IGMC). To predict a potential link (i.e., rating), it rst extracts a 1-hop subgraph around the target link and then relabels the node w.r.t the distance to the target nodes. Finally, a GNN is applied to each subgraph to learn the local graph patterns that can be generalized to an unseen graph. By learning local graph patterns, IGMC has a better performance on the sparse rating matrix and enables inductive matrix completion. However, extracting subgraphs in both training and inference processes is time-consuming for the real recommendation. Moreover, the performance degradation on the dense rating matrix in IGMC also hinder us from applying it to real recommender systems. In this paper, we propose an inductive matrix completion method using GAE (IMC-GAE) that achieves ecient and inductive learning for matrix completion, and meanwhile obtain good performance on both sparse and dense rating matrices. As summarized in Table 1, IMC-GAE combines the advantages of both the GAE-based models and IGMC together, which uses GAE to learn both node-specic representation for personalized recommendation, and local graph patterns for inductive matrix completion. Specially, we incorporate two informative node features into IMC-GAE to represent two types of user-item interactions and design a layer-wise node dropout scheme in GAE to learn local graph patterns for inductive matrix completion. In summary, this work makes the following main contributions: •(Sec. 3.1) To better understand local graph patterns, we conduct a quantitative analysis on ve real datasets. Based on this quantitative analysis, we have multiple observations that reveal the properties of local graph patterns in matrix completion. It motivates us to design our model, IMC-GAE. •(Sec. 3.2) We design two informative features, the identical feature and the role-aware feature, for the model to learn the expressive graph patterns. Moreover, these graph patterns can be easily generalized to unseen graphs. •(Sec. 3.5) We design a layer-wise node dropout schema that drops out more nodes in the higher layers. With the layer-wise node dropout, link representation in our model contains more node information in a 1-hop local graph around the target link. Accordingly, our model is able to learn local graph patterns associated with the target link, which enhances the capability of the inductive learning of our model. •(Sec. 5) To illustrate the eectiveness of the proposed IMC-GAE, we conduct empirical studies on ve benchmark datasets. Extensive results demonstrate the state-of-the-art performance of IMC-GAE and its eectiveness in learning both local graph patterns and node-specic representations. In this section, we will briey review existing works on GAE-based matrix completion methods and inductive matrix completion methods based on GNN, which are most relevant with this work. Here, we highlight their dierences to IMC-GAE, and illustrate how we combine the advantages of them to build a more eective model for real recommendation. Figure 1: Model Overview. The rating matrix is formulated as a bipartite user-item graph, in which the nodes represent users (or items) and the links represent the corresponding ratings. In addition, the input features of each no de in this graph consist of the identical feature, the role-aware feature, and the one-hot index feature. In addition, the encoder of our model has multiple layers (e.g., Layer 1) with multiple rating-subgraph (e.g., Rating 1). As stacking more layers, the node dropout probability increases, which is referred to as layer-wise node dropout. The model aggregated the latent embedding which is learned by one-hot index feature and structure embedding of a node which is learned by role-aware feature and identical feature in all layers by the weighted sum operator. At last, we reconstruct the links by a bilinear decoder. In this way, the output of our model contains the information of both latent link representation and structure representation The majority of GNN-based matrix completion methods is based on Graph Autoencoder (GAE) [12], which applies a GNN to the entire network to learn a representation for each node. The representations of the user and item nodes are aggregated to predict potential ratings. For example, Monti et al. [18]propose a multi-graph CNN model to extract user and item latent features from their nearestneighbor networks. Berg et al. [2]propose graph convolutional matrix completion (GC-MC) which uses one-hot encoding of node IDs as initial node features, learns specic node representations by applying a GNN-encoder to the bipartite user-item graph, and reconstructs the rating links by a GNN-decoder. To the best of our knowledge, our method is the rst inductive GAE-based matrix completion method that achieves a good performance in both sparse and dense rating matrices. There are mainly two types of GNN-based matrix completion methods that are applicable to inductive settings. One is attempting to handle inductive matrix completion without using node content, such as IGMC [28]. IGMC rst extracts enclosing subgraphs around target links, then relabels the nodes in subgraphs according to their distances to the source and target nodes, and nally applies a GNN to each subgraph to learn a link representation for link prediction. IGMC applies GNN to those enclosing subgraphs to learn local graph patterns, which can easily generalize to the users (or items) unseen during training. Moreover, local graph patterns help IGMC obtain a better performance than the GAE-based models on the sparse rating matrices. However, applying IGMC to real recommender systems yields two crucial challenges. First of all, IGMC replaces nodes’ one-hot index embedding with local structure features, which does not capture diverse user preferences and item attributes for personalized recommendation. Second, IGMC extracts subgraphs around target links during both the training and inference process, which is time-consuming for large-scale recommendation. In contrast, IMC-GAE maintains the ability to give a node-specic representation, which is important in personalized recommendation for the users with historical ratings. In addition, instead of extracting subgraphs and relabeling each node, we incorporate two informative features into the input features of each node and design a layer-wise node dropout scheme in IMC-GAE to help the GAE to learn local graph patterns. By using GAE to learn local graph patterns, the inference process of IMC-GAE becomes ecient and inductive. Another previous inductive GNN-based matrix completion methods are content-based models; such as PinSage [25], which uses node content as initial node features. Although being inductive and successful in real recommender systems, content-based models Table 2: Quantitative Analysis on multiple datasets. YahooMusic < 0.0001 0.1915 0.0745 0.3585 0.4713 rely heavily on the rich content of each node, which is not easily accessible in most real recommender systems. In comparison, our model is inductive and does not rely on any node content. As aforementioned, matrix completion has been formulated as the link prediction problem on a bipartite user-item graph in recent GNN-based matrix completion methods. Specially, we consider a matrix completion that deals with a rating matrix𝑀of shape 𝑁× 𝑁, where𝑁is the number of users and𝑁is the number of items. Some entries in this matrix exist and other entries are missing. Existing entry𝑀is a historical rating from a user𝑖to an item𝑗. The task of matrix completion is to predict the value of missing entries. GNN-based matrix completion views the matrix as a bipartite graph and predicts the missing links in this graph. In this section, we rst present some ndings on multiple real-world datasets, which reveal the properties of local graph patterns in both sparse and dense rating matrices. Based on these observations, we then elaborate on how the proposed learning algorithm, IMCGAE, integrates the GAE-based model and IGMC to obtain a more eective model for real recommender systems. Then, we show an overview of IMC-GAE in Figure 1. Specially, IMC-GAE is a GAEbased model consisting of three major components: 1) embedding layer whose input features consist of the one-hot index of nodes, the identical feature and the role-aware feature, 2) relational GCN encoder, 3) a bilinear decoder that combines the representations of target nodes to reconstruct links representation. In the previous works, some handcrafted heuristics in a local graph around the target link (i.e.,local graph patterns) are designed for link prediction on graphs [13]. IGMC rst adopts the labeling trick in GNN-based matrix completion that automatically learns suitable graph patterns from the local graph. These local graph patterns can be easily generalized to new local graphs or unseen links. To develop a better understanding of local graph patterns in matrix completion, we do a quantitative data exploration on ve real-world datasets, the density of which ranges from less than 0.0001 to 0.063. In particular, we examine the Pearson’s correlation coecient (PCC) between the true ratings and four heuristic scores [13,27]: average user rating (AUR), average item rating (AIR), most common rating between source nodes and target nodes (MCR) and a simple collaborative signal (SCF) in ve datasets. Specially, we nd a user node that has the most common neighbors with the source node as guider in SCF. The link prediction in SCF is based on the rating that guider rates the target item node. From Table 2, we can extract multiple ndings, •The PCCs between the true ratings and four heuristic scores in ve datasets are all positive, which indicates that the true ratings are correlated with these four heuristic scores in each dataset. Furthermore, it suggests that local graph patterns are eective to predict the missing ratings in matrix completion. •The PCCs between the true ratings and four heuristic scores are all smaller than 6.0, which indicates that a single local graph pattern is not enough to predict the missing ratings. It suggests that the model needs to learn more complex local graph patterns from rating matrix or specic node representations for personalized recommendation to obtain better performance. •Among the four heuristic scores, AUR and AIR are simple statistics that only depend on one type of user-item interaction (i.e., the interactions with the target user or the interactions with the target item), while MCR is a statistic depending on these two types of interactions. We nd that the performance of MCR is more stable across dierent datasets than that of AUR (or AIR) . It suggests that MCR is eective in both sparse and dense rating matrices. Moreover, stable local graph patterns like MCR are eective across dierent datasets, which makes inductive matrix completion possible. Furthermore, SCF considers all the interactions with the target nodes and their neighbors within 1-hop, which outperforms MCR on all datasets. It suggests that local graph patterns which consider more user-item interactions may be more powerful. Motivated by our earlier ndings, we now introduce two input features, identical feature and role-aware feature for the GAE-based model to learn local graph patterns. The identical feature is an identical index, which helps GNN model aggregate one-hop user-item interactions (user-to-item interaction or item-to-user interaction) by message passing function. It aims to represent some simple local heuristics scores such as AIR or AUR, which have been demonstrated to be eective to predict potential ratings in the above quantitative analysis. To model two-hop user-item interactions, we design the second structure feature, the role-aware feature, using two extra indexes to distinguish user and item in the input space of the model. It helps the model distinguish user nodes with item nodes, and therefore distinguish the interactions from user to item with the interactions from item to user. Furthermore, after the user-item interactions around the target link are aggregated by the message passing function, the model can distinguish the user-item interactions from the 1-hop neighbors with the user-item interactions from 2-hop neighbors. By distinguishing these two types of user-item interactions, the model is capable of learning more complicated and powerful local graph patterns such as the aforementioned MCR or SCF. Furthermore, the model needs more expressive patterns for personalized recommendation. Accordingly, we incorporate the onehot index into the input space of IMC-GAE, which is same as previous GAE-based models that learns specic node representations for personalized recommendation. Altogether, we adopt two informative features and one-hot index feature in IMC-GAE, which aims to help GAE learn structure link representation and latent link representation, respectively. The structure link representation represents local graph patterns around the target link, and the latent link representation represents the user-specic preference to the item. In our paper, matrix completion is formulated as the link prediction problem on a heterogeneous graph. In the heterogeneous graph, rating edges of the same type are collected into a rating subgraph (e.g., if the graph consists of four types of ratings, there are four rating subgraphs). Correspondingly, each rating subgraph contains a copy of all the nodes. Then IMC-GAE applies a node-level GNN encoder to these subgraphs that learn a distinct representation for each node in each subgraph. There are three components in our GNN encoder: 1) embedding layer, 2) message passing layer, and 3) accumulation layer. 3.3.1 Embedding layer. In each rating subgraph, the representation of each node consists of three dierent embeddings (identical node embedding𝑢, role-aware embedding𝑟, and rating embedding𝑙). We assume that there are𝑇rating types in the rating matrix so that we have𝑇rating subgraphs in our model. With three dierent embeddings in each rating subgraphs, each node has 3× 𝑇embeddings in IMC-GAE. In order to reduce the number of parameters while allowing for more robust pattern learning, we use the same identical node embedding and role-aware embedding in each rating subgraph. Therefore, there are𝑇 +2 embeddings to represent a node in𝑇rating subgraphs. Moreover, we concentrate (denoted by𝐶𝑜𝑛𝑐𝑎𝑡 (¤)) these three embeddings (denoted by𝑈,𝑅,𝐿) in embedding layer, which is the output of the embedding layer, where𝑥[𝑖]denotes node𝑖’s embedding vector in𝑡-th rating subgraph. The node embedding vectors are the input of message passing layer. 3.3.2 Message passing layer. In IMC-GAE, we adopt a traditional GCN message passing layer to do local graph convolution, which has the following form: where𝑥[𝑖]denotes node𝑖’s feature vector at layer𝑙 +1 in the𝑡-th rating subgraph. In addition, we chose symmetric normalization as the degree normalization factor in our message passing layer, where the|N(𝑖)|represents the number of neighbors of node𝑖in the 𝑡 -th rating subgraph. 3.3.3 Accumulation layer. In each𝑡-th rating subgraph, we stack 𝐿message passage layer with ReLU activations [1] between two layers. Following [8], node𝑖’s feature vectors from dierent layers are weighted sum as its nal representationℎ[𝑖]in the𝑡-th rating subgraph, Then we accumulate all node𝑖’s nal representationℎ[𝑖]from all𝑇rating subgraphs into a single vector representation by sum Figure 2: Layer-wise Node Dropout. In this subgraph extracted in ML-100k, red nodes indicates target nodes; blue nodes indicates the 1-hop neighbors of target nodes; white nodes indicates the 2-hop neighbors of target nodes. operator, To obtain the nal representation of user or item node, we transform the intermediate output ℎ [𝑖] by a linear operator, The parameter matrix𝑊of user nodes is the same as that of item nodes, which because the model is trained without side information of the nodes. In IMC-GAE, following [2], we use a bilinear decoder to reconstruct links in the user-item graph and treat each rating level as a separate class. Given the nal representation𝑛[𝑖]of user𝑖and𝑛[ 𝑗]of item 𝑗, we use billinear operator to produce the nal link representation 𝑒[𝑖, 𝑗] in the 𝑡-th rating subgraph, where𝑊is a learnable parameter matrix. Thus, we can estimate the nal rating score as, wheree(i, j)is the vector that concentrate the nal link representations of user𝑖and item𝑗on all𝑇rating subgraph, and the𝑆is the softmax probability on 𝑡-th dimension of e(i, j) vector. The layer-wise node dropout is inspired by node dropout from [2], aiming to help model grasp patterns in local graphs which can be better generalized to unobserved ratings. In previous works, GAE-based models always adopt a node dropout scheme which randomly drops out all outgoing messages of a particular node with a probability𝑝. However, our method adopts dierent node dropout probabilities in dierent layers, which we call it layer-wise node dropout. Specially, layer-wise node dropout is𝑝= 𝑝− 𝑙𝜃, where 𝑝is the node dropout probability in the𝑙-th layser,𝑝is the initial node dropout probability, and 𝜃 is the hyperparameter. In our paper, layer-wise node dropout facilitates node representation learning due to the following two reasons. The rst reason is the same as [2], which we adopt to overcome the over-smoothing problem in GNN representation and improve the generalization ability of our model. The second reason is to help the model learn local graph patterns which consider more user-item interactions in a 1-hop subgraph around the target link. As shown in Figure 2, the target nodes are node 0 and node 1. In the previous GAE-based models, the representations of node 0 and node 1 in the 2-th layer aggregate too much node information beyond the 1-hop subgraph around them (e.g., node 4, node 5, node 8 and node 9 in the example), which prevents the model learning graph patterns from the user-item interactions around target nodes. 3.6.1 Loss function. We minimize the cross entropy loss (denoted by 𝐶𝐸) between the predictions and the ground truth ratings, L =1|(𝑖, 𝑗)|Ω= 1|𝐶𝐸 (𝑟 [𝑖, 𝑗],ˆ where we use𝑟 [𝑖, 𝑗 ]andˆ𝑟 [𝑖, 𝑗 ]to denote the true rating and the predicted rating of(𝑖, 𝑗), respectively, and the 0/1 matrixΩserves as a mask for unobserved ratings in rating matrix 𝑀. 3.6.2 Node representation regularization. It is inspired by adjacent rating regularization in [28]. Since each two rating types are comparable in matrix completion (e.g., ratings 5 is bigger than ratings 4 and ratings 4 is bigger than 3), we need to consider the magnitude of ratings. Accordingly, we propose node representation regularization to encourages the representation of each node in rating subgraph that adjacent to each other to have similar representations. Specially, we assume that the representation of the𝑖-th node in the𝑡-th rating subgraph isℎ[𝑖], where 0≤ 𝑡 ≤ 𝑇. Then, the NRR regularizer is, where𝐶𝑜𝑠is cosine similarity between two vectors, and𝑁is the total number of users and items in the matrix. Finally, we combine these two loss functions to the nal loss function, where 𝜆 is a hyperparameter that trade-o two losses. In IMC-GAE, the inductive link representation for unseen nodes has two parts, inductive structure representation which is learned from the identical feature and the role-aware feature, and inductive latent representation which is learned from one-hot index of the node. For the inductive structure representation, we just leverage message passing, propagating learned structure representation from neighbors to target nodes. For the inductive latent representation, we also rst accumulates the latent presentation of the neighbors of the target nodes. However, there may exist some unseen nodes during training in their neighbors, which lacks the latent representation. In our method, we use the average latent representation of the other nodes to represent the unseen nodes in each rating subgraph, where𝑙[𝑖]is the initial latent representation of the𝑖-th node in 𝑡-th rating subgraph (in equation 1) andIis a set of nodes which we have seen during training. It is a simple but eective method, which is demonstrated in the following experiments. To shed more light on local graph patterns learning in GNN-based models, we provide a comparison with GAE-based matrix completion, IGMC, and IMC-GAE through a typical example in Figure 3. Here, we assume the ratings in our example are within {1, -1} (like, denoted by bold black line, and dislike, denoted by bold coee line). The solid lines are observed ratings for training and dash lines are test ratings. In a train case, user 1 and 2 all like item 3, while they all dislike item 4. It indicates that user 1 may have a similar taste with user 2, which is a common local graph pattern in matrix completion. Furthermore, since user 2 dislikes item 5, we inference that user 1 may dislike item 5 based on the "similar taste" pattern. When trained with the existing rating between user 2 and item 4, IGMC rst extracts the 1-hop local graph around user 2 and item 4, and relabels user 1, user 2, item 3, item 4 and item 5 as index 2, 0, 3, 1 and 3, respectively. Finally, the model applies a GNN to the local graph, where the new node labels are the input features of the model. Without introducing the user-item interactions beyond the 1-hop local graph around target nodes, the "similar taste" pattern is easily learned by the model. However, previous GAE-based models apply the GNN to the entire graph for learning graph patterns. Accordingly, when trained with the existing rating between user 2 and item 4, the representations of user 2 and item 4 are aggregated with many node embeddings beyond 1-hop local graph around user 2 and item 4, which makes the model hardly focus on the interactions in this local graph, and fails to learn "similar taste" pattern. To solve this problem, IMC-GAE designs the layer-wise node dropout scheme for the GAE-based model to avoid aggregating too many embeddings of the nodes beyond the 1-hop local graph into the target nodes representation. Although the target node representations are still aggregated a few node representations beyond the local graph, the model is capable of learning the "similar taste" pattern between user 1 and 2. Furthermore, if given a new graph in Figure 3 which has the same graph structure as the original graph, previous GAE-based models need to be retrained to inference the missing rating between user 8 and item 9. However, with labeling trick in IGMC and inductive structure representation in IMC-GAE, the models learn the "similar taste" pattern into structural link representation, which can be generalized to the new graph. Despite the eectiveness of local graph patterns learning in IGMC, the labeling trick introduces extra computational complexity. The reason is that for every rating(𝑢, 𝑖)to predict, IGMC needs to relabel the graph according to(𝑢, 𝑖). The same node𝑢will be labeled dierently depending on the target link and will be given a dierent node representation by the GNN when it appears in dierent links’ labeled graphs. This is dierent from previous GAEbased models and IMC-GAE, where we do not relabel the graph and each node only has a single embedding vector. For a graph with 𝑛nodes and𝑚ratings to predict, the GAE-based model needs to apply the GNNO(𝑛)times to compute an embedding for each node, while IGMC needs to apply the GNNO(𝑚)times for all ratings. Figure 3: We compare local graph patterns learning in IMC-GAE with that in GAE and IGMC in two cases. In the rst case, the model is to infer the link between node1and node5in original graph. In the second case, the model is to infer the link between node 6 and node 0 in a new graph. When𝑚 ≫ 𝑛, IGMC has worse time complexity than GAE-based models, which is not suitable for real recommendation. We perform experiments on ve datasets to evaluate our proposed method. We aim to answer the following research questions: • RQ1:How does IMC-GAE perform compared with state-of-theart matrix completion methods when facing both sparse and dense rating matrices? • RQ2:How does the dierent hyper-parameter settings (e.g., depth of layer, weighted layer combination, and node representation regularization (NRR)) aect IMC-GAE? • RQ3:How does the local graph patterns learning in IMC-GAE benet from two informative features and layer-wise node dropout scheme respectively? • RQ4:How does the IMC-GAE perform on few-shot or even unseen users (or items) as compared with GAE-based models and IGMC? To evaluate the eectiveness of IMC-GAE, we conduct experiments on ve common matrix completion datasets, Flixster [10], Douban datasets [14], YahooMusic [4], MovieLens-100K [15] and MovieLens-1M [15], which are publicly accessible and vary in terms of domain, size, and sparsity. Moreover, Flixster, Douban, and YahooMusic are preprocessed subsets of the original datasets provided by [19]. These datasets contain sub rating matrix of only 3000 users and 3000 items, which we consider as sparse rating matrices in real recommendation. The MovieLens-100K and MovieLens-1M are widely used datasets for evaluating many recommender tasks, which we consider as dense rating matrices in real recommendation. For ML-100k, we train and evaluate on canonical u1.base/u1.test train/test split. For ML-1M, we randomly split into 90% and 10% train/test sets. For the Flixster, Douban, and YahooMusic, we use the splits provided by [19]. 5.2.1 Baselines. To demonstrate the eectiveness, we compare our proposed IMC-GAE with the following methods: • Traditional methods.matrix completion (MC) [3], inductive matrix completion (IMC) [9], geometric matrix completion (GMC) Table 3: RMSE of dierent algorithms on Flixster, Douban and YahooMusic. Table 4: RMSE test results on MovieLens-100K (left) and MovieLens-1M (right). [11], PMF [16], I-RBM [21], NNMF [5], I-AutoRec [22] and CFNADE [30] are traditional matrix competion methods, which use the user-item ratings (or interactions) only as the target value of their objective function. • GAE-based methods.sRGCNN [18], NMTR [6], GC-MC [2] are GAE-based matrix completion methods, which use one-hot index as the initial feature of each node. • IGMC.IGMC [28] is an inductive matrix completion method, which learns local graph patterns to generalize to new local graphs for inductive learning. • Content-based GNN methods.Content-based matrix completion methods are inductive GNN-based methods adopting side information as initial features of each node, which includes PinSage [25] and IGC-MC [2]. PinSage is originally used to predict related pins and is adapted to predicting ratings here. IGC-MC is a content-based GC-MC method, which uses the content features instead of the one-hot encoding of node IDs as its input features. • Other GNN methods.GRALS [20] is a graph regularized matrix completion algorithm and F-EAE [7] uses exchangeable matrix layers to perform inductive matrix completion without using content. In addition, given dierent datasets, we compare IMC-GAE with dierent baseline methods under RMSE in Table 3. The RMSE is a common evaluation metric in matrix completion [2,28]. The baseline results are taken from [28]. 5.2.2 Hyperparameter Seings. We implement our model based on DGL [23] and use the Adam optimizer. We apply a grid search for hyperparameters, the number of layers is searched in{1,2, ...,5}, the𝜆in equation 10 is searched in{4𝑒,4𝑒,4𝑒,4𝑒}, the embedding size of each vector in embedding layer is chosen from {90,120, ...,1800}and the embedding size of each vector in bilinear decoder is searched in{30,40, ...,80}. Besides, the initial node dropout probability is tuned in{0.1,0.2,0.3}and the decay ratio𝜃 is tuned in{0.05,0.1,0.2}. All implementation codes can be found at https://github.com/swtheing/IMC-GAE. We start by comparing our proposed IMC-GAE with baselines on ve benchmark datasets and then explore how the combination of the local graph patterns learning and specic node representations improves the performance in matrix completion. For Flixster, Douban, and Yahoomusic, we compare our proposed model with GRALS, sRGCNN, GC-MC, F-EAE, PinSage, IGC-MC and IGMC. We show the result in Table 3. Our model achieves the smallest RMSEs on Douban and YahooMusic datasets, but slightly worse than IGMC on the Flixster dataset. Furthermore, as a GAE-based model, our method outperforms signicantly all the GAE-based baselines (sRGCNN and GC-MC), which highlights the successful designs (two informative features, layer-wise dropout scheme) of our model. For ML-100k, we compare IMC-GAE with MC, IMC, as well as GRALS, sRGCNN, GC-MC, F-EAE, PinSage, NMTR and IGMC. For ML-1M, besides the baselines GC-MC, F-EAE, PinSage, NMTR and IGMC, we further include PMF, I-RBM, NNMF, I-AutoRec, and CFNADE. Our model achieves the smallest RMSEs on these datasets without using any content, signicantly outperforming all the compared baselines, regardless of whether they are GAE-based models. Altogether, our model outperforms all GAE-based models on all datasets. It demonstrates that the local graph patterns learned in our model truly help model inference the missing ratings in both sparse and dense rating matrices. As the GNN encoder plays a pivotal role in IMC-GAE, we investigate its impact on the performance. We start by exploring the inuence of Table 6: Eect of weighted layer combination and NRR layer numbers. We then study how the weighted layer combination and NRR aect the performance. 5.4.1 Eect of Layer Numbers. To investigate whether IMC-GAE can benet from multiple layers in the GNN encoder, we vary the model depth. In particular, we search the layer numbers in the range of{1,2,3,4,5}. Table 5 summarizes the experimental results, wherein IMC-GAE-𝑖indicates the model with𝑖embedding propagation layers, and similar notations for others. By analyzing Table 5, we have the following observations: •Increasing the depth of IMC-GAE substantially enhances the performance of the model. Clearly, IMC-GAE-2 achieves consistent improvement over IMC-GAE-1 across all the board, which considers the 1-hop neighbors only. We attribute the improvement to the eective modeling of local graph structure: structure features and layer-wise node dropout help model grasp eective patterns from local graph around target nodes. •When further stacking propagation layer on the top of IMC-GAE2, we nd that IMC-GAE-3 leads to performance degradation on ML-100k, but performance improvement on Douban and YahooMusic. This might be caused by the deeper layer with a higher node dropout probability might introduce noise in latent link representation. More specically, the deeper layers (e.g., the third layer in IMC-GAE-3) lose the original graph connectivity, which makes the model fail to learn the latent link representation from the neighbors. Moreover, the marginal improvements on the other two datasets verify that the local graph patterns beyond 1-hop neighbors still improve the performance of the model in the sparse rating matrix. 5.4.2 Eect of weighted layer combination and NRR. Dierent from prior works [2,28], we adopt the weighted sum operator for layer combination instead of sum or concentration operator and NRR to encourages node representation in rating subgraph that adjacent to each other to have similar parameter matrice. Table 6 shows the result of the ablation experiments. From the ablation experiments, we have the following observations. First, the weighted sum combination shows a performance improvement over the sum or Table 7: Ablation study on three datasets, where IMC-GAE-R indicates the IMC-GAE-R trained with only role-aware feature; IMC-GAE-I indicates IMC-GAE trained with only identical feature; IMC-GAE-OD indicates IMC-GAE trained with original node dropout scheme. Table 8: Inference time (s) of IMC-GAE, IGMC and GCMC on Douban, MovieLens-100K, and MovieLens-1M. concentration operator in both sparse and dense rating matrices. This might be because that sum or concentration operator does not assign lower importance to the node representations in deeper layers, which introduces more noise into the representation of the nodes. Second, we can see that disabling NRR results in the performance drop on all three datasets. It demonstrates that NRR is an eective way to regularize the model. 5.5.1 Performance Comparison. In this section, we attempt to understand how the identical feature, the role-aware feature, and the layer-wise node dropout scheme aect local graph patterns learning in IMC-GAE, and how local graph patterns learning aects the performance of IMC-GAE in both sparse and dense rating matrices. Towards this end, we compare the performance of original IMCGAE with IMC-GAE trained with only identical feature, IMC-GAE trained with only role-aware feature, IMC-GAE with normal node dropout on the three datasets. From the result shown in Table 7, we conclude the following ndings: •With only role-aware feature or only identical feature for training, IMC-GAE obtains a competitive performance with IMC-GAE. It demonstrates that local graph patterns learning in IMC-GAE is eective in both sparse and dense rating matrices. •The IMC-GAE outperforms the other three baselines in all datasets, which demonstrates that each design in IMC-GAE for local graph patterns learning (i.e., role-aware feature, identical feature, and layer-wise node dropout scheme) is essential, which helps GAE learn a series of eective local graph patterns. 5.5.2 Inference Time Comparison. We compare the inference time of IMC-GAE with IGMC and GC-MC (a typical GAE-based model) on three datasets. Specially, we infer the 20% samples in each dataset and conduct the experiment on GN7 on Tencent Cloud, which is Figure 4: ML-1M results under dierent sparsity ratios. equipped with 4*Tesla T4. We repeat this experiment ve times and report the average inference time of each model in Table 8. The results show that the inference time of IMC-GAE is slightly longer than that of GAE but signicantly shorter than that of IGMC. To investigate the model performance on few-shot or even unseen users (or items), we test the model on dataset under dierent sparsity levels of the rating matrix [2,28]. Here we construct several sparse datasets by using 100%, 20%, 10%, 5%, 1% and 0.1% training ratings in Movielens-1M, and then compare the test RMSEs of our method with GC-MC and IGMC, a typical GAE-based model and an inductive GNN model. As shown in Figure 4, we have two observations, •As the dataset becomes sparser, the performance of all the models suer from a drop, but the drop rate of our model is much smaller compared with GC-MC. •The performance of our model in the datasets with 100%, 20%, and 10% training ratings is better than IGMC, but worse in other datasets. From the observations, we nd that the way we adopt GAE to learn local graph patterns that truly improves its inductive learning ability. However, the performance of IMC-GAE on three sparer datasets is worse than that of IGMC. It suggests that local graph patterns learned in IMC-GAE is not as good as those learned in IGMC which can be generalized to sparser datasets containing more few-shot or unseen users (or items). In this paper, we propose Inductive Matrix Completion using Graph Autoencoder (IMC-GAE), which uses GAE to learn both graph patterns for inductive matrix completion and specic node representations for personalized recommendation. Extensive experiments on real-world datasets demonstrate the rationality and eectiveness of the way IMC-GAE learns local graph patterns by GAE. This work represents an initial attempt to exploit local structural knowledge in GAE-based matrix completion, which is more suitable to be applied to real recommender systems.