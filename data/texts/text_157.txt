N recent years, personalized recommender systems have emerged in an extensive range of Web applications to predict the preferences of its users and provide them with new and relevant items based on the scarce data about the users and/or items [2]. There are two major paradigms of recommender systems: (i) content-based ﬁltering systems; (ii) collaborative ﬁltering systems. Content-based ﬁltering approach exploits a proﬁle of users’ preferences and/or properties of the items to carry out the recommendation task. On the other hand, collaborative ﬁltering approach recommends new items to the users based on similarity measures between users and items. The main advantage of collaborative ﬁltering over content-based ﬁltering is that they do not require domain knowledge since the embeddings are automatically learned, and the more interactions the users have with the items, the more accurate and relevant the new recommendations are. Inspired by the Netﬂix challenge, a well-known technique to predict the missing ratings in collaborative ﬁltering frameworks is low-rank matrix completion, which is the main interest of this paper. Given partial observation of a matrix of users by items, the goal is to develop an algorithm to accurately predict the values of the missing ratings. One of the prime challenges of collaborative ﬁltering systems that rely on user-item interactions is the “cold start problem” in which high-quality recommendations are not feasible for the new users/items that bear little or no information. One prominent technique to overcome the problem with cold start users is to incorporate the community information into the framework of recommender systems in order to enhance the recommendation quality. Motivated by the social homophily theory [3] that users within the same community are more likely to share similar preferences, sociallyaware collaborative ﬁltering approach exploits the social network among the users and provides recommendations based on the similarity measures of the users that have direct or indirect social relationship with a given user. A plethora of research works have explored the idea of exploiting the information inferred by social graphs to enhance the performance of recommender systems from an algorithmic perspective [2], [4]–[22]. However, few works dedicated to developing theoretical insights on the usefulness of graph side-information on the quality of recommendation, and characterizing the maximal achievable gain due to side-information, e.g., [23], [24]. Recently, a number of works [25]–[28] have investigated the problem of interest from an information-theoretic perspective. Ahn et al. [25] considered a matrix completion problem with We study the matrix completion problem that leverages hierarchical similarity graphs as side information in the context of recommender systems. Under a hierarchical stochastic block model that well respects practically-relevant social graphs and a low-rank rating matrix model, we characterize the exact information-theoretic limit on the number of observed matrix entries (i.e., optimal sample complexity) by proving sharp upper and lower bounds on the sample complexity. In the achievability proof, we demonstrate that probability of error of the maximum likelihood estimator vanishes for sufﬁciently large number of users and items, if all sufﬁcient conditions are satisﬁed. On the other hand, the converse (impossibility) proof is based on the genie-aided maximum likelihood estimator. Under each necessary condition, we present examples of a genie-aided estimator to prove that the probability of error does not vanish for sufﬁciently large number of users and items. One important consequence of this result is that exploiting the hierarchical structure of social graphs yields a substantial gain in sample complexity relative to the one that simply identiﬁes different groups without resorting to the relational structure across them. More speciﬁcally, we analyze the optimal sample complexity and identify different regimes whose characteristics rely on quality metrics of side information of the hierarchical similarity graph. Finally, we present simulation results to corroborate our theoretical ﬁndings and show that the characterized information-theoretic limit can be asymptotically achieved. Recommender systems, matrix completion, graph side information. n users and m items, and studied a simpliﬁed model where there are two clusters of users, and the users of each cluster share the same rating over the items. A sharp threshold on the sample complexity is derived as a function of the quality of the social graph information, and the gain due to the information provided by social graph is theoretically quantiﬁed. Furthermore, the authors proposed an efﬁcient rating estimation algorithm that provably achieves the minimum sample complexity for reliable recovery of the ground truth rating matrix. Follow-up works have investigated different models of the matrix completion problem proposed in [25]. Yoon et al. [26] considered a general setting where there are K hidden communities of possibly different sizes, and each community is associated to only one feature, and hence the users of each community are assumed to provide the same binary rating over the items. Unlike [25], [26] where one-sided graph side-information (i.e., user-to-user similarity graph) is considered, Zhang et al. [27] studied the beneﬁts of two-sided graph side-information depicted by userto-user and item-to-item similarity graphs. Interestingly, the theoretical analysis demonstrates that there is a synergistic effect, under some scenarios, stemmed from considering two pieces of graph side-information. This implies that observing both graphs is necessary in order to slash the sample complexity under those scenarios. Jo et al. [28] relaxed the assumption in [25] on the preference matrix whose element at row i and column j denotes the probability that user i likes item j, and proposed a new model in which the unknown entries of the preference matrix can take discrete values drawn from a known ﬁnite set of probabilities. While the works of [25]–[28] lay out the theoretical foundation for the problem, they impose a number of strict assumptions on the system model such as the users of the same cluster have same ratings over all items, which limits the practicality of the proposed models for real-world data. A natural hypothesis in the theory of recommender systems is that the unknown rating matrix has an intrinsic structure of being low rank. This hypothesis is sensible because it is generally believed that only a few factors contribute to one’s preference. Prior works [25]–[28] assume that each cluster is represented by a rank-one matrix, and users within a cluster share the same rating vector over items. In this work, we relax this assumption and study a more generalized framework where each cluster is represented by a rank-r matrix. More speciﬁcally, we consider a matrix completion problem where the users are categorized into c clusters, each of which comprises g sub-clusters, or what we call “groups”, producing a hierarchical structure in which the features of different groups within a cluster are broadly similar to each other, however, they are different from the features of the groups in other clusters. The goal is to reliably retrieve the rating matrix under the proposed generalized model, utilizing the information provided by the noisy partial observation of the rating matrix, as well as the hierarchical social graph. 1) Connection to Low-Rank Matrix Completion Problem: The objective of low-rank matrix completion, a recurring problem in collaborative ﬁltering [2], is to recover an unknown low-rank matrix from partial, and possibly noisy, sampling of its entries [29] . Since the rank minimization problem is NP-hard, accurate reconstruction is generally ill-posed and computationally intractable. However, exploiting the fact that the structure of the matrix is of low-rank makes the exploration for solution worthwhile. One direction of research is geared towards studying low-rank matrix completion where the observed subset of matrix entries are exactly known. Under certain conditions, upper bounds on the number of observed entries, which are uniformly drawn at random, are developed to ensure successful reconstruction with high probability [30]–[32]. A fundamental open question in the literature of low-rank matrix completion with exact observation is how to ﬁnd a low-rank matrix that is consistent with the partial observation of its entries. This question stems from the fact that the sparse basis of the low-rank matrix is unknown, and that the basis are drawn from a continuous space. Performance guarantees provided by existing algorithms only hold when certain incoherence assumptions on the singular vectors of the matrix are satisﬁed. By and large, theoretical guarantees on the reconstruction performance are not established even for the rank-one case, and hence, our understanding of the problem is far from complete. Numerous algorithms for low-rank matrix completion have been proposed over the years. If the rank information of the original low-rank matrix is unknown, various techniques based on nuclear norm minimization are proposed [30]–[36]. On the other hand, if the rank is known in advance, techniques based on Frobenius norm minimization are proposed [37]–[44]. Another interesting and practical research direction is investigating low-rank matrix completion when the observed entries are contaminated by noise. The objective is to seek a low-rank matrix that best approximates the original matrix, and ﬁnd an upper bound on the root-mean squared error [31], [45]. 2) Algorithms for Recommender Systems with Graph Side-Information: The idea of exploring the value of incorporating graph side-information into collaborative ﬁltering approaches has gained a lot of attention from the research community [4]. There are two primary approaches of collaborative learning [2]: (i) latent factor approach, and (ii) neighborhood approach. Latent factor approach learns latent features for users and items from the observed ratings. Most successful realizations of this approach hinge on matrix factorization which characterizes the latent characteristics of users and items by two low-rank userand item-feature matrices inferred from the rating patterns. One direction to integrate graph side-information in this approach is through adding some regularization terms in the loss function of the matrix factorization model [5]–[9]. Another direction is to develop matrix factorization frameworks that fuse the user-item rating matrix with the social network of the users [10]–[13]. Moreover, a robust online matrix completion on graphs is designed and analyzed in [14] that exploits the graph information to recover the incomplete rating matrix entires in the presence of outlier noise. On the other hand, for neighborhood approach, the prediction of rating information is based on computing the relationships among items or users. The recommendation accuracy in this approach can be enhanced by incorporating the information provided by the social graphs into the neighborhood deﬁnition [15]–[20]. Lately, recent works have proposed novel architectures for graph convolutional neural networks that fully exploit the structure of item/user graphs [21], [22]. Few works have provided theoretical insights on the usefulness of side-information for matrix completion problem, e.g., [23], [24]. Chiang et al. [23] proposed a dirty statistical model to exploit the feature-based side information, yet to be robust to feature noise, in matrix completion applications. They provided theoretical guarantees that the proposed model achieves lower sample complexity than the standard matrix completion (with no graph information) under the condition that the features are not too noisy. Rao et al. [24] proposed a scalable graph regularized matrix completion, and derived consistency guarantees to demonstrate the gain due to the graph side-information. It is worth mentioning that the maximal achievable gain due to graph side-information is not characterized in these works. 3) Connection to Community Detection in Stochastic Block Model: Stochastic Block Model (SBM) [46] with two communities, which is also known as the planted bisection model, is an extension to Erd exhibits community structure. It is based on the assumption that agents (nodes) in a network (graph) connect depending on their community assignment, unlike Erd reconstruct the community assignment upon observing the unlabeled graph. This problem has been well investigated from an information-theoretic perspective where a sharp threshold is characterized for exact recovery of communities [47]–[55]. Inspired by applications in physics, computational social science, and machine learning, several generalizations of the classical SBM have been developed to explore the beneﬁts of considering various types of side-information in the task of community recovery. Generalizations have been made to incorporate inﬁnite number of blocks [56], probabilistic block membership [57], [58], heterogeneous degree within blocks [59], weighted edges between nodes [60], and node attributes [61]. One crucial and practical generalization to SBM, that we consider in this work, is the hierarchical (nested) organization of blocks, where vertices are partitioned into blocks, each of which is further partitioned into sub-blocks [62]–[65]. The edge probabilities in a hierarchical random graphs are inhomogeneous and determined by the topology structure. 4) Connection to Clustering Problem with Side-Information: There has been some recent works that investigate the clustering problem with side-information, where the learner is allowed to interact with a domain expert. Ashtiani et al. [66] proved that having access to few pairwise queries leads to more efﬁcient k-means clustering, which is NP-hard in general. Mazumdar et al. [67] explored the beneﬁts of the similarity matrix, that is used to cluster similar points together, to slash the adaptive query complexity. In both works, information-theoretic lower bounds are proved, and efﬁcient clustering algorithms are designed. Matrix Completion problem with graph side-information can be seen as a natural extension to clustering problem if we shift our focus to recovering the structure of (hierarchical) clusters instead of reconstructing the rating matrix. The main contributions of this paper are summarized as follows. We conduct a rigorous theoretical study and characterize an information-theoretic threshold for optimal rating matrix recovery, rather than starting with a certain algorithmic approach. We establish matching upper and lower bounds on the sample complexity for reliable recovery of the rating matrix, and hence exactly characterize the optimal sample complexity that is a function of the quality of social graph side-information. In the proof of achievability bound, we show that probability of error of the maximum likelihood estimator vanishes for sufﬁciently large number of users and items, if all sufﬁcient conditions are met. On the other hand, the proof of the converse bound is based on the genie-aided maximum likelihood estimator. Under each necessary condition, we present examples of a genie-aided estimator to prove that the probability of error does not vanish for sufﬁciently large number of users and items. While numerous low-complexity matrix completion algorithms have been proposed, it remains an open problem to develop optimization algorithms with provable performance guarantees for a generic class of matrices [29]. This work makes substantial progress on this long-standing open problem. We also emphasize on the fact that this work is a non-trivial extension of [25] and [26], as will be delineated in the following sections. A preliminary version of the main results of this paper has been reported in [1] for (c, g, r, q) = (2, 3, 2, 2). In this paper, we characterize the optimal sample complexity result for any (c, g, r, q), and present the complete achievability and converse proofs. Row vectors and matrices are denoted by lowercase letters (e.g., v) and uppercase letters (e.g., X), respectively. Random matrices are denoted by boldface uppercase letters (e.g., X), while their realizations are denoted by uppercase letters (e.g., X). Sets are denoted by calligraphic letters (e.g., Z). Let F of X. Let X(r, t) denote the matrix entry at row r and column t. Furthermore, let X(i, :) and X(:, j) denote the i jcolumn of matrix X, respectively. Furthermore, for sets I and J, the submatrix of X, that is obtained by rows i ∈ I and columns j ∈ J, is denoted by X(I, J). Let Λ (X, Y ) denote the number of different entries between matrices X and Y for X, Y ∈ F be all-zero and all-one matrices of dimension n ×m, respectively. For a matrix X ∈ F, let Xdenote the transpose distance between two vectors u and v is denoted by d Let kuk and m where n ≤ m, deﬁne [n : m] as {n, n + 1, ··· , m}. Let {0, 1, ··· , q − 1} digits are drawn from F graph G where [n] is the set of n vertices labeled by integers in [n]; and E is the set of undirected edges. The elements of E are given by pairs (a, b) for a, b ∈ [n]. The remainder of this paper is organized as follows. We ﬁrst present the problem formulation of the rating matrix and hierarchical similarity graph in Section II. Then, the main results are presented in Section III. In Section IV, informationtheoretic interpretation of the main results is provided, and extensive numerical results are presented. Next, the achievability proof is presented in Section V, while the converse proof is presented in Section VI. In Section VII, we present simulation results that corroborate our main results. Finally, the paper is concluded and directions for future research are discussed in Section VIII. Consider a rating matrix X ∈ F of the r graphs (e.g., social graphs) are leveraged as side-information during the matrix completion procedure to enhance the item recommendation quality. More speciﬁcally, we consider a hierarchical similarity graph that consists of c disjoint clusters, and each cluster comprises g disjoint sub-clusters (or that we call “groups”). For the sake of tractable mathematical analysis, we assume equal-sized clusters and groups. The theoretical guarantees, however, hold as long as the group sizes are order-wise same (See Section III). According to social homophily theory [3], users within the same community (i.e., who are more likely to be connected in a social graph) are more likely to share similar preferences of items. This results in a low rank structure of the rating matrix since the rows of the rating matrix associated with such users are highly likely to be similar [29]. To capture this crucial fact in our model, we make the following assumptions: (i) the rating vectors of the users who belong to the same group are equal, and hence there are gc distinct rating vectors in total; (ii) the rating vectors of the groups of a given cluster are different, yet intimately-related to each other through a linear subspace of r basis vectors for some integer r ≤ g. Let v denotes the rating vector of the users in cluster x and group i for x ∈ [c] and i ∈ [g]. Let R rows are the rating vectors of the groups in cluster x for x ∈ [c]. The set of g rows of R groups in cluster x) is spanned by any subset of r rows of R Let X be represented by a set of rating vectors V problem with n = 12 users in c = 2 clusters and g = 3 groups. If the rating matrix is given by then we have the set of rating vectors as and the user partitioning as The main goal of the problem of interest is to ﬁnd the best estimate of X 1) partial and noisy observation Y of X denote the `norm of vector u. For an integer n ≥ 1, let [n] denote the set of integers {1, 2, . . . , n}. For integers n user over m items are given by the rating vector that corresponds to the rrow of X for r ∈ [n]. The user similarity denote the ground truth rating matrix. Each instance of the problem corresponds to a rating matrix X, which can = {Z(1, 1)= {1, 12}, Z(1, 2)= {2, 6}, Z(1, 3)= {5, 9}, Z(2, 1)= {3, 8}, Z(2, 2)= {7, 11}, Z(2, 3)= {4, 10}}. ={Z(x, i)}: Z(x, i) ⊆ [n], Z(x, i) ∪ Z(y, j) = ∅ for (x, i) 6= (y, j),Z(x, i) = [n] (x, i) denotes the set of users who belong to cluster x and group i for x ∈ [c] and i ∈ [g]. no observation. Let the set of observed entries of Xbe denoted by Ω = {(r, t) ∈ [n] × [m] : Y (r, t) 6= ∗}. The partial observation is modeled by assuming that each entry of Xis observed with probability p ∈ [0, 1], independently from others. Moreover, the potential noise in the observation is modeled by considering a random uniform noise distribution, that is, the noise is not adversarial (i.e., not deterministic). In particular, we assume that each observed entry X(r, t), 2) user similarity graph G = ([n], E). A vertex represents a user, and an edge captures a social connection between two users. Let ψ denote an estimator (decoder) that takes as input a pair (Y, G) where Y is the incomplete and noisy rating matrix and G is the user similarity graph, and outputs a completed rating matrix X ∈ F and the user partitioning of notation, we may interchangeably use X or (V, Z) as the output of the estimator. The former notation is adopted when we are interested in the entries of the rating matrix, while the latter notation is used when we are interested in either the set of rating vectors or the user partitioning. One key parameter, that is instrumental in expressing the main result (see Section III) as well as proving the main theorem, is the discrepancy between the rating vectors. Let δ rating vectors of groups within the same cluster. Let δ different clusters. More formally, δ As will be elaborated in the next section, our result hinges on δ of all rating matrices M in which the rating vectors maintain a minimum level of dissimilarity. Formally, we deﬁne M be the set of rating matrices M = (V, Z) such that the following properties are satisﬁed: The performance metric we consider to provide theoretical guarantees on the quality of recommendation is the worst-case probability of error P for (r, t) ∈ Ω, can be possibly ﬂipped to any element of the set {0, 1, . . . , q} \ X(r, t) with a uniform probability of θ/(q − 1) for θ ∈ [0, (q − 1)/q). The reasons behind choosing this noise model are: (i) the uniform noise distribution is the worst case distribution in discrete channels; and (ii) this model captures the fact that there may exist a fraction of group members whose ratings are close to the majority ratings, yet they are not exactly identical. Hence, the majority ratings can be considered as the ground truth, while the ratings of this fraction of users can be seen as noisy version of the ground truth. Furthermore, since this fraction of users have some ratings that are different from the majority, each of such ratings can take a value that is randomly and uniformly selected from the set of all possible ratings different from that of the majority; The set [n] of vertices is partitioned into c disjoint clusters, each of which is of size n/c users. Each cluster is further partitioned into g disjoint groups, each of which is of size n/(cg) users. The user similarity graph is generated as per the hierarchical stochastic block model (HSBM) [47], [64], which is a generative model for random graphs exhibiting hierarchical cluster behavior. In this model, each two nodes in the graph are connected by an edge, independent of all other nodes, with probabilities given by Here, we assume that edge probabilities are scaling with the size of the problem. In particular, we assume where α, β and γ are positive real numbers such that α ≥ β ≥ γ. In other words, there is an edge between two users in the same group within a cluster with probability α; there is an edge between two users in different groups but within the same cluster with probability β; and there is an edge between two users in different clusters with probability γ. Note that the considered edge probabilities guarantee the disappearance of isolated vertices (i.e., vertices of degree zero) in the user similarity graph, which is a necessary property for exact recovery in stochastic block model (SBM) [49]. Furthermore, motivated by the social homophily theory [3], we study the problem of interest when users within the same group (or cluster) are more likely to be connected than those in different groups (or clusters). That is why we assume that α ≥ β ≥ γ. vectors in different groups within the same cluster and those in different clusters are not smaller that δand δ, respectively; c/(ng) users. ground truth matrix M = (V, Z) ∈ M ﬁnd the estimator that minimizes the maximum risk (i.e., minimizes the worst-case probability of error). This can be expressed Based on the proposed problem formulation and performance metric, we aim at characterizing the optimal sample complexity (i.e., the minimum number of entries of the rating matrix that is required to be observed), concentrated around nmp limit of n and m, for exact rating matrix recovery. Here, p the following conditions, in the limit of n and m, are satisﬁed: Theorem 1 (Optimal Sample Complexity). Let m = ω(log n) and log m = o(n). Let q, θ, c, g and r be constants such that q is prime, θ ∈ [0, (q − 1)/q), and r ≤ g. For any constant  > 0, if p ≥ then there exists an estimator ψ that outputs a rating matrix X ∈ M conversely, if p ≤ then lim works [25], [26]. We defer the complete achievability proof to Section V, and the converse proof to Section VI. The achievability proof is based on maximum likelihood estimation (MLE). We ﬁrst evaluate the likelihood for a given clustering/grouping of users and the corresponding rating matrix. Next, we provide an upper bound on the worst-case probability of error, which is given by the probability that the likelihood of the ground truth rating matrix is less than that of a candidate rating matrix. Finally, we perform typical and atypical error analyses and demonstrate that if p > p the likelihood is maximized only by the ground-truth rating matrix, and hence the worst-case probability of error vanishes, in the limit of n and m. This completes the achievability proof. On the other hand, the converse (impossibility) proof hinges on the genie-aided maximum likelihood estimation. We ﬁrst establish a lower bound on the error probability, and show that it is minimized when employing the maximum likelihood estimator. Next, we prove that if p is smaller than any of the three terms in the RHS of (11), then there exists another solution that yields a larger likelihood, compared to the ground-truth matrix. More precisely, for any estimator and any ground truth rating matrix, we have the following three cases: Proof: We provide a concise proof sketch of Theorem 1, along with the technical distinctions with respect to the prior truth rating matrix with a carefully chosen sequence, and yields a larger likelihood than the one of the ground truth rating matrix; the rating vectors of two users in the same cluster yet from distinct groups such that the hamming distance between their rating vectors is mδ. We show that the likelihood of any rating matrix from this class is greater than the one of the ground truth rating matrix; is obtained by swapping the rating vectors of two users in distinct clusters such that the hamming distance between their For each case, we show that the maximum likelihood estimator will fail in the limit of n and m by selecting one of the rating matrices from the respective class instead of the ground truth rating matrix. This completes the converse proof, and concludes the proof of Theorem 1. The technical distinctions with respect to the prior works [25], [26] are four folded: (i) the likelihood computation requires more involved combinatorial arguments due to the hierarchical structure of the similarity graph; (ii) sophisticated upper/lower bounding techniques are developed in order to exploit the relational structure across different groups; (iii) novel typical and atypical error analyses are derived for the achievability proof; and (iv) novel failure proof techniques are proposed in the converse proof. The assumptions m = ω(log n) and log m = o(n) are introduced in order to be able to apply large deviation theories, as will be shown in the proof of Theorem 1. These assumptions are also practically relevant as they eliminate the possibility of having extremely tall and wide matrices, respectively. The following remark demonstrates that the problem setting considered in [1] is a special case of the general setting considered in this paper, and their result is subsumed by our generalized result presented in Theorem 1. Remark 1. Setting (c, g, r, q) = (2, 3, 2, 2), the optimal observation probability p which is equal to the shape threshold on p characterized We investigate the relationship between the optimal sample complexity nmp and different parameters related to the rating matrix as well as the hierarchical user similarity graph. The optimal sample complexity increases as δ between rating vectors of two users in different groups within the same cluster (or in different clusters) decreases, it becomes harder to distinguish the rating vectors, and hence it leads to imperfect user grouping (or clustering). Thus, one has to sample more entries of the rating matrix in order to exactly recover the rating matrix. It is evident that the optimal sample complexity increases as θ increases. Furthermore, as θ approaches (q − 1)/q, each sampled entry of the rating matrix can take any of the q possible values with a uniform probability of 1/q, and hence an inﬁnite sample complexity is theoretically required to exactly recover the entries of the rating matrix. In order to better illustrate the relationship between the optimal sample complexity and the quality of the hierarchical graph, we deﬁne the following quality parameters: Intuitively, as I hand, larger values of I depending on the quality parameters of the hierarchical graph. More speciﬁcally, we deﬁne three regimes as follows: 1) the ﬁrst term in the RHS of (11) is activated when I 3) the third term in the RHS of (11) is activated when I rating vectors is mδ. We demonstrate that any rating matrix from this class yields a larger likelihood than the one of the ground truth rating matrix. 1 − θ −θ information are reliable. Hence, this regime is coined as “perfect clustering/grouping regime”; this regime is coined as “grouping-limited regime”; reliable, andδ> δ. Thus, this regime is coined as “clustering-limited regime”. In what follows, we analyze the optimal sample complexity under each regime, and highlight the novel technical contributions in the achievability proof (See Section V) as well as the converse proof (See Section VI). For illustrative simplicity, we focus on the noiseless case where θ = 0. 1) Perfect Clustering/Grouping Regime: The optimal sample complexity reads (gc/(g −r + 1))m log m. Since the grouping and clustering information are reliable, one can recover the groups and clusters from the similarity graph. However, further increments of the values of these quality parameters do not yield further improvement in the sample complexity, and hence the sample complexity gain from the similarity graph is saturated in this regime. Moreover, it should be noted that a naive generalization of [25], [26] requires crm log m observations since there are r independent rating vectors to be estimated for each the c clusters, and each rating vector requires m log m observations under the considered random sampling due to the coupon-collecting effect. On the other hand, we leverage the relational structure (i.e, linear dependency) across rating vectors of different group, reﬂected by the underlying linear MDS code structure (to be detailed in Section V), and hence this serves to estimate the rc rating vectors more efﬁciently, precisely by a factor of r(g − r + 1)/g improvement, thus yielding (gc/(g − r + 1))m log m. 2) Grouping-Limited Regime: The optimal sample complexity reads which is a decreasing function of I graph consists of only gc clusters. This implies that exploiting the relational structure across different groups does not help improving sample complexity when grouping information is not reliable. Furthermore, since the clustering information is reliable, clusters can be recovered from the similarity graph. However, further increments of I reduction in the sample complexity, and hence the sample complexity gain from these two quality parameters is saturated in this regime. 3) Clustering-Limited Regime: The optimal sample complexity reads which is a decreasing function of I works. Since the clustering information is not reliable, it is not possible to recover the groups and clusters from the similarity graph. Moreover, it should be noted that when β = γ, i.e., groups and clusters are indistinguishable, we have I Comparing to the optimal sample complexity expression for the grouping-limited regime, the only distinction appears in the denominator, in which δ Consider a problem setting where n = 4000, m = 500, θ = 0, c = 10, g = 5, r = 3 and q = 5. Fig. 1a and Fig. 1b depict the different regimes of the optimal sample complexity as a function of (I δ= 1/6, the region depicted by diagonal stripes corresponds to perfect clustering/grouping regime and the ﬁrst term in the RHS of (11) is active. The graph quality parameters I rich enough to perfectly retrieve the clusters and groups. The region depicted by dots corresponds to grouping-limited regime, where the second term in the RHS of (11) is active. In this regime, graph information sufﬁces to exactly recover the clusters, but we need to rely on rating observation to exactly recover the groups. Finally, the third term in the RHS of (11) is active in the region captured by horizontal stripes. This indicates the clustering-limited regime, where neither clustering nor grouping is exact without the side information of the rating vectors. On the other hand, Fig. 1b depicts the case where δ δ= 1/3. It is worth noting that in practically-relevant systems, we have δ cluster are expected to be more similar than those in a different cluster. Therefore, the third regime, i.e., clustering-limited regime, vanishes in Fig. 1b. Consider a problem setting where n = 4000, m = 500, θ = 0, c = 10, g = 5, r = 3 and q = 5. Fig. 2 compares the optimal sample complexity, as a function of I and γ = 1. It should be noted that [26] leverages neither the hierarchical structure of the graph, nor the linear dependency among the rating vectors. Thus, the problem formulated in Section II will be translated to a graph that consists of gc clusters whose rating vectors are linearly independent in the setting of [26]. Also note that the minimum hamming distance for [26] is δ. In Fig. 2, we can see that the noticeable gain in the sample complexity of our result in the diagonal parts of the plot (i.e., = 0. As a result, it boils down to a problem setting of gc clusters, and hence the optimal sample complexity reads clustering-limited and grouping-limited regimes on the left side) is due to leveraging the hierarchical graph structure, while the improvement in the sample complexity in the ﬂat part of the plot (i.e, perfect clustering/grouping regime) is a consequence of exploiting the relational structure (i.e., linear dependency) among the rating vectors within each cluster. In this section, we prove the achievability part of Theorem 1, that is if the condition on p in (9) holds, then there exists an estimator ψ such that lim likelihood (ML) estimator, if all the following inequalities hold: Throughout the proof, let p = Θ((log n)/n), and let q and θ be constants such that q is prime and θ ∈ [0, 1]. We ﬁrst present the structure of the ground truth rating matrix and the underlying linear MDS code structure in Section V-A. Next, we introduce a number of auxiliary lemmas in Section V-B. Finally, we present the achievability proof of Theorem 1 in Section V-C. Let the ground truth rating matrix be denoted by X where Z stacking all the rating vectors of cluster x given by {u its r matrix) be denoted by X = (V, Z) where X ∈ F where Z also follows similar conditions listed in (4). First, we construct a ground truth rating matrix X sized groups. The set of g rating vectors of cluster x is spanned by any subset of r rating vectors for x ∈ [c]. Without loss of generality, assume that the set of users who belong to cluster x and group i is given by {k + 1, k + 2, . . . , k + x ∈ [c], i ∈ [g] and k = (x − 1) Fhas a minimum distance of g − r + 1, and hence reaches the Singleton bound [68]. Furthermore, according to the MDS conjecture [68], there exists a (g, r) MDS code in F even and r ∈ {3, q − 1}, in which case g ≤ q + 2. If these conditions are satisﬁed, then the existence of such an MDS code is guaranteed by the construction of Generalized Reed-Solomon codes [68]. Consider a (g, r) linear MDS code in F g is the length of the code and r is its dimension. Let the set of g ground truth rating vectors of the groups in cluster x be a (g, r) MDS code. Hence, the set of g rows of R matrix of the (g, r) MDS code, and W Without loss of generality, we make the following assumptions: Based on the aforementioned assumptions, the entries of each column of X vectors. This is due to the fact that the ﬁrst row of X be constructed by linear combinations of its ﬁrst r rows. Hence, we have a total of q Let the set of column of X the possible q and row u We assume that the MDS code structure is assumed to be known a priori, and hence the output matrix follows the MDS code structure imposed on the construction of X construction of the ground truth rating matrix X δmI+Igclog n +(g − 1)Igc follows the conditions given in (4), but omitted for the sake of brevity. Let R∈ Fbe a matrix obtained by row equals to uif and only if r ∈ Z(x, i). Furthermore, let the output of an estimator ψ (i.e., the completed rating . Recall from Section II that the considered hierarchical graph consists of c clusters, and each cluster comprises g equalensures that the ﬁrst r rows of Φare linearly independent, and hence the ﬁrst r rows of Rare linearly independent by (20). of Xbe partitioned into qsections, denotedby {u(`) : ` ∈ {0, 1, . . . , q − 1}}, for x ∈ [c] and i ∈ [g]. 1) Illustrative Example: Consider the setting of (c, g, r, q) = (2, 3, 2, 2). Under this setting, the generator matrix and the basis matrix of each cluster are given by where 0 ≤ s We present six auxiliary lemmas that are used to prove the achievability part of Theorem 1. Before each lemma, we introduce the terminologies and notations needed for the statement of the lemma. We show that the likelihood expression hinges on two factors: (i) the difference between the estimated ratings (entries of X) and the observed ratings (elements of Y ); and (ii) the dissimilarity between the graph induced by the partitioning in Z and the observed graph G. As deﬁned in Section I-C, we denote by Λ(X, Y ) the number of mismatched entries between X and Y . For a user partitioning Z, let P in different groups within any cluster; and P Recall from Section II that the user partitioning induced by any rating matrix in M groups have equal size of n/(cg) users. This implies that the sizes of P for any user partitioning. Furthermore, for a graph G and a user partitioning Z, deﬁne e any group; e clusters. More formally, we have for µ ∈ {eα, Lemma 1. For a given ﬁxed input pair (Y, G) and any X ∈ M where eα, The following lemma provides an upper bound on the worst-case probability of error P For a ground truth rating matrix X following disjoint sets: Let B independent Bernoulli random variables: eβ, eγ}. The following lemma gives a precise expression of L(X). L(X) = log(q − 1)1 − θθΛ (Y, X) +log1 − µµe(G, Z) − log(1 − µ) |P(Z)| eβ and eγ are the edge probabilities deﬁned in (5). Proof: We refer to Appendix A for the proof of Lemma 1. Proof: We refer to Appendix B for the proof of Lemma 2. the same cluster in X(and therefore they are connected with probabilityeβ), but they are estimated to be in the same group in X (and hence, given the estimator output, the belief for the existence of an edge between these two users is eα). Formally, we have P= {(a, b) : a ∈ Z(x, i) ∩ Z(y, j), b ∈ Z(x, i) ∩ Z(y, j), for x, y ∈ [c], i, i, j ∈ [g], i6= i P= {(a, b) : a ∈ Z(x, i) ∩ Z(y, j), b ∈ Z(x, i) ∩ Z(y, j), for x, y ∈ [c], i, j, j∈ [g], j6= j in X(and therefore they are connected with probability eγ), but they are estimated to be in the same group in X (and hence, given the estimator output, the belief for the existence of an edge between these two users is eα). Formally, we have P= {(a, b) : a ∈ Z(x, i) ∩ Z(y, j), b ∈ Z(x, i) ∩ Z(y, j), for x, x, y ∈ [c], x6= x, i, i, j ∈ [g]}. (33) P= {(a, b) : a ∈ Z(x, i) ∩ Z(y, j), b ∈ Z(x, i) ∩ Z(y, j), for x, y, y∈ [c], y6= y, i, j, j∈ [g]}; (34) in X(and therefore they are connected with probability eγ), but they are estimated to be in different groups of the same cluster in X (and hence, given the estimator output, the belief for the existence of an edge between these two users is eβ). Formally, we have P={(a, b): a ∈Z(x, i)∩Z(y, j), b∈Z(x, i)∩Z(y, j), for x, x, y ∈[c], x6=x, i, i, j, j∈[g], j6=j}. P={(a, b): a ∈Z(x, i)∩Z(y, j), b∈Z(x, i)∩Z(y, j), for x, y, y∈[c], y6=y, i, i, j, j∈[g], i6=i}. denote the iBernoulli random variable with parameter σ ∈ {p, θ,, eα,eβ, eγ}. Deﬁne the following sets of B: i ∈ P,B: i ∈ P,B: i ∈ P,B: i ∈ P, µ, ν ∈eα,eβ, eγ, µ 6= ν In the following lemma, we write each summand in (29) in terms of (38). The following lemma provides an upper bound of the RHS of (39). We show that the interested error event {L(X which dictate the relationship between X and X 1) the ﬁrst set includes counters to identify the number of users in cluster x and group i whose rating vector u Based on these two parameters, the set of rating matrices M Here, each matrix class X(T ) is deﬁned as the set of rating matrices that is characterized by a tuple T where Next, we analyze the performance of the ML decoder by comparing the ground truth user partitioning with that of the decoder. For a non-negative constant τ ∈ (0, ( log m − (2 + ) log(2q))/(2(1 + ) log m)), where  > max{(2 log 2)/ log n, (2(g − r + 1) log 2)/ log(2qm), (2 log(2q))/ log(m/2q)}, deﬁne σ(x, i) as the set of pairs of cluster and group in Z whose number of overlapped users with Z Note that τ < 0.5, which implies that |σ(x, i)| ≤ 1 since the size of any group is n/(gc) users. For |σ(x, i)| = 1, let σ(x, i) = {(σ(x), σ(i|x))}. Accordingly, partition the set T Proof: We refer to Appendix C for the proof of Lemma 3. P [B ≥ 0] ≤ exp−(1 + o(1))|P|I+ PIlog nn+ PIlog nn+ PIlog nn Proof: We refer to Appendix D for the proof of Lemma 4. changed to the rating vector vof users in cluster y and group j in X, for x, y ∈ [c] and i, j ∈ [g]. Formally, we deﬁne as the set of all non-all-zero tuples T . Therefore, we can write M=SX(T ). =T ∈ T: ∀(x, i) ∈ [c] × [g] such that |σ(x, i)| = 1, d≤ τ m min{δ, δ} Intuitively, when T ∈ T when T ∈ T mass. The following two lemmas provide an upper bound on the RHS of (40) under different classes of candidate rating matrices, and evaluating the limits as n and m tend to inﬁnity. C. The Achievability Proof of Theorem 1 The worst-case probability of error P where (50) follows from Lemma 2; (51) follows from the deﬁnition of negative log-likelihood in (24); (52) follows from the deﬁnition of the tuples characterizing matrix classes in (44); (53) follows from Lemma 3 and Lemma 4; and ﬁnally (54) follows from the deﬁnitions of T =T ∈ T: ∃(x, i) ∈ [c] × [g] such that (|σ(x, i)| = 0) ∪T ∈ T: ∀(x, i) ∈ [c] × [g] such that |σ(x, i)| = 1, ∃(x, i) ∈ [c] × [g] such that d> τ m min{δ, δ}. limexp−(1 + o(1))|P|I+ PIlog nn+ PIlog nn+ PIlog nn= 0. (48) Proof: We refer to Appendix E for the proof of Lemma 5. limexp−(1 + o(1))|P|I+ PIlog nn+ PIlog nn+ PIlog nn= 0. (49) Proof: We refer to Appendix F for the proof of Lemma 6. In this section, we prove the converse part of Theorem 1, that is if the condition on p in (10) holds, then lim for any estimator ψ. To this end, we prove that lim where I such that q is prime and θ ∈ [0, 1]. We ﬁrst present a number of auxiliary lemmas in Section VI-A. Then, we present the converse proof of Theorem 1 in Section VI-B. We present three auxiliary lemmas that are used to prove the converse part of Theorem 1. Before each lemma, we introduce the terminologies and notations needed for the statement of the lemma. First, let S denote the success event that a rating matrix is correctly estimated (i.e., exactly recovered). It is deﬁned as where L(X) is the negative log-likelihood of a candidate rating matrix X, deﬁned in (24). The following lemma introduces a lower bound on the inﬁmum of the worst-case probability of error. Next, the following lemma provides, together with Lemma 3, a lower bound on the probability that L(X or equal to L(X). Lemma 8. For any {P where the random variable B is deﬁned in (38). We ﬁnally present a lemma that guarantees the existence of two subsets of users with speciﬁc properties. Lemma 9. Consider the sets Z approaching 1, there exists two subsets |eZ(y, j)| ≥ , if either of the following conditions holds: δmI+Igclog n +(g − 1)Igc , I, Iand Iare deﬁned in (17). Throughout the proof, let p = Θ((log n)/n), and let q and θ be constants Proof: We refer to Appendix G for the proof of Lemma 7. Proof: We refer to Appendix H for the proof of Lemma 8. Proof: We refer to Appendix I for the proof of Lemma 9. B. The Converse Proof of Theorem 1 In order to prove the converse part of Theorem 1, we demonstrate that lim by (56), (57) or (58) holds. In the following, we show the claim for each condition in (56), (57) or (58) separately. 1) Failure Proof for the Perfect Clustering/Grouping Regime: In this proof, we introduce a class of rating matrices, where each matrix in this class is obtained by replacing one column of X (56) holds, then with high probability the ML estimator will fail by selecting one of the rating matrices from this class, instead of X Recall the sections of the columns of X matrix that is identical to X the submatrix of X with generator matrix Φ The existence of such a column vector w is guaranteed due to the fact that the (g, r) MDS code in F of g − r + 1. Consequently, the entries of X Furthermore, given X according to their deﬁnitions in (30)–(35). Thus, the cardinalities of the sets in (65) are given by For each X bounded by where (67) follows from Lemma 3 and (65); (68) follows from (66); and (69) is an immediate consequence of Lemma 8. |/m is bounded away from zero (i.e., not vanishing with m and n). For each k ∈ S, deﬁne X∈ Fas a rating PL(X) > L(X)= 1 − PL(X) ≤ L(X)"# where (71) follows from the deﬁnition in (59); (72) holds since the events {L(X independent due to the fact that each event corresponds to a different column k within S follows from the condition in (56). Therefore, we obtain which shows that if the condition in (56) holds, then the ML estimator will fail in ﬁnding X 2) Failure Proof for the Grouping-Limited Regime: Without loss of generality, assume δ rating vectors of groups 1 and 2 in cluster 1 have the minimum Hamming distance among distinct pairs of rating vectors of groups within the same cluster. In this proof, we introduce a class of rating matrices, which are obtained by switching two users between groups 1 and 2 in cluster 1. Then, we prove that if (57) holds, then with high probability the ML estimator will fail by selecting one of the rating matrices from this class, instead of X Set (x, i) = (1, 1) and (y, j) = (1, 2) in Lemma 9. Thus, there exist subsets with | which are swapped. More formally, the entries of X Furthermore, given X according to their deﬁnitions in (30)–(35). Thus, the cardinalities of the sets in (79) are given by L(X eZ(1, 1)| = |eZ(1, 2)| =, such that the subgraph induced by the vertices ineZ(1, 1) ∪eZ(1, 2) is edge-free. Deﬁne ∈ F, for a ∈eZ(1, 1) and b ∈eZ(1, 2), as a rating matrix that is identical to Xexcept for its aand brows, ) − LX= log(q − 1)1 − θθB1 + B()B− 1+ log(1 −β)eαeB− B where (81) follows from Lemma 3 and (79); and (82) follows from (80). Therefore, the probability that the negative loglikelihood of X where (83) follows from (82) and Lemma 8. Finally, the probability of exact rating matrix recovery is upper bounded by where (85) follows from the deﬁnition in (59); (86) holds since the events {L are mutually independent due to the fact that there are no edges among the vertices in follows from (84); (88) holds since | we obtain which shows that if the condition in (57) holds, then the ML estimator will fail in ﬁnding X 3) Failure Proof for the Clustering-Limited Regime: The proof follows the same structure as that presented in Section VI-B2 where the condition in (57) holds. Without loss of generality, assume that the rating vectors of group 1 in cluster 1 and group 2 in cluster 2 have the minimum Hamming distance among distinct pairs of rating vectors across different clusters, i.e., opposed to the same cluster in Section VI-B2. In this proof, we introduce a class of rating matrices, which are obtained by switching two users between group 1 in cluster 1 and group 2 in cluster 2. Then, we prove that if (58) holds, then with high probability the ML estimator will fail by selecting one of the rating matrices from this class, instead of X Set (x, i) = (1, 1) and (y, j) = (2, 2) in Lemma 9. Hence, there exist subsets with | to (77) and (78) in Section VI-B2, deﬁne X The corresponding user partitioning Z u, u= δm. Note that the corresponding groups deﬁned by such rating vectors belong to different clusters, as eZ(1, 1)| = |eZ(2, 2)| =, such that the subgraph induced by the vertices ineZ(1, 1)∪eZ(2, 2) is edge-free. Similar Furthermore, given X according to their deﬁnitions in (30)–(35). Thus, the cardinalities of the sets in (93) are given by For each X where (95) follows from Lemma 3 and (93); and (96) follows from (94). Therefore, the probability that the negative loglikelihood of X where (97) follows from (96) and Lemma 8. Finally, the probability of exact rating matrix recovery is upper bounded by L(X) − LX= log(q − 1)1 − θθB1 + B()B− 1 where (99) follows from the deﬁnition in (59); (100) holds since the events {L are mutually independent due to the fact that there are no edges among the vertices in (101) follows from (97); and (102) follows from the condition in (58), and | which shows that if the condition in (58) holds, then the ML estimator will fail in ﬁnding X We conduct Monte Carlo experiments clusters, g = 3 groups per cluster, ﬁnite ﬁeld of order q = 2, and r = 2 basis vectors per group. The MDS code structure is given u θ = 0.1, (β, γ) = (10, 0.5) and (δ Section II. In Figs. 3a and 3b, we evaluate the performance of the proposed algorithm, and quantify the empirical success rate as a function of the normalized sample complexity over 10 graphs. We vary n and m, preserving the ratio n/m = 3. Fig. 3a depicts the case of α = 40 which corresponds to perfect clustering/grouping regime, while Fig. 3b illustrates the case of α = 17 which corresponds to grouping-limited regime. In both ﬁgures, we observe a phase transition Figs. 3a and 3b corroborate Theorem 1 in different regimes when the graph side information is not scarce. In this paper, we consider a rating matrix that consists of n users and m items, and a hierarchical similarity graph that consists of c disjoint clusters, and each cluster comprises g disjoint groups. The rating vectors of the groups of a given cluster are different, but related to each other through a linear subspace of r basis vectors. We characterize the optimal sample complexity to jointly recover the hierarchical structure of the similarity graph as well as the rating matrix entries. We propose a matrix completion algorithm that is based on the maximum likelihood estimation, and achieve the characterized sample complexity. The optimality of the proposed achievable scheme was demonstrated through a matching converse proof. We demonstrate that the optimal sample complexity hinge on the quality of side information of the hierarchical similarity graph. We also highlighted the fact that leveraging the graph side information enables us to achieve signiﬁcant gain in sample complexity, compared to existing schemes that identiﬁes different groups without taking into consideration the hierarchical structure across them. One potential research direction is to develop a computationally efﬁcient algorithm to achieve the sharp threshold on the optimal sample complexity characterized in this paper. Another research direction is to characterize the optimal sample complexity for a more general case of c clusters, each of which comprise arbitrary number of groups of possibly different number of users. ≤ exp−nexp (−2(1 + o(1))(1 − ) log n)(102) = u+ ufor x ∈ [2]. Furthermore, the parameters of observation noise, graph and rating vectors are set to