In this paper, we formulate a Collaborative Pure Exploration in Kernel Bandit problem (CoPE-KB), which provides a novel model for multi-agent multi-task decision making under limited communication and general reward functions, and is applicable to many online learning tasks, e.g., recommendation systems and network scheduling. We consider two settings of CoPE-KB, i.e., Fixed-Condence (FC) and Fixed-Budget (FB), and design two optimal algorithms are equipped with innovative and ecient kernelized estimators to simultaneously achieve computation and communication eciency. Matching upper and lower bounds under both the statistical and communication metrics are established to demonstrate the optimality of our algorithms. The theoretical bounds successfully quantify the inuences of task similarities on learning acceleration and only depend on the eective dimension of the kernelized feature space. Our analytical techniques, including data dimension decomposition, linear structured instance transformation and (communication) round-speedup induction, are novel and applicable to other bandit problems. Empirical evaluations are provided to validate our theoretical results and demonstrate the performance superiority of our algorithms. Additional Key Words and Phrases: collaborative pure exploration, kernel bandit, communication round, learning speedup ACM Reference Format: Yihan Du, Wei Chen, Yuko Kuroki, and Longbo Huang. 2021. Collaborative Pure Exploration in Kernel Bandit. In Woodsto ck ’21: ACM Symposium on Neural Gaze Detection, June 03–05, 2021, Woodsto ck, NY . ACM, New York, NY, USA, 44 pages. https://doi.org/10.1145/ 1122445.1122456 Pure exploration [ sequentially chooses options (often called arms) and observes random feedback, with the objective of identifying the best option (arm). This problem nds various applications such as recommendation systems [ and neural architecture search [ cannot be directly applied to many real-world distributed online learning platforms, which often face a large volume of user requests and need to coordinate multiple distributed computing devices to process the requests, e.g., geographically distributed data centers [ information in order to attain globally optimal performance. To handle such distributed pure exploration problem, prior works [ Exploration (CoPE) model, where there are multiple agents that communicate and cooperate in order to identify the best arm with learning speedup. Yet, existing results only investigate the classic multi-armed bandit (MAB) setting [ 3,9,12,18,21,24] is a fundamental online learning problem in multi-armed bandits, where an agent and focus only on the fully-collaborative setting, i.e., the agents aim to solve a common task. However, in many real-world applications such as recommendation systems [31], it is often the case that dierent computing devices face dierent but correlated recommendation tasks. Moreover, there usually exists some structured dependency of user utilities on the recommended items. In such applications, it is important to develop a more general CoPE model that allows heterogeneous tasks and complex reward structures, and quantitatively investigate how task similarities impact learning acceleration. Motivated by the above facts, we propose a novel Collaborative Pure Exploration in Kernel Bandit (CoPE-KB) problem, which generalizes traditional single-task CoPE problems [20,22,38] to the multi-task setting. It also generalizes the classic MAB model to allow general (linear or nonlinear) reward structures via the powerful kernel representation. Specically, each agent is given a set of arms, and the expected reward of each arm is generated by a task-dependent reward function with a low norm in a high-dimensional (possibly innite-dimensional) Reproducing Kernel Hilbert Space (RKHS) [33,42], by which we can represent real-world nonlinear reward dependency as some linear function in a high-dimensional space, and can go beyond linear rewards as commonly done in the literature, e.g., [10,13,14,35,40]. Each agent sequentially chooses arms to sample and observes noisy outcomes. The agents can broadcast and receive messages to/from others in communication rounds, so that they can exploit the task similarity and collaborate to expedite learning processes. The task of each agent is to nd the best arm that maximizes the expected reward among her arm set. Our CoPE-KB formulation can handle dierent tasks in parallel and characterize the dependency of rewards on options, which provides a more general and exible model for real-world applications. For example, in distributed recommendation systems [31], dierent computing devices can face dierent tasks, and it is inecient to learn the reward of each option individually. Instead, CoPE-KB enables us to directly learn the relationship between option features and user utilities, and exploit the similarity of such relationship among dierent tasks to accelerate learning. There are also many other applications, such as clinical trials [41], where we conduct multiple clinical trials in parallel and utilize the common useful information to accelerate drug development, and neural architecture search [17], where we simultaneously run dierent tests of neural architectures under dierent environmental setups to expedite search processes. We consider two important pure exploration settings under the CoPE-KB model, i.e., Fixed-Condence (FC), where agents aim to minimize the number of used samples under a given condence, and Fixed-Budget (FB), where the goal is to minimize the error probability under a given sample budget. Note that due to the high dimension (possibly innite) of the RKHS, it is highly non-trivial to simplify the burdensome computation and communication in the RKHS, and to derive theoretical bounds only dependent on the eective dimension of the kernelized feature space. To tackle the above challenges, we adopt ecient kernelized estimators and design novel algorithmsCoopKernelFCandCoopKernelFBfor the FC and FB settings, respectively, which only costPoly(𝑛𝑉 )computation and communication complexity instead of Poly(dim(H))as in [10,43], where𝑛is the number of arms,𝑉is the number of agents, andHis the high-dimensional RKHS. We also establish matching upper and lower bounds in terms of sampling and communication complexity to demonstrate the optimality of our algorithms (within logarithmic factors). Our work distinguishes itself from prior CoPE works, e.g., [20,22,38], in the following aspects: (i) Prior works [20,22, 38] only consider the classic MAB setting, while we adopt a high-dimensional RKHS to allow more general real-world reward dependency on option features. (ii) Unlike [20,22,38] which restrict tasks (given arm sets and rewards) among agents to be the same, we allow dierent tasks for dierent agents, and explicitly quantify how task similarities impact learning acceleration. (iii) In lower bound analysis, prior works [20,38] mainly focus on a 2-armed case, whereas we derive a novel lower bound analysis for general multi-armed cases with high-dimensional linear reward structures. Moreover, when reducing CoPE-KB to prior CoPE with classic MAB setting (all agents are solving the same classic MAB task) [20, 38], our lower and upper bounds also match the existing state-of-the-art results in [38]. The contributions of this paper are summarized as follows: •We formulate a novel Collaborative Pure Exploration in Kernel Bandit (CoPE-KB) problem, which models distributed multi-task decision making problems with general reward functions, and nds applications in many real-world online learning tasks, and study two settings of CoPE-KB, i.e., CoPE-KB with xed-condence (FC) and CoPE-KB with xed-budget (FB). •For CoPE-KB with xed-condence (FC), we propose an algorithm cient kernelized estimator to signicantly reduce computation and communication complexity from existing and Δ •For CoPE-KB with xed-budget (FB), we design an ecient algorithm rounds is also established to validate the communication optimality of factors). Here inX to RKHS (dened in Section 5.1.1). •Our results explicitly quantify the impacts of task similarities on learning acceleration. Our novel analytical techniques, including data dimension decomposition, linear structured instance transformation and roundspeedup induction, can be of independent interests and are applicable to other bandit problems. Due to space limit, we defer all the proofs to Appendix. 2 RELATED WORK This work falls in the literature of multi-armed bandits [ of research, i.e., collaborative pure exploration and kernel bandit. Collaborative Pure Exploration (CoPE). considers the classic MAB and fully-collaborative settings, and designs xed-condence algorithms based on majority vote with upper bound analysis. Tao et al single-agent xed-condence algorithms, and completes the analysis of round-speedup lower bounds. Karpov et al [22]extend the formulation of [ xed-budget algorithms with tight round-speedup upper and lower bounds, which give a strong separation between best arm identication and the extended best MAB and fully-collaborative settings in the above works [ communication eciency due to the high-dimensional reward structures. Collaborative Regret Minimization. regret minimization objective. Bistritz and Leshem bandit with collisions motivated by cognitive radio networks, where multiple players simultaneously choose arms from the same set and receive no reward if more than one player choose the same arm (i.e., a collision happens). Bubeck and Budzinski Poly(dim(H))to onlyPoly(𝑛𝑉 ). We derive matching upper and lower bounds of sample complexity log 𝛿)and communication rounds𝑂 (log Δ). Here𝜌is the problem hardness (dened in Section 4.2), is the minimum reward gap. exp−𝑛𝑉and communication rounds𝑂 (log(𝜔 (˜X))). A matching lower bound of communication [6], Bubeck et al. [7]investigate a variant multi-player bandit problem where players cannot communicate but have access to shared randomness, and they propose algorithms that achieve nearly optimal regrets without collisions. Chakraborty et al. [11]introduce another distributed bandit problem, where each agent decides either to pull an arm or to broadcast a message in order to maximize the total reward. Korda et al. [25], Szorenyi et al. [36]adapt bandit algorithms to peer-to-peer networks, where the peers pick arms from the same set and can only communicate with a few random others along network links. The above works consider dierent learning objectives and communication protocols from ours, and do not involve the challenges of simultaneously handling multiple dierent tasks and analyzing the relationship between communication rounds and learning speedup. Kernel Bandit.There are a number of works for kernel bandit with the regret minimization objective. Srinivas et al. [35]study the Gaussian process bandit problem with RKHS, which is the Bayesian version of kernel bandits, and designs an Upper Condence Bound (UCB) style algorithm. Chowdhury and Gopalan[13]further improve the regret results of [35] by constructing tighter kernelized condence intervals. Valko et al. [40]consider kernel bandit from the frequentist perspective and provides an alternative regret analysis based on eective dimension. Deshmukh et al. [14], Krause and Ong[26]study the multi-task kernel bandits, where the kernel function of RKHS is constituted by two compositions from task similarities and arm features. Dubey et al. [16]investigate the multi-agent kernel bandit with a local communication protocol, with the learning objective being to reduce the average regret suered by per agent. For kernel bandit with the pure exploration objective, there are only two works [10,43] to our best knowledge. Camilleri et al. [10]design a single-agent algorithm which uses a robust inverse propensity score estimator to reduce the sample complexity incurred by rounding procedures. Zhu et al. [43]propose a variant of [10] which applies neural networks to approximate nonlinear reward functions. All of these works consider either regret minimization or single-agent setting, which largely diers from our problem, and they do not investigate the distributed decision making and (communication) round-speedup trade-o. Thus, their algorithms and analysis cannot be applied to solve our CoPE-KB problem. 3 COLLABORATIVE PURE EXPLORATION IN KERNEL BANDIT (COPE-KB) In this section, we present the formal formulation of the Collaborative Pure Exploration in Kernel Bandit (CoPE-KB), and discuss the two important settings under CoPE-KB that will be investigated. Agents and rewards.There are𝑉agents[𝑉 ] = {1, . . . ,𝑉 }, who collaborate to solve dierent but possibly related instances (tasks) of the pure exploration in kernel bandit (PE-KB) problem. For each agent𝑣 ∈ [𝑉 ], she is given a set of 𝑛armsX= {𝑥, . . . , 𝑥} ⊆ R, where𝑑is the dimension of arm feature vectors. The expected reward of each arm𝑥 ∈ Xis𝑓(𝑥), where𝑓:X↦→ Ris an unknown reward function. LetX = ∪X. Following the literature in kernel bandits [14,26,35,40], we assume that for any𝑣 ∈ [𝑉 ],𝑓has a bounded norm in a Reproducing Kernel Hilbert Space (RKHS) specied by kernel𝐾:X × X ↦→ R(see below for more details). At each timestep𝑡, each agent𝑣pulls an arm𝑥∈ Xand observes a random reward𝑦= 𝑓 (𝑥) + 𝜂, where𝜂is an independent and zero-mean 1-sub-Gaussian noise (without loss of generality).We assume that the best arms𝑥= argmax𝑓(𝑥)are unique for all 𝑣 ∈ [𝑉 ], which is a common assumption in the pure exploration literature, e.g., [3, 12, 18, 24]. Multi-Task Kernel Composition.We assume that the functions𝑓are parametric functionals of a global function 𝐹 : X × Z ↦→ R, which satises that, for each agent 𝑣 ∈ [𝑉 ], there exists a task feature vector 𝑧∈ Z such that HereXandZdenote the arm feature space and task feature space, respectively. Eq.(1)allows tasks to be dierent for agents, whereas prior CoPE works [20, 22, 38] restrict the tasks (Xand 𝑓) to be the same for all agents 𝑣 ∈ [𝑉 ]. Denote has a bounded norm in a global RKHS 𝜙 :˜X ↦→ H satises that for any 𝑧, 𝑧 where𝐾 measures the similarity of functions and we have that dierent, then learning. We give a simple 2-agent (2-task) illustrating example in Figure 1. Agent 1 is given Items 1,2 with the expected rewards𝜇 𝜇, 𝜇, respectively, denoted by 𝜙 (˜𝑥) = 𝜙 ( information on the second dimension of 𝜃 Note that the RKHS and any direct operation on of𝜙 (˜𝑥)and explicit expression of the estimate of impracticable. In this paper, all our algorithms only query the kernel function analysis, which is dierent from existing works, e.g., [ 43]. Communication. tion protocol in existing CoPE works [ allow these munication rounds, in which each agent can broadcast and receive messages from others. While we do not restrict the exact length of a message, for practical implementation it should be bounded by number of arms for each agent, and we consider the number of bits for representing a real number as a constant. In the CoPE-KB problem, our goal is to design computation and communication ecient algorithms to coordinate dierent agents to simultaneously complete multiple tasks in collaboration and characterize how the take similarities impact the learning speedup. Fixed-Condence and Fixed-Budget. (FC) and the other with xed-budget (FB). Specically, in the FC setting, given a condence parameter agents aim to identify used by each agent. In the FB setting, on the other hand, the agents are given an overall ˜X = X × Zand˜𝑥 = (𝑥, 𝑧). As a standard assumption in kernel bandits [14,16,26,35], we assume that𝐹 and an unknown parameter 𝜃∈ Hsuch that :=𝜃𝜃≤ 𝐵for some known constant𝐵 >0.𝐾:˜X ×˜X ↦→ Ris a product composite kernel, which is the arm feature kernel that depicts the feature structure of arms, and𝐾is the task feature kernel that 𝑧=1 for all𝑣 ∈ [𝑉 ],𝐾(𝑧, 𝑧) =1 for all𝑧, 𝑧∈ Z, and𝐾 = 𝐾. On the contrary, if all tasks are rank(𝐾) = 𝑉.𝐾allows us to characterize the inuences of task similarities (1≤ rank(𝐾) ≤ 𝑉) on , 𝜇, respectively, denoted byX= {𝑥, 𝑥}. Agent 2 is given Items 2,3 with the expected rewards ˜𝑥) = [0,1,0],𝜙 (˜𝑥) = [0,0,1], and𝜃= [𝜇, 𝜇, 𝜇]. The two agents can share the learned 𝜙 (˜𝑥)and𝜃are only used in our theoretical Following the popular communica- 𝑉agents to exchange information via comsamples per agent), and aim to use at most𝑇 ·𝑉samples to identify𝑥for all𝑣 ∈ [𝑉 ]and minimize the error probability. In both FC and FB settings, agents are requested to minimize the number of communication rounds. To evaluate the learning acceleration of our algorithms, following the CoPE literature, e.g., [20,22,38], we also dene the speedup metric of our algorithms. For a CoPE-KB instanceI, let𝑇denote the average number of samples used by each agent in multi-agent algorithmAto identify𝑥for all𝑣 ∈ [𝑉 ], and let𝑇denote the average number of samples used by each task for a single-agent algorithmAto sequentially (without communication) identify𝑥for all 𝑣 ∈ [𝑉 ]. Then, the speedup of Aon instance I is formally dened as It can be seen that 1≤ 𝛽≤ 𝑉, where𝛽=1 for the case where all tasks are dierent and𝛽can approach𝑉for a fully-collaborative instance. By taking𝑇and𝑇as the smallest numbers of samples needed to meet the condence constraint, the denition of 𝛽can be similarly dened for error probability results. In particular, when all agents𝑣 ∈ [𝑉 ]have the same arm setX= X = {𝒆, . . . , 𝒆}(i.e., standard bases inR) and the same reward function𝑓(𝑥) = 𝑓 (𝑥) = 𝑥𝜃for any𝑥 ∈ X, all agents are solving a common classic MAB task, and then the task featureZ = {1}and𝐾(𝑧, 𝑧) =1 for any𝑧, 𝑧∈ Z. In this case, our CoPE-KB problem reduces to prior CoPE with classic MAB setting [20, 38]. 4 FIXED-CONFIDENCE COPE-KB We start with the xed-condence (FC) setting and propose theCoopKernelFCalgorithm. We explicitly quantify how task similarities impact learning acceleration, and provide sample complexity and round-speedup lower bounds to demonstrate the optimality of CoopKernelFC. 4.1 Algorithm CoopKernelFC 4.1.1 Algorithm.CoopKernelFChas three key components: (i) maintain alive arm sets for all agents, (ii) perform sampling individually according to the globally optimal sample allocation, and (iii) exchange the distilled observation information to estimate reward gaps and eliminate sub-optimal arms, via ecient kernelized computation and communication schemes. The procedure ofCoopKernelFC(Algorithm 1) for each agent𝑣is as follows. Agent𝑣maintains alive arm setsB for all𝑣∈ [𝑉 ]by successively eliminating sub-optimal arms in each phase. In phase𝑡, she solves a global min-max optimization, which takes into account the objectives and available arm sets of all agents, to obtain the optimal sample allocation𝜆∈ △and optimal value𝜌(Line 4). Here△is the collection of all distributions onX.𝜉is a regularization parameter such that which ensures the estimation bias for reward gap to be bounded by 2and can be eciently computed by kernelized transformation (specied in Section 4.1.2). Then, agent𝑣uses𝜌to compute the number of required samples𝑁, which guarantees that the condence radius of estimation for reward gaps is within 2(Line 5). In algorithmCoopKernelFC, we use a rounding procedure ROUND(𝜆, 𝑁 ) with approximation parameter 𝜖 from [2, 10], which rounds the sample Algorithm 1: Distributed Algorithm CoopKernelFC: for Agent 𝑣 (𝑣 ∈ [𝑉 ]) ˜𝑠, . . . , Broadcast on arm any allocation 𝜆 ∈ △ By calling a sub-sequence the number of samples overall observation information, she estimates the reward gap 𝑣∈ [𝑉 ] and discards sub-optimal arms (Lines 13-14). 4.1.2 Computation and Communication Eiciency. Here we explain the eciency of CoPE-KB, due to its high-dimensional reward structures, directly using the empirical mean to estimate rewards will ,˜X, . . . ,˜X,𝐾 (·, ·):˜X×˜X ↦→ R,𝐵, rounding procedureROUND(·, ·)with approximation parameter𝜀. ∈ [𝑉 ], |B| > 1 do and 𝜌be the optimal solution and optimal value of max∥𝜙 (˜𝑥) − 𝜙 (˜𝑥)∥, where𝜉is a regularization parameter that ˜𝑠) ← ROUND(𝜆, 𝑁); ˜𝒔be the sub-sequence of (˜𝑠, . . . ,˜𝑠) which only contains the arms in˜X; // generate the sample ˜𝒔and observe random rewards 𝒚; {(𝑁,¯𝑦)}, where𝑁is the number of samples and¯𝑦is the average observed reward , . . . , B; into the integer numbers of samples 𝜅 ∈ N, such thatÍ𝜅= 𝑁 and ROUND(𝜆, 𝑁), agent𝑣generates an overall sample sequence(˜𝑠, . . . ,˜𝑠)according to𝜆, and extracts ˜𝒔that only contains the arms in˜Xto sample (Lines 6-8). After sampling, she only communicates cause loose sample complexity, and naively calculating and transmitting innite-dimensional parameter 𝜃will incur huge computation and communication costs. As a result, we cannot directly compute and communicate scalar empirical rewards as in prior CoPE with classic MAB works [20, 22, 38]. Computation Eciency. CoopKernelFCuses three ecient kernelized operations, i.e., optimization solver (Line 4), condition for regularization parameter𝜉(Eq.(3)) and estimator of reward gaps (Line 13). Unlike prior kernel bandit algorithms [10, 43] which explicitly compute 𝜙 (˜𝑥) and maintain the estimate of 𝜃on the innite-dimensional RKHS, CoopKernelFConly queries kernel function𝐾 (·, ·)and signicantly reduces the computation (memory) costs from Poly(dim(H)) to only Poly(𝑛𝑉 ). Below we give the formal expressions of these operations and defer their detailed derivation to Appendix A.1. Kernelized Estimator. We rst introduce the kernelized estimator of reward gaps (Line 13). Following the standard estimation procedure in linear/kernel bandits [10,19,23,43], we consider the following regularized least square estimator of underlying reward parameter 𝜃 Note that this form ofˆ𝜃has𝑁terms in the summation, which are cumbersome to compute and communicate. Since the samples(˜𝑠, . . . ,˜𝑠)are composed by arms˜𝑥, . . . ,˜𝑥, we merge repetitive computations for same arms in the summation and obtain (for notational simplicity, we combine the subscripts𝑣, 𝑖in˜𝑥, 𝑁,¯𝑦by using ˜𝑥, 𝑁,¯𝑦, respectively) Here𝑁is the number of samples and¯𝑦is the average observed reward on arm˜𝑥for any𝑖 ∈ [𝑛𝑉 ].Φ= is the kernel matrix, and¯𝑦= [𝑁¯𝑦, . . . ,𝑁¯𝑦]is the average observations. Equality (a) rearranges the summation according to dierent chosen arms, and (b) follows from kernel transformation. Then, by multiplying𝜙 (˜𝑥) − 𝜙 (˜𝑥), we obtain the estimator of reward gaps 𝑓 (˜𝑥) − 𝑓 (˜𝑥) as where𝑘(˜𝑥) = Φ𝜙 (˜𝑥) = [𝑁𝐾 (˜𝑥,˜𝑥), . . . ,𝑁𝐾 (˜𝑥,˜𝑥)]for any˜𝑥 ∈˜X. This estimator not only transforms heavy operations on the innite-dimensional RKHS to ecient ones that only query the kernel function, but also merges repetitive computations for same arms (equality (a)) and only requires calculations dependent on 𝑛𝑉 . Kernelized Optimization Solver/Condition for Regularization Parameter. Now we introduce the optimization solver (Line 4) and condition for regularization parameter 𝜉(Eq. (3)). For the kernelized optimization solver, we solve the min-max optimization in Line 4 by projected gradient descent,Í which follows the procedure in [10]. Specically, let𝐴(𝜉, 𝜆) = 𝜉𝐼 +𝜆𝜙 (˜𝑥)𝜙 (˜𝑥)for any𝜉 >0, 𝜆 ∈ △. We dene function the gradient of ℎ(𝜆) is given by which can be eciently calculated by the following kernel transformation where 𝑘( For condition Eq. (3) on the regularization parameter 𝜉 where𝜆 Both the kernelized optimization solver and condition for dimensional RKHS by querying the kernel function, and only cost (Eqs. (7),(8) only contains scalar 𝐾 ( Communication Eciency. tive computations for the same arms and only transmits of transmitting all 𝑂 (𝑛𝑉 ) bits (Lines 9-10). 4.2 Theoretical performance of CoopKernelFC Dene the problem hardness of identifying the best arms where𝜉 linear/kernel bandit pure exploration [ of samples used by each agent in algorithm CoopKernelFC. The sample complexity and number of communication rounds of CoopKernelFC are as follows. Theorem 1 (Fixed-Confidence Upper Bound). With probability at least 1 correct answers and communication rounds 𝑂 (log Δ = [, . . . ,]is the uniform distribution on˜X,𝑘(˜𝑥) = [𝐾 (˜𝑥,˜𝑥), . . . ,𝐾 (˜𝑥,˜𝑥)]and𝐾= 𝑁samples as in [16]. This signicantly reduces the communication cost from𝑂 (𝑁)bits to = min𝜉.𝜌is the information-theoretic lower bound of the CoPE-KB problem, which is adapted from Remark 1. 𝜌is comprised of two sources of problem hardness, one due to handling dierent tasks and the other due to distinguishing dierent arms (We will decompose the sample complexity into these two parts in Corollary 1(c)). We see that the sample complexity ofCoopKernelFCmatches the lower bound (up to logarithmic factors). For fully-collaborative instances where single-agent algorithms [19,23] have˜𝑂 (𝜌log 𝛿)sample complexity, ourCoopKernelFCachieves the maximum𝑉-speedup (i.e., enjoys˜𝑂 (log 𝛿)sample complexity) using only logarithmic communication rounds. Interpretation.We further interpret Theorem 1 via standard expressive tools in kernel bandits [14,35,40], i.e., eective dimension and maximum information gain, to characterize the relationship between sample complexity and data structures, and demonstrate how task similarity inuences learning performance. To this end, dene the maximum information gain over all sample allocation 𝜆 ∈ △as Denote𝜆= argmaxlog det𝐼 +𝜉𝐾and𝛼≥ ··· ≥ 𝛼the eigenvalues of𝐾, and dene the eective dimension of 𝐾as We then have the following corollary. Corollary 1. The per-agent sample complexity of algorithmCoopKernelFC, denoted by𝑆, can also be bounded as follows: Here 𝑔(Δ, 𝛿) = log Δlog+ log log Δ. Remark 2.Corollary 1(a) shows that, our sample complexity can be bounded by the maximum information gain of any sample allocation on˜X, which extends conventional information-gain-based results in regret minimization kernel bandits [13, 16, 35] to the pure exploration setting in the view of experimental (allocation) design. In terms of dimension dependency, it is demonstrated in Corollary 1(b) that our result only depends on the eective dimension of kernel representation, which is the number of principle directions that data projections in RKHS spread. We also provide a fundamental decomposition of sample complexity into two compositions from task similarities and arm features in Corollary 1(c), which shows that the more tasks are similar, the fewer samples we need for accomplishing all tasks. For example, when tasks are the same (fully-collaborative), i.e.,rank(𝐾) =1, each agent only spends a fraction of samples used by single-agent algorithms [10,43]. Conversely, when the tasks are totally dierent, i.e., rank(𝐾) = 𝑉, no advantage can be attained by multi-agent deployments, since the information from neighboring agents is useless for solving local tasks. 4.3 Lower Bound for Fixed-Confidence Seing We now present lower bounds for the sample complexity and a round-speedup for fully-collaborative instances, using a novel measure transformation techniques. The bounds validate the optimality of and communication. Specically, Theorems 2 and 3 below formally present our bounds. In the theorems, we refer to a distributed algorithm at least 1 −𝛿. Theorem 2 (Fixed-Confidence Sample Complexity Lower Bound). Consider the xed-condence collab orative pure exploration in kernel bandit problem with Gaussian noise A must have per-agent sample complexity Ω ( Remark 3. still requires at least logarithmic factors of the optimal sampling. Theorem 3 (Fixed-Confidence Round-Speedup Lower Bound). There exists a fully-collaborative instance of the xed-condence CoPE-KB problem with multi-armed and linear reward structures, for which given any 𝛿-correct and 𝛽-speedup distributed algorithm A must utilize communication rounds in expectation. In particular, when expectation. Remark 4. which validates that CoPE with classic MAB setting [ bounds (Theorems 1 and 3) match the state-of-the-art results in [38]. Novel Analysis for Fixed-Condence Round-Speedup Lower Bound. bound for the FC setting analysis has the following novel aspects. (i) Unlike prior CoPE work [ on a preliminary 2-armed case without considering reward structures, we investigate multi-armed instances with high-dimensional linear reward structures. (ii) We develop a linear structured progress lemma (Lemma 3 in Appendix A.5), which eectively handles the challenges due to dierent possible sample allocation on multiple arms and derives the required communication rounds under linear reward structures. (iii) We propose multi-armed measure transformation and linear structured instance transformation lemmas (Lemmas 4,5 in Appendix A.5), which bound the change of probability measures in instance transformation with multiple arms and high-dimensional linear rewards, and serve as basic analytical tools in our proof. 5 FIXED-BUDGET COPE-KB We now turn to the xed-budget (FB) setting and design an ecient algorithm xed-budget round-speedup lower bound to validate its communication optimality. Theorem 2 shows that even if the agents are allowed to share samples without limitation, each agent Theorem 3 exhibits that logarithmic communication rounds are indispensable for achieving the full speedup, Algorithm 2: Distributed Algorithm CoopKernelFB: for Agent 𝑣 (𝑣 ∈ [𝑉 ]) Input: Per-agent budget 𝑇 ,˜X, . . . ,˜X, 𝐾 (·, ·) :˜X ×˜X ↦→ R, regularization parameter 𝜉, rounding procedure ˜𝑠, . . . ,˜𝑠) ← ROUND(𝜆, 𝑁); 5.1 Algorithm CoopKernelFB 5.1.1 Algorithm.CoopKernelFBconsists of three key steps: (i) pre-determine the numbers of phases and samples according to data dimension, (ii) maintain alive arm sets for all agents, plan a globally optimal sample allocation, (iii) communicate observation information and cut down alive arms to a half in the dimension sense. The procedure ofCoopKernelFBis given in Algorithm 2. During initialization, we determine the number of phases 𝑅 and the number of samples for each phase 𝑁 according to the principle dimension 𝜔(˜X) (Line 1), dened as: i.e., the principle dimension of data projections in˜Sto the RKHS. In each phase𝑡, each agent𝑣maintains alive arm sets Bfor all agents𝑣∈ [𝑉 ], and solves an integrated optimization to obtain a globally optimal sample allocation𝜆 (Line 3). Then, she generates a sample sequence(˜𝑠, . . . ,˜𝑠)according to𝜆, and selects the sub-sequence˜𝒔that only contains her available arms to perform sampling (Lines 4-5). During communication, she only sends and receives the number of samples𝑁and average observed reward¯𝑦for each arm to and from other agents (Lines 7-8). Using the shared information, she estimates rewards of alive arms and only selects the best half of them in the dimension sense to enter the next phase (Lines 11-14). 5.1.2 Computation and Communication Eiciency. solver (Eqs. estimate the rewards in Line 11. Moreover, communication costs. 5.2 Theoretical performance of CoopKernelFB We present the error probability of CoopKernelFB in the following theorem, where 𝜆 Theorem 4 (Fixed-Budget Upper Bound). Suppose Ω(𝜌log(𝜔 ( . With at most probability and communication rounds 𝑂 (log(𝜔 ( Remark 5. samples, which matches the sample complexity lower bound (Theorem 2) up to logarithmic factors. In addition, CoopKernelFB rounds, which also matches the round-speedup lower bound (Theorem 5) within double logarithmic factors. Technical Novelty in Error Probability Analysis. multi-agent setting. The single-agent analysis in [ bound. Instead, we establish novel estimate concentrations and high probability events for each arm pair and each agent to handle the distributed environment, and build a connection between the principle dimension problem hardness 𝜌 Interpretation. maximum information gain and eective dimension in kernel bandits [ into two compositions from task similarities and arm features. Corollary 2. The error probability of algorithm CoopKernelFC, denoted by 𝐸𝑟𝑟 , can also be b ounded as follows: (6),(7)) to solve the min-max optimization in Line 3 and employs the kernelized estimator (Eq.(4)) to ˜X))), and the regularization parameter𝜉>0 satises𝜉max∥𝜙 (˜𝑥) −𝜙 (˜𝑥)∥≤ Theorem 4 implies that, to guarantee an error probability𝛿,CoopKernelFBonly requires𝑂 (log()) attains the maximum𝑉-speedup for fully-collaborative instances with only logarithmic communication Similar to Corollary 1, we can also interpret the error probability result with the standard tools of (a) 𝐸𝑟𝑟 = 𝑂exp−𝑇𝑉 Δ˜·𝑛𝑉 log(𝜔 (˜X)), where Υ is the maximum information gain. eective dimension. Remark 6.Corollaries 2(a), 2(b) bound the error probability by maximum information gain and eective dimension, respectively, which capture essential structures of tasks and arm features and only depend on the eective dimension of the feature space of kernel representation. Furthermore, we exhibit how task similarities inuence the error probability performance in Corollary 2(c). For example, in the fully-collaborative case whererank(𝐾) =1, the error probability enjoys an exponential decay factor of𝑉compared to conventional single-agent results [23] (achieves a𝑉-speedup). Conversely, when the tasks are totally dierent withrank(𝐾) = 𝑉, the error probability degenerates to conventional single-agent results [23], since in this case information sharing brings no benet. 5.3 Lower Bound for Fixed-Budget Seing In this subsection, we establish a round-speedup lower bound for the FB setting. Theorem 5 (Fixed-Budget Round-Speedup Lower Bound). There exists a fully-collaborative instance of the xedbudget CoPE-KB problem with multi-armed and linear reward structures, for which given any𝛽 ∈ [,𝑉 ], a 𝛽-speedup distributed algorithm A must utilize communication rounds in expectation. In particular, when𝛽 = 𝑉,Amust useΩ()communication rounds in expectation. Remark 7.Theorem 5 shows that under the FB setting, to achieve the full speedup, agents require at least logarithmic communication rounds with respect to the principle dimension 𝜔(˜X), which validates the communication optimality ofCoopKernelFB. In the degenerated case when all agents solve the same non-structured pure exploration problem, same as in prior classic MAB setting [20,38], both our upper (Theorem 4) and lower (Theorem 5) bounds match the state-of-the-art results in [38]. Novel Analysis for Fixed-Budget Round-Speedup Lower Bound.Dierent from the FC setting, here we borrow the proof idea of prior limited adaptivity work [1] to establish a non-trivial lower bound analysis under Bayesian environments, and perform instance transformation by changing data dimension instead of tuning reward gaps. In our analysis, we employ novel techniques to calculate the information entropy and support size of posterior reward distributions in order to build induction among dierent rounds and derive the required communication rounds. 6 EXPERIMENTS In this section, we conduct experiments to validate the empirical performance of our algorithms. In our experiments, we set𝑉 =5,𝑑 =4,𝑛 =6,𝛿 =0.005 and𝜙 (˜𝑥) = 𝐼˜𝑥for any˜𝑥 ∈˜X. The entries of𝜃form an arithmetic sequence that starts from 0.1 and has the common dierenceΔ, i.e.,𝜃= [0.1,0.1+ Δ, . . . ,0.1+ (𝑑 −1)Δ]. For the FC setting, we vary the gapΔ∈ [0.1,0.8]to generate dierent instances (points), and run 50 independent simulations to plot the average sample complexity with 95% condence intervals. For the FB setting, we change the budget𝑇 ∈ [7000,300000] to obtain dierent instances, and perform 100 independent runs to show the error probability across runs. The specic values of gap Δand budget 𝑇 can be seen in X-axis of the gures. Fixed-Condence.In the FC setting (Figures 2(a)-2(c)), we compareCoopKernelFCwith ve baselines:CoopKernel-IndAlloc is an ablation variant ofCoopKernelFCwhich individually calculates sample allocations for dierent agents.IndRAGE[19], IndALBA[34] andIndPolyALBA[15] are single-agent algorithms, which use𝑉copies of single-agentRAGE[19], divides the sample complexity of the best single-agent algorithm achieves the best sample complexity in Figures 2(a),2(b), which demonstrates the eectiveness of our sample allocation and cooperation scheme. Moreover, the empirical results also reect the impacts of task similarities on learning speedup, and keep consistent with our theoretical analysis. Specically, in the fully-collaborative case (Figure 2(a)), matchesIndRAGE-𝑉 ofCoopKernelFC decrease of task similarity; in the totally-dierent-task ( the single-agent algorithm IndRAGE, since information sharing among agents brings no advantage in this case. Fixed-Budget. is an ablation variant of andIndUniformFB sampling policies, respectively. As shown in Figures 2(d),2(e), our all other algorithms. In addition, these empirical results also validate the inuences of task similarities on learning performance, and match our theoretical analysis. Specically, as the task similarity decreases in Figures 2(d) to 2(f), the error probability of speedup. 7 CONCLUSION In this paper, we propose a novel Collaborative Pure Exploration in Kernel Bandit (CoPE-KB) problem with FixedCondence (FC) and Fixed-Budget (FB) settings. CoPE-KB aims to coordinate multiple agents to identify best arms with ] andPolyALBA[15] policies to solve the𝑉tasks independently.IndRAGE/𝑉is a𝑉-speedup baseline, which lies betweenIndRAGE/𝑉andIndRAGE, since it only achieves smaller than𝑉speedup due to the In the FB setting (Figures 2(d)-2(f)), we compareCoopKernelFBwith three baselines:CoopKernelFB-IndAlloc CoopKernelFBgets closer to that of single-agentIndRAGE-FB, due to the slow-down of its learning general reward functions. We design two computation and communication ecient algorithmsCoopKernelFCand CoopKernelFBbased on novel kernelized estimators. Matching upper and lower bounds are established to demonstrate the statistical and communication optimality of our algorithms. Our theoretical results explicitly characterize the impacts of task similarities on learning speedup and avoid heavy dependency on the high dimension of the kernelized feature space. In our analysis, we also develop novel analytical techniques, including data dimension decomposition, linear structured instance transformation and (communication) round-speedup induction, which are applicable to other bandit problems and can be of independent interests.