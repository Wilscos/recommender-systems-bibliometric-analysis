Our goal is to identify inhibitors and catalysts for productive longterm scientiﬁc software development. The inhibitors and catalysts could take the form of processes, tools, techniques, environmental factors (like working co nditions) and software artifacts (such as user manuals, unit tests, design documents and code). The effort (time) invested in catalysts will pay oﬀ in the long-term, while inhibitors will take u p resources, and can lower product quality. Developer surveys on inhibitors and catalysts will yield responses as varied as the education and experiential backgrounds of the respondents. Although well-meaning, respo nses will predictably be biased. For instance, developers may be guilty of the sunk cost fallacy, promoting a technology they have invested considerable hours in learning, even if the current costs outweigh the beneﬁts. Likewise developers may recommend against spending time on proper requirements, not as an indication that requirements are not valuable, only that current practice doesn’t promote requirements [ 2]. Another perceived inhibitor is time spent in meetings. For instance, the lack of visible short-term beneﬁts renders department retreats unpopular, even though relationship building and strategic decision making may provide signiﬁcant future rewards. Evaluating the usefulness of meetings is diﬃcult. Rather than relying o n preference and perception, as these examples illu strate, we need to measure the long-term impact of development choices to make wise ones. A scientiﬁc approach requires a solid foundation. The building blocks for scientiﬁc discourse are: communicating co ncepts via an unambiguous language, for mulating hypotheses, p lanning data collection, and analyzing models and theories. To start with, we need to classify the software under discussion. Likely dimensions include: general purpose scientiﬁc tools versus special purp ose physical models, scientiﬁc domain, open source versus commercial software, project maturity, project size, and level of safety criticality. We also nee d to be precise about our software quality goals. Qualities such as reliability, sustainability, reproducibility and productivity need precise deﬁnitions. Attempts have been made since the 1970s [7], but the resulting deﬁnitions aren’t usually speciﬁc t o scientiﬁc software (as shown by the confusion between precision and accuracy is the ISO/IEC deﬁnitions [4]). Moreover, the deﬁnitions often focus on measurability, where the ﬁrst priority should be conceptual clarit y, analogous to the unmeasurable, but conceptually clear, deﬁnition of forward error, which req uires knowing the (usually unknown) true answer. For each relevant quality we recommend collecting as many distinct deﬁnitions as possible. Once collected, they can be assessed against the following criteria (b ased on IEEE [3]): completeness, consistency, modiﬁability, traceability, unambiguity and abstractness. The understanding gained from this systematic survey and analysis can be used to either choose solid deﬁnitions, or propose new ones. In all cases, the deﬁnitions should enable reasoning about quality. Our deﬁnition of long-term productivity [9] provides an example of our vision, and meets our criteria. We deﬁne productivity as: where 𝑃 is productivity, 𝐼 is the inputs, 𝑂 is the outputs, 0 is the time the project started, 𝑇 is the time in the future where we want to take stock, 𝐻 is the total number of hours available by all personnel, 𝐶 represents diﬀerent classes of users (external as well as internal), 𝑆 is user satisfaction and 𝐾 is eﬀective knowledge, and 𝐹 is a weighing function that indicates “value”. Thus productivity is measured in “value per year.” and is a mixture of external and internal value produced. Value should not be equated with money; measuring the productivity of free software development is just as important as for commercial software. While the most straightforward use of such a formula is t o measure productivity of a team, it can also be used in “what if” scenarios to assist in planning interventions, i.e. changes intended to improve productivity. Measuring over too short a time-frame will assuredly give warped results. This leads so me to argue that productivity shouldn’t even be measured [5]. Proper science requires measurement. We can only d etermine whether a given intervention is a catalyst or inhibitor by measuring its impact. Let us examine in more details t he consequences of our proposed deﬁnition. First, the time integrals emphasize that productivity is something that happens over time. The most interesting kind of productivity is that of an organization over the span of years. Measuring over too short a time frame is one of the main sources of t echnical debt [6] as it devalues planning, team work, being strategic, etc. Secondly, as Drucker [1] reminds u s, quality is at least as import ant as quantity. Here we use a proxy for quality, namely user satisfaction. It is important to note that unreleased prod ucts and unreleased features induce no user satisfaction. A broken prod uct might be even worse, and produce negative satisfaction. The input 𝐻 is the number of hours worked by the team, including managers and support staﬀ, as appropriate. To o ptimize productivity, we want to make 𝐼 , and thus 𝐻 , small. This is the raw input being applied, whether eﬀect ive or not. We use user satisfaction (𝑆) as a proxy for eﬀective quality. How to measure this is left for future study. It can be approximated by measures such as numbers of users, number of citations, number of forks of a repository, number of “stars”, surveys of existing users, number of mentions in the issue tracker, and usability experiments. Probably the trickiest part is eﬀective knowledge (𝐾). The idea is that while source code embodies operational knowledge that has the potential to directly lead to user satisfaction, a p roject usually also generates a lot of tacit knowledge about design, including the rationale for various choices. This is the kind of knowledge that is lost when employees leave, and is the most co stly to build and replace. In other words, human-reusable knowledge such as documentation factors in here. The best measure for knowledge is an area for future exploration. Software development typically produces many artifacts, such as requirements, speciﬁcations, user manuals, unit tests, system tests, usability tests, build scripts, API (Ap plication Programming Interface) documentation, READMEs, license d ocuments, process documents, and code. We regard all of these as containing knowledge, albeit encoded in diﬀerent forms. Furt hermore, it is cr ucial to recognize that the knowledge of a single product is distributed amongst those artifacts. In particular, the various artifacts contain many copies of the same core knowledge — by design. To understand the importance of certain artifacts, it makes sense to look at the productivity impact of their presence/absence. For example, long-lived projects will inevitably encounter contributor turnover. How long should it t ake for new contributors to be productive? How much training by peer mentors will it t ake? Could some documentation be written that would shorten this learning perio d and, just as imp ortantly, reduce the time it takes from experienced peo ple? Of course, documentation that is out-of-date could be even worse: a false sense of knowledge that results in even more wasted work that needs repairing. As we gain understanding on measures of value, we can use them to evaluate the state of practice in diﬀerent research software domains. We can estimate the knowledge 𝐾 embedded in, and the user value 𝑆 derived from, existing artifacts. In particular, we can compare these to the artifacts produced by recommended processes from standard so ftware engineering textbooks. For example, we can test the hypothesis that knowledge duplication between code and requirements, coupled with the fact that requirements get de-synchronized from the code and the tenuous link to user value, is the likely reason for low adoption of requirements in scientiﬁc software development [2]. Nevertheless, documentation remains useful, especially for the very long term. Another means to judge the utility of documentation is to loo k at assurance cases. An assurance case [8] presents an organized and explicit argument for correctness (or whatever other software q uality is deemed important) through a series of sub-arguments and evidence. Assurance cases gives at least one measure of which documentation is relevant and necessary. One way to improve productivity is to waste less on non-productive or counter-productive activities. That code is the most visible artifact that contributes user-value, along with with testing (because quality is an extremely important factor in user-value) explains the inordinate focus on just those artifacts. Furthermore, the deemphasis on documentation, even to the extreme of some metho dologies having none, can feel like p roductivity improvements in the short term! A better approach would be to capture knowledge in ways that keeps it continuously synchronized between the various artifacts where it appears. One promising approach is to generate all artifacts from a single knowledge base [10]. This relies on a solid understanding of the contents of all of the artifacts present in t he software engineering process. Our proof-of-concept shows that this is possible. As the artifacts are now generated, knowledge duplication is not a problem. Even better, the knowledge is synchronized-by-construction. Furthermore, it b ecomes easy to tailor artifacts, documentation as well as code, to diﬀerent classes of “users”. Our positio n is that decisions on processes, tools, techniques and software artifacts should be driven by science, not by personal preference. Decisions should not be based on anecdotal evidence, gut instinct or the path of least resistance. Moreover, decisions should vary depending on the users and the context. In most cases of interest, this means that a longer term view should be adopted. We need to use a scientiﬁc approach b ased on unambiguous deﬁnitions, empirical evidence, hypothesis testing and rigorous p rocesses. By developing an understanding of where input hours are spent, what most contributes to user satisfaction, and how to leverage knowledge produced, we can determine what has the greatest return o n investment. We will be able to recommend software production processes that justify their value because the long-term output beneﬁts are high compared to the required input resources.