In sequential recommender system applications, it is important to develop models that can capture users’ evolving interest over time to successfully recommend future items that they are likely to interact with. For users with long histories, typical models based on recurrent neural networks tend to forget important items in the distant past. Recent works have shown that storing a small sketch of past items can improve sequential recommendation tasks. However, these works all rely on static sketching policies, i.e., heuristics to select items to keep in the sketch, which are not necessarily optimal and cannot improve over time with more training data. In this paper, we propose a differentiable policy for sketching (DiPS), a framework that learns a data-driven sketching policy in an end-to-end manner together with the recommender system model to explicitly maximize recommendation quality in the future. We also propose an approximate estimator of the gradient for optimizing the sketching algorithm parameters that is computationally efﬁcient. We verify the effectiveness of DiPS on real-world datasets under various practical settings and show that it requires up to50%fewer sketch items to reach the same predictive quality than existing sketching policies. Recommender Systems (RSs) have seen great success in matching users with items they are interested in when largescale user-item interaction datasets are available. Early approaches in RS assume that a user’s interest is static over time and uses a RS model to compute their latent interest state from historical interactions and predict their ratings on future items (He et al. 2017; Xue et al. 2017). Sequential RS (SRS) is an emerging research topic on how to effectively capture user preference changes over time. The general idea is to keep track of a user’s latent interest state over time, e.g., using a recurrent neural network (Wu et al. 2017; Chung et al. 2014; Belletti, Chen, and Chi 2019). Since recurrent neural networks are prone to forget interactions in the distant past, in many practical scenarios, sequential RS approaches prioritize on making decisions according to a user’s recent interactions in the current session (Hidasi et al. 2015; Hidasi and Karatzoglou 2018). However, these approaches are not adept at capturing a user’s static interests from their history, Copyright©2022, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. which can also be critical to future recommendations. Recently, (Wang et al. 2018; Guo et al. 2019) found that storing a small set of historical items is highly beneﬁcial in sessionbased SRS. Therefore, the sketching policy, i.e., how to select which items to keep in the sketch, is key to the effectiveness of SRS approaches. The sketching policy plays an important role in real-world SRS applications to reduce memory consumption and has been extensively studied in the context of problems such as moment ﬁnding, k-minimum value, and distinct element counting with probabilistic guarantees (Cormode 2011). Various data structures can be used for sketching (Cormode 2011); in this paper, we will focus on sample-based sketching as used in previous RS literature (Wang et al. 2018; Guo et al. 2019) where each sketch item is a past user-item interaction. This setting is related to problems such as data summarization and coreset construction where inﬂuence score and item hardness are often used to select representative samples (Aljundi, Kelchtermans, and Tuytelaars 2019; Koh and Liang 2017). Another related problem is active learning where the label uncertainty of future items is often used to select the next item to query (Settles 2011). A common theme for all these methods is that they deﬁne a measure of informativeness to select the most informative item(s). The signature of RS applications is that items come in a streaming fashion (Chang et al. 2017); thus, at each time step, we observe a single item, decide whether to store it in the sketch, and decide which item to remove from the sketch if the current item is stored. This streaming nature is in stark contrast to the aforementioned problems where one often have access to the full set of items available to select from. This “one-pass” sketching setup is more challenging than the sketching setup in other problems. In practice, simple sketching strategies such as uniform reservoir sampling are often adopted in RS applications (Guo et al. 2019; Wang et al. 2018). These approaches usually keep a reservoir of random historical items to compute the gradient required for model updates. Although static sketching policies often work well, two major limitations hinder their further development. First, these methods deﬁne a heuristic informativeness measure to select items to add or remove from the sketch that is not optimized on the real objective of RS: predictive quality of the user’s interaction with future items. Second, these informativeness metrics are static and cannot exploit abundant information that can be extracted from larger and larger training datasets that are made available in recent years. Recently, there have been approaches for differentiable sample selection policy learning in data streams in other application domains (Ghosh and Lan 2021); however, these approaches are only applicable when the prediction objective is computed on a static set of known items. In contrast, in streaming RS sketching, items included in the prediction task are constantly changing. Contributions.In this paper, we proposeDiPS, a DifferentiablePolicy forSketching framework to learn a sketching policy that optimizes the performance on the ﬁnal recommendation tasks in SRS. The sketching policy is learned in an end-to-end manner together with the base RS model; at each time step, the policy takes the past sketch (of Kitems) and the recent item (or items) and produces a new sketch (ofKitems) for use in subsequent time steps. We make three key contributions: First, we formulate the sketch update and recommendation tasks as a bi-level optimization problem (Franceschi et al. 2018) with a learnable sketching policy. In the outer-level optimization problem, we learn both the base RS model and the sketching policy by explicitly maximizing predictive quality on future recommendations. In the inner-level optimization problem, we adapt the base RS model for the user using the items in the current sketch. The sketching policy is learned in a fully differentiable manner with the sole objective of maximizing performance on future time steps. Second, we propose an approximate estimator of the true gradient of the sketching policy parameters using a separate queue module that is computationally efﬁcient. Since at any time step, the sketch is dependent on all the prior decisions made by the sketching policy, we need to back-propagate gradient to all the previous time steps. This gradient computation requires a re-computation of the entire sketching process from the start until the current step using the current policy parameters, which is computationally intensive. Instead, we show that our approximation effectively alleviates these cumbersome computations. Third, we verify the effectiveness of DiPS through extensive experiments on ﬁve real-world datasets. We observe that the learned sketching policy outperforms existing sketching policies using static informativeness metrics on future recommendation and prediction tasks, requiring up to50%fewer sketch items to reach the same predictive quality. Our implementation will be publicly available at https://github.com/arghosh/DiPS. We now detail the DiPS method. We will start with notations and the generic sketching problem setup, followed by details on base RS models, the sketching policy, and how to efﬁciently learn the sketching policy. Notation and Problem Setup We use the notation[N]to denote the set{1, · · · , N }and use shorthand notationxfor the set{x, · · · , x}. There are a total ofNusers, indexed byi ∈ [N ]andMitems, indexed byj ∈ [M ]. For notation simplicity, we will only discuss the sketching process for a single user, with a total ofTdiscrete time steps, i.e., interactions, indexed by t. We consider two commonly studied RS settings. In the explicitRS setting, for a user, the sequence of interactions is denoted as[(x, r), · · · , (x, r)]; each element in the sequence is an item-rating pair denoted as a tuple, where x∈ [M]is thetitem they interacted with andris the rating they gave to the itemx. The sketching policy keeps a sketch ofKpairs. Therefore, the sketch at timet is denoted asS= {(x, r), · · · (x, r)}where x∈ xandk ∈ [K]; for the ﬁrstKtime steps, the sketchScontains all the past history. Our goal is to predict their (real/binary/categorical-valued) rating on the item they interact with at the next time step,r, using the sketchS, given that we know which item they interact with next. In the implicitRS setting, for a user, the sequence of interactions is denoted as[x, · · · , x]wherex∈ [M ]is the item they interacted with at timet. There are no explicit ratings; items the user interacts with are considered positively rated while items that the user does not interact with are considered to be negatively rated. The sketching policy keeps a sketch ofKitems. Therefore, the sketch at timetis denoted as S= {x, · · · x}wherex∈ xandk ∈ [K]. Our goal is to predict the item that they interact with next,x, out of the entire set of items [M], using the sketch S. We consider two sketching policy updating settings depending on how frequently the sketchSis updated. In the online setting, we update the sketch at each time step. Specifically, given the sketch at the last time step,Sand the current interaction/item(x, r)(orx) for the explicit (or implicit) case at timet, the sketching policy decides whether to include the current item in the sketch; if so, it also decides which item to remove fromSto keep the size of the sketch ﬁxed, arriving at the sketch for the current time step,S. We useˆS= S∪ {(x, r)}(or{x}) to denote the intermediate sketch ofK + 1items and the sketching policy decides which single item to remove fromˆS to get the new sketchS. In thebatch setting, we update the sketch once everyτtime steps; settingτ = 1results in the online setting. Our method is equally applicable to the case of varying update time periodsτ, τ, · · ·; for notation simplicity, we will only detail the case of a ﬁxed time periodτin this paper. Speciﬁcally, given the sketch at the last time step,S, and a batch of current interactions/items [(x, r), · · · , (x, r)](or[x, · · · , x]) for the explicit (or implicit) case at timet + τ, the sketching policy decides whether to include the current items in the sketch and in that case which items to remove, arriving at the sketch for the current time stepS. Similarly, we useˆS= S∪ {(x, r), · · · , (x, r)} (or{x, · · · , x}) to denote the intermediate sketch of K + τitems and the sketching policy decides whichτitems to remove fromˆSto get the new sketch S. We solve the following bilevel optimization problem (for one user only for notation simplicity) (Franceschi et al. 2018): minimize`(r,g(x;θ(Θ,Φ))),`(θ) (1) s.t. θ=argmin`(r,g(x; θ))+R(θ;Θ), L(S; θ) where S= π(S, (x, r Here,ΘandΦare the global RS model and sketching policy parameters, respectively.g(·)is the RS model that takes an itemxas input and predict its explicit or implicit rating (which we denote asr= 1).π(·)is the sketching policy that takes as input the sketch at the last time step,S, the current itemsx, and outputs the updated sketchS. The outer-level optimization problem minimizes the loss, `(r, g(x; θ))across all users and all time steps to learn both the global RS model and the sketching policy. The inner-level optimization problem minimizesL(S; θ), the loss on the sketch for each user at each time step, to adapt the global RS model locally, resulting in a user, time step-speciﬁc parameterθ.R(θ; Θ)is a regularization term that penalizes large deviations of the local parameters from global values. Note thatθis a function of the global parametersΘandΦ, reﬂected through both the regularization term in (2) and the items the sketching policy selects for the user in (3). Recommender System Model Since our focus in this paper is differentiable sketching policy learning, which is agnostic to the underlying base RS model, we adopt a standard neural collaborative ﬁltering (NCF) model as the base RS model (He et al. 2017) since NCF works well with gradient-based optimization; we use NCF to compute the loss`(r, g(x; θ))in both the inner and outer optimization problems. We emphasize that our approach is model agnostic and equally applicable to any differentiable RS model; in the experiments, we also use case studies to show that a learned sketching policy under one RS model is still highly effective for another RS model. The prediction model parameterΘcontains the embedding of a userΘ(u)and a neural network with parameterΘ(p) corresponding to the parameters of the items. For simplicity, we will useΘto denote{Θ(u), Θ(p)}. For the explicit RS setting, given the local parameterθand the next itemx, we predict the ratingrasg(x; θ). For real-valued ratings, we deﬁne a Gaussian likelihood function and use the meansquared error loss`; for binary (or categorical) ratings, we deﬁne a logistic (softmax) likelihood function resulting in the binary (or categorical) cross-entropy loss`(or`). For the implicit RS setting, we predict the next itemxasg(x; θ) among all the items[M]. We deﬁne a softmax function over allMitems, resulting in a categorical cross-entropy loss. The number of items is often large; therefore, several alternative loss functions such as the bayesian personalized ranking loss or the Top1 loss, together with negative sampling, are often used instead (Rendle et al. 2012; Hidasi et al. 2015). We emphasize that our method is agnostic to the loss function; for simplicity, we use the standard categorical cross-entropy `loss in our experiments. Sketching Policy We use a sparse vectorz∈ {0, 1}to represent the indices of each item in the sketchSat timet, withz= 1if and only if item indexjis present in the current sketch. This vector has a one-to-one correspondence with the sketch S. We also use the vectory = [r, · · · , r] ∈ Rto represent the user’s ratings of all items. These ratings are realvalued under the explicit RS setting and binary-valued under the implicit RS setting.The ratings on the non-interacted items do not need to be deﬁned; the DiPS algorithm masks ratings on these items. In the online (τ = 1) and batch update settings, using the intermediate sketch we deﬁned above, at time t + τ, we have wheree∈ {0, 1}represents the unit vector with a1only at index xand 0 at all other indices. The sketching policyπonly has access to items in the intermediate sketch. Therefore, we can represent this rating information using the vectorˆz y ∈ Rwheredenotes element-wise multiplication. The policyπ(ˆz, y; Φ) updates the intermediate sketchˆStoS. In particular, it outputs a sparse vectorw∈ {0, 1}that indicates whether each item in the sketch should be kept or removed. In the online setting, the policy outputs the item index to remove, w∈ {0, 1}∩ ∆, where∆is the probability simplex. In the batch setting, the policy outputs theKitem indices to keep, w∈ {0, 1}∩ {w : 1w = K}. The sketching policyπcomputes a score for each item that is in the intermediate sketchˆzusing a neural networkf(·)with the observed ratings asf(ˆz, y; Φ) = f(ˆz y; Φ). In the online setting, we use the softmax distributionσ(f(ˆz y; Φ))to select the item to remove, w(σ(f(ˆz y; Φ))). We can do this either in a deterministic way by selecting the item with the highest score or in a stochastic way by sampling from the softmax probability distribution. The item indices included in the updated sketch Sare then computed as In the batch setting, we need to selectKitems fromK + τ items. We employ the Top-K projection layer (Amos, Koltun, and Kolter 2019) deﬁned as µ(f(ˆz y; Φ)) =argmin−f(ˆzy; Φ)u−H(u) whereH(u)is the binary cross entropy function and f(ˆz y; Φ)is the score for theMitems. Similarly, we Our framework allows multiple interactions with the same item; for notation simplicity, we detail our method in the case where a user interacts with each item at most once. Figure 1: Top/bottom: true/approximate gradient computation at time stept. The approximate gradient calculated using intermediate sketches, obtained from past parametersΦ, is close to the true gradient when the learning rate is small. can sample theKpoints to keep,w(µ(f(ˆz, y; Φ))), in either a deterministic way or a stochastic way. The sketch at time t + τ is given by In both cases, the sketching policy output is only deﬁned over items in the intermediate sketch. This constraint can be satisﬁed by addinglogˆzas input to the ﬁnal softmax or Top-K projection layer of the sketching policy network. Optimization At the inner-level, we adapt the user parameterθfrom the global parameterΘusing the sketched itemsSat each time step. In practice, we keep item-speciﬁc neural network parametersΘ(p)ﬁxed and adapt only the user embeddingΘ(u) to minimize the loss on theKitems in the sketch. Following the model agnostic meta learning approach (Finn, Abbeel, and Levine 2017), we setθ(u), θ(p) ← Θ(u), Θ(p)and take a ﬁxed number of gradient descent (GD) steps as A ﬁxed number GD steps in Eq. 7 is equivalent to implicit regularization (Grant et al. 2018); thus, we do not impose any explicit regularization in the inner optimization problem. Sinceθis a function ofΘ, computing the gradient w.r.t.Θ in the outer optimization objective (1) requires us to compute the gradient w.r.t. the gradient in (7), i.e., the meta gradient, which can be computed using automatic differentiation (Paszke et al. 2017). Similarly, to learn the sketching policy parametersΦ, we need to compute the gradient of the outer optimization objective w.r.t.Φthrough the user parametersθ(Θ, Φ)in (2). However, the discrete item indices to remove from the intermediate sketch are non-differentiable. Therefore, we need to develop a method to approximate this gradient, which we detail next. Sketching Policy Optimization.The inner-level optimization in (2) usesz, the vector version of the sketchS, to compute the inner-level loss, which is used to adapt the user speciﬁc parameterθ. This loss is computed on all items, regardless of whether they are part of the sketch, and multiplied with the weight vectorzbefore taking gradient steps. Therefore, we can still compute the gradient w.r.t. to the weight of all the itemseven if their corresponding weight is zero. We start with the online setting and denote the outer optimization objective at timet + 1as`(θ). Thus, we need to compute. Note that`is a function ofθ, which is a function of Φ (from (2) and (3)) as θ= argminz(Φ)`(r, g(j, θ)) + R(θ; Θ). (8) We also note that the sketch item indiceszat timetis a function of{z, · · · , z}which are themselves a function ofΦ. We can write the total derivative ofzw.r.t.Φin terms of the partial derivative as where the partial derivativesw.r.t.Φare computed by keeping the inputˆzconstant. The main challenge here is that in order to compute the gradient for the loss on itemx, we need to re-generate the computation graph fromztoz, i.e., the entire sketching history, using the current policy parameterΦ(at time stept), which cannot be computed in previous time steps with past policy parameters (with multiple SGD steps in between). This regeneration is often infeasible due to its high computational cost. An alternative is to run the sketching with the current policy parameter and solve the inner optimization for each time step at once; however, that leads to enormous memory requirements for the backward gradient propagation even for a few time steps. We propose to approximate the total derivative in (9) without recomputing the entire sketching process at every time step. For each iteration, we take a mini-batch of users (with multiple time steps for multiple interactions) for stochastic gradient descent (SGD) optimization. We represent the policy parameterΦat timetasΦin the training iteration. Note that in (9), every pastz(andˆz) correspond to the sketch indices obtained using the current parameterΦ. However, we can use a queueMstoring the intermediate sketch indicesM = [ˆz, · · · ,ˆz]computed from old policy parameters,Φ, Φ, · · · , Φrespectively. If the learning rate is small enough in the SGD steps, we can assume that the past sketches stored in queue, which were computed from the old sketching policy parameters, to be close to that computed from the new parametersΦ. At time stept, we can then run the sketching policy with the current parameterΦon the stored intermediate sketch indicesMin parallel to obtain zand compute|efﬁciently. We can approximate the Jacobianwith the identity matrix since they are additive in (4), which does not need to be explicitly generated in (9). We can compute the gradientv =and obtain the vector-Jacobian productefﬁciently as wherevis ﬁxed and onlyz’s are a function ofΦfor computing the partial derivatives. We note that in (10), there is no sequential dependency; all the terms can be computed in parallel. We use a ﬁxed-size queueM(with sizeQ ∼ 50 − 100) where we remove the oldest sketch indices when the queue gets full. We update the queue after everyτtime steps. This approximate gradient computation process is visualized in Figure 1. In the supplementary material, we show that this approximate gradient remains close to the true gradient. Since the sketch item indicesz(ˆz, y, Φ)are sampled from the Softmax or Top-K projection layer, they are not differentiable w.r.t the policy parametersΦ. Thus, we need to approximate the partial derivative, which we can re-write in the online setting as whereσis the softmax layer andwcontains the sketch indices after sampling in (4). We need to approximate since they are non-differentable; we can leverage the approximationw≈ σ(f(·))since it holds if the item to be removed has almost all the probability mass. This approximation is known as the straight-through (ST) estimator and it is often found to have lower empirical variance than the REINFORCE gradient estimator (Williams 1992; Bengio, Léonard, and Courville 2013). In general, one can test other differentiable approximations, such as ST-Gumbel softmax estimator (Jang, Gu, and Poole 2017), for the sampling operation in (4) and (6), which we leave for future work. The ﬁnal termcan be easily computed as the gradient of the softmax layer w.r.t. the policy parameters. In the batch setting, the softmax layer (σ) is replaced by the Top-K projection layer (µ) that scores topKitems close to1and other items close to0. We can approximate the second term asw≈ µ(f (·))when the topKitems that are selected have the highest scores among all items. We can further use the KKT conditions and the implicit function theorem to compute the gradient; for details, refer to (Lee et al. 2019; Amos, Koltun, and Kolter 2019). Connection to Inﬂuence Function.In (8), we can compute the gradient ofθw.r.t.z,∀j ∈ xusing the implicit function theorem (Cook and Weisberg 1982) as dθ=−(∇L(S; θ))∇`(r, g(j, θ))|. (11) The gradient for loss on the next rating prediction is given by d`= −(∇`(r; g(x; θ)))(∇L(S; θ)) whereI(j), the inﬂuence function (Koh and Liang 2017) score of itemj, computes the change in the loss on the next time step under small perturbations in the weight of this item, zin (8). Intuitively, we would want to keep items that have gradients similar to that for the future items in sketch, i.e., those that are the most informative of future recommendations. Therefore, in the online setting, the sketching policy Algorithm 1: Training of DiPS 1:Initialize global parametersΘ, Φ, learning ratesη(outer level), α (inner level) sketch size K, queue size Q. 2: while not converged do 4:For each user, initialize empty queue of past sketch indicesM ← φ, sketchSand sketch indicesz∈ {0, 1}, encode ratings into vector y ∈ R. 7:Compute loss`on item(x, r)usingθ. 12:Compute∇`using Eq.10 and updateΦ: 17: end while will tend to select items that are the least informative (to replace from the sketch) and in the batch setting, it will tend to select items that are most informative (to keep in the sketch). Datasets and Evaluation Metric.We use ﬁve publicly available benchmark datasets: the Movielens 1Mand 10M datasets (Harper and Konstan 2015) and the Netﬂix Prize datasetfor explicit RSs and the Amazon Book and Foursquaredatasets for implicit RSs. The Movielens datasets contain at least 20 ratings for each user; for the Netﬂix dataset, we ﬁlter out users with less than 20 ratings. We use 20-core settings for the Foursquare and the Amazon book dataset; thus, all users (items) interact with at least 20 items (users). For Amazon Book dataset, we keep reviews with ratings more than 3.5 (from 1-5 scale) as the implicit positively rated items (He and McAuley 2016). The foursquare dataset contains global user check-in datasets on the Foursquare platform from Apr. 2012 to Jan. 2014 (Yang et al. 2019). https://grouplens.org/datasets/movielens/1m/ https://grouplens.org/datasets/movielens/10m/ https://www.kaggle.com/netﬂix-inc/netﬂix-prize-data,https: //www.netﬂixprize.com/ https://jmcauley.ucsd.edu/data/amazon/ https://sites.google.com/site/yangdingqi/home/foursquaredataset Figure 2: On implicit RS datasets, (a) Recall@20 with a GRU4Rec RS model using sketching policies learned by DiPS (with NCF as the base RS model) and (b) Recall@20 with a session-based GRU4Rec model augmented withK historical sketch items. See Table 1 for detailed statistics. For explicit RSs, we use root mean square error (RMSE) as the evaluation metric. For implicit RSs, we use Recall@20 (= E) as the evaluation metric where rank is computed among all possible items; we also provide additional results with Mean Reciprocal Rank (MRR)@20 as the evaluation metric where MRR@K =E. We randomly split60-20-20%of the users in the datasets into training-validation-testing sets. We run all experiments ﬁve times with different splits and report the average and standard deviation (std) numbers across all ﬁve runs. Methods and Baselines.We compare our method,DiPS, against various baselines including reservoir sampling (Vitter 1985) that has been primarily used in RS applications, which keeps items with uniform probability (Guo et al. 2019; Wang et al. 2018). We dub this heuristic sketching policy asRandom. There are several other heuristic sketching policies in streaming settings used in various applications. TheHardest sample heuristic keeps the hardest data point to classify in the sketch and has been highly successful in continual learning and active learning tasks (Aljundi, Kelchtermans, and Tuytelaars 2019). For binary classiﬁcation, it is equivalent to uncertainty sampling. Another closely related method is to construct a coreset in online and batch RS settings. We use anInﬂuencefunction-based score to construct the sketch by selecting the most representativeKdata points from the K + τintermediate sketch items (Borsos, Mutn`y, and Krause 2020). In contrast, our bi-level optimization framework explicitly minimizes the loss incurred on predicting future items. We also experiment with a simpler version of our method, dubbed asDiPS@1, where we do not keep the queue of past intermediate sketches and ﬂow gradient only for the current items, i.e., the ﬁrst term on the right-hand side of (10). We test different sketch sizes asK ∈ {2, 4, 8}. For the batch setting, we set the sketch update period asτ = 4to cover three cases: the update period is less than, equal to, or larger than the sketch size. Model details and parameter settings can be found in the supplementary material. In Table 2, we list the mean RMSE and std numbers across all runs for all methods on all explicit RS datasets under the online setting(τ = 1). On all datasets, for all values of the sketch sizeK, DiPS signiﬁcantly outperforms other methods, followed by DiPS@1. On all datasets, DiPS reaches similar predictive quality to that of static informativeness-based policies using up to50%fewer sketch items. DiPS@1 does not perform as well as DiPS, which suggests that storing past sketch steps in queueMfor more than one time step is beneﬁcial to obtaining a more accurate gradient approximation and better predictive quality. We also observe that reservoir sampling slightly outperforms the Hardest and Inﬂuence heuristics on the Movielens1M and Netﬂix datasets while the Hardest heuristic slightly outperforms the other two on the Movielens10M dataset. Somewhat surprisingly, reservoir sampling performs well in many cases, without using any informativeness measures. We postulate that the reason behind this observation is that the Hardest and Inﬂuence heuristics operate locally since they make decisions only in the context of the local sketch; this restriction means that they favor items that are more informative to the most recent user interactions over those that are representative of longer-term history. Re-weighting these heuristics based on time difference can be beneﬁcial and is left for future work. In Table 2, we list the mean RMSE across all runs for all methods on all explicit RS datasets under the batch setting (τ = 4). On all datasets, DiPS and DiPS@1 signiﬁcantly outperform other informativeness-based policies. On the smaller datasets (Movielens 1M), DiPS@1 slightly outperforms DiPS for larger sketch sizesK ∈ {4, 8}. We postulate that the reason behind this observation is that the policy is not frequently updated (only once everyτ = 4time steps), which reduces the beneﬁt of more accurate gradient approximation by keeping the past sketches. Moreover, the fact that Movielens 1M is signiﬁcantly smaller than the other two datasets might also contribute to this observation. In Table 3, we list the mean Recall@20 metric across all runs for all the methods on all implicit RS datasets under the online setting(τ = 1). On all datasets, DiPS and DiPS@1 signiﬁcantly outperform other informativeness-based policies. On the Book dataset, DiPS with the smallest sketch size ofK =2outperforms static informativeness-based policies with the largest sketch sizeK = 8by at least30%. On the foursquare dataset, DiPS reaches similar predictive quality to that of static policies using up to50%fewer sketch items. We observe that DiPS@1 slightly outperforms DiPS on the smaller Book dataset. Combined with a similar observation in the explicit RS case, this observation suggests that storing past sketches is more beneﬁcial on the larger datasets. We also observe that the Inﬂuence policy works better than other static policies on implicit RS datasets. This observation suggests that recent context might be more important under the implicit RS setting. In Table 3, we list the mean Recall@20 metric across all runs for all the methods on all implicit datasets under the batch setting(τ = 4). On all datasets, DiPS signiﬁcantly outperforms other informativeness-based policies while DiPS@1 slightly outperforms DiPS on the smaller Book dataset. These observations fall in line with those in the online setting. In Table 4, we list the mean and standard deviation of MRR@20 scores for all methods on all implicit datasets under the online setting (τ = 1) and batch Table 2: Mean and std RMSE for all methods under the online (τ = 1) and batch setting (τ = 4) on all explicit RS datasets. Online (1)8 0.0845±0.0005 0.0876±0.0008 0.0877±0.0005 0.1325± 0.0007 0.1275±0.00072 0.1329±0.0002 0.1294±0.0003 0.1316±0.0003 0.1396±0.0001 0.1406± 0.0003 Batch (4)8 0.0714±0.0005 0.0631±0.0006 0.0844±0.0006 0.1046± 0.0006 0.1011±0.0006 Table 3: Mean and std Recall@20 for all methods under the online ( setting (τ = 4). We observe similar trends for the MRR@20 metric as the Recall@20 metric. Policy Transfer.We perform additional experiments to show that the sketching policy learned using one base RS model, NCF in our case, can be effective even if used in conjunction with a different base RS model. In particular, we train a sequential GRU4Rec model (Hidasi et al. 2015) on the Book and foursquare datasets, where at each time stept, all historyxis used to recommend the next item x. We also train three DiPS models (K ∈ {2, 4, 8}) with a base NCF RS model (on the same training set of users) and only retain the learned sketching policiesπ. We test how this GRU4Rec model would perform when we keep a sketch of onlyKitemsxon the test users to recommend the next itemx. In Figure 2(a), we plot the performance of the GRU4Rec model under different values ofKfor both the DiPS policy and the reservoir sampling policy. These policies are identical atK = ∞when the entire history is available. We see that the DiPS policy requires up to50%less τ = 1) and batch setting (τ = 4) on all implicit RS datasets. sketch items to reach the same recommendation quality than reservoir sampling. This observation suggests that sketching policies learned with a particular base RS model can potentially be transferred to other base RS models effectively. We note that the DiPS sketching policy exploits items that are highly predictive of future items, making them amenable to other base RS models. Augmented Session-based SRS.We perform additional experiments to show that a few historical sketch items can effectively augment base RS models to improve sessionbased SRS. In particular, we split the user’s history into non-overlapping sessions of four items. At each step, the model has access to items from the current session (0-3 items) and a sketch ofK ∈ {2, 4, 8}items from the full history. We train a modiﬁed GRU4Rec model that computes hidden states using items from the current session plus the sketch, and concatenate the two hidden states for the ﬁnal prediction layer. We use uniform reservoir sampling and the DiPS sketching policy (trained with NCF) to build the sketch; Online (1)2 0.0342±0.0001 0.0332±0.0001 0.0333±0.0002 0.0365±0.0001 0.0373± 0.0001 Batch (4)8 0.0164±0.0002 0.0149±0.0001 0.0206±0.0002 0.0263± 0.0002 0.0247±0.0002 Table 4: Mean and std MRR@20 for all methods under the online ( Figure 3: Visualization of the DiPS sketching policy (K = 4) on the Book dataset for a selected user over15time steps. Cell(i, j)represents whether itemiis present in the sketch and used to successfully recommend item j. we train a modiﬁed GRU4Rec model on the session items and the sketched items to recommend the next item. In Figure 2(b), we plot the performance of the modiﬁed GRU4Rec model. Note thatK = 0represents the standard GRU4Rec model using only the session data. We see that augmenting historical sketch items improve the performance of the session-based GRU4Rec model by more than20%on both datasets. Moreover, the DiPS sketching policy achieves the same predictive quality as uniform reservoir sampling with 50% fewer sketch items. Policy Visualization.In Figure 3, we plot the sketching process (K = 4) following the policy learned by DiPS for a selected user in the Book dataset for15time steps. We color-coded the columns (time steps) based on successful (blue)/unsuccessful (red) recommendations (from a total of 23, 774distinct books), i.e., whether the actual item is included in the top-20 recommendations. This user is interested in the “Mystery/Suspense” and “Fiction” genres. The third book, “Moon Dance”, is kept in the sketch between time steps4and12and used to successfully predict similar items, such as “Moon Child”. The book “The Girl with the Dragon Tattoo” is kept in the sketching memory and used to successfully predict similar items “The Girl Who Played with Fire’‘ and “The Girl Who Kicked the Hornet’s Nest”. The book τ = 1) and batch setting (τ = 4) on all implicit RS datasets. “The Girl Who Kicked the Hornet’s Nest” is not kept in the sketch, possibly since it is the last book in the original “Millennium” series; its information is already well-captured in the sketch by the ﬁrst book. Although we cannot successfully predict the book “Samantha Moon: The First Four Vampire”, it is kept in the sketch to capture the user’s interest on ﬁction, which is later used to successfully predict “Dead Until Dark” with the same theme. We note that although the model is not able to always successfully recommend items, the sketching policy captures item properties and adds/removes incoming items to improve future recommendations. In this paper, we developed a framework for differentiable sketching policy learning for recommender systems applications. The policy decides which past items to keep in a small sketch to explicitly maximize future predictive quality using items in the sketch. We use a bi-level optimization setup to directly learn such a sketching policy in a data-driven manner. Extensive experimental results on real-world datasets under various recommender systems settings show that our framework can sometimes signiﬁcantly outperform existing static, informativeness-based sketching policies. Although side information (or metadata) often plays an important role in recommender systems, we did not use any side information in this paper. We brieﬂy discuss how to incorporate metadata in the DiPS framework in the supplementary material. Avenues for future work include i) using more sophisticated recommender systems models that take item metadata into account, and ii) using interpretable recommender systems architecture to explicitly interpret how past items in the sketch help us predict future items (Shi et al. 2020). A. Ghosh and A. Lan are partially supported by the National Science Foundation via grants IIS-1917713 and IIS-2118706.