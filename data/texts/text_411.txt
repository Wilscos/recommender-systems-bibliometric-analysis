As the volume of long-form spoken-word content such as podcasts explodes, many platforms desire to present short, meaningful, and logically coherent segments extracted from the full content. Such segments can be consumed by users to sample content before diving in, as well as used by the platform to promote and recommend content. However, little published work is focused on the segmentation of spoken-word content, where the errors (noise) in transcripts generated by automatic speech recognition (ASR) services poses many challenges. Here we build a novel dataset of complete transcriptions of over 400 podcast episodes, in which we label the position of introductions in each episode. These introductions contain information about the episodes’ topics, hosts, and guests, providing a valuable summary of the episode content, as it is created by the authors. We further augment our dataset with word substitutions to increase the amount of available training data. We train three Transformer models based on the pre-trained BERT [ augmentation strategies, which achieve signicantly better performance compared with a static embedding model, showing that it is possible to capture generalized, larger-scale structural information from noisy, loosely-organized speech data. This is further demonstrated through an analysis of the models’ inner architecture. Our methods and dataset can be used to facilitate future work on the structure-based segmentation of spoken-word content. ACM Reference Format: Elise Jing, Kristiana Schneck, Dennis Egan, and Scott A. Waterman. 2021. Identifying Introductions in Podcast Episodes from Automatically Generated Transcripts. 1, 1 (December 2021), 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn As digital spoken-word content, such as podcasts, increases in quantity and availability, content creation and distribution platforms face many challenges related to organizing, understanding, and promoting their content. The large scale of spoken-word content also presents challenges to users who would like to explore the catalogs and choose content to listen to. Many podcasts are long and require focused attention, so users may spend longer in deciding whether to commit to them. Video platforms, including Netix and YouTube, address this issue by editing and producing previews and shorter clips from long-form content. Such previews and clips typically consist of short segments that are meaningful, entertaining, logically coherent, and self-contained, providing a compelling way for users to sample content before committing to watching the full episode or show. Additionally, they can be used to promote content o-platform, and further increase users’ engagement with the content by personalizing recommendations and user interfaces [ creation tools, and are even oered as subscription APIs [ 1,9]. Automatic methods for video segmentation (e.g. [21]) are commonly used in editing and content are currently not available. Methods to automatically identify reusable segments from longer speech content have clear practical applications in summarization, trailers, and highlights and quote extraction. In this work, we focus on the problem of identifying the introductory part of a podcast episode based on automatic speech recognition (ASR) transcriptions. An introduction in a podcast episode typically describe the episode’s main subject(s), contents, and speakers, and is a good representation of the whole episode. Dialectically, the introduction is used to inform and prepare the listener for the performance ahead. Listening to a podcast episode’s introduction can often stimulate listeners’ interest and help them decide whether to listen to the whole episode. Additionally, the introduction can be used in recommendation systems or o-platform promotion to attract users to the content. From a linguistic perspective, the task of identifying introductions is dierent from many common segment extraction tasks in that an introduction is recognizable based on its structural uniqueness within the longer content, rather than topical dierences. While an introduction shares similar vocabulary and topics with the rest of the episode, a human listener can usually recognize the macro-level discourse structure that denes the introduction. In the computational studies of discourse structure, most work is focused on detailed phrasal elements and detection of elementary discourse units (EDUs), with application on specic tasks such as argument detection [16] and dialogue generation [12]. Although recurrent neural network (RNN) and attention models have recently been applied to build discourse-aware models for segmentation and summarization tasks [8,25,30], they also focus on EDUs or small, clausal units, rather than larger-scale units similar to what we tackle here. Mining spoken-word data in the form of ASR transcripts poses many additional challenges. Most natural language processing (NLP) techniques assume well-structured, written text divided into sentences, while ASR transcripts explicitly represent speakers’ disuencies, restarts and other errors. Neither do they have reliable punctuation or paragraphing. ASR transcripts are also aected by word errors caused by out-of-vocabulary terms or failed recognition. Moreover, diculty in speaker detection makes it hard to identify the conversational exchanges. While existing work have addressed the segmentation of ASR data based on topic changes [4,7] and dialogue exchanges [13,19], little work has focused on the structural segmentation of ASR data necessary for identifying these introductory segments. While many corpora have been created for topic-based text segmentation [17], the relatively few corpora for text segmentation based on discourse structure focus on sentence-level connectives [26] or EDUs under the Rhetorical Structure Theory (RST) framework [5], and no dataset is available for the structure-based segmentation of ASR transcripts to the best of our knowledge. We therefore create a new dataset of labeled podcast transcripts for our task. After obtaining transcripts using ASR tools, we recruited lightly-trained volunteers to annotate the introductions, obtaining labeled data for 417 podcast episodes. We explore the annotator agreement, showing that human annotators achieve reasonable agreement on the locations and components of introductions. For the details, see Section 2. Inspired by recent works on text segmentation using neural network models [3,14,22], we formulate our task as a supervised sequence labeling task. We train our models to label each token in the text, and nd the best split position based on the token labels, using ne-tuning over a pre-trained BERT model [10]. To highlight that our models recognize the structures of discourse data, rather than relying only on lexical cues, we compare to a baseline created using GloVe embeddings [18]. We also apply two data augmentation strategies to increase the amount of available training data (see Section 3 for details). Our models are evaluated using two metrics: the accuracy of identifying the segmentation boundaries (accuracy), and the overlap between the predicted introduction and the gold standard (overlap score). Fig. 1. Agreement on podcast episode introductions with annotation from three annotators. The majority of episodes have a perfect or majority agreement, and few have no agreement. We nd that our models outperform the baseline by signicant margins. Compared to the base BERT model, data augmentation improves the accuracy by up to three percentage points, as well as decreasing the variance in model performance. Moreover, while the baseline model performs poorly on data that is structurally dierent from the training data, our models show an ability to generalize (see Section 4). We further analyze the learned hidden representations within our models, demonstrating that they are able to learn structural information in additional to topical or lexical cues. Our methods and dataset can be used to facilitate future work in this domain. 2 DATA Our dataset consists of podcast transcripts collected using Google’s speech-to-text service. Our work focuses on English content, and we use the transcriptions produced by the ASR systems without post-editing or clean-up. In order to create a varied dataset, we collect recent episodes from popular programs across 20 topical categories on our platform for manual labeling. At the time of submission, 417 episodes have been annotated. A group of annotators were recruited to label the dataset. Each annotator listens to a podcast episode while looking at its transcript. Annotators are instructed to identify the episode introduction, which is a short description of a specic episode’s topic, host, guest, or other important subjects discussed in it. Podcasts may also contain program introductions which give on overview of the program as a whole. These are often trivial to identify, as they repeat from episode to episode. We focus on the episode introductions We ask the annotators to label the starting and ending words of each of the episode introductions, or mark “none" if they are not present. We do not provide a detailed guideline of what is required for an introduction, but encourage the annotators to use their own judgement in order to obtain more spontaneous reaction to the data. Since ASR can be prone to errors, an annotator may have trouble identifying or labeling the exact start and end of an introduction. Even if the transcription is perfect, annotators may disagree on what constitutes an introduction. Taking account of these issues, we have a number of episodes labeled by three independent annotators. We examine the annotator agreement on 117 episodes with three annotations. We dene the gold standard to be majority agreement—where at least two out of three annotators agree. If all three annotators agree, we consider it a perfect agreement. Because of the noise in the data, if two labels dier by less than 2 seconds, we consider them as having an agreement. Figure 1 shows the number of perfect and majority agreements for these episodes. For the annotations labeling the start of introductions, 72 out of the 117 episodes have perfect agreement. Forty-one episodes have majority agreement, and only 4 have no agreement. If we only consider episodes with perfect or majority agreement, we obtain Fig. 2. Locations of episode introductions in the transcripts. Each line shows the start, end, and duration of an episode’s introduction by word positions, sorted by start position 113 episodes or 96.6% of the data. Similarly, considering the agreement on the end position, we are left with 110 episodes or 94.0% of the data. For the podcast programs with 100% annotator agreement, we include all episodes in these programs in our dataset even if they have not been labeled by three independent annotators. From the annotated data, we notice that the structures of podcast episodes are not consistent. For example, some episodes have program introductions before episode introductions, and some vice versa. A number of episodes have no introduction at all. Music and advertisements may also appear before or after the introductions. Figure 2 shows the locations of episode introductions. We found these locations to vary widely, with episode introductions starting or ending as late as near 1,000 words into the transcripts. While the formats of podcasts vary, the episodes in a single podcast program usually share similar topic(s) and format(s). For example, all episodes in the B&H Photography Podcastare about photography. All episodes in the Song Exploderprogram share a similar structure: introduction, a song, and then an interview with the creator(s) of the song. Intuitively, it will be much easier for a model to perform well on given episodes if it is trained on other episodes in the same program. We therefore stratify the data, leaving 5% of the programs out and use all episodes in these programs as “test set of unseen programs", and another 5% of all programs as validation set. From the rest of programs, we further keep 10% of their episodes as “test set of seen programs" and another 10% as validation set. The rest of the dataset is used as training set. The number of episodes in the training and test sets are summarized in Table 1. 3 APPROACH We start with learning contextualized vector representations for each token in the transcript by ne-tuning a BERT model. We rst tokenize the documents with BERT’s WordPiece tokenizer. If a document is longer than 512 tokens, we divide it into overlapping spans using a sequence length of 512 with 128 overlapping tokens between spans following the practice in Devlin et al in the same paper. We then train a fully connected layer to assign the probability of each token belonging to one of two classesIs-intro Is-intro dierence algorithm inspired by Salloum et al likely each token is the introduction’s start position by averaging the scores of tokens before and after it: where𝑃 a chosen window size. The token position of the introduction in a similar manner. Additionally, we create a baseline using non-contextual word embeddings. We use GloVe vectors with 6B tokens and 100 dimensions provided by the Stanford NLP group [ classier to predict the detection using the method described above. We use the pre-trained BERT model model for 300 epochs, using the used for learning rate adjustment. The cross entropy function is used as the loss function. Automatic data augmentation techniques have been widely applied to alleviate the lack of labeled data in recent years [11]. Here we experiment with two augmentation strategies that were found to consistently perform well [ rst is random word replacement based on TF–IDF scoring [ token swap, deletion, or crop [ choose to generate 5 augmented samples for each original sample. The Python package the augmentation. 4 RESULTS We begin by showing the overall alignment between each models’ predictions and the ground truth in Figure 3. We rst examine the base BERT model and the BERT models with data augmentation. We found that although all three models have a few misses as the base and word replacement models assign a few tokens with low Is-intro scores within the true introduction, and the random augmentation model assigns high scores to a few tokens before the beginning of the true introduction, they correctly identify blocks of text that mostly align with the ground truth. Meanwhile, our token-based baseline predicts high with consistent high scores, much less a segment that overlaps with the labeled introduction. With a general understanding of the models’ behaviour, we formally evaluate our model’s performance by two metrics inspired by the practice in Rajpurkar et al boundaries. We consider the accuracy with regard to given osets, i.e. how far away the predicted boundary is from orNot-intro. As the predicted probability score falls between 0 and 1, the tokens predicted with high probabilities may be found throughout a document (see Figure 3). We therefore create a simple maximum is the likelihood for a token to be the start position,𝑆is theIs-introscore assigned by our model, and𝑘is Fig. 3. Distribution of the models’ prediction scores. The x-axis shows the index of tokens. Spans are truncated to show details of the introductions beer. Each vertical line is the probability for beingIs-introfor a token. The darker lines indicate higher probabilities, while the gray bars show the raw probability scores. The ground truth boundaries are indicated by triangles. Table 2. Accuracy versus oset on the test sets with seen and unseen programs, where the oset is how many tokens there are between the predicted position and the true position. The average results across 3 dierent runs are shown for the BERT models. 95% confidence intervals are shown in parenthesis. the ground truth, in number of tokens. We consider osets from 0 to 9; with the noisy and swift nature of podcasts, 9 spoken words happen in 2-3 seconds, and is a small margin compared to an entire podcast which can be longer than an hour. The accuracy corresponding to varied oset is shown in Table 2. We found that the BERT with word replacement in general has the best performance on the test set of seen programs, reaching an accuracy over 51% at oset=9 and 25% over the GloVe baseline when predicting the start position. It also has an accuracy over 34% at predicting the end position. Meanwhile, the BERT model with random augmentation performs best on the test set of unseen programs, potentially more resilient to unfamiliar data due to the variation introduced by augmentation. Notably, it also performs better in predicting the end position than predicting the start position, even though the end is harder to identify for other models as well as human readers (c.f. Section 2). Overall, our models are able to generalize into data that is structurally dierent from the training data, while the baseline generalizes poorly. Fig. 4. Distribution of overlap scores. The x-axis shows the overlap score between 0 and 1. Each point is a document in the test sets. (a) shows the test data from previously seen programs, and (b) shows the test data from unseen programs. We also evaluate the performance of our models using the overlap between the predicted introduction and true introduction on a token level. Because good predicted introductions need to not only contain the right tokens, but also be at the correct position, it is not suitable to compute the F1 score using a bag-of-tokens model. We compute the overlap score as: The overlap score distribution are summarized in Figure 4. We found that although all BERT models work well on the test set of seen programs, the word replacement model has a signicantly higher rst quartile. However, on the test set of unseen programs, the random augmentation model fares better while all other models perform poorly, conrming our observation above. We further analyze our models to better understand their learning behaviour. We extract the output of each hidden layer using the base BERT model as an example. Inspired by the method used in van Aken et al a principal component analysis (PCA) on the output token vectors, which have 768 dimensions each. We then plot the tokens using the rst two principal components. In this way, we expect to nd clusters of tokens that the model considers to be closely related. Figure 5 shows the token clustering from output of layer 1 and layer 12. We found that in layer 1, words with similar syntactic functions are close to each other; for example, there is a cluster of verbs on the upper left corner and one of auxiliary verbs on the right. Meanwhile, there is no separation between the Not-intro respectively. This is consistent with the ndings in van Aken et al higher layers in BERT are found to contain dierent types of information. In the lower layers, such as layer 1, the model learns and maintains syntactic representation, while the higher layers focus on task-specic information. We speculate tokens. However, at layer 12, two distinct clusters appear, containing theIs-introandNot-introtokens that such learning phases allow our models the versatility to learn structural information in addition to lexical and topic information. Fig. 5. Token clustering from the base BERT model’s outputs. (a) output of layer 1. (b) output of layer 12. Red markers show the Is-intro tokens, and black show the Not-intro tokens. 5 DISCUSSION In this work, we proposed the task of identifying introductions in podcast episodes as a type of segment that can be used for listeners to sample content as well as for recommendation. We created a novel dataset of annotated ASR transcripts for our task, and developed Transformer-based models that agree well with human judgement in identifying the boundaries of introductions. Our models may be integrated into a pipeline that includes human validation of automatically discovered segments and uses them for downstream tasks. The main challenge in our task is that the structures of the introductions are both not conventional, and greatly varied across dierent podcast genres. Although we did nd that some programs had very regular introduction structure (e.g. Song Exploder), many had a looser, more conversational style, without a standardized structure. Additionally, transcription error is also a factor in our dataset, as is the fundamental dierence between written and speech data. The speech data is generally less organized, less grammatically well-formed, and also error-prone, with restarts, disuencies, over-talking, and the like. We demonstrate that our model handles these challenges well. In particular, our model signicantly out-performs the baseline on the test set of unseen programs, showing an ability to extend to unfamiliar data. This result also suggest that although the denition of an introduction is not crisp, there exists some embedded linguistic knowledge that human readers leverage to recognize structure, which the deep learning models are also able to learn. Our dataset is relatively small in scale due to the limit in labeling resources, and only a subset was labeled by multiple annotators. The noise in ASR data poses additional challenges to the annotators, and not all annotators agree on the positions of introductions even with corrected transcripts. Despite these limitations, our dataset is one of the rst labeled dataset for the structural segmentation of ASR transcripts. Compared to synthetic datasets constructed by concatenating dierent segments, our dataset is more internally coherent and challenging for machine learning models. We believe that our dataset will be a benecial addition for future work in this domain, including better characterization of the narrative structure and stylistic variation of podcasts which would allow us to identify other stable segment types within spoken word programming.