Individualized decision making is an increasingly attractive artiﬁcial intelligence paradigm that proposes to assign each individual a given treatment based on their observed characteristics. In particular, such a paradigm has been recently employed in precision medicine to tailor individualized treatment decision rule. Among all individualized decision rules (IDR), the one that maximizes the expected outcome is referred to as an optimal IDR. There is a huge literature on learning the optimal decision rule. Some popular methods include Q-learning (Watkins and Dayan, 1992; Chakraborty et al., 2010; Qian and Murphy, 2011; Song et al., 2015), A-learning (Robins, 2004; Murphy, 2003; Shi et al., 2018), policy search methods (Zhang et al., 2012, 2013; Wang et al., 2018; Nie et al., 2020), outcome weighted learning (Zhao et al., 2012, 2015; Zhu et al., 2017; Meng et al., 2020), concordance-assisted learning Fan et al. (2017); Liang et al. (2017), decision list-based methods (Zhang et al., 2015, 2018), and direct learning (Qi et al., 2020). We note however, all these methods consider settings where the number of available treatment options is ﬁnite. settings. These studies occur in a number of real applications, including personalized dose ﬁnding (Chen et al., 2016) and dynamic pricing (den Boer and Keskin, 2020). For instance, in personalized dose ﬁnding, one wishes to derive a dose level or dose range for each patient. Due to patients’ heterogeneity in response to doses, it is commonly assumed that there may not exist a uniﬁed best dose for all patients. Thus, one major interest in precision medicine is to develop an IDR that assigns each individual patient with a certain dose level or a speciﬁed range of doses based on their individual personal information, to optimize their health status. Similarly, in dynamic pricing, we aim to identify an IDR that assigns each product an optimal price according to their characteristics to maximize the overall proﬁt. ized decision making with a continuous treatment domain has been less studied. Among those available, Rich et al. (2014) modeled the interactions between the dose level and covariates to recommend personalized dosing strategies. Laber and Zhao (2015) developed a tree-based method to derive the IDR by dividing patients into subgroups and assigning In this paper, we consider individualized decision making in continuous treatment In contrast to developing the optimal IDR under discrete treatment settings, individualeach subgroup the same dose level. Chen et al. (2016) proposed an outcome weighted learning method to directly search the optimal IDR among a restricted class of IDRs. Zhou et al. (2018) proposed a dimension reduction framework to personalized dose ﬁnding that eﬀectively reduces the dimensionality of baseline characteristics from high to a moderate scale. Kallus and Zhou (2018) and Chernozhukov et al. (2019) evaluated and optimized IDRs for continuous treatments by replacing the indicator function in the doubly-robust approach with the kernel function, and by modeling the conditional mean outcome function (i.e., the value) through a semi-parametric form, respectively. Zhu et al. (2020) focused on the class of linear IDRs and proposed to compute an optimal linear IDR by maximizing a kernel-based value estimate. Schulz and Moodie (2020) proposed a doubly robust estimation method for personalized dose ﬁnding. The estimated optimal IDRs computed by these methods typically recommend one single treatment level for each individual, making it hard to implement in practice. (I2DR) that returns a range of treatment levels based on individuals’ baseline information. Compared to the IDRs recommended by the existing works, the proposed I2DR gives more options and is thus more ﬂexible to implement in practice. Take personalized dose ﬁnding as an illustration. First, interval-valued dose levels may be applied to patients of the same characteristics, when arbitrary dose within the given dose interval could achieve the same eﬃcacy. Studies of the pharmacokinetics of vancomycin conducted by Rotschafer et al. (1982) suggested that adults with normal renal function should receive an initial dosage of 6.5 to 8 milligram of vancomycin per kilogram intravenously over 1 hour every 6 to 12 hours. In the review of warfarin dosing reported by Kuruvilla and Gurk-Turner (2001), when the international normalized ratio (INR) approaches the target range or omit dose, they suggested to give 1-2.5 milligram vitamin K1 if a patient has a risk factor for bleeding, otherwise provide Vitamin K1 2-4 milligram orally. Second, a range of doses gives instructions for designing the medicine speciﬁcation and helps to save cost on manufacturing dosage. As such, an I2DR is preferred in these applications due to its necessity and ﬂexibility. making in a continuous treatment domain is a vital problem in many applications such as The focus of this paper is to develop an individualized interval-valued decision rule Our contributions are summarized as follows. Scientiﬁcally, individualized decision precision medicine and dynamic pricing. To the best of our knowledge, this is the ﬁrst work on developing individualized interval-valued decision rules. Our proposal thus ﬁlls a crucial gap, extends the scope of existing methods that focus on recommending IDRs, and oﬀers a useful tool for individualized decision making in a number of applications. alized decision making with multi-scale change point detection (see Niu et al., 2016, for a selective overview). Our proposal makes useful contributions to the two aforementioned areas simultaneously. a continuous treatment domain. Our proposal is motivated by the empirical ﬁnding that the expected outcome can be a piecewise function on the treatment domain in various applications. Speciﬁcally, in dynamic pricing (Qiang and Bayati, 2016; den Boer and Keskin, 2020), the expected demand for a product has jump discontinuities as a function of the charged price. This motivates us to impose a piecewise-function model for the expected outcome. We then leverage ideas from the change point detection literature and propose a jump-penalized regression to estimate the conditional mean of the expected outcome as a function of the treatment level and the baseline characteristics (outcome regression function). This partitions the entire treatment space into several subintervals. The proposed I2DR is a set of decision rules that assign each subject to one of these subintervals. In addition, we further develop a procedure to construct a conﬁdence interval (CI) for the expected outcome under the proposed I2DR and the optimal IDR. either focused on models without covariates, or required the underlying truth to be piecewise constant (see e.g., Boysen et al., 2009; Frick et al., 2014; Fryzlewicz, 2014, and the references therein). Our work goes beyond those cited above in that we consider a more complicated (nonparametric) model with covariates, and allow the underlying outcome regression function to be either a piecewise or continuous function over the treatment space. To approximate the expected outcome as a function of baseline covariates, we propose a linear function model and a deep neural networks model. We refer to the two procedures as L-JIL and D-JIL, respectively. Here, the proposed L-JIL yields a set of linear decision rules that Methodologically, we propose a novel jump interval-learning (JIL) by integrating person- First, to implement personalized decision making, we propose a data-driven I2DR in Second, we note that most works in the multi-scale change point detection literature is easy to interpret. See the real data analysis in Section 6 for details. On the contrary, the proposed D-JIL employs deep learning (LeCun et al., 2015) to model the complicated outcome-covariates relationships that often occur in high-dimensional settings. We remark that both procedures are developed by imposing a piecewise-function model to approximate the outcome-treatment relationship. Yet, they are valid when the expected outcome is a continuous function of the treatment level as well. estimators with linear regression or deep neural networks. Our theoretical approaches can be applied to the analysis of general covariate-based change point models. The model could be either parametric or nonparametric. Speciﬁcally, we establish the almost sure convergence rates of our estimators. When the underlying outcome regression function is a piecewise function of the treatment, we further derive the almost sure convergence rates of the estimated change point locations, and show that with probability 1, the number of change points can be correctly estimated with suﬃciently large sample size. These ﬁndings are nontrivial extensions of classical results derived for models without covariates. For instance, deriving the asymptotic behavior of change point estimators for these models typically relies on the tail inequalities for the partial sum process (see e.g., Frick et al., 2014). However, these technical tools are not directly applicable to our settings where deep learning is adopted to model the outcome regression function. Moreover, we expect our theories to also be of general interest to the line of work on developing theories for deep learning methods (see e.g., Imaizumi and Fukumizu, 2019; Schmidt-Hieber et al., 2020; Farrell et al., 2021). framework, deﬁne the notion of I2DR, and posit our working model assumptions. In Section 3, we propose the jump interval-learning method and discuss its detailed implementation. Statistical properties of the proposed I2DR and the estimator for the mean outcome under the proposed I2DR are presented in Section 4. We further develop a conﬁdence interval for the expected outcome under the estimated I2DR. Simulation studies are conducted in Section 5 to evaluate the ﬁnite sample performance of our proposed method. We apply our method to a real dataset from a warfarin study in Section 6. In Section 7, we provide the Theoretically, we systematically study the statistical properties of the jump-penalized The rest of this paper is organized as follows. In Section 2, we introduce the statistical technical proof for one of our main theorem, followed by a concluding discussion in Section 8. The rest of proofs are provided in the supplementary article. This section is organized as follows. We ﬁrst introduce the model setup in Section 2.1. The deﬁnition of I2DR is formally presented in Section 2.2. In Section 2.3, we posit two working models assumptions for the expected outcome as a function the treatment level. We aim to develop a method that works under both working assumptions. We begin with some notations. Let selected individual in the population from a compact interval. Without loss of generality, suppose support Y ∈ R Let a ∈ have been observed if they were receiving treatment covariate-treatment-outcome triplets i.i.d. copies of ( possibly maximize the expected outcome of future subjects using their baseline information. that maps the covariate space to maximize the expected outcome (value function) The following assumptions guarantee the optimal IDR is identiﬁable from the observed data. (A1.) Stable Unit Treatment Value Assumption (SUTVA): Y = Y (A2.) No unmeasured confounders: {Y (A3.) Positivity: there exists some constant Abelongs to [0,1]. LetX ∈ Xbe that individual’s baseline covariates where the Xis a subset inR. We assume the covariance matrix ofXis positive deﬁnite. Let denote that individual’s associated outcome, the larger the better by convention. p(•|x) denote the probability density function ofAgivenX=x. In addition, for any [0,1], deﬁne the potential outcomeY(a) as the outcome of that individual that would Formally speaking, an individualized decision rule (IDR) is a deterministic functiond(·) a ∈ [0, 1]. In other words, there is no interference eﬀect between individuals. Assumption (A2) requires that the baseline covariates have included enough confounders given which the potential outcomes and the received treatment are independent. (A2) and (A3) automatically hold in randomized studies. These three assumptions are commonly imposed in the literature for estimating an optimal IDR (see e.g., Chen et al., 2016; Zhu et al., 2020; Schulz and Moodie, 2020). Under (A1)-(A3), we have their received treatment and baseline covariates. We refer to this function as the outcome regression function. As a result, the optimal IDR for an individual with covariates given by We have V The focus of this paper is to develop an optimal individualized interval-based decision rule (I2DR). As commented in the introduction, these decision rules are more ﬂexible to implement in practice when compared to single-valued decision rules in personalized dose ﬁnding and dynamic pricing. and outputs an interval agents might assign diﬀerent treatments to patients / products. The actual treatments that subjects receive in the population will have a distribution function Π this paper, we assume Π Apparently, we have hold, the associated value function under an I2DR d(·) equals SUTVA requires that the outcome of each individual depends on their own treatment only. ) =E(Y |X=x, A=a) is the conditional mean of an individual’s outcome given arg maxQ(x, a). LetVdenote the value function under the optimal IDR. We deﬁne an I2DR as a functiond(·) that takes an individual’s covariatesxas input search the optimal I2DR based on the estimated value function. However, such a value search method has the following two limitations. First, a nonparametric estimator of requires to specify the preference function though a nonparametric estimator of eﬃciently compute the I2DR that maximizes the estimated value (see Section 8.2.2 for details). To overcome these limitations, we propose a semiparametric model for the outcome regression function and use a model-assisted approach to derive the optimal I2DR. We formally introduce our method in Section 3. In this section, we introduce two working models for the outcome regression function, corresponding to a piecewise function and a continuous function of the treatment level. for some partition number of intervals in of mutually disjoint intervals ··· < τ expect the above model assumption holds in real-world examples such as dynamic pricing. and x, for any x ∈ X and a ∈ [0, 1]. Model I or Model II holds. In this section, we ﬁrst present the proposed jump interval-learning and its motivation in Section 3.1. We next introduce two concrete proposals, i.e., linear jump interval-learning Given the dataset, one may estimateV(d) nonparametrically for anyd(·) and directly Model II (Continuous Functions).SupposeQ(x, a) is a continuous function ofa We aim to propose an optimal I2DR that optimizes the value function when either and deep jump interval-learning, to detail our methods in Section 3.2. We then present the dynamic programming algorithm to implement jump interval-learning (see Algorithm an overview) in Section 3.3. Finally, we provide more details on tuning parameter selection in Section 3.4. We use Model I to present the motivation of our jump interval-learning. In view of equation 1, any treatment level within an interval The optimal I2DR is then given by independent of the preference function π For any I2DR deﬁnition. It follows that where the inequality is due to that the optimal I2DR, it suﬃces to estimate paper, we denote V to those of the form [ any partition q(X)I(a ∈ I) =Q(X, a), almost surely for anya ∈[0,1]. Therefore, to derive From now on, we focus on a subset of intervals in [0,1]. By interval we always refer point locations, i.e, proposed method yields a partition number of intervals in will contain many short intervals, making the resulting decision rule hard to implement in practice. Yet, a smaller value of method adaptively determines | In the ﬁrst step, we estimate the outcome regression function using jump penalized least squares regression. Then we derive the corresponding I2DR from the resulting estimator bq(·). To begin with, we cut the entire treatment range into m initial intervals: The integer be speciﬁed by the clinical physician such that the output dose interval for each individual is at least of the length mto be proportional to that we recommend. Nor is it equal to looking for a partition these form requirement: the end-points of each interval We associate to each partition for I. We propose to estimate where We next detail our method. Jump interval-learning consists of the following two steps. mintervals. In other words, we will adaptively combine some of these intervals to More speciﬁcally, letB(m) denote the set of partitionsPthat satisfy the following Qas some class of functions, whereθis the underlying parameter associated to interval arg minI(A∈ I)Y− q(X; θ)+ λ|I|kθk+ γ|P|,(3) λandγare some nonnegative regularization parameters speciﬁed in Section 3.4, and kdenote the Euclidean norm of the model parameterθ. The purpose of introducing the The purpose of introducing the jumps. When the above optimization corresponds to the jump-penalized least square estimator proposed by Boysen et al. (2009). We refer to this step as jump interval-learning (JIL). outcome regression functions regression and can be solved via existing statistical or machine learning approaches. We provide two concrete study cases below, based on linear regression and deep learning. These estimated outcome regression functions can be viewed as functions of adaptively determined by minimizing the penalized least square function in equation 3. When the argmax in equation 4 is not unique, smallest treatment. each interval score function Following the estimation strategy in Zhang et al. (2012), we propose the following value estimator under equation 4, 4. Although we use the example of piecewise functions to motivate our procedure, the proposed method allows the outcome regression function to be a continuous function of and x as well. See Section 4 for detail. `-type penalty termλ|I|kθkis to help to prevent overﬁtting in largepproblems. For a ﬁxedP, solving the optimization function in equation 3 yields its associated To maximize the expected outcome of interest, our proposed I2DR is then given by We next evaluate the value function under the proposed I2DRV(bd) andV(d). For Statistical properties of the estimates in equation 4 and equation 5 are studied in Section In practice, we consider two concrete proposals to implementing jump interval-learning, by considering a linear function class and a deep neural networks class for Q We use a linear regression model for interval assumption, we have I2DR corresponds to a linear decision rule, i.e., linearity assumption ensures our I2DR is interpretable to the domain experts. equation 3 is reduced to where The ridge penalty parameter estimate in high-dimensional settings. We next consider using deep neural networks (DNNs) to approximate the outcome regression function, so as to capture the complex dependence between the outcome and covariates. Speciﬁcally, the network consists of to the covariates layers. Each unit in the hidden layer is determined as a nonlinear transformation of a linear combination of the nodes from the previous layer. The total number of parameters in the network is denoted by Iandx ∈ X, where¯xis a shorthand for the vector (1, x). Adopting the linearity We next discuss how to computebPand{bθ:I ∈bP}. The objective function in X= (1, X). We refer to this step as linear jump interval-learning (L-JIL). I(A∈ I)XXis not invertible. It also prevents over-ﬁtting and yields more accurate solved using a stochastic gradient descent algorithm. In our implementation, we apply the Multi-layer Perceptron (MLP) regressor Pedregosa et al. (2011) for parameter estimation. We refer to the resulting optimization as deep jump interval-learning (D-JIL). takes the covariate-treatment pair ( function. However, the resulting estimator for the outcome regression function is not guaranteed to be a piecewise function of the treatment. As such, it cannot yield an I2DR. In this section, we present the computational details for jump interval-learning. We employ the dynamic programming algorithm (see e.g., Friedrich et al., 2008) to ﬁnd the optimal partition for multi-scale change point detection are equally applicable (see e.g., Scott and Knott, 1974; Harchaoui and Lévy-Leduc, 2010; Fryzlewicz, 2014). Speciﬁcally, we adopt the PELT method proposed by Killick et al. (2012) that includes additional pruning steps within the dynamic programming framework to achieve a linear computational cost. Given set of functions Finally, we remark that alternative to our approach, one may directly apply DNN that bPthat minimizes the objective function equation 3. Meanwhile, other algorithms network. where D-JIL, respectively. partitions {j/m : j = 0, 1, . . . , r}. Set B(m, m) = B(m), we deﬁne the Bellman function Let formula, where R during each iteration with the set of candidate change points and removes values that can never be the minima of the objective function. It speeds up the computation, leading to a cost that is linear in the number of observations (Killick et al., 2012). change point location requires to apply the linear / MLP regression to learn each The optimal partition more details is given in Algorithm 1. To detail our procedure, for any interval I ∈ [0, 1], we deﬁne the cost function Qis a class of linear functions or deep neural networks, corresponding to L-JIL and For any integer 1≤ r < m, denote byB(m, r) the set consisting of all possible B(0) =−γ, the dynamic programming algorithm relies on the following recursion is the candidate change points list updated by We brieﬂy summarize our algorithm below. For a given integerr, we search the optimal j ∈ R. Letjbe the corresponding minimizer. We then deﬁne the change points list ) ={j, τ(j)}. This procedure is iterated to computeB(r) andτ(r) forr= 1, . . . , m. Our proposal requires to specify the tuning parameters choice of m and n are of the same order. In our simulation studies, we tried several diﬀerent values Thus, the proposed I2DR is not overly sensitive to the choice of this constant. Detailed empirical results can be found in Section 5.3. the concrete proposal to approximate the outcome regression function. We elaborate below. Global: data {(X, A, Y) : i = 1, . . . , n}; sample size n; covariates dimension p; Local: integers l, r ∈ N; cost dictionary C; a vector of integers τ ∈ N; Output:bP and {bq: I ∈bP}. I. Initialization. 1. Set B(0) ← −γ;bP ← Null; τ ← Null; R(0) ← {0}; 2. Deﬁne the cost function C(I): II. Apply the PELT method. For r = 1, . . . , m: 1. B(r) = min{B(j) + C([j/m, r/m)) + γ}; 2. j← arg min{B(j) + C([j/m, r/m)) + γ}; 3. τ(r) ← {j, τ(j)}; 4. R(r) ← {j ∈ R(r − 1) ∪ {r − 1} : B(j) + C([j/m, (r − 1)/m)) ≤ B(r − 1)}; III. Get Partitions. τ← τ (m); r ← m; l ← τ[r]; While r > 0: 1. Let I = [l/m, r/m) if r < m else I = [l/m, 1]; 3. bq(·) ← arg minI(A∈ I){Y− q(X)}; 4. r ← l; l ← τ[r]; returnbP and {bq: I ∈bP}. m. In practice, we recommend to setm=n/cwith some constantc >0 such that and found the resulting estimated I2DRs have approximately the same value function. We next discuss the choices ofλandγ. Selection of these tuning parameters relies on For L-JIL, we choose requirements of develop an algorithm that substantially reduces the computation complexity resulting from the use of cross-validation. candidate tuning parameters. For a given integer for k ∈ { computed based on the data in G times to compute the minimizer of equation 6 over the set of candidate tuning parameters. We develop an algorithm to facilitate the computation. The key observation is that, for any interval can be obtained simultaneously over the set of candidate tuning parameters. This forms the basis of our algorithm. More details are provided in Section A of the supplementary article. As for D-JIL, we ﬁnd that the MLP regressor is not overly sensitive to the choice of so we set requirement of cross-validation, we randomly split the data into To be more speciﬁc, let Λ={λ, ··· , λ}and Γ={γ, ··· , γ}be the set of equal sized subgroups. LetGdenote indices of the subsamples in thekth subgroup, k= 1, ··· , K. LetGdenote the complement ofG. For anyλ∈Λ,γ∈Γ, 1, ··· , K}, let (bP, {bθ:I ∈bP}) denote the optimizer equation 6, To solve equation 9, we remark that there is no need to apply Algorithm 1|Λ| × |Γ| I ⊆[0,1] andk ∈ {1, ··· , K}, the set of estimators{bθ:γ∈Γ, λ∈Λ} λ= 0. The parameterγis chosen based on cross-validation. The theoretical , . . . , K, we compute the estimatorsbPandbq(·) based on the sub-dataset in We also remark that other tuning parameters, such as the learning rate, the numbers of hidden nodes and hidden layers, are set to the default values of the MLP regressor implementation (Pedregosa et al., 2011). We establish the statistical properties of our proposed method in this section. As we have commented, we consider both cases where the outcome regression function is either a piecewise or continuous function of the treatment. We ﬁrst study the statistical properties of L-JIL and D-JIL under Model I, respectively. We next outline a procedure to construct a conﬁdence interval for the value under the proposed I2DR and prove its validity under these two methods. Finally, we investigate the properties of our proposed method under Model II. The theoretical results justify that our method will work when the outcome regression function is either piecewise or continuous function. To establish the theoretical properties of the I2DR obtained by L-JIL, we ﬁrst assume equation 1 holds with outcome regression function of generality, assume that the representation in equation 1 is unique. We write deﬁned by }, {b}if there exists some universal constantc ≥1 such thatcb≤ a≤ cb. Deﬁne ·) =θI(· ∈ I). Giving (bP, {bθ:I ∈bP}), our estimator for the functionθ(·) is This yields a piecewise constant approximation of properties of probability tails of X and Y . (A4) Suppose there exists some constant {1, . . . , p} given the treatment A, outcomes are bounded. Theorem 1 Ahas a bounded probability density function on [0 that the following events hold with probability at least 1 − O(n estimator. Results in (ii) imply that the estimated change point locations converge at a rate of bθ(·). As discussed in the introduction, derivation of Theorem 1 is nontrivial. A number of technical lemmas (see Lemma 1-4 in Section 7) are established to prove Theorem 1. These results can be easily extended to study general covariate-based change point models. quantity under the proposed I2DR. The smaller the diﬀerence, the better the I2DR. Notice that impose the following condition. , and that for any random variableZ,kZkdenotes the conditional Orlicz norm We remark that Condition (A4) is automatically satisﬁed when the covariates and the }satisﬁesγ→0 andγn/ log n → ∞. Then, there exists some constant¯c >0 such (i) |bP| = |P|. (ii) maxmin|ˆτ − τ| ≤ ¯cnlog n. (iii)kbθ(a) − θ(a)kda ≤ ¯cn In Theorem 1, results in (i) show the model selection consistency of our jump penalized O(nlog n). In (iii), we derive an upper error bound for the integrated`loss of We next establish the convergence rate ofV− V(bd), whereV=V(d). The V− V(bd) represents the diﬀerence between the optimal value and the value ≥ V(d) for any I2DRd(·). It suﬃces to provide an upper bound forV− V(bd). We (A5.) Assume for any I where the big-O term is uniform in 0 < t ≤ δ for the value function under the estimated optimal IDR (Qian and Murphy, 2011; Luedtke and Van Der Laan, 2016; Shi et al., 2020). It is very similar to the margin condition (Tsybakov, 2004; Audibert and Tsybakov, 2007) used in the classiﬁcation literature. This condition is automatically satisﬁed with density function for any I ∈ P Theorem 2 Then, we have for some constant ¯c > 0, with probability at least 1 − O(n value at a rate of Theorem 1 and 2 occur with probability at least 1 application of Borel-Cantelli lemma implies that these events will occur for suﬃciently large n almost surely. We study the theoretical properties of the proposed I2DR under Model I when D-JIL is applied. Similar to the linear case, we assume layers and W is a smooth function of the baseline covariates (see assumption (A6) below). Meanwhile, D-JIL is valid when Condition (A5) is commonly assumed in the literature to derive sharp convergence rate When (A5) holds withγ= 1, Theorem 2 suggests thatV(bd) converges to the optimal ∈ P. For anyI, we set the regression classQto a class of DNN withLhidden To derive the theoretical properties of D-JIL, we assume the outcome regression function Fukumizu, 2019). Speciﬁcally, deﬁne the class of smooth functions with exponent β) as for some constant We introduce the following conditions. (A6.) Suppose Q(•, a) ∈ Φ(β, c), and p(a|•) ∈ Φ(β, c) for any a. (A7.) Functions {bq assumptions are commonly imposed in the literature to derive the convergence rates of DNN estimators (see e.g., Farrell et al., 2021). Combining (A7) with (A6) allows us to derive the uniform rate of convergence for the class of DNN estimators theorem summarizes the theoretical properties of the proposed method via deep neural networks. Theorem 3 variables, and satisﬁes DNN classes resulting D-JIL estimator computed by equation 3 satisﬁes with probability at least 1 − O(n is piecewise function in the treatment. Results in (i) imply that D-JIL correctly identiﬁes Φ(β, c) =sup|Dh(x)| ≤ c, supsup≤ c, denotes the diﬀerential operator Ddenote the diﬀerential operator: Assumption (A7) ensures that the optimizer would not diverge in the`sense. Similar γ→0 andγ nlogn. Then, there exist some constant¯c >0 and (i) |bP| = |P|; (ii) maxmin|ˆτ − τ| ≤ ¯cnlogn; (iii) E|Q(X, A) −I(A ∈ I)bq(X)|da ≤ ¯cnlogn, Theorem 3 establishes the properties of our method under settings where theQ(x, a) the number of change points. Results in (ii) imply that any change point in consistently identiﬁed at a convergence rate of factors. Notice that we use the piecewise function outcome regression function. In (iii), we show our estimator for function at a rate of and Fukumizu, 2019; Farrell et al., 2021). These DNN architectures ensure the convergence rate of our estimator for function rate of convergence under (A6) (see e.g., Stone, 1982). theorem. Theorem 4 Then, we have with probability at least 1 − O(n up to some logarithmic factors. This rate is slower than the rate ( logarithmic factor) we obtained in Theorem 2 where we posit a parametric (linear) model. Suppose the condition Here, the extra margin parameter due to the estimated decision rule require the smooth parameter (β > p/ (see e.g., Chernozhukov et al., 2017; Farrell et al., 2021). Suppose Model I holds. When L-JIL is used, it follows from Theorem 2 that Win Theorem 3 are consistent with the literature of DNN estimators (Imaizumi and We next establish the convergence rate ofV− V(bd) under Model I in the following Theorem 4 suggests thatV(bd) converges to the optimal value at a rate ofO{n} n). This observation forms the basis of our inference procedure in Section 4.1.3. 2) is commonly assumed in the literature on evaluating average treatment eﬀects +o(n). When D-JIL is used, if the smoothness parameterβ(see (A6)) and the margin parameter 4 that normal as well. (A8.) [ functions with VC-index upper bounded by a detailed deﬁnition of the VC-type class), for any I ∈ I(m). to converge at certain rates. Similar assumptions are commonly imposed in the causal inference literature to derive the asymptotic distribution of the estimated average treatment eﬀect (see e.g., Chernozhukov et al., 2017). The second part of (A8) essentially controls the model complexity of the estimator index. Under (A6), we can show (A8) holds when DNN is used to model the generalized propensity score. Theorem 5 from zero. Further assume that for any for some σ Wald-type 1 V(bd) =V+o(n). In the following, we derive the asymptotic normality of (bV − V). By Slutsky’s theorem, this implies thatn{bV − V(bd)}is asymptotically E{be(I|X)−e(I|X)}]=o(n) and thatbe(I;•) belongs to the class of VC-type The ﬁrst part in Assumption (A8) requires the generalized propensity score function (X)) = 0. (i) Suppose conditions in Theorem 2 are satisﬁed. Then, under L-JIL, we have (ii) Suppose conditions in Theorem 4 are satisﬁed with 4β(1 +γ)>(2β+p)(2 +γ). We now introduce the estimator for the asymptotic varianceσorσ, and derive a V (d) with probability tending to 1 − α. We estimate σ where {bq quantile of a standard normal distribution. Similar to Theorem 5, we can show that consistent. This shows the validity of our inference procedure. We ﬁrst consider the case when the outcome regression function can be represented by a varying coeﬃcient model and investigate the theoretical properties of the proposed L-JIL. Speciﬁcally, suppose the true outcome regression function takes the following form where assume the conditional mean of the outcome is a linear function of individuals’ covariates for any treatment step function, or an arbitrary continuous function of Models of this type belong to the class of varying coeﬃcient models popularly applied in many scientiﬁc areas (see e.g., Fan and Zhang, 2008, for an overview). there exist some constants L > 0, 0 < α integer k > 0, we deﬁne θ (·)} corresponds to the value estimations under L-JIL or D-JIL. The corresponding 1−αCI is given bybV ±zbσ, wherezdenotes the upperα/2-th ¯x= (1, x)andθ(·) is some continuous (p+ 1)-dimensional function. That is, we Here, we consider the following class of Hölder continuous functions forθ(·). Suppose We ﬁrst sketch a few lines to see why our method works under equation 14. For a given (a) =θI(j ≤ (k + 1)a < j + 1) + θI((k + 1)a ≥ k). Apparently, show that see that points increases. piecewise linear function assumption. Based on the above discussion, we expect that jump interval-learning also works when the model equation 13 holds. We formally establish the corresponding theoretical results in the following theorem. Theorem 6 on [0 the model equation 13, there exists some constant probability at least 1 − O(n In addition, assume such that the following occurs with probability at least 1 − O(n converges at a rate of compared to the results in Theorem 1, since When proposed I2DR will converge to the optimal value function at a rate of O We next consider the general case when the outcome regression function is speciﬁed by model II and study the theoretical properties of the proposed D-JIL. The following theorem proves the consistency of the proposed estimator. θ(·) can be uniformly approximately by a step function as the number of change In Theorems 1 and 2, we have shown the proposed I2DR is consistent under the ,1]. Assumem  n,λ=O(nlog n),γsatisﬁesγ→0 andγ nlog n. Under It is worth mentioning that with proper choice ofγ, the integrated`loss ofbθ(·) θ(·) is Lipschitz continuous, it follows from equation 15 that the value under our Theorem 7 (A6)-(A7) hold. Assume density function on [0 satisﬁes a ∈ I. The consistency of the value in (ii) thus follows. In this section, we focus on scenarios where the outcome regression function takes the form of Model I and examine the coverage probability of the proposed CI in Section 4.1.3. Simulated data are generated from the following model: where following two scenarios with diﬀerent choices of Q(X, A). Scenario 1: Under Scenario 1, the outcome regression function is piecewise constant as a function of and is linear as a function of some calculations, one can show that the optimal value V (n|I|)log(n|I|) such that the resulting D-JIL estimator computed by equation 3 (i) maxsupE|bq(X) − Q(X, a)|= o(1); (ii) V− V (bd) = o Theorem 7 establishes the properties of our method under settings whereQis continuous . Results in (i) imply thatbq(x) can be used to uniformly approximateQ(x, a) for any Y |X, A ∼ N(Q(X, A), 1), A|X ∼ Unif[0, 1] and X, X, . . . , X∼ N(0, 1), Unif[a, b] denotes the uniform distribution on the interval [a, b]. We consider the Scenario 2: Under Scenario 2, the outcome regression function is piecewise constant as a function of a, but is nonlinear as a function of corresponding to is applied to Scenario 1 only, as it requires the outcome regression function to be linear in the baseline covariates. The detailed implementation is discussed in Section 3.3. We set described in Section 4.1.3. Reported in Table 1 are the estimated value function standard error for V value as the sample size increases for both two methods. For instance, when obtained an estimated value of 1 value of 1 | = 3. The optimal value equals 1.35, based on Monte Carlo approximations. For each scenario, we setp= 4 and consider three diﬀerence choices of the sample size, n/5,λ= 0,γ= 4nlog(n), and construct the CI forVbased on the procedure , and the number of estimated partitions |bP|, aggregated over 500 simulations. Based on the results, it is clear that the estimated value function approaches the optimal .349 under Scenario 2. These values are very close to the truths 1.34 and 1.35, respectively. The performance of our proposed L-JIL and D-JIL are comparable under the Scenario 1. In addition, as the sample size increases, the coverage probability of the Wald-type CI approaches to the nominal level. This veriﬁes our theoretical ﬁndings in Theorem 5. Moreover, the averaged estimated number of partitions for all settings. This supports our theoretical ﬁndings in Theorems 1 and 3. In this section, we consider more general settings and compare the proposed procedure with the existing state-of-the-art methods that outputs single-valued decision rule. Similar to Section 5.1, we generate the data from the following model: In addition to Scenarios 1 and 2, we consider several other choices of the outcome regression function, allowing the working model assumption in Model I or Model II to be violated in some scenarios. Scenario 3: The outcome regression function in Scenario 3 is piecewise function of with |P Scenario 4: where continuous function. One can show that V Scenario 5: Y |X, A ∼ N(Q(X, A), 1), A|X ∼ Unif[0, 1] and X, X, . . . , X∼ N(0, 1). with complex treatment-covariates interactions. We haveJ(P) ={0.25,0.5,0.75,1} | = 4, and the optimal value equals 0.76. θ= (1,2, −2,0). By settingθ(a) = 2|a −0.5|θ, it is immediate to see that ) =¯xθ(a) and satisﬁes the condition in equation 13. Note thatθ(·) here is a This scenario is considered in Chen et al. (2016). Note that the outcome regression function is continuous in both the baseline covariate and the treatment. With some calculations, one can show that V with according to Section 3.4. Here, we set In Section 5.3, we report results with I2DRs are very similar to those with c = 10. under estimated optimal IDRs obtained by linear outcome weighted learning (L-O-L) and the nonlinear outcome weighted learning based on the Gaussian kernel function (K-O-L). Both methods were proposed by Chen et al. (2016). To implement L-O-L and K-O-L, we ﬁx the parameter We apply the proposed L-JIL and D-JIL to estimate the optimal I2DR for Scenario 1-5, p= 20 andn ∈ {50,100,200,400,800}. The tuning parameters in JILs are speciﬁed To evaluate the proposed I2DRs, we compare its value functionV(bd) with the values in Chen et al. (2016). All the value functions are evaluated via Monte Carlo simulations. The average value function as well as its standard deviation over 200 replicates are summarized in Table 2. I (Scenarios 1-3) holds, and perform reasonably well when Model II (Scenario 4 and 5) holds or the sample size is small. For instance, the proposed L-JIL achieves a value of 1.297 in Scenario 1 and 1 the optimal values, given by 1 consistently better than L-JIL, due to the capacity of deep neural networks in approximating complicated non-linear relationships. In addition, it can be seen that D-JIL achieves the best performance with small sample size in most cases. Moreover, the value of the proposed I2DR increases with the sample size in most cases. This supports our theoretical ﬁndings in Section 4. more than half of the optimal value, for each setting in Scenario 1 to 3. In Scenario 4, both L-JIL and D-JIL achieve much larger value functions than outcome weighted learning. In Scenario 5, L-O-L and K-O-L have better performance, as the true optimal decision rule is linear and the outcome regression function is very sensitive to the change of the treatment level n = 800, the two competing methods achieve similar value functions. Recall that we set results with in Tables 3 and 4 for L-JIL and D-JIL, respectively. We also include results with for completeness. It can be seen that the value functions are very similar across diﬀerent choices of c. It can be seen from Table 2 that both L-JIL and D-JIL are very eﬃcient when Model In comparison, the value function under the estimated IDR using L-O-L or K-O-L is no a(by noticing that the coeﬃcient of the quadratic term in Scenario 5 is 10). When In this section, we illustrate the empirical performance of our proposed method on a real data from the International Warfarin Pharmacogenetics Consortium. Warfarin is commonly used for preventing thrombosis and thromboembolism. High doses of Warfarin are more beneﬁcial than its lower doses, but may lead to a high risk of bleeding as well. Proper dosing of Warfarin is thus of signiﬁcant importance. (2009) for analysis. We choose 6 baseline covariates, including age, height, weight, gender, the VKORC1.AG genotype and VKORC1.AA genotype. This yields a total of 3848 with complete records of baseline information. The outcome is deﬁned as the absolute distance between the international normalized ratio (INR, a measurement of the time it takes for the blood to clot) after the treatment and the ideal value 2 use the min-max normalization to convert the range of the dose level A into [0, 1]. as in Section 5.2. To further evaluate the empirical performance of the proposed I2DRs, we compare their values with the value under the IDR estimated by K-O-L. Speciﬁcally, we randomly select 70% of the data to compute the proposed I2DR and the IDR obtained by K-O-L, and evaluate their value functions using the remaining dataset. We then iterate this procedure 50 times to calculate the average value function. For each iteration, the value function is estimated based on the nonparametric estimator proposed by Zhu et al. (2020). by K-O-L, we consider the following nonparametric estimator for its value function, where parameters. The tuning parameters in Section 5 of Zhu et al. (2020). We use the dataset provided by the International Warfarin Pharmacogenetics Consortium To implement L-JIL and D-JIL, we setm=n/5, and selectγandλvia cross validation, Speciﬁcally, letGdenote observations in the testing dataset. For the IDRedcomputed ed) =YK(h(x − X))K(h(d(x) − A))PK(h(x − X))dx, K(·) denotes the Gaussian kernel function, andhandhare some bandwidth To evaluate the value function under the proposed I2DRbd(·), we setπto be a uniform density function and compute The integration is calculated via a midpoint rule with a uniform grid. and D-JIL are K-O-L. In the following, we focus on one particular iteration in L-JIL and D-JIL, respectively, to further interpret our results. the largest as iteration, L-JIL partitions [0 Let subintervals. We report these estimators in Table 5. According to Table 5, the proposed I2DR under L-JIL gives us a clear interpretation about the eﬀect of baseline information on the dose assignment rule. For instance, patients whose genotype VKORC1 is AG or AA are more likely to receive low doses of Warfarin to prevent bleeding; older patients with larger weight shall be treated with higher dose levels. Future experiments are warranted to conﬁrm these scientiﬁc ﬁndings. In addition, we also explore one particular iteration under D-JIL whose value nearly achieves the largest as under D-JIL recommends most patients to receive dose level in [0 [0.20, 1], respectively. This ﬁnding accords with the partition results under L-JIL. 1YK(h(x − X))K(h(d(x) − A))PXK(h(x − X)) |bd(x)|K(h(x − X))K(h(d(x) − A))|G|h Over 50 iterations, the average value functions of our proposed I2DRs computed by L-JIL Finally, we show the results of one particular iteration under L-JIL whose value achieves bθ,bθandbθdenote the corresponding regression coeﬃcients associated with these three We provide the proof for Theorem 1 in this section. We present an outline of the proof ﬁrst. Let the following event occurs with probability at least 1 − O(n This together with equation 17 proves (i) in Theorem 1. In the last part, we show (iii) holds. present the proofs for Part 1, 2, 3 and 4. where the one deﬁned in equation 3 for any equation 13, the deﬁnition of model θ δ=min|I|/3>0. We divide the proof into four parts. In Part 1, we show that In the following, we ﬁrst introduce some notations and auxiliary lemmas. Then, we Notations and technical lemmas: For any interval I ⊆ [0, 1], deﬁne X= (1, X). It is immediate to see that the deﬁnition ofbθhere is consistent with (a) =θI(a ∈ I) for any I ∈ P. Let I(m) denote the set of intervals I(m) = {[i/m, i/m) : for some integers iand ithat satisfy 0 ≤ i< i< m} Let points of θ Lemma 1 any interval I ∈ I(m) that satisﬁes |I| ≥ ¯c {τ}with 0< τ< τ< ··· < τ<1 be the locations of the true change 0,c≥1 such that the following events occur with probability at least 1− O(n): for 0, c≥1 such that the following events occur with probability at least 1− O(n): for Lemma 3 nand any interval n → ∞, we have either τ for some integer k such that 2 ≤ k ≤ K and some constant c for some integer k such that 3 ≤ k ≤ K and some constant c interval I ∈ I(m) that satisﬁes for some constant ¯c Lemma 4 at least 1 − O(n Part 1: Assume have In view of equation 24, we obtain with probability at least 1 − O(n In addition, the following events occur with probability at least 1− O(n): for any {[0,1]}which consists of a single interval and a zero vector0. By deﬁnition, we This implies that under the event deﬁned in equation 24, we have for suﬃciently large n, and hence for suﬃciently large n. large in equation 20-equation 24 hold for any interval I ∈ By equation 20 and equation 21, we obtain that with probability at least 1 we have with probability at least 1 − O(n Under the event deﬁned in Lemma 4, we havemin|I| ≥ ¯cnlog nfor suﬃciently n, sinceγ nlog n. Thus, with probability at least 1−O(n), the events deﬁned Under the events deﬁned in equation 22 and equation 23, it follows that where the last inequality is due to Cauchy-Schwarz inequality. By equation 29 and the condition that γ with probability at least 1 − O(n Combining equation 32 with equation 33, we’ve shown that with probability at least 1 For any integerksuch that 1≤ k ≤ K −1, letτbe the change point location the oracle partition formed by the change point locations{τ}. Setτ= 0, = 1 andθ=θfor 1≤ k ≤ K −1 andθ=θ. Let kand suﬃciently largen, we can ﬁnd an intervalI ∈ I(m) with length between log n and 2¯cnlog n that covers ∆. It follows that λ=O(nlog n), combining equation 35 together with equation 24 and equation 25, In view of equation 34 and equation 36, we obtain that with probability at least 1 deﬁned in equation 37. Otherwise, there exists some for all ˆτ ∈ J( This apparently violates equation 38. equation 16 thus holds with probability at least 1 − O(n Part 2: By equation 30 and equation 31, we have with probability at least 1 To summarize, we’ve shown that with probability at least 1 − O(n It follows from equation 35 and equation 36 that for some constants condition that λ Denoted byT(m) the set of intervalsI ∈ I(m) withkθ(a)− θkda ≥ ¯cnlog n. +kθ(a) − θkda − 3cnkθ(a) − θkda log n. + η+ nγ|bP| − 2(c+ ¯c)|bP|log n. with probability at least 1 probability at least 1 − O(n By deﬁnition, Thus, we have with probability at least 1 − O(n and hence, + nγ|bP| − (2c+ 2¯c)|bP|log n − clog n − nγ|P|. |bP| ≥2|P|, it follows from the conditionnγ log nthat for suﬃciently largen, In view of equation 41, we have with probability at least 1 − O(n for some constant c > 0. Thus, with probability at least 1 − O(n for any 1 − O(n or [ from Lemma 3 such that min(|i Part 3: Using similar arguments in proving equation 28, we can show that the following events occur with probability at least 1 − O(n for some constant C > 0. can show the following event occurs with probability at least 1 − O(n I ∈bP ∩ T(m). By the deﬁnition ofT(m), we obtain that with probability at least Consider a given change pointτ ∈ P, there exists an intervalI ∈bPof the form [i, i) i, i] withi= 1 such thati≤ τ < i. Under the event deﬁned in equation 42, it follows By equation 42, using similar arguments in proving equation 39 and equation 40, we for some constant C > 0. By deﬁnition, Since if |P| > |P since γ Part 4: In the ﬁrst three parts, we’ve shown that with probability tending to 1. For suﬃciently large implies that following occurs with probability at least 1 − O(n ≤ 2 where the ﬁrst inequality is due to Cauchy-Schwarz inequality. This proves (iii). The proof is hence completed. γ nlog n, the above event occurs only when|bP| ≤ |P|. To see this, notice that b|, we have nγ− C log n −b− nγb≥ nγ− C log n −− nγ  nlog n. This proves equation 19. Under the events deﬁned in equation 42, equation 43 and equation 44, we have Under Model I, we assume Our theoretical results can be generalized to the situation where well. Take L-JIL as an example. Similar to Theorem 6, we can show that the loss satisﬁes 1, the convergence rate here is slower by a factor required to guarantee the consistency of details), we consider a more general framework and establish the by assuming θ By deﬁnition, AE for any θ(·) and we have with probability at least 1 − O(n where large, we have with probability at least 1 − O(n where O(1) denotes some positive constant. we have We next present more technical details. In the proof of Theorem 6 (see Section B.12 for Whenθ(·) is a step function with number of jumps equal to|P|, we haveAE(θ) = 0 k ≥ |P|. As a result,θsatisﬁes the conditionlim supkAE(θ)< ∞for any >0. As a result, the assertion equation 148 in the proof of Theorem 6 also holds for O(1) denotes some positive constant. As|P| → ∞andαcan be made arbitrarily In Theorem 6, we requireγ nlog n. However, this condition can be relaxed to ≥ Mnlog nfor some suﬃciently large constantM>0. Under the latter condition, This yields the convergence rate of the ` In this paper, we focus on modeling the outcome regression function to derive I2DR. Below, we outline two other potential approaches and discuss their weaknesses. Let’s assume eliminate the baseline function example, Robinson, 1988; Zhao et al., 2017; Chernozhukov et al., 2018, and the references therein) and compute eq where be obtained by some generic machine learning methods with good prediction performance. for some tuning parameter for a given partition result, standard change point detection algorithms such as dynamic programming or binary segmentation (Scott and Knott, 1974) cannot be applied. Exhaustive search among all possible partitions is computationally infeasible. It remains unknown how to eﬃciently solve the above optimization problem. We leave it for future research. As commented in Section 2.2, to apply value search, we need to specify a preference function π. To better illustrate the idea, let us suppose is, the preference function is the same as the one we observe in our data. Then, for a bµ(x) correspond tos some nonparametric estimators forE(Y |X=x). Bothbµandbecan When Pis unknown, one might consider estimating Pand {q: I ∈ P} jointly by arg minY− bµ(X) −{I(A∈ I) − be(I|X)}q(X)+ γ|P|, given I2DR be computed by maximizing Suppose we consider the class of linear decision rules, i.e., Similar to Section 8.2.1, for a given partition be jointed estimated. As a result, dynamic programming cannot be applied. It remains unknown how to eﬃciently solve the above optimization problem. We leave it for future research. In L-JIL, We use a ridge penalty in equation 6 to prevent overﬁtting in large When the true regression coeﬃcient the ridge penalty with the LASSO Tibshirani (1996) to improve the estimation accuracy. However, optimizing the resulting objective function requires to compute the LASSO estimator method. It remains unknown whether the computation can be simpliﬁed. We leave it for future research. For a given partitionP, letDdenote the space of I2DRs that we consider. Thenbdcan m(m −1)/2 times. This is far more computationally expensive than the proposed