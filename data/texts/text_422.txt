The classic Prediction with Expert’s Advice problem, also known as the canonical framework for online learning (Cesa-Bianchi and Lugosi, 2006). This problem is usually formulated as a two-player sequential game played between a learner and an adversary. Assume that Nexperts are indexed by the set [ reward vector current round), the learner selects an expert (possibly randomly) and then receives a reward equal to the reward of the chosen expert. The goal of the learner is to design an online policy that incurs a small regret. Recall that the regret of an online policy over a time horizon of length T is deﬁned as the diﬀerence between the reward accumulated by the best ﬁxed expert in hindsight and the total expected reward accrued by the online policy over the time horizon (viz. Eqn. policies achieving sublinear regrets in this problem are known, most notably, Freund and Schapire, 1997). In this paper, we introduce and study the problem. In the selects a subset received by the learner at round Table 1 describes some variants of the forms for the reward function at round given the history of the game. Then, we can express the (conditional) expected reward for the round as This paper introduces and studies thek-expertsproblem - a generalization of the classic Prediction with Expert’s Advice (i.e., theExperts) problem. Unlike theExpertsproblem, where the learner chooses exactly one expert, in this problem, the learner selects a subset ofkexperts from a pool ofNexperts at each round. The reward obtained by the learner at any round depends on the rewards of the selected experts. Thek-expertsproblem arises in many practical settings, including online ad placements, personalized news recommendations, and paging. Our primary goal is to design an online learning policy having a small regret. In this pursuit, we proposeSAGE(Sampled Hedge) - a framework for designing eﬃcient online learning policies by leveraging statistical sampling techniques. We show that, for many related problems,SAGE improves upon the state-of-the-art bounds for regret and computational complexity. Furthermore, going beyond the notion of regret, we characterize the mistake bounds achievable by online learning policies for a class of stable loss functions. We conclude the paper by establishing a tight regret lower bound for a variant of thek-expertsproblem and carrying out experiments with standard datasets. tis given by the sum of the rewards of the experts in the chosen setS. In particular, denote the marginal (conditional) probability that theiexpert is included in the setS, Eq(S, r) =r· p.TheMax-reward,`-reward, andPairwise-rewardvariants are deﬁned analogously. However, unlike the the latter variants depends on higher-order joint inclusion probabilities as well (as opposed to only marginals). For each of the above variants, we formulate the problem of designing an online expert selection policy that minimizes the regret R In the above deﬁnition, the expectation in the second term is taken with respect to any random choice made by the learner. Related problems: N-ary prediction with k-sets a learner sequentially predicts the next symbol in an unknown chosen by an adversary. The symbols are revealed to the learner sequentially in an online fashion. However, instead of predicting a single symbol a subset be correct if the predicted set correct prediction, the learner receives unit reward, else, it receives zero rewards for that round. The goal of the learner is to maximize its cumulative reward over a time period. It is easy to see that the above problem is a special case of the k-experts problem with the Sum-reward variant, where the adversary’s actions are constrained as r In a seminal paper, Cover (1966) studied the fundamental limits of online binary prediction, which corresponds to a special case of the characterization of the set of all stable reward proﬁles achievable by online policies. Fifty years later, Rakhlin and Sridharan (2016) generalized Cover’s result to an arbitrary alphabet of size requiring with an arbitrary N and k has long remained an open problem. Coming back to the problem of minimizing the static regret for the dirty approach can be used to reduce the problem to an instance of the set of experts, which we call meta-experts. In this reduction, a meta-expert is identiﬁed with one of the Hedge (2010) referred to the resulting approach is to overcome the severe computational ineﬃciency of the resulting online policy, which, apparently, needs to keep track of exponentially many experts. To resolve this issue, Koolen et al. (2010) proposed the Component Hedge ( tight regret bound. However, the which costs projection and decomposition procedures for the problems. The the authors referred to as the paging problem. The authors alleviated the complexity of the naïve Hedge distribution. Unfortunately, the resulting policy is still suﬃciently complex (Ω( Bhattacharjee et al. (2020) studied the paging problem and proposed an eﬃcient and regret-optimal Follow-the-Perturbed-Leader-based policy. Although simple to implement, their algorithm does not admit a data-dependent small-loss bound. Our contributions: Our contributions can be summarized as follows: S,consisting ofksymbols at roundt. The learner’s prediction for roundtis considered to k= 1. The fundamental limits of the achievable prediction error for thek-setsproblem possible subsets of sizek. One can then use any known low-regret prediction policy, such as , on the meta-experts to design an online learning policy for thek-setsproblem. Koolen et al. O(N). We refer the readers to Takimoto and Hatano (2013) for a survey of the eﬃcient implementation by reducing it to a problem of sequential sampling from a recursively deﬁned We conclude this section by giving a brief overview and key intuition for the SAGE framework. As pointed out earlier, the expected marginal inclusion probabilities and not on the higher-order joint inclusion distribution. In particular, any two online prediction policies, that have the same conditional marginal inclusion probabilities, yield exactly the same expected sum reward per round. This simple observation leads to the meta-algorithm described below. Start with a low-regret base online prediction policy base policy π Clearly, the above procedure yields the same performance (regret) as the base policy However, the advantage of the above three-step process stems from the fact that, unlike the base policy (which could be computationally intractable), the for many problems. We show that when Hedge is used as a base policy for the k-sets problem, the marginalization in line 3 reduces to the evaluation of certain elementary symmetric polynomials, that can be eﬃciently computed using Fast Fourier Transform (FFT) techniques. Furthermore, an eﬃcient sampler for line 4 can be borrowed from the statistical sampling literature, reviewed in Section 2. SAGE in line 3 is diﬃcult to obtain. This unlocks the potential application of a host of oﬀ-the-shelf algorithms from the Probabilistic Graphical Model literature to online learning problems. However, we would like to remind the reader that expected reward depends on higher-order inclusion probabilities (such as the with Pairwise-reward or Max-reward), for which we need other techniques (see Section 5). Our proposed online policy makes critical use of systematic sampling techniques from statistics, which we brieﬂy review in this section. Consider the problem of sampling without replacement where one needs to randomly select a In Section 3, we generalize Cover (1966)’s result on binary sequence prediction by characterizing the set of all stable error proﬁles achievable by online learning policies for the onlineN-ary prediction problem with k-sets. In Section 4, we presentSAGE- an eﬃcient, projection-free, regret-optimal online prediction framework. UsingSAGE, we design an eﬃcient online policy for thek-setsproblem. The proposed policy runs in linear time, admits a small-loss bound, and breaks the existing quadratic computational barrier (see Table 2). We also design a diﬀerent OCO-based prediction policy for a generalized version of the k-sets problem. In Section 5, we present aSAGE-based improper learning policy that achieves aO(T) regret for the Pairwise-reward version of the k-experts problem. In Section 6, we establish a tight regret lower bound for theMax-rewardversion of the k-experts problem. Finally, in Section 7, we numerically compare the performance of the proposed policies with other benchmarks on some standard datasets. Eﬃciently compute the ﬁrst-order marginal inclusion probabilities (p) corresponding to the policyπ.This step amounts to marginalizing the joint distribution induced by the policy is ﬂexible and can be easily augmented with heuristics when the exact marginalization with probability without replacement, for any sides with respect to the randomness of the sampler, we conclude that the following consistency condition must be satisﬁed for any feasible marginal inclusion probability vector p: Surprisingly, it turns out that condition that leads to the marginal inclusion probability vector studied in the statistical sampling literature under the heading of unequal probability sampling design (Tillé, 1996; Hartley, 1966; Hanif and Brewer, 1980). In this paper, we use a linear-time exact sampling scheme proposed by Madow et al. (1949) as outlined in Algorithm 2. Algorithm 2 Madow’s Sampling Scheme Input: vector p = (p Output: A random k-set S with |S| = k such that, P(i ∈ S) = p Correctness: necessary condition the element distributed in [0 We begin with the following prediction problem studied by Cover (1966) - assume that an adversary secretly selects a binary sequence seeing the initial segment of the sequence ˆyfor the prediction. Let Afor a binary sequence with respect to the randomness of the prediction algorithm. As shown in Eqn. follows that irrespective of the prediction algorithm µ(·) over all possible 2 FTPL (Gaussian perturbation) Cohen and Hazan (2015) 22T k ln˜O(N)q SAGE (with π2kT ln˜O(N) with a pre-speciﬁed marginal probabilityp, ∀i ∈[N].Formally, if thek-setSis sampledP A universe [N] of sizeN, cardinality of the sampled setk, marginal inclusion probability jis selected if the random variableU ∈ t[Π− i,Π− i).SinceUis uniformly telement of the sequencey. The actual value ofyis revealed to the learner after the to be achievable if there exists an online prediction policy any sequence is upper bounded by the function is to characterize the set of all achievable loss functions φ(·). For a given sequence We call a loss function y ∈ {±1} In this setup, Cover (1966) proved the following result: Theorem 1 achievable if and only if distribution over [N] We emphasize that although Theorem 1 is stated in terms of an expectation, no probabilistic assumption was made on the sequence extended the above result to the a (randomized) subset prediction policy A for the sequence y is given by: where the expectation is taken with respect to the randomness of the policy the loss function sequences in [N] where the equality (a) follows from the fact that |S loss function y ∈ [N ] We now state the following theorem that generalizes Cover’s result by showing that conditions and (7) are also suﬃcient for the achievability. Theorem 2. online policy if and only if Eφ(z) ≥ 1 − distribution over [N] The necessity part of Theorem 2 has already been established in Eqn. suﬃciency is constructive and proceeds in two phases. In Phase-I, at each round vector probability of the element sample a Please refer to Section A.1 of the supplementary material for the proof of Theorem 2. and for all 1 ≤ t ≤ T : ((Cover, 1966)).Suppose the loss functionφ:{±1}→[0,1] is stable. Thenφ(·) is -setsproblem where, instead of predicting a single valueˆy,the learner is allowed to predict φ: [N]→[0,1] to be stable if it satisﬁes the following two conditions for all sequences psatisfying the feasibility condition(2), such thatpgives the correct marginal inclusion k-setS⊆[N] consistent with the marginal inclusion probabilitiespusing Algorithm 2. Discussion: achievability proof of Theorem 2 could be intractable in terms of computation or memory requirements. A more serious issue with the generic prediction policy is that it requires the loss function to be stable, which limits its applicability. Similar to the treatment in Rakhlin and Sridharan (2016), it might be possible to work with some relaxation of the loss function to derive a tractable policy. In the rest of the paper, we show that near-optimal inclusion probabilities may be quickly computed via alternative methods, resulting in low-regret eﬃcient online prediction policies. In this section, we propose two diﬀerent eﬃcient online policies for the ﬁrst policy uses Regularized-Leader framework. For the simplicity of exposition, we use the the standard conjunction with the policy may also be used, such as 2011), leading to more reﬁned regret bounds. 1. The Base Policy: Introduction. Deﬁne a collection of Assume that the learner predicts the set accrued by the learner when the adversary chooses the symbol y where the indicator variables round Sat round times the round t with the following probability: where w 2. Eﬃcient Computation of the Inclusion Probabilities: for each of the N elements can be obtained by marginalizing the probability in Eqn. (9): where {i}). We can verify that p(i) :=Pp(S) is the marginal inclusion probability of theielement in the predicted S. We now use theHedgealgorithm as our base policy for the resultingExpertsproblem. Let τ. Furthermore, let the variabler(S)≡r(i) denote the reward accrued by the expert τ. The cumulative reward accumulated by the expertSup to the roundt −1 is givenP (S) =r(S).Overloading the notation, let the variableR(i) denote the number of ielement appears in the sub-sequencey. TheHedgepolicy chooses the expertSat (S) ≡ exp(ηR(S)), η > 0 is the learning rate. where step (a) follows from the fact that for any exactly condition naïvely computing the probability products - a computationally intractable task. This diﬃculty can be alleviated upon realizing that both the numerator and denominator of Eqn. polynomials as shown below. For any vector elementary symmetric polynomial (ESP) of order l as follows: Furthermore, for any index the sub-vector with its ric polynomials that can be eﬃciently computed in methods (see, e.g., Shpilka and Wigderson (2001)). Further speedup is possible by exploiting the fact that the weight of only one of the components change at a round. This faster iterative method is deduced in Section A.2 of the supplementary material. 3. Sampling the predicted set: use Madow’s systematic sampling scheme outlined in Algorithm 2 to sample a prediction policy is summarized below: Algorithm 3 k-sets via SAGE with π Input: w ← 1, learning rate η > 0. Recall that, in expectation, the performances of Algorithm 3 and the base policy It is well-known that by adaptively tuning the learning rate with n experts admits the following data-dependent small-loss regret bound Koolen et al. (2010) where loss matrix. In the case of the Hence, the bound: where y. Clearly, the regret bound is sublinear in the horizon-length as Eqn. (14) could be much smaller if the oﬄine oracle incurs a small loss for the particular sequence. It is possible to design eﬃcient online policies for the Hedge be augmented with the systematic sampling schemes to design an online prediction policy for a ktimes. Therefore, the marginal inclusion probabilities in Eqn.(10)satisfy the feasibility (2). Hence, Algorithm 2 may be used to eﬃciently sample the predictedk-set. However, ← wexp(η1(y= i)), ∀i ∈ [N ]. ldenotes the cumulative loss incurred by the best ﬁxed expert in the hindsight for the given SAGEprediction framework withHedgeas the base policy yields the following regret l(y) is the number of mistakes incurred by the best ﬁxedk-set in hindsight for the sequence . In particular, we now show how the standard Follow-the-Regularlized-Leader framework can generalized version of the essential to the rest of the paper, due to space limitations, we move this discussion to Section A.3 of the supplementary section. A drawback of the not yield a small-loss regret bound. In this section, we design an online prediction policy for a special case of the with where the adversary chooses a single item at each round (so that only one component of the reward vector (so that exactly two components of the reward vector chosen by the adversary are included in the predicted receives zero rewards for that round. The following hardness result is immediate. Proposition 1. Proof. (Sotirov, 2020) to the oﬄine optimization problem. Consider an arbitrary graph thek-experts pair of items corresponding to the vertices of the edge subgraph of Densest maximize the cumulative reward in the k-experts problem with pairwise-rewards. In principle, we can use the and then sample - (1) unlike Eqn. inclusion probability vector when given a feasible pairwise inclusion probability vector, it is not known how to eﬃciently sample items accordingly. The above roadblocks are not surprising given the hardness of the oﬄine problem. This prompts us to propose the following approximate policy described in Algorithm 4. Since any item may be a part of in Algorithm 4 includes an item multiple times. However, it is easy to see that the number of items contained in the union of any with Algorithm 4: Oﬄine oracle reward with at most 2k items where is an instance of improper learning algorithm where the online policy competes with a weaker oracle. In this section, we consider regret lower bounds for diﬀerent variants of the given in Table 1. Consider the setting where the adversary chooses binary rewards with exactly one non-zero reward per round. In this setting, Bhattacharjee et al. (2020) established the following regret lower bound for the Sum-reward variant of the k-experts problem: pairwise-rewardfunction and binary rewards (see Table 1). Unlike thek-setsproblem, ris one, the rest are zero), here, the adversary secretly chooses a pair of items at each round The proof follows from a simple reduction of theNP-HardDensestk-subgraph problem edges denoted bye, e, . . . , e(arranged in some arbitrary order). Construct an instance of kvertices such that the number of edges in the induced subgraph is maximum (i.e., the k-subgraph ofG) reduces to the oﬄine problem of selecting the most rewardingkitems to (the number of super-items) in Eqn. (14), we get the following performance guarantee for lis the number of errors made by the optimal oﬄine oracle containing 2kitems. Algorithm 4 Theorem 3 (Regret Lower bound for Sum-reward). For any online policy with Note that with the above rewards structure, the (where the same ﬁle is requested twice in each round) variants of the identical. Hence, Theorem 3 also yields a lower bound to all of the above variants of the problem. However, from the standard be immediately seen that the upper and lower regret bounds diﬀer by a logarithmic factor. Our main result in this section is the following tight regret lower bound for the the k-experts problem, that gets rid of the above logarithmic gap. A distinguishing feature of the above regret lower bound, compared to the standard lower bounds (Cesa-Bianchi and Lugosi, 2006) is its non-asymptotic nature. Proof outline: is lower bounded by the average regret over an ensemble of analysis becomes complex as the reward accrued at each round vector. This nonlinearity complicates the computation of the optimal expected cumulative reward in hindsight. To overcome this diﬃculty, we ﬁrst partition the pool of We select the cumulative best expert in hindsight from each subset in order to lower bound the optimal oﬄine reward. Please see Section A.4 in the supplementary material for detailed proof. Figure 1: Comparison among diﬀerent prediction policies with Dataset. k-sets: Assume that there is a corpus of at each round. The learner sequentially predicts (possibly randomly) a set of user is likely to watch at a given round. At each round, the learner receives a unit reward if the movie chosen by the user is in the predicted set; else, it receives zero rewards for that round. The learner’s goal is to maximize the total number of correct predictions over a given time interval. In our experiments, we use the MovieLens 1M dataset (Harper and Konstan, 2015) for generating the sequence of movies chosen by the user. The dataset contains along with the timestamps. Here, we assume that a user rates a movie immediately after watching it. The plot in Figure 1 compares the regrets of the proposed (Regret Lower Bound forMax-reward).For any online policy withT ≥16k ln() and 2400 for the MovieLens/N= 0.02 for the Reality Mining/N= 0.01 for the MovieLens In our ﬁrst experiment, we consider the following stylized version of thek-setsproblem. FTPL policy proposed in Bhattacharjee et al. (2020), and two other baseline prediction policies LFU and LRU, which treat the prediction problem as a paging problem (Geulen et al., 2010). From the plot, it is clear that the SAGE decisively outperforms all other policies under consideration. k-experts with Pairwise-reward: (Eagle and Pentland, 2006) to understand the eﬃcacy of the approximate prediction policy proposed in Section 5. The dataset contains timestamped human contact data among 100 MIT students collected using standard Bluetooth-enabled mobile phones over 9 months. In our experiments, we consider a subset of predict a sequence of As described in Section 5, we design an approximate prediction policy by considering each pair of students as a super-item and use the achieved by the policy is plotted in Figure 2. To compute the optimal static oﬄine reward, we used a brute-force search. From the plots, we see that the time-averaged regret for this policy approaches zero for long-enough time-horizon given in Proposition 1. k-experts with Max-reward: with genres so that if the movie max requirement that if the requested movie is not in the predicted set, then it is preferable to predict a closely related movie than a completely diﬀerent one. In Figure 3, we plot the normalized regret of theSAGE we can see that the normalized regret shows a downward trend with albeit there is a non-trivial gap with the lower bound. This gap is expected as the optimal for the are not aware of any eﬃcient policy for the refer to Section A.5 of the supplement for additional experimental results. In this paper, we formulated the its variants using the variant of the k-sets In particular, designing an eﬃcient online policy for the a regret close to the lower bound given in Theorem 4 will be of interest. T ∼7000 ratings forN= 200 movies. We now assume that the movies are sorted according to 1 −|j − i|for predicting the setS. This reward function roughly emulates the practical policy withπ=FTRL,along with the lower bound given in Theorem 4. From the plot, problem achievable by online policies. However, many interesting questions are still left open.