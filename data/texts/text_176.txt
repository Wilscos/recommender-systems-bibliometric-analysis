With the advent of end-to-end deep learning approaches in machine translation, interest in word alignments initially decreased; however, they have again become a focus of research more recently. Alignments are useful for typological research, transferring formatting like markup to translated texts, and can be used in the decoding of machine translation systems. At the same time, massively multilingual processing is becoming an important NLP scenario, and pretrained language and machine translation models that are truly multilingual are proposed. However, most alignment algorithms rely on bitexts only and do not leverage the fact that many parallel corpora are multiparallel. In this work, we exploit the multiparallelity of corpora by representing an initial set of bilingual alignments as a graph and then predicting additional edges in the graph. We present two graph algorithms for edge prediction: one inspired by recommender systems and one based on network link prediction. Our experimental results show absolute improvements in Fof up to 28% over the baseline bilingual word aligner in different datasets. Word alignment is a challenging NLP task that plays an essential role in statistical machine translation and is useful for neural machine translation (Alkhouli and Ney, 2017; Alkhouli et al., 2016; Koehn et al., 2003). Other applications of word alignments include bilingual lexicon induction, annotation projection, and typological analysis (Shi et al., 2021; Rasooli et al., 2018; Müller, 2017; Lewis and Xia, 2008). With the advent of deep learning, interest in word alignment initially decreased. However, recently a new wave of publications has again drawn attention to the task (Jalili Sabet et al., 2020; Dou and Neubig, 2021; Marchisio et al., 2021; Wu and Dredze, 2020). Figure 1: Bilingual alignments of a verse in English, German, Spanish, and French. Two of the alignment edges not found by the bilingual method are German “Schritt” to French “pas” and Spanish “largo” to English “thousand miles”. By looking at the structure of the entire graph, one can infer the correctness of these two edges. In this paper we propose MPWA (MultiParallel Word Alignment), a framework that employs graph algorithms to exploit the information latent in a multiparallel corpus to achieve better word alignments than aligning pairs of languages in isolation. Starting from translations of a sentence in multiple languages in a multiparallel corpus, MPWA generates bilingual word alignments for all language pairs using any available bilingual word aligner. MPWA then improves the quality of word alignments for a target language pair by inspecting how they are aligned to other languages. The central idea is to exploit the graph structure of an initial multiparallel word alignment to improve the alignment for a target language pair. To this end, MPWA casts the multiparallel word alignment task as a link (or edge) prediction problem. We explore standard algorithms for this purpose: Adamic-Adar and matrix factorization. While these two graphbased algorithms are quite different and are used in different applications, we will show that MPWA effectively leverages them for high-performing word alignment. Link prediction methods are used to predict whether there should be a link between two nodes in a graph. They have various applications like movie recommendations, knowledge graph completion, and metabolic network reconstruction (Zhang and Chen, 2018). We use the Adamic-Adar index (Adamic and Adar, 2003); it is a second-order link prediction algorithm, i.e., it exploits the information of neighbors that are up to two hops aways from the starting target nodes (Zhou et al., 2009). We use a second-order algorithm since a set of aligned words in multiple languages (representing a concept) tends to establish a clique (Dufter et al., 2018). This means that exploring the inﬂuence of nodes at a distance of two in the graph provides informative signals while at the same time keeping runtime complexity low. Matrix factorization is a collaborative ﬁltering algorithm that is most prominently used in recommender systems where it provides users with product recommendations based on their interactions with other products. This method is especially useful if the matrix is sparse (Koren et al., 2009). This is true for our application: Given two translations of a sentence with lengthsMandN, among all possible alignment links (M × N), only a few (O(M + N )) are correct. This is partly due to fertility: words in the source language generally have only a few possible matches in the target language (Zhao and Gildea, 2010). A multiparallel corpus provides parallel sentences in more than two languages. This type of corpus facilitates the study of multiple languages together, which is especially important for research on low resource languages. As far as we know, out of all available multiparallel corpora, the Parallel Bible Corpus (Mayer and Cysouw, 2014) (PBC) provides the highest language coverage, supporting 1334 different languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of monolingual corpora. Complementarily, MPWA assumes the existence of a very small (a few 10,000s of sentences in our case) parallel corpus and then takes advantage of information from the other languages in the parallel corpus. This is an alternative approach that is especially important for low resource languages for which monolingual data often are not available. The PBC corpus does not contain a word alignment gold standard. To conduct the comparative evaluation of our new method, we port three existing word alignment gold standards of Bible translations to PBC, for the language pairs EnglishFrench, Finnish-Hebrew and Finnish-Greek. We also create artiﬁcial multiparallel datasets for four widely used word alignment datasets using machine translation. We evaluate our method with all seven datasets. Results demonstrate substantial improvements in all scenarios. Our main contributions are: 1.We propose two graph-based algorithms for link prediction (i.e., the prediction of word alignment edges in the alignment graph), one based on second-order link prediction and one based on recommender systems for improving word alignment in a multiparallel corpus and show that they perform better than established baselines. 2.We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3.We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our codeand data. Bilingual Word Alignerstake different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eﬂomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Eﬂomal and SimAlign. Multiparallel corpus alignment.Most work on word alignment has focused on bilingual corpora. To the best of our knowledge, only one method speciﬁcally designed for multiparallel corpora was previously proposed: (Östling, 2014). However this method is outperformed by a “biparallel” method by the same author, Eﬂomal (Östling and Tiedemann, 2016). We compare with Eﬂomal in our experiments. Cohn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of beneﬁt to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction.Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and NLP tasks like word sense disambiguation (Schütze, 1998). In 2009, Netﬂix’s recommender system competition revealed that this technique effectively works for collaborative ﬁltering (Koren et al., 2009). Since then it has been a state of the art method in recommender systems. Link prediction algorithms are widely used in different areas of science since many social, biological, and information systems can be described as networks with nodes and connecting links (Zhou et al., 2009). Link prediction algorithms compute the likelihood of links based on different heuristics. One can categorize available methods based on the maximum number of hops they consider in their computations for each node (Zhang and Chen, 2018). First order algorithms, such as common neighbors (CN), only consider one hop neighborhoods, e.g., (Barabási and Albert, 1999). Second order methods consider two hops, e.g., (Zhou et al., 2009). Finally, higher order methods take the whole network into account for making predictions (Brin and Page, 1998; Jeh and Widom, 2002; Rothe and Schütze, 2014). In this paper, we use a two-hop method since it offers a good tradeoff between effectiveness and efﬁciency. While a bilingual aligner considers each language pair separately, MPWA utilizes the synergy between all language pairs to improve word alignment performance. In Figure 1, Eﬂomal alignments of a sentence from PBC in four different languages are depicted. Although Eﬂomal has failed to ﬁnd the link between German “Schritt” and French “pas”, we can easily ﬁnd this relation by observing that the four nodes “step”, “Schritt”, “paso”, and “pas” are fully connected, except for the edge from “Schritt” to “pas”. In this case, the inference amounts to a completion of a clique. However, most cases are not that simple. In the ﬁgure, English “thousand miles” is mistakenly aligned to Spanish “siempre” although its alignments to German “lange” and French “mille” are correct. We would like to infer that “thousand miles” should be aligned to “largo”, but in this case creating a fully connected subgraph, i.e., a clique (which would include “siempre”), would add many incorrect edges. Given the complexity and error-proneness of initial bilingual alignments, inferring an alignment between two languages from a multiparallel alignment in general is a complex problem. Starting from a multiparallel corpus, we ﬁrst generate bilingual alignments for all language pairs. MPWA then employs a prediction algorithm to ﬁnd and add new alignment links. In this paper, we focus on two prediction algorithms: non-negative matrix factorization and Adamic-Adar link prediction. Non-negative matrix factorization (NMF) has been used in many different applications. After discovery of its effectiveness for collaborative recommendation (Koren et al., 2009), it was widely accepted as a standard method for recommender systems. In a standard recommender system withmusers andnitems, ratings (a number from 1 to 5) from each user for the items they have seen so far are known. The aim is to predict the ratings the user would give to unseen items and, based on these predictions, recommend new items to the user. As described by (Luo et al., 2014), letW = [w] ∈ Rbe the matrix of ratings. For NMF to work it is essential that the matrix be sparse, thus if a user’s rating for an item is unknown, the corresponding cell is zeroed. The matrixWis then decomposed into two low-rank non-negative matrices,T = [t] ∈ RandV = [v] ∈ R such thatT V ≈ Wandr  min(m, n).ris a hyperparameter. By multiplication of these two matrices we end up with a reduced matrixW= T V in which each zeroed cellwfrom matrixWis replaced with a valuewthat represents a prediction for the rating that useruwould give to itemi. NMF solves the following optimization program: This optimization problem can be solved by gradient descent using the following updates: In this equation,ηis the learning rate. To guarantee non-negativity, it is deﬁned as: Note that the objective function only takes account of non-zero cells. Luo et al. (2014) propose an approach that takes advantage of the sparseness of the matrix for faster computation. In addition, Figure 2: An example of how the original matrix is ﬁlled for a sentence in three languages. Zero entries are left blank for readability. Tikhonov regularization is integrated to improve precision, recall, and convergence rate. We use the implementaion of NMF provided by the Surpriselibrary, with default hyperparameters (r = 15, n_epochs = 50). 3.2.1 NMF in MPWA framework We create a separate matrixWfor each sentence in the multiparallel corpus. Tokens in the sentence play the role of both users and items, i.e., we consider each token both as a row and as a column. Figure 2 shows an example of a sentence in a toy English-German-French multiparallel corpus. If two tokens are aligned using the bilingual aligner, we ﬁll the corresponding cell with the highest rating (5). To give a few negative examples to the algorithm, if a tokenxfrom languageLis aligned to tokenyin languageL, we pick another random tokenzfromLand ﬁll the corresponding cell ofxtozwith the lowest rating (1). We zero out all other cells. Next we apply the matrix factorization algorithm to this matrix and then compute the reduced matrixWfrom the factors. Now we grab the predicted alignment scores between source and target languages fromW. To extract new alignment edges we apply the Argmax algorithm (Jalili Sabet et al., 2020). Argmax creates an alignment edge between wordwfrom language Land wordwfrom languageLif among all words fromL,whas the highest alignment score withw, and among all words fromL,whas the highest alignment score with w. A multiparallel sentence annotated with bilingual word alignments can be considered to be a graph with words from all translations as nodes and the word alignments as edges. Link prediction algorithms such as Common Neighbors (CN) and Adamic-Adar (AdAd) estimate the likelihood of having an edge between two nodesxandyin the graph based on the similarity of their neighborhoods. The CN index weights all common neighbors equally. In contrast, AdAd gives higher weight to neighbors with low degrees because sharing a neighbor that in turn has few neighbors is more signiﬁcant. Therefore, we use the AdAd index. It is deﬁned as: AdAd=1log |Γ(z)|(1) where Γ(x) is the neighborhood of x. If we use a word aligner that produces a score for each alignment edge, we can use Weighted AdamicAdar (Lü and Zhou, 2010): WAdAd=w(x, z) + w(z, y)log(1 + S(z))(2) wherew(x, z)is the similarity score ofxandzgen-P erated by the aligner andS(x) =w(x, z). For embedding-based aligners we use embedding similarity as the scorew(x, z). If an aligner does not provide scores, we set all weights to 1.0. Given a scored word alignment, we create a multilingual word alignment matrixWfor each sentence as shown in Figure 2. Each cell contains 0 or 1 for Adamic-Adar or the alignment score for Weighted Adamic-Adar. We again apply Argmax to extract new alignment edges and then add them to the original alignment. 4.1 PBC The PBC corpus (Mayer and Cysouw, 2014) contains 1758 editions of the Bible in 1334 languages. The editions are aligned at the verse level and tokenized. A verse can contain more than one sentence, but we treat it as one unit in the parallel corpus since a true sentence level alignment is not available. There are some errors in tokenization (e.g., for Tibetan, Khmer and Chinese), but the overall quality of the corpus is good. For the majority of languages one edition is provided, while a few languages (in particular, English, French and German) contain up to dozens of editions. The verse coverage also differs from language to language. Some languages have translations of both New Testament and Hebrew Bible while others contain only one. Table 2 gives corpus statistics. PBC does not provide gold word alignments. To evaluate MPWA, we port two word alignment gold datasets of the Bible to PBC: Blinker (Melamed, 1998) and the recently published HELFI (Yli-Jyrä et al., 2020). We further experiment with bilingual datasets, using Machine Translation (MT) to create multiparallel corpora. Table 1 gives dataset statistics. The HELFI datasetconsists of the Greek New Testament, the Hebrew Bible and translations of both into Finnish. In addition, morpheme alignments are provided for Finnish-Greek and FinnishHebrew. We reformatted this dataset to the format used by PBC. In more detail, we added three new editions for the three languages to PBC. We identiﬁed the PBC verse identiﬁer for each verse of HELFI to ensure proper verse alignment of these three new editions. The Finnish-Hebrew dataset has 22,291 verses and the Finnish-Greek dataset 7,909. We split these datasets 80/10/10 into train, validation and test. The Blinker Bible datasetprovides word level alignments of 250 Bible verses between English and French. The French side of this dataset matches with the edition Louis Segond 1910 in PBC. However, the tokenizations (Blinker vs PBC) are different. We therefore create a mapping of the tokens using character n-gram matching. For English, we created and added a new edition to PBC. MT datasets. To more broadly evaluate MPWA, we also create multiparallel datasets for four nonBible word alignment gold standards; these are listed in Table 1 as “Non-Bible” corpora. For these gold standards, we translate from English to all languages available in Google Translate, using their API.For the added languages, we create alignments for the gold standard sentences using SimAlign. We compare with two state of the art models, one statistical, one neural. Eﬂomal (Östling and Tiedemann, 2016) is a Bayesian statistical word aligner using Markov Chain Monte Carlo inference. SimAlign (Jalili Sabet et al., 2020) obtains word align- Table 1: Overview of datasets. We use ISO 639-3 language codes. # Sentences: the number of available verses (i.e., sentences). FIN-HEB and FIN-GRC datasets split into train, validation and test. ments from multilingual pretrained language models with no need for parallel data. For the symmetrization of Eﬂomal, we use grow-diag-ﬁnal-and (GDFA) and intersection, and for SimAlign we use Argmax and Itermax. Intersection and Argmax generate accurate alignments while GDFA and Itermax are less accurate but have better coverage (Jalili Sabet et al., 2020). We evaluate on a target language pair parallel sentence as follows: First, we create the matrix (Figure 2) for this sentence for all languages in the multiparallel corpus. Then we run link prediction on the matrix – this accumulates evidence from a set of languages in the multiparallel corpus. Finally, we take the predictions for the target language pair and add them to the original (bilingual) alignment. NMF works best if it starts with high-accuracy (i.e., non-noisy) bilingual alignments – errors can result in incorrectly predicted alignment edges. We therefore use SimAlign Argmax and Eﬂomal Intersection, two word alignment methods with high precision, to create the initial alignments that are then fed into NMF. We then add the predictions to any desired original alignments; e.g., NMF (GDFA) uses Eﬂomal Intersection as the initial alignments and adds the predictions to Eﬂomal GDFA. See the Appendix for more details. SimAlign offers high quality word alignments for well-represented languages from pretrained language models; however, our experiments show that its performance is far behind Eﬂomal for less well resourced languages like Biblical Hebrew and Koine Greek. Also, Eﬂomal is a better match for MPWA because it can provide word alignments for all languages available in a multiparallel corpus. In contrast, SimAlign is limited to languages supported by pretrained multilingual embeddings. To feed Eﬂomal with enough training data for a target language pair, we use all available data from different translations of the language pair. For example if one language has two translations and the other one has three translations, Eﬂomal’s training data will contain six aligned translation pairs for these two languages. We use the standard evaluation measures for word alignment: precision, recall,Fand Alignment Error Rate (AER) (Och and Ney, 2003b; Östling and Tiedemann, 2016; Jalili Sabet et al., 2020). We perform the ﬁrst set of experiments on the Blinker Bible and the HELFI gold standards in the PBC. The baseline results are calculated on the original language pairs. MPWA can be applied to both Eﬂomal and SimAlign alignments. Since the default version of SimAlign can only generate alignments for the 84 languages that multilingual BERT supports,for a better comparison, we use the same set of languages in the alignment graph for both SimAlign and Eﬂomal. Table 3 shows the results for our methods applied on SimAlign and Eﬂomal baselines.AdAd, NMF and WAdAd substantially improve the performance for all language pairs. SimAlign generates high-quality alignments for the English-French dataset, but cannot properly align underresourced languages like Biblical Hebrew and Koine Greek. Table 3: Comparison of results from different methods on PBC. The best results in each column are in bold. The three methods exploiting multiparallelism (AdAd, WAdAd, NMF) generally outperform the baselines on F AER. In such cases, MPWA uses the accumulated information from all other language pairs in the graph to improve the performance. When starting with the SimAlign alignment (“Init SimAlign”), both methods improve the result for both FIN-HEB and FIN-GRC. Eﬂomal generates better alignments for FINHEB and FIN-GRC. This means that Eﬂomal also generates better alignments between FIN, HEB and GRC on the one hand and the other languages in the graph on the other hand and therefore can provide a better signal for MPWA. The improvements of our models applied on Eﬂomal are higher than the ones applied on SimAlign for these language pairs. When changing the initial alignments from Eﬂomal (intersection) to Eﬂomal (GDFA), we see different behaviors: GDFA improves the results for Blinker while it does not help for HELFI. We believe this is caused by the different ways the two datasets were annotated. In Blinker, many phrases are “exhaustively” aligned: if a phrase DE in English is aligned with FG in French then all four alignment edges (D-F, D-G, E-F, E-G) are given as gold edges. So Blinker contains a lot of many-to-many links. In contrast, most alignments are one-to-one in HELFI. This partially explains why intersection as initial alignment works much better for HELFI than GDFA and vice versa for Blinker. In summary, compared to the baselines, we see very large improvements through exploiting multiparallelism for one type of alignment methodology (HELFI,Fimproved by up to 20% for FIN- HEB) and improvements of up to 3.5% for the other (ENG-FRA). We perform the second set of experiments on gold standard alignments for language pairs that are not part of a multiparallel corpus such as PBC. To this end, we create artiﬁcial multiparallel corpora by translating the English side to all languages available in the Google Translate API. The main goal is to give broader evidence for the effectiveness of our method, beyond the specialized domain of the Bible. Eﬂomal’s alignments generally have good quality. However, they get worse when less parallel data is available (Jalili Sabet et al., 2020). Since the size of the multiparallel corpus created by machine translation is rather small, we use SimAlign for generating initial alignments. SimAlign has been shown to have good performance even for very small parallel corpora; in fact, it does not need any parallel data at all. Table 4 shows the results of the experiments. Both NMF and WAdAd, improve the performance of the baseline by using the alignment graph. Improvements range from 0.8% (ENG-DEU) to 3.3% (ENG-HIN). This again demonstrates the utility of exploiting multiparallelism for word alignment. It is worth mentioning that in this case the translations are noisy since they were automatically generated. But even with these noisy translations (instead of a “true” multiparallel corpus), our models effectively leverage multiparallelism. Table 4: Results with gold standards translated into other languages using machine translation. The best results in each column are in bold. The three methods exploiting multiparallelism (AdAd, WAdAd, NMF) outperform the baselines on Fand AER. Figure 3: Fof MPWA for the target language pair FIN-HEB as a function of the number of additional languages. There is a clear rise initially. The curve ﬂattens around 75. 5.3.1 Effect of number of languages The effect of adding more languages to the alignment graph is depicted in Figure 3. This plot shows Ffor FIN-HEB. As seen in the ﬁgure, the slope is pretty steep up to 25 languages, but even adding just three languages can still improve the results. For 75 languages we have almost reached the peak and after 100, adding more languages is not improving the results. This means that MPWA can also be helpful for corpora with a smaller number of languages – a massively parallel corpus with thousands of languages is not required. 5.3.2 Size of the training set To assess the effect of dataset size on the performance of MPWA, we perform a set of experiments on ENG-FRA and NMF. To this end, we take the training data for ENG-FRA and train models on subsets of it. The training data consists of 6.4M sentence pairs – this number is so high because we use the crossproduct of all editions in English and French (§4.3). The results are shown in Figure 4. Eﬂomal performance increases with training set size initially Figure 4: Word alignment Fon ENG-FRA as a function of the size of the training set, ranging from 30K to 6.4M training sentence pairs and is then less predictable. NMF consistently improves the scores. 5.3.3 Effect of task difﬁculty Table 3 shows large improvements for all datasets, and especially for FIN-HEB and FIN-GRC. To get more insight into the reasons for this improvement, we stratify FIN-HEB verses by dividing the interval [0, 1]of initialFperformance of Eﬂomal into ﬁve equal-sized subintervals: [0, 0.2], . . . , (0.8, 1]. Figure 5 indicates that MPWA is most effective for difﬁcult verses, but brings little improvement for easy verses. We attribute this to two reasons: 1.An easy to align verse in a language pair cannot use help from other languages since it already has good alignment links (although the language pair would still be of beneﬁt in improving alignments for the sentence in other languages). So there is no way for MPWA to get better results in this scenario. 2.MPWA only tries to get better results by adding new alignments, and as an easy verse already has many alignment links, adding new links almost inevitably results in a drop in pre- Figure 5: How helpful is MPWA for different difﬁculty levels? For this analysis, FIN-HEB verses were stratiﬁed according to Eﬂomal F(x-axis). We see that MPWA brings the largest improvements for difﬁcult sentences. Table 5: The ﬁve most helpful languages and WAdAd’s absolute improvements in Fover the initial bilingual aligner SimAlign. For example, MPWA improves the bilingual FIN-GRC alignment by 7.5% if applied to the trilingual corpus FIN-GRC-LAT, i.e., Latin can be viewed as the best bridge between Finnish and Greek. cision. It may also be possible to inspect and prune existing Eﬂomal links using MPWA to get better results in this scenario. 5.3.4 Most helpful languages For each dataset, the ﬁve most helpful languages with their corresponding improvements are listed in Table 5. We hypothesize that these languages serve to bridge the typological gap between the two target languages. Table 5 suggests one should be able to achieve excellent results – even for a corpus with a small number of languages – if we utilize an intelligent selection of languages. 5.3.5 Multiple translations in two languages There are some datasets that contain few languages, but many translations of a text in one language. PBC is one example of such a dataset, many literary works another (e.g., many novels have many translations in English). To see whether MPWA can also help in this scenario, we picked all available 49 English and French editions from PBC and used them as additional translations for the ENGFRA dataset. The outcome of this experiment is Table 6: Ffor ENG-FRA. MPWA can exploit a multiparallel corpus with languages different from the target languages (“other languages”) better than one that contains additional translations in the target languages (“target languages”). compared with the outcome of the same setup, but with translations from languages other than French and English in Table 6. From this table we can conclude that translations from the target language pair can also assist, but not as much as translations from other languages. We presented MPWA, a framework for leveraging multiparallel corpora for word alignment. We used two prediction methods, one based on recommender systems and one based on link prediction algorithms. By adding new alignment edges to the output of bilingual aligners, both methods show large improvements over the bilingual baselines, with absolute improvements ofFof up to 20%. We have also ported Blinker and HELFI word alignment gold standards to the Parallel Bible Corpus in the hope that this will help foster more work on exploiting multiparallel corproa. Future work.In this paper, we have mainly focused on adding new alignment edges to baseline word alignments based on properties of the multiparallel alignment graph. This increases recall, but can harm precision. In future work, we plan to expand on the possibility of deleting edges based on evidence from the multiparallel alignment graph (cf. 5.3.3), thereby potentially improving both precision and recall. This work was supported by the European Research Council (ERC, Grant No. 740516) and the German Federal Ministry of Education and Research (BMBF, Grant No. 01IS18036A). The fourth author was supported by the Bavarian research institute for digital transformation (bidt) through their fellowship program. We thank the anonymous reviewers for their constructive comments.