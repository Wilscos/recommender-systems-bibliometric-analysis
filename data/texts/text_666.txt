Considering the rapidly increasing number of academic papers, searching for and citing appropriate references have become a nontrial task during the wiring of papers. Recommending a handful of candidate papers to a manuscript before publication could ease the burden of the authors, and help the reviewers to check the completeness of the cited resources. Conventional approaches on citation recommendation generally consider recommending one groundtruth citation for a query context from an input manuscript, but lack of consideration on co-citation recommendations. However, a piece of context often needs to be supported by two or more co-citation pairs. Here, we propose a novel scientic paper modelling for citation recommendations, namely Multi-Positive BERT Model for Citation Recommendation (MP-BERT4CR), complied with a series of Multi-Positive Triplet objectives to recommend multiple positive citations for a query context. The proposed approach has the following advantages: First, the proposed multi-positive objectives are eective to recommend multiple positive candidates. Second, we adopt noise distributions which are built based on the historical co-citation frequencies, so that MP-BERT4CR is not only eective on recommending high-frequent co-citation pairs; but also the performances on retrieving the low-frequent ones are signicantly improved. Third, we propose a dynamic context sampling strategy which captures the “macro-scoped” citing intents from a manuscript and empowers the citation embeddings to be content-dependent, which allow the algorithm to further improve the performances. Single and multiple positive recommendation experiments testied that MP-BERT4CR delivered signicant improvements. In addition, MP-BERT4CR are also eective in retrieving the full list of cocitations, and historically low-frequent co-citation pairs compared with the prior works. • Information systems → Recommender systems;Content analysis and feature sele ction. Citation Recommendation, Text Recommendation, Document Embedding ACM Reference Format: Yang Zhang and Qiang Ma. 2021. Recommending Multiple Positive Citations for Manuscript via Content-Dependent Modeling and Multi-Positive Triplet. In IEEE/WIC/ACM International Conference on Web Intelligence (WI-IAT ’21), December 14–17, 2021, ESSENDON, VIC, Australia. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3486622.3494002 Considering the massive amount of academic papers, which is accounted for over 300 million since 2018, and growing at 5% per year according to Johnson et al. [11]. It becomes a challenging task for researchers to search and nd appreciate papers for citing when they are writing their own papers, and for the reviewers to check the completeness of the cited resources. Currently, researchers are generally relying on keyword-based search engines (such as Google Scholar) for searching relevant papers via input keywords. However, it is argued that the input keywords might be over-simplied to reect the users’ searching needs [9,10], and hence they often lead to unsatisfactory recommendations. To appropriately detect the citing intent of the users, a line of studies considered to adapt a collection of seed papers (e.g. the papers previously cited or read by the user) to nd the topically relevant articles via collaborative ltering [16], random-walk methods [7,10,14], or matrix decomposition methods [3]. Seed papers could potentially reect richer information than keywords. However, these methods still lack considerations on extracting the citing intent from the user’s manuscript. Later researches utilized the “local context” (surrounding words around a target citation) to extract the semantics of the citing intent and then rank the most similar papers via citations embeddings [8, 29]. Context-based approaches [8,29] detect citing intents from the sentences needed support, and based on which it nds the bestmatched candidate papers. These approaches could be potentially applied in the real world to assist researchers in nding citations during the writing of papers. Nevertheless, context-based methods may still be constrained in four aspects. First, they generally consider recommend one ground-truth citation for the query context. However, it is common that a piece of context cites more than one reference to form co-citations. Second, we aim to improve the performances on retrieving both high and low frequent co-citation pairs by adapting noise frequency distributions which are built based on historical co-citation frequencies. Prior works ([6,24]) generally have been testied eectively [8,13] on retrieving the most frequently occurred co-citation pairs in history. However, Figure 1: Recommending multiple positive citations by adapting historical co-citation frequencies co-citation pairs that were published in later years, or by nonscientic-elites [19] may come with lower occurrences in history. We consider low-frequent co-citation pair could also demonstrate high topic relatedness as studied by Small[24], and therefore can potentially be utilized to improve recommendation performances. Third, “local context” is helpful to determine the citing intent in a micro-scope [8,28]. However, we argue that a lack of consideration of the rest of the body content may lead to inaccurate judgments on extracting the topic semantics of the manuscript. Forth, the prior works might be limited to representing the content semantics by adapting the label-based embeddings [8,28] that do not contain content knowledge, or content embeddings generated solely from abstracts [4]. To this end, we propose aMulti-PositiveBERTModel forCitation Recommendation (MP-BERT4CR), which is a content model for citation recommendations coupled with multi-positive triplet objectives. It has the following advantages: First, the proposed multipositive triplet objective functions allow the algorithm to be optimized for recommending both single and multiple positive candidates, which can eectively nd multiple positive candidates than conventional triplet objective. Second, we leverage the historical co-citation frequencies to generate positive samples. The underlying logic is: if two (or more) citations historically appeared as co-citations in the past, and one of them is cited by a manuscript at the present, then it is likely that other co-cited papers should be cited, as illustrated in Figure 1. We build noise distributions based on historical frequencies to draw co-citation samples, which is eective in retrieving both high and low frequent co-citations in history. Third, a dynamic context sampling strategy is proposed to extract the citing intents in a macro-scope by extracting the topic semantics from the remaining part of the body content of a manuscript, and producing citation embeddings carrying content semantics from candidate papers. Recommendations made by matching the content semantics of candidate papers with the “macro-scoped” citing intents produced superior performances compared with the baselines. We conduct experiments on single and multiple citation recommendations to evaluate MP-BERT4CR with the conventional baseline models in the eld on four datasets. The results revealed that the MP-BERT4CR had provided signicant improvements for both single and multiple positive recommendations compared to the baselines. Citation recommendation denotes the task of nding relevant papers based on an input query. A line of studies use a collection of seed papers as the input query to nd topically relevant papers via collaborative ltering [16], random-walk methods [7,10,14], or matrix decomposition methods [3]. These methods could be helpful during the surveying of a topic at an early stage. However, when applying to assist the writing of a paper, they generally lack considerations on detecting the citing intents of users. Contextbased methods [8,29] aimed to extract citing intent via an input local context (surrounding words around a target citation), and hence nd the most relevant papers based on the detected citing intent. Nevertheless, their methods still leave room for further improvements. First, previous methods only consider recommending one positive citation to a context, which may constraint the usability; second, local context may only reveal the “micro-scoped” citing intent which could not infer the topic in a macro-scope of the manuscript, and therefore may lead to inaccurate judgments on extracting the true citing intent of users; third, these method adapted label-based embeddings without content semantics, which may further constraint the performances. Document Embedding refers to the studies on representing documents as continuous vectors. The early approaches, Word2Vec [17] and Doc2Vec [15] were proposed to learn word embeddings by preserving the contextual information via DNN-like networks. However, Word2Vec and Doc2Vec generally treated the input documents as “plain texts” which may lead to information loss issues. Later approaches were considered to ne-tune with specic information from academic papers, such as hyperlinks [8], and section headers and word-wise relations in the local context [29]. The recent language modelling models such as BERT [5], SciBERT [2] are eective on multiple NLP tasks. In this study, we further extend the language modeling for citation recommendations, considering multiple positive candidates. We adapt part of terminologies and denitions for academic papers, citation relationships, and citation recommendation following the past studies [8, 29]. Let w∈W represent a word from a vocabulary set W , andH represent the i-th paper from a text collection H. Denition 3.1 (Academic Paper). The textual information of paper His represented as its content words, and cited papers, i.e.,H:= Denition 3.2 (Citation Relationship). Given a source paper,H:= citation relationship is constructed and expressed using a tuple, which is composed content words of the source papers, and the target paper, i.e., R := ⟨ˆW,ˆW⟩. Given a query paperH, a collection of papersH, and an embedding model𝔼, the task is dened to nd top𝑘ranked papers{H, H, ..., H}based on the geometric distances between embeddings ofH, i.e.,D = 𝔼(H)and the embedding ofH, i.e., The algorithm rstly composes the content words of the input papers into a hierarchical structural for encoding. Given a paper dened in Denition 3.1, whereH:=ˆW∪ˆD, the content words,ˆW are categorized into sentences, i,e,ˆW:= {S, S, ..., S}, whereS:= {𝑤, 𝑤, ..., 𝑤}. At the end of each sentence, a special end-of-sentence (EOS) token is added. We adapt the hierarchical transformer [27] to pre-train the model. However,faewfwfwe replace the EOS pooling to MEAN pooling at the sentence-level encoder from the original model. Fine-tuning is conducted after pre-training to maximize the similarity between the citing intent of an input manuscript and the content semantics of its ground-truth citations. We rst conduct dynamic context sampling for two purposes: 1) to extract the citing intent from the input manuscript from both of “micro-scoped” and “macro-scoped” view; 2) to generate contentdepend embeddings carrying content semantics for the citations. And then, we adapt a triplet-encoder neural architecture [22,23] coupled with multi-positive triplet objectives to encode the sampled contexts, for optimizing the algorithm for single and multiple recommendations. 4.1.1 Dynamic Context Sampling. It aims to extract essential contexts from the manuscript regarding a predicting location for detecting the micro and macro scoped citing intent, as well as capturing content semantics of the cited papers and other positive samples. The sampled contexts involve two components: afundamental contextfunctions as the backbone for inferring the citing intent or content semantics, and asupplemental contextaims to provide additional knowledge on the topic semantics of the paper. The detail of the sampling strategy is illustrated in Figure 2(b). As illustrated in Figure 2(b), for a manuscript, thefundamental contextis dened to include three sentences: the sentences include the target citations, the sentence before the target citation, and the sentence after it, which is functionally the same as the local context dened in prior works [8,29] to infer the citing intent in a micro-scope. Thesupplemental contextis dened as a pre-set number of sentences randomly selected from the nished content (the sentences appearing before the citation excluding the base context) to infer the overall topic of the manuscript, and thus to help determine the citing intent in a macro-scope. For a cited paper, thefundamental contextis dened to be the sentences from the abstract, which works as the backbone for inferring the content semantics. Thesupplemental contextis dened as a pre-set number of randomly selected sentences from the body content to provide additional information on the semantics. The default settings of the dynamic context sampling strategy were as follows: For the manuscript, we set the total number of sentences to 30, which includes the fundamental context composed of three sentences (the sentence including the target citation, the sentence before it, and the sentence after). Supplemental contexts include 27 sentences, which are randomly selected from the content before the base context. For a citation paper, we set the total number of sampling contexts to 40, which includes 10 sentences for the fundamental context (as it is found that the abstract averagely contain 10 sentences from our datasets) and 30 for the supplemental context (randomly selected from the body content). The number of the selected sentence was chosen according to the maximum memory of our GPUs. In addition, 40 sentences cover 10% of the contents from our dataset (papers come with 317 sentences overage). We presume that the abstract and 10% (maximumly 20% for 2 iterations of ne-tuning) of the body content are sucient to capture the content semantics. 4.1.2 Neural Architecture. It comprises a manuscript enco der for encoding the sampled context from the manuscript regarding a ground-truth citation, a citation encoder for encoding the sampled context from the ground-truth citation and other positively sampled citations, and a pre-dened number of negative-citation encoders for encoding the negatively sampled citations (Figure 2(b)). The architecture was inspired by the prior works [22,23], where the three encoders are identical to the encoders proposed in the pre-training model of MP-BERT4CR. The parameters of the three encoders are initialized from the pre-trained encoder. During training, the parameters of the citation encoder and negative-citation encoders are completely shared, whereas the manuscript enco der is individually updated. Additionally, a sum pooling layer and a normalization layer [1] are added: the former summarizes the sentence vectors into a document vector, and the latter facilitates the convergence of the objective functions. 4.1.3 Multi-Positive Sampling. For the query context with multiple ground-truth citations, in addition to the target citation, we sample a pre-dened number of citations according to their historical co-citation frequencies with the target citation as the additional positive samples. Noise distribution is built based on the historical frequencies of co-citation pairs are built, so that both high frequent and low frequent pairs are assigned a probability to be drawn. High frequent pairs are assigned a relatively higher probability, since it is likely for a pair to be co-cited again, if they are frequently cited before. The probability for low frequent pairs can be adjusted by the power value, which is set tofor a default value in Equation 1. Specially, givenHas the target ground-truth citation for prediction, andHfor the full list of co-citations, the algorithm samples 𝑛number (a pre-dened value) of positive citations from the paper collection Hvia the noise probability distribution [17]: where𝑓 𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦denotes the count of the two input papers being appeared as co-citations from the dataset. The number of positive Figure 3: Illustration of Optimization Strategies samples is set to be 3, as it is found that the number of co-citations with greater than 3 items are neglectable small in our datasets. 4.1.4 Negative Sampling. A negative sampling strategy is adapted to pick the papers which are not cited by the input context as the targets for similarity minimization. The objective is that the irrelevant papers should not appear in the top recommendation list. The negative citations are sampled based on their occurrences as citations. The underlying intuition is that if a paper is frequently cited by other papers, however, it is not cited by the input context, then it would be drawn more frequently for similarity minimization between it and the input context. Similar to positive sampling, a noise distribution is constructed based on their occurrences as citations: whereHdenotes all the papers in the dataset except the positive citations; and𝑐𝑜𝑢𝑛𝑡denotes the number of occurrences as citations of a paper. A pre-dened𝑚number of papers are picked from the distribution as negative samples, noted asH. We set𝑚to be 4 in this study. 4.1.5 Multi-Positive Triplet Objectives. They are designed to optimize the algorithm for recommending multiple positive citations. Suppose a piece of context in the manuscript,Hcontaining the ground-truth citation,H, along with𝑁number of co-citations H, i.e.,H, ..., H. The algorithm retrieves𝑛positive samples {H|1≤ 𝑛 ≤ 𝑁 }from Equation 1, and𝑚negative samples {H|1 ≤ 𝑚} from Equation 2. We propose multiple positive objectives considering the multiple positive samplings by modifying the original triplet-encoder [22,23] which originally considers one target and one negative sample: where||.||denotes euclidean distance, and𝜀is the margin which is normally set to 1. The original triplet loss implies that the embedding of the manuscript is only guaranteed to be geometrically closed to one target citation. When applying for recommending multiple positive citations, this training objective might be limited. Hence, the three multi-positive triplet objectives are proposed, so that the embedding of the manuscriptdshould not only be geometrically closed to one target citationd, but also be closed to the other positive citations{d|1≤ 𝑐 ≤ 𝑛}. Meanwhile, the manuscript embedding should be distanced to the𝑚number of negative embeddings {d|1 ≤ 𝑚}. Three strategies were designed to achieve the objective, which are illustrated in Figure 3: • “Target-based” optimization strategy: Minimize the distance betweendandd, and the distances betweendand {d|1≤ 𝑐 ≤ 𝑛}. Hence, whendis found to be similar, embeddings {d|1 ≤ 𝑐 ≤ 𝑛} could also be recommended. • “Source-based” optimization strategy: Minimize the distance betweendandd, and the distances betweendand {d|1≤ 𝑐 ≤ 𝑛}, so that givend, the both of the target and positive citations could be retrieved. • “Source-target-based” optimization strategy: Combining“target-based”and“source-basedstrategies, it is proposed to minimize the distances betweend, and both of the target and positive embeddings, and the distances between the target and positive embeddings. Based on the N-tuplet loss function [25], we propose three designs of multi-positive triplet objectives following the aforementioned strategies: • Multi-positive target-based triplet (mpt-tgt): • Multi-positive manuscript-based triplet (mpt-src): • Multi-positive source-target-based triplet (mpt-src-tgt): We set the maximum number of positive samples to be 3, since the number of co-citations with greater than 3 pairs is neglectable small from our datasets.Mpt-src-tgtis set to be the default objective for experiments in Section 5.3, since it is testied to be the most eective objective according to Section 5.5. Four datasets including ACL Anthology (2013 release) and three datasets generated from the DBLP corpus were adapted for experiments. The ACL Anthology corpus includes 20,405 papers with 108,729 citations, whereas the DBLP corpus contains 649,114 papers, with 2,874,303 citations. Three datasets were produced from the DBLP corpus, i.e., DBLP-1, DBLP-2, and DBLP-3, each of which includes 50,000 papers. A “biased-individuality” dataset generating strategy was adapted to produce the three DBLP datasets, by which DBLP-2 and DBLP-3 shares 20% papers (10,000) in-common to evaluate the stability on the performance of the model; whereas the DBLP-1 dataset contains completely dierent papers to DBLP-1 and DBLP-2. The complete ACL corpus was adapted for the fourth dataset. We split the datasets into train and test sets as listed in Table 1. MP-BERT4CR was developed based on Fairseq 0.4.0 [18], Gensim 2.3.0 [21], and Pytorch 1.6.0 [20]. For the baseline models, Word2Vec and Doc2Vec were implemented using Gensim 2.3.0; HyperDoc2Vec was developed based on Gensim 2.3.0; DACR was developed based on Pytorch 1.6.0 and Gensim 2.3.0; SciBERT and Specter were implemented using Hugginface 4.2.0 [26]. Adam optimizer [12] was adapted to optimize MP-BERT4CR with parameters illustrated in Table 2. The batch sizes are set to be 7 for pre-training, and 1 for ne-tuning. We run 10 iterations of pre-training and 2 iterations of ne-tuning on DBLP-1, DBLP-2, and DBLP-3 datasets; or 50 iterations of pre-training and 5 iterations of ne-tuning on the ACL dataset. The the number of negative samples is set to 4, and maximum number of positive samples is set to 3. We adapted six baseline modesl: Word2Vec (W2V) [17], Doc2Vec (D2V) [15], HyperDoc2Vec (HD2V) [8], DACR [29], SciBERT [2] and Specter [4]. We train the models with default parameters. SciBRET and Specter were ne-tuned with the abstract combined with fundamental contexts (i.e. augmented abstracts), so that the learnt vectors carrying the semantics of the abstract and the citing intents. We present the methods for generating recommendations for MPBERT4CR (MB4R thereafter) and the baselines. Recall and MAP at top 10 recommendations were reported for analyses. For MB4R, we rstly conduct dynamic sampling for the papers in the test and train set. The sampled manuscript and citation contexts are then encoded via the ne-tuned manuscript encoder or citation encoder to getquery vectorsandcitation vectors. Recommendations were selected as the top 10 citations by cosine similarities. For W2V, D2V, HD2V, and DACR, we use the same method from the original studies [8,29]. Thequery vectorsare computed by averaging the word vectors of the local context. Thecitation vectorsare computed via dierent methods. ForW2V: we use the input embedding of the citation ids ascitation vectors; for D2V: we use the trained model to infercitation vectorsfrom the content words; and forHD2V,DACR: use the output embedding of the citation ids ascitation vectors. ForSciBERTandSpecter, we generatequery vectorsby using the fundamental context sampled. The citation vectors are produced by encoding and sum-pooling the augmented abstracts. Recommendations are made by cosine similarities. 5.4.1 Single Positive. Two points could be drawn from Table 3 (𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 =1 cases). First, MB4R outperformed all the baseline models across all the datasets and metrics by signicant margins, from which MB4R’s superiority compared with baselines testied the eectiveness of the proposed neural architecture, and MB4Rfurther testied the eectiveness of the proposed multi-positive objectives. Second, the multi-positive objective function is not only helpful to identify multiple ground-truth citations, the single positive performances are also improved when comparing MB4Rto MB4R. In addition, owing to the more hierarchical transformer and dynamic sampling strategy, MB4R outperformed Sci-BERT and Specter. 5.4.2 Multiple Positive . According to the scores with multiple ground-truth citations (cases of𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 ≥1 and𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 ≥2) in Table 3, the scores for multiple positive recommendations from MB4Rremained eective comparing with the baselines and MB4R. However, as the𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 ≥2 test samples are much less than the test samples for𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 ≥1, so the ability of mpt might not be fully demonstrated, especially on DBLP-3 and ACL datasets. We will produce datasets with a higher number of positive samples in the later stage for further tests. We compare the three designs of multi-positive objectives in Table 4. First, all the three proposed multi-positive objectives (MB4R, Table 3: Recommendation Scores for Single and Multiple Positive Citations (* 𝑝 < 0.05 for paired t test against best baselines) Table 4: Comparison on Multi-Positive Triplet Objectives with Conventional Triplet on DBLP-1 MB4R, and MB4R) outperformed the best baseline (HD2V), and the original triplet objective (MB4R). Second, the two target-based strategies MB4Rand MB4R are superior than the source-based strategy (MB4R), which means that the distances between the ground-truth candidate and other positive candidates play the central role for multiple retrieving. Third, MB4Rproduced superior performances comparing with MB4Rfor case positive≥2, but very close performances for cases positive=1 and positive≥1, and hence it was set as the default. We also tested MB4R with or without dynamic sampling for an ablation analysis, it is found that the dynamic sampling mechanism can signicantly improve the performances, by comparing the resutls from𝑀𝐵4𝑅and In this study, we proposed MP-BERT4CR, a content modeling for multi-positive citation recommendations. It has the following advantages: rst, it comes with a series of multiple positive objectives to optimize the model for multi-positive recommendation; second, it can eectively recommend co-citations by leveraging the historical patterns; Third, the proposed dynamic context sampling strategy empowers the citation embeddings to carry content semantics, to further improve the performances. This research has been supported in part by JSPS KAKENHI under Grant Number 19H04116.