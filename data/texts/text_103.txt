Gradient Boosted Decision Trees (GBDTs) are widely used for building ranking and relevance models in search and recommendation. Considerations such as latency and interpretability dictate the use of as few features as possible to train these models. Feature selection in GBDT models typically involves heuristically ranking the features by importance and selecting the top few, or by performing a full backward feature elimination routine. On-the-ﬂy feature selection methods proposed previously scale suboptimally with the number of features, which can be daunting in high dimensional settings. We develop a scalable forward feature selection variant for GBDT, via a novel group testing procedure that works well in high dimensions, and enjoys favorable theoretical performance and computational guarantees. We show via extensive experiments on both public and proprietary datasets that the proposed method oﬀers signiﬁcant speedups in training time, while being as competitive as existing GBDT methods in terms of model performance metrics. We also extend the method to the multitask setting, allowing the practitioner to select common features across tasks, as well as selecting task-speciﬁc features. Gradient Boosting methods Friedman [2001] are widely used in several ranking and classiﬁcation tasks for web-scale data Zheng et al. [2008], Li et al. [2008]. GBDTs allow for eﬃcient training and inference for large datasets Ke et al. [2017], Chen and Guestrin Proceedings of the 23International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108. Copyright 2020 by the author(s). [2016]. Eﬃcient inference is of key importance for applications such as search, where real-time vending of results at web scale in response to a search query is vital. A key consideration for the models is the number of features used. A large number of features selected in the model severely impacts latency. Selecting a small number of features also allows for better model ﬁtting and helps yield explainable models. While it is generally accepted that ﬁtting a parsimonious model to the data is useful, past work on learning such models for GBDTs have been few and far between. In Ke et al. [2017], sparsity inducing penalties are used to reduce the number of trees; in Chen and Guestrin [2016], a similar technique is applied to penalize the number of leaves in each tree. One common method for feature selection in gradient boosting involves ﬁtting the model on all the features, ranking the features in the order of importance Ke et al. [2017], Chen and Guestrin [2016] and selecting the top-s, where s is a positive, predeﬁned number of features that one can handle. This kind of post-hoc thresholding is suboptimal compared to learning a sparse set of features during training itself. A second, and more often used method is backward feature elimination Mao [2004]: recursively ﬁt a model on the (leftover) set of features, and eliminate the least important feature. The second method becomes cumbersome in the case of most real world applications, which have a small number of target features and a large number of potential features to choose from. To alleviate this, Xu et al. [2014] proposed a forward feature selection method for gradient boosting, based on a sparsity-inducing penalty over the features. The resulting subroutine to select features is linear in the number of features. This is both wasteful and cumbersome in high dimensional settings where the number of features we want to use is signiﬁcantly smaller than the total number of features available. Moreover, the sparsity penalty in the algorithm does not explicitly account for the distribution of targets in the training data for each tree. The diﬀerence in variance across the trees means a sparsity penalty that works well for one tree might not work well for subsequent ones. In this paper, we help address both of the above concerns. We ﬁrst show how the forward feature selection method for GBDT needs to be modiﬁed to account for diﬀerent variances in the residuals being ﬁt, which we refer to as A-GBM (Adaptive Gradient Boosting Machine), since it adapts to the residual variance while ﬁtting successive trees. The main contribution of our work is the introduction of a scalable variant of AGBM, called GT-GBM (Group Testing GBM) that uses a group testing procedure to signiﬁcantly speed up the training procedure for GBDTs. For cases where we want to select s out of d features, we show that so long as the number of samples in a node n to split is at least the order oflog log, GT-GBM selects the optimal feature to split on. GT-GBM also enjoys com- putational speedups so long as n is Oexp. Thus, so long as the rather easy-to-satisfy slog logs. n . expslogs, condition holds, GT-GBM is guaranteed to be fast as well as accurate. This covers a wide range of real world applications. For example in web search cases, the number of samples is in the millions, number of features is in the hundreds and a few tens of features need to be selected. Another major contribution is the extension of GTGBM to the multitask setting, where our novel penalization helps us tradeoﬀ between selecting common features across tasks, as well as task speciﬁc features. By sharing some features across tasks and selecting a few task-speciﬁc features, we can achieve better performance than standard multitask learning. We experimentally show that GT-GBM matches other feature selection methods for GBDT in performance, while being signiﬁcantly faster. Results on multitask learning show the power of the ﬂexibility to select features provided by our method. GBDT based feature selection has been shown to outperform other baselines such as the L1-regularized linear models and random forests Xu et al. [2014], so we omit these redundant comparisons to those methods in this paper. Prior Work : The LARS Efron et al. [2004] and Lasso Tibshirani [1996] methods, along with variants Needell and Tropp [2009], Chen et al. [2001], Rao et al. [2015] allow for highly eﬃcient training and inference on large datasets for linear models. In the nonlinear setting, kernel methods Song et al. [2012] can be trained with methods similar to the above ones, but their computational and memory complexity typically grow super-linearly with the number of samples in the data. The method in Xu et al. [2014] (referred to as GBFS, stands for Gradient Boosted Feature Selection) is a form of forward feature selection in the GBDT setting, but the tree splitting routine(s) still takes linear time with respect to the number of features in the data. We show how to avoid this. We also make a modiﬁcation to GBFS to make the method more robust to the variances in the residuals as we ﬁt more trees into the model. Multitask learning (MTL) Caruana [1997] aims to improve model performance across multiple “tasks" by learning joint representations. Such methods are useful in cases where there is not enough data to train individual models, as in the case of neuroscience Rao et al. [2013] or where there are similarities across tasks Chen et al. [2010], Yang et al. [2009]. Work on MTL has focussed on linear models Maurer et al. [2013], where novel sparsity-aware penalties have been proposed to share models, and neural networks (Collobert and Weston [2008] for languages for example); the former being too restrictive in web search and recommendations domain, and the latter not lending itself well to real-time inference. We formally set up the problem we intend to solve and introduce the GBFS procedure of Xu et al. [2014] in Section 2, and the variance adaptive variant of the same. In Section 3, we introduce our multitask learning method for forward feature selection. In Section 4 we derive a scalable method for forward feature selection in GBDT, and provide theoretical performance guarantees. We conduct extensive experiments in Section 5, and conclude the paper in Section 6. Let (x, y)be a dataset of m samples, with x∈ X ⊂ R. yis a T− dimensional vector (for the multitask case). Our aim is to train a GBDT model f(X) → Y, by using a small subset of features of size s  d. We denote by [d] the set {1, 2, ··· , d}. 1{C} is the indicator function, taking the value 1 if condition C is satisﬁed, 0 otherwise. Given µ > 0, Xu et al. [2014] proposed the GBFS method, that penalizes the selection of new features via an additive penalty. Let h correspond to a tree, and Ω ⊂ [d] be the set of features used by the model, and gbe the residual. At iteration k, GBFS solves h= arg min(g− h(x)) with H being the space of trees we are optimizing over. (1) can be solved by modifying the CART algorithm, which builds the tree by choosing the split to minimize the square error loss L. At each node, one chooses the best split among features j ∈ [d] and split points s∈ {x, i ∈ [m]} that minimizes L(j, s) = SSE(j, s) + SSE(j, s) + µ1 We have used the shorthand 1to denote the indicator function for the event that feature j has not been previously used. SSE(j, s) =(y− ¯y)1 {x< s} and are the sum of squared errors for left and right child if we split at feature j and split points s. ¯y= the corresponding node. Adaptive Gradient Boosted Feature Selection : When optimizing to choose h, the value of the objective function in the root of the tree being built may have high variance across trees. Consequently, a penalty parameter µ that worked well until iteration k−1 might not be good for iteration k. Picking a good penalty parameter µ in this case becomes challenging, since we are using the same parameter for feature selection across all boosting rounds. To alleviate this situation, we propose to scale the loss function being used to ﬁt each tree to account for the current tree root variance. That is, we modify L to be ˜L(j, s) =SSE(j, s) + SSE(j, s)+ µ1 where SSE=P(y− ¯y), yis the label in the current tree root. We now only need to choose µ ∈ [0, 1] since the scaled split criterionis always ∈ [0, 1]. More importantly, this variance scaling ensures that the behavior of µ remains stable across each ﬁtting round, avoiding the alternative of potentially “re-tuning" µ for each boosting round. We refer to this method as A-GBM (the ‘A’ referring to adaptive), since the method adapts to the variance on a per-tree bases. A-GBM training proceeds exactly like GBFS, except for the scaling part. We refer the interested reader to Appendix A for the pseudocode. The above modiﬁcation that adapts to the data variance as we grow the model becomes more crucial in the multitask learning setting, where we now have T diﬀerent but related tasks. Let t ∈ [T ] denote the task id, and let the data for task t be (x, y), i ∈ [m] and the corresponding features be f, j ∈ [d]. For ease of presentation, we assume that all tasks have the same number of features d. In the case where the tasks have diﬀerent features, we can ‘zero-pad’ the data and since there is no variance along these features, they will not be considered for selection in the GBDT model. As in standard MTL, we can form groups of features, where each group is a single feature grouped across tasks Maurer et al. [2013]. Then we have d groups of features G= {f, t = 1, ., T}, j = 1, 2, .., d. Assuming the tasks are related, grouping the features in this manner helps us learn a joint set of features that are useful across all tasks. However, this constraint might be too restrictive: we would like to account for slight variations across tasks, and have the ﬂexibility to select task-speciﬁc features as well. To this end, we propose to use a group sparse penalty (only penalize if the feature is from a previously unused group of features, see the formula below for details) + sparse penalty for MTL. Note that now, the function to be ﬁt depends on the task as well as the feature, giving: ˜L(j, s) =SSE(j, s) + SSE(j, s)(2) Where Ωis the set of features that have been selected across all tasks, and Ωis the set of features selected for task t, t ∈ [T ]. µ, µare respectively the common group sparsity parameter for all the tasks and the task speciﬁc sparsity parameter. The pseudocode for this method is presented in Algorithm 1. Using a combination of the group sparse and sparse penalizations has been shown to be eﬀective in multitask learning settings for linear regression and classiﬁcation Rao et al. [2013], Simon et al. [2013]. To the best of our knowledge, this has not been proposed before in the tree learning setting. The methods described above end up having to compute the SSE(j, s) and SSE(j, s) functions deﬁned previously for all the features in the dataset and for each split to be performed while ﬁtting a tree. This procedure is linear in the number of features d, and the number of samples n per node where the split is being computed. For many real world applications in web search and recommendations, the total number of feature is large while the number of feature used by the model is signiﬁcantly smaller. In these cases, Algorithm 1 Pseudocode for Multitask A-GBM Require: Data {x, y}, i ∈ [m], t ∈ [T ], shrinkage , iterations N, tree growth parameter α, group penalty parameter 0 ≤ µ< 1, individual task penalty parameter 0 ≤ µ< 1, also µ+ µ< 1 1: for t = 1, 2, . . . T do and selected feature set Ω= ∅, Ω= ∅ 3: end for 4: for k = 1, 2, . . . N do {{x, g}, i ∈ [m]} and loss function (2) 12: end for 13: Output H, Ω∀ t ∈ [T ] and Ω we expect that checking all the d features is not only time consuming but also redundant. If we can quickly identify those small number of s good features without checking them all during each node split, training time will be reduced greatly. We address this now. The idea is to compare groups of randomly selected features and perform a binary search to eliminate the set of features that are relatively uninformative. Random selection helps reduce the bias in the ordering of the features. At each time we can eliminate half of features in this way. This depends on a key consideration: we require a metric that can be computed eﬃciently on a group of features, and one that is also indicative of the presence of an important feature in the group. An ineﬃcient method will not yield computational gains, and a non-indicative metric is not going to yield an accurate solution. Suppose we have a function GT(G,M) that takes in a subset of features G ⊂ [d] and a subset of samples M ⊂ [m] as input arguments. Suppose the number of operations it takes to evaluate a split for this group of features is Φ(|G|, |M|): the computational complexity of this procedure depends on the number of samples as well as the number of features. We will address how to construct such a function in Section 4.2. Assuming for now we do have such a function at our disposal, we give our general procedure of group testing and binary search in GBDT. We refer to this method as GT-GBM, the “GT" referring to the group-testing scheme. The pseudocode for GT-GBM is identical to that of A-GBM (Algorithm 3) except line 3 will be replaced by the subroutine we provide below in Algorithm 2. For the multitask case, line 2 in Algorithm 1 will be replaced by the subroutine. Algorithm 2 Tree Fitting Subroutine for GT-GBM Require: (in addition to usual hyperparameters) s = desired number of features, δ ∈ (0, 1) (see Theorem 4.1 for details) 1: Check previously used feature set Ω for splitting and record the best standardized MSE. Call it l. 2: Independently generate es log() random subsets from [d] with size. If s = 1,we just select [d] . Assign these to G 3: Initialize candidate set C = ∅ 4: for Each random subset G ∈ G do 10: end for 11: check features in C for splitting and record the squared error value with penalty l+µ if the feature is not used by previous trees. 12: if l+ µ < l then In general, the number of operations for group testing and binary search in a node splitting step is Os log(s) log()Φ(d, n). Our aim is to construct a function GT() such that Φ(d, n)  O(nd). We do this as follows: given a group of features and the samples to make the split, we sum the features up to obtain a new “pseudo-feature”. We will then test this “pseudofeature” for a split point in a fashion identical to the usual tree-splitting procedure in A-GBM. For speeding up this computation, we can compute the preﬁxedsum Cormen et al. [2009] of all the features in data. Φ(d, n) now is n log(n) for sorting the pseudofeature and the check for splitting. Comparing GT-GBM with the usual procedure of GBDT, we will gain a boost in We will standardize feature values by subtracting the min value and dividing the max value, so that all feature values are within [0,1] after computing the preﬁxed sum for features in each random subset and storing the result in one-pass, getting the "pseudofeature" value will just be O(1) Table 1: Complexity comparisons between A-GBM (and hence GBFS) and GT-GBM. P and T refer to the precomputation and training phases respectively. training speed if (see table 1 for details) which is easy to satisfy in real world applications. More detailed complexity comparision is the following. The main preprocessing of data for training A-GBM or any other sort-based GBDT algorithm involves getting and storing the sorted value pairs of features and targets. This takes O ((n log n) d) operations and needs O (nd) space. GT-GBM, however, does not need to precompute the sorted value pairs since target values will be sorted based on the pseudofeature during binary search and split. Instead, it calculates and records the preﬁxed sum for each of O (s log s) random subset of features with size d/s. So the precomputation for GT-GBM takes O ((s log s) nd/s) = O ((n log s) d) time and space. With a bit more space used during precompute, GT-GBM needs O (n) instead of O (nd) space since sorted value pairs doesn’t need to be stored and passed to child nodes during growing the tree. Table 1 shows this comparison. If s = 1, then the important feature will be in either Gor G. All other features will act as random noise. Intuitively the procedure will select the group that contains the relevant feature with high probability as long as it is highly correlated with the target. This process recurses until we ﬁnd the important feature. If there are multiple relevant features in the same group, however, their eﬀects can cancel each other out. An idea then is to generate several random subsets of features and apply our GroupTest to these subsets, as we do in Algorithm 2 (line 2). If a subset contains only one of the important features, then it reduces to the case for one feature and we can ﬁnd that feature with high probability. The following result bounds this probability as a function of the number of subsets generated: Theorem 4.1. Suppose that there are s important features. To ensure that for every important feature there is a random subset that only cover this feature with probability 1 − δ, it is suﬃcient to generate p random subsets of features, where p ≥ es logand e = 2.71.. is the base of the natural logarithm. We refer the reader to Appendix B.2 for the Proof. Next, we show that the method we proposed is guaranteed to recover the correct set of features with high probability, under mild asumptions. Theorem 4.2. Suppose X = (X, . . . , X) are independent of each other, and 0 ≤ X≤ 1, with nonzero variance. Let B= Var(X+ .. + X). Assume limB= ∞. Suppose there is an unknown subset S⊂ [d], |S| = s, such that Y =P µ+f(X)+, where µ = EY is the population mean and  is noise (mean 0, bounded and independent of all other variables). fs are unknown univariate monotonic functions with Ef(X) = 0. Suppose at a node we have n i.i.d. samples. Then for δ ∈ (0, 1), if d ≥ dand GTGBM ﬁnds the best split feature with probability at least 1−δ., where Cand dare positive constants that only depend on the ﬁxed unknown functions f, i ∈ S. Note that the assumptions made above are based on Sparse Additive Models Ravikumar et al. [2009], and encompass a wide variety of practical settings. Proof Sketch. Recall the split criterion of CART algorithm. For a split variable Z (a feature or the pseduo-feature in GTGBM that represents a group of variables) and threshold t, the criterion is to minimize The population split criterion (corresponds to when we have inﬁnite amount of data) is to minimize L(Z, t) = E[(Y − E [Y |Z < t])1{Z < t} For an important feature index i ∈ S, we consider the random subset S generated in GTGBM that only covers i. Then during binary search for active feature within S, we only need to prove for the split subset S, S(assume Scontains the important index i), that minL(Z, t) < minL(Z, t) w.h.p. Let Z=PX. For the population version, we can prove L(Z, t) = EY, ∀t (no variance reduction), minL(Z, t) < EYand the diﬀerence only depends on the signal strength of fand how correlated are Zand Y . To investigate the sample split criterion, we need to quantify : (a) How the amount of variance reduced decays with the increase of |S| (Lemma B.1 states ≈≈.) (b) How the uniform approximation error between empirical and population split criterion decays with n. (Lemma B.2 statesq sup|L(Z, t) − L(Z, t)| = O()) Combining the above gets us the result. We refer the reader to Appendix B.5 for the detailed proof. Combining equations (3) and (4) in Theorem 4.2 show that so long as the number of samples n at a node to split satisﬁes slog logs. n . expslogs, GT-GBM will ﬁnd the correct feature to split signiﬁcantly faster than GBFS. This condition is easily satisﬁed in most real world applications, where the number of samples and the number of features are large, and relatively shallow trees are used to train the models which is the case for gradient boosting procedures. An experiment on synthetic data shows the bound in Theorem 4.2 is quite conservative. Figure 1 indicates that the dependence between n and d is potentially linear. We leave the tightening of the bound for future work. For the experiment, we ﬁx s = 3, δ = 0.1 and generate y = 2x− 3 ∗ 2+ log(1 + x) +  where x, x, xand other irrelevant features are i.i.d uniform on [0, 1] and  ∼ N(0, 1). We replicate each experiment 50 times and calculate the ratio of success (success means the candidate feature set found by GTGBM contains both x, x, x). Figure 1: Average success rate as a function of ambient dimension d and sample size n. Dark regions indicate values near 1, and light closer to 0. Note the near linear dependence between n and d. First, we extensively test A-GBM and GT-GBM on publicly available datasets. Next, we apply the methods to proprietary datasets, and evaluate GT-GBM for ranking and multiclass classiﬁcation tasks. Results on an internal dataset for classiﬁcation are provided in Appendix E. We compare our methods with other GBDT feature selection methods, as that is the main focus in this paper. We compare A-GBM and GT-GBM methods with GBFS Xu et al. [2014] and the GBDT method with ranking all features, and retraining with K most important features (referred to as GBDT-topK here). For GBDT-topK, we use LightGBM Ke et al. [2017] and use it’s default feature scoring mechanism to rank the features by importance. We train the models on the Gisette, Epsilon, and the Flight Delaydatasets. They are all for classiﬁcation tasks. For the latter, we use the variant with 100K samples, and the same script to generate the data as provided in the repository. Details for all the datasets are provided in Table For each of the methods we use, we tune all the parameters on a held out validation set, and report the results on a separate test set. For GBFS, AGBM and GT-GBM, we choose the corresponding µ that achieves the best performance on the validation dataset, regardless of the number of features they select. For this reason, we end up picking diﬀerent number of features for diﬀerent methods. For GBDT-topK, we train on all the features, and pick top K features, where K is the maximum of the number of features picked by the 3 other methods. We then retrain the model with these K and report results on the test set. Optimal hyperparameter values to reproduce our results are provided in Appendix C. Speed and Performance Comparisons : First, we show that the proposed methods perform either comparatively, or outperform the baselines. Table 3 shows the performance metrics for the methods we compare, indicating that there’s very little performance loss over the baseline methods. For the sake of https://archive.ics.uci.edu/ml/datasets/ Gisette https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/binary.html https://github.com/szilard/benchm-ml completeness, we also report the results obtained from training the GBDT model on all the features, with no feature selection in Appendix D. Furthermore, the ﬂight delay dataset has a large number of categorical features, and a large number of data points compared to features. Even in this case, GT-GBM outperforms the other baselines. Table 3: Performance comparison on various datasets. Note that GT-GBM consistently picks fewer features while still outperforming or competing with A-GBM and GBFS. As expected, GBDT-topK suﬀers from poor approximation as a result of picking top K features after ﬁtting on the whole set of features. Next, we compare the training time for all the methods in Figure 2. The Figure shows that GT-GBM is signiﬁcantly faster than the competing methods on all the datasets, by an order of magnitude for Gisette, and two orders of magnitude for Epsilon. The gap is smaller for Flight dataset, since the ratio of the number of samples to the number of features is much smaller. Evaluating Correlations : In Figure 3 we show that the features selected by the GT-GBM methods are less correlated than those picked by ﬁtting all the features, and selecting the top K (via the feature importance scores obtained via GBDT). We ﬁx K = 20, and plot the Pearson correlation coeﬃcient for the Gisette data. When the number of features we want to select is constrained, it is important to select features that are as uncorrelated from each other as possible, as this allows for maximal information gain. Figure 3: Pairwise pearson correlations for the top 20 features selected by GBDT-topK (left) and GT-GBM (right) methods. The lighter squares indicate values closer to 0. Next, we apply the GT-GBM and A-GBM methods on proprietary datasets. We use aggregated data sets containing only de-identiﬁed data from search logs of an e-commerce engine (i.e. they don’t include personally identifying information about individuals in the dataset). We make use of 4 datasets across 2 tasks. C1 and C2 are classiﬁcation tasks, and R1 and R2 are ranking tasks. Results on C1 and C2 are in Appendix E, since the previous experiments already evaluated GT-GBM on classiﬁcation data. In all the cases below, we choose 20 as the desired number of features in our models so as to illustrate an example where extreme latency constraints are enforced. The ranking task is akin to the standard relevance task in a search engine: in response to a query, and a set of items that are matched, the job is to rank the items in the order of relevance. Since this is a ranking task, we report the Mean Reciprocal Rank (MRR) for the datasets. Again, we see that GT-GBM is competitive with the other methods (while being faster) (Table 4). Table 4: Comparison of various methods on the Ranking tasks (R1 and R2). Similar to the classiﬁcation setting, GT-GBM is competitive with the baselines, and achieves the same result in signiﬁcantly less time. Finally, we test the multitask variant of our algorithm on two other proprietary datasets: M1 and M2. M1 is a classiﬁcation dataset that categorizes a query into 3 categories (head, torso, tail). The idea is to see if there are highly predictive features in one task that can be used in other tasks where there is a lack of data. At the same time, there might be task-speciﬁc features that are useful, which our model accounts for as well. M2 is a dataset that uses query-items across countries, similar to the dataset used in Chapelle et al. [2010]. Due to space constraints, details about M2 and results are provided in Appendix F. We tune the two parameters µand µwhich control the proportion of common active features and taskspeciﬁc important features via cross-validation, and report the results on a held out test set. In Figure 4 (and 5 in Appendix), SingleTask refers to training the model on the combined training data in the single task mode with the task number used as a categorical feature. Multitask_GroupSparse refers to the Multi- Figure 2: Timing comparisons of all the methods on various datasets, Gisette (top), Epsilon (middle), and Flight (bottom). In all the cases, we see that GT-GBM outperforms the other methods, by orders of magnitude. We plot the tree root variance (left), RMSE (middle) and Area under ROC curve (right) for all datasets as a function of time. task model we developed, but forcing all the features across tasks to be the same, which is the standard multitask learning framework (eﬀectively µ= 0). Multitask refers to the model that has the full ﬂexibility, where both sparse and group sparse parameters can be nonzero. “Total" refers to the overall metric, after taking a weighted average of the scores across the tasks, weighted proportional to the number of samples in each task. The ﬁgures show that the Multitask model outperforms both the other methods, across all tasks as well as overall. Figure 4: Performance on M1, for Area Under Precision-Recall curves. We see that having the ﬂexibility to choose both task speciﬁc and common features across tasks helps boost performance. T1, T2, T3 refer to the three query level tasks respectively. In this paper, we developed a feature selection procedure for gradient boosted decision trees that adapts itself to the variations in the data, and built a scalable version of the same. The scalable algorithm we developed uses a novel group testing and binary search heuristic to achieve signiﬁcant speedups over baseline methods, with almost no change in performance. We provided theoretical performance guarantees that establish both the speedup and correctness, and empirical results corroborating the same. We also developed a multitask variant of this algorithm, that is ﬂexible enough for the practitioner to transition between choosing the same set of features and training independent models across tasks. Experiments on multiple ranking and classiﬁcation datasets show that the developed method compares to state of the art methods in performance, while at the same time takes signiﬁcantly less time to train.