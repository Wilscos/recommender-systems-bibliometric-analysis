Cross-domain recommendation (CDR) aims to provide better recommendation results in the target domain with the help of the source domain, which is widely used and explored in real-world systems. However, CDR in the matching (i.e., candidate generation) module struggles with the data sparsity and popularity bias issues in both representation learning and knowledge transfer. In this work, we propose a novel Contrastive Cross-Domain Recommendation (CCDR) framework for CDR in matching. Specically, we build a huge diversied preference network to capture multiple information reecting user diverse interests, and design an intra-domain contrastive learning (intra-CL) and three inter-domain contrastive learning (inter-CL) tasks for better representation learning and knowledge transfer. The intra-CL enables more eective and balanced training inside the target domain via a graph augmentation, while the inter-CL builds dierent types of cross-domain interactions from user, taxonomy, and neighbor aspects. In experiments, CCDR achieves signicant improvements on both oine and online evaluations in a real-world system. Currently, we have deployed CCDR on a well-known recommendation system, aecting millions of users. The source code will be released in the future. • Information systems → Recommender systems. recommendation, contrastive learning, cross-domain recommendation, matching Personalized recommendation aims to provide attractive items for users according to their proles and historical behaviors, which has been widely implemented in various elds of our lives. Real-world large-scale recommendation systems usually adopt the classical two-stage architecture containing ranking and matching (i.e., candidate generation) [4,42,54]. The matching module focuses more on the eciency and diversity, which rst retrieves a small subset of (usually hundreds of) item candidates from the million-level large corpora. Next, the ranking module gives the specic ranks of items for the nal display. Figure 1: An example of CDR in matching. With the increase of recommendation scale and the expansion of recommendation scenarios, real-world recommendations usually need to bring in additional data sources (i.e., domains) as supplements to improve their content coverage and diversity. These cold-start items of new data sources only have very few user behaviors at their warm-up stage. Hence, it is dicult to recommend these cold-start items appropriately. Cross-domain recommendation (CDR), which aims to make full use of the informative knowledge from the source domain to help the target domain’s recommendation [5], is proposed to solve this issue. EMCDR [23] is a classical CDR method, which focuses on building user mapping functions via aligned user representations in the source and target domains. CoNet [14] proposes another approach that jointly models feature interactions in two domains via a cross connection unit. However, existing CDR methods often heavily rely on aligned users for crossdomain mapping (e.g., EMCDR), ignoring other rich information in recommendation such as taxonomy. It will harm the knowledge transfer between dierent domains, especially in cold-start scenarios. Moreover, lots of CDR methods are designed for ranking that consider complicated cross-domain user-item interactions (e.g., CoNet), which cannot be directly adopted in matching due to the online eciency. CDR in the matching module should consider not only recommendation accuracy, but also diversity and eciency. In this work, we aim to improve the matching module’s performance on new (few-shot or strict cold-start) domains via the CDR manner. Fig. 1 shows an illustration of this task. Precisely, CDR in matching mainly has the following three challenges: (1) How to address the data sparsity and popularity bias issues of CDR in matching? Real-world recommendation usually suers from serious data sparsity issues when modeling the interactions between million-level users and items. Moreover, these sparse interactions are even highly skewed to popular items with high exposure owing to the Matthew eect [26], which makes hot items become hotter. These two issues inevitably harm the representation learning of cold-start and long-tail items, whose damages will even be multiplied in matching where all items should be considered. (2) How to conduct more eective knowledge transfer for the (coldstart) target domain with few user behaviors? As stated above, conventional CDR methods strongly depend on aligned users and their behaviors. The performance of CDR in matching will be greatly reduced, if most users and items have few interactions and models cannot learn reliable representations in cold-start domains. Moreover, other heterogeneous information (e.g., taxonomy) should also be fully considered in CDR to bridge dierent domains. We should build more eective and robust cross-domain knowledge transfer paths to well learn both popular and long-tail objects. (3) How to balance the practical demands of accuracy, diversity and eciency of CDR in matching? Online eciency requirements need to be strictly followed. Moreover, matching is more responsible for the diversity than ranking, for it determines the inputs of ranking. A good CDR matching model should comprehensively transfer user diverse preferences via multiple paths to the target domain. To address these issues, we propose a novelContrastive CrossDomain Recommendation (CCDR)to transfer user preferences in matching. Specically, we build two global diversied preference networks for two domains, containing six types of objects to enhance diversity and cross-domain connections. We conduct a GNN aggregator with a neighbor-similarity based loss on heterogeneous interactions to capture user diverse interests. To strengthen the cross-domain knowledge transfer, we design theintra-domain contrastive learning (intra-CL)andinter-domain contrastive learning (inter-CL)in CCDR. The intra-CL conducts an additional self-supervised learning with sub-graph based data augmentations to learn more reliable representations for matching in the target domain. In contrast, the inter-CL designs three contrastive learning tasks focusing on the cross-domain mapping between aligned users, taxonomies, and their heterogeneous neighbors. The mutual information maximization with dierent types of objects multiplies the eectiveness of cross-domain knowledge transfer. Finally, all additional CL losses are combined with the original CDR losses under a multi-task learning (MTL) framework. We conduct a cross-domain multi-channel matching to further improve the diversity in online. CCDR has the following three advantages: (1) The intra-CL brings in self-supervised learning for long-tail users and items, which can alleviate the data sparsity and popularity bias issues in matching. (2) The inter-CL introduces new CL-based cross-domain interactions, which enables more eective and robust knowledge transfer for CDR in matching. (3) The diversied preference network, multiple CL tasks, and the cross-domain multi-channel matching cooperate well to capture user diverse preferences, which meets the requirements of diversity and eciency in online system. In experiments, we compare CCDR with competitive baselines on real-world recommendation domains, and achieve signicant improvements on both oine and online evaluations. Moreover, we also conduct some ablation tests and parameter analyses to better understand our model. The contributions are concluded as: •We propose a novel contrastive cross-domain recommendation for CDR in matching. To the best of our knowledge, we are the rst to conduct contrastive learning to improve both representation learning and knowledge transfer in CDR. •We propose the intra-CL task with a sub-graph based data augmentation to learn better node representations inside the single domain, which can alleviate the data sparsity issue in CDR of matching. •We also creatively design three inter-CL tasks via aligned users, taxonomies, and their neighbors in our diversied preference networks, which enable more eective and diverse knowledge transfer paths across dierent domains. •We achieve signicant improvements on both oine and online evaluations. CCDR is eective, ecient, and easy to deploy. Currently, it has been deployed on a real-world recommendation system, aecting millions of users. Matching in Recommendation.The matching (i.e., candidate generation) module aims to retrieve a small subset of (usually hundreds of) items from large corpora eciently [42]. Conventional recommendation matching algorithms often rely on information retrieval based models [30]. Recently, embedding-based retrieval [1,15] is also widely used in practical systems with the help of fast retrieval servers [16]. Due to the need for eciency, embeddingbased methods usually adopt a two-tower architecture, which conducts two dierent towers for building user and item representations separately. Dierent feature interaction methods such as FM [29], Youtube candidate generation [4], DeepFM [8], AutoInt [32], ICAN [42], AFN [3], DFN [40], and AFT [10] could be used in these towers. In contrast, tree-based matching models [54,56] give another way to address the matching problem with structured item trees. Graph-based matching models [41] are also proposed to learn user/item representations. However, few works focus on the CDR in matching, which often exists in practical recommendations. Cross-domain Recommendation.Cross-domain recommendation attempts to learn useful knowledge from the source domain to help the target domain’s recommendation [5]. EMCDR [23] is a classical embedding mapping approach, which builds the mapping function via aligned users’ representations. SSCDR [17] designs a semi-supervised manner to learn item mapping based on EMCDR. In contrast, CoNet [14] is another classical type of CDR method that uses the cross connection unit to model domain interactions. Zhao et al. [49]combines items of both source and target domains in one graph to learn representations. Neural attentive transfer [6], dual transfer [19,53], source-target mixed attention [25], dual generator [48], and meta-learning [55] are also proposed for CDR. Additional review information is also used to enhance the attentive knowledge transfer [50]. Some works explore non-overlapping CDR [46]. However, most of these CDR models are specially designed for the ranking module (which involve user-item interactions in cross-domain modeling). ICAN [42] is the most related work, which captures eld-level feature interactions to improve matching in multiple domains. In CCDR, we introduce several novel CL tasks. To the best of our knowledge, we are the rst to conduct CL to jointly improve representation learning and knowledge transfer in CDR. Contrastive Learning.Contrastive learning (CL) is a representative self-supervised learning (SSL) method, which aims to learn models by contrasting positive pairs against negative pairs [9]. Contrastive predictive coding (CPC) [24] designs the widely-used InfoNCE loss. MoCo [11] builds a large dynamic dictionary with a queue and a moving-averaged momentum encoder. SimCLR [2] designs a simple contrastive learning framework with a composition of data augmentations and projectors for CL. BYOL [7] relies on its online and target networks, which iteratively bootstraps the outputs of a network to serve as targets for learning. Qiu et al. [28] proposes a graph contrastive coding for GNN pre-training. CL in recommendation.Recently, SSL and CL are also veried in recommendation [47]. S-Rec [52] builds contrastive learning tasks among items, attributes, sentences, and sub-sentences in sequential recommendation. UPRec focuses on user-aware SSL [39]. Xie et al. [44]designs an adversarial and contrastive VAE for sequence modeling. Moreover, CL has also been used in disentangled recommendation [22,51], session modeling [38], social recommendation [45], and cold-start recommendation [35]. For graph-based CL, Wu et al. [37]introduces embedding, node, edge dropouts to graphbased recommendation. Diering from these works, we build three CL tasks to facilitate the user preference transfers between dierent domains in cold-start cross-domain recommendation. In this work, we propose CCDR to enhance the cross-domain recommendation in matching via contrastive learning. In this section, we rst describe our task and the overall framework of CCDR (Sec. 3.1). Second, we introduce the diversied preference networks and the single-domain GNN-based aggregator (Sec. 3.2 and Sec. 3.3). Next, we introduce the intra-domain and inter-domain contrastive learning (Sec. 3.4 and Sec. 3.5). Finally, we combine three losses with a multi-task optimization (Sec. 3.6). The online system and deployment of CCDR will be introduced in Sec. 4. More detailed discussions are given in Appendix. CDR in matching.We concentrate on the matching module of the classical two-stage recommendation systems [4]. Matching is the rst step before ranking, which attempts to eciently retrieve hundreds of items from million-level item candidates. It cares more about whether good items are retrieved (often measured by hit rate), not the specic top item ranks which should be considered by the following ranking module (often measured by NDCG or AUC) [21,42]. The CDR in matching task attempts to improve the target domain’s matching module with the help of the source domain. Overall framework.CCDR is trained with three types of losses, including the original source/target single-domain losses, the intradomain CL loss, and the inter-domain CL loss. (1) We rst build a huge global diversied preference network separately for each domain as the sources of user preferences. This diversied preference network contains various objects such as user, item, tag, category, media, and word with their interactions to bring in user diverse preferences from dierent aspects. (2) Next, we train the single-domain matching model via a GNN aggregator and the neighbor-similarity based loss. (3) Since the cold-start domain lacks sucient user behaviors, we introduce the intra-domain CL inside the target domain to train more reliable node representations with a sub-graph based data augmentation. (4) To enhance the cross-domain knowledge transfer, we design three inter-domain CL tasks via aligned users, taxonomies, and their neighbors between two domains, which cooperate well with the diversied preference network. All three losses are combined under a multi-task learning framework. Conventional matching [4] and CDR [17,23] models usually heavily rely on user-item interactions to learn CTR objectives and crossdomain mapping. However, it will decrease the diversity of matching due to the popularity bias issue. Moreover, it does not take full advantage of other connections (e.g., tags, words, medias) besides users between dierent domains, which is particularly informative in cross-domain knowledge transfer. Therefore, inspired by [20,41], we build a global diversied preference network for each domain, considering 6 types of important objects in recommendation as nodes and their heterogeneous interactions as edges. Specically, we use item, user, tag, category, media, and word as nodes. Tags and categories are item taxonomies that represent users’ ne- and coarse- granularity interests. Media indicates the item’s producer. Words reect the semantic information of items extracted from items’ titles or contents. To alleviate data sparsity and accelerate our oine training, we also gather users into user groups according to their basic proles (all users having the same gender-age-location attributes are clustered in the same user group). These user groups are viewed as user nodes in CCDR. As for edges, we consider the following six item-related interactions: (a) User-item edge (U-I). This edge is generated if an item is interacted by a user group at least 3 times. We jointly consider multiple user behaviors (i.e., click, like, share) to build this edge with dierent weights. (b) Item-item edge (I-I). The I-I edge introduces sequential information of user behaviors in sessions. It is built if two items appear in adjacent positions in a session. (c) Tag-item edge (T-I). The T-I edges connect items and their tags. It captures items’ ne-grained taxonomy information. (d) Category-item edge (C-I). It records items’ coarse-grained taxonomy information. (e) Media-item edge (M-I). It links items with their producers/sources. (f) Word-item edge (W-I). It highlights the semantic information of items from their titles. Each edge is undirected but empirically weighted according to the edge type and strength (e.g., counts for UI edges). Comparing with conventional U-I graphs, our diversied preference network tries its best to describe items from dierent aspects via these heterogeneous interactions. The advantages of this diversied preference network are as follows: (1) it brings in additional information as supplements to user-item interactions, which jointly improve both accuracy and diversity (see Sec. 3.3.2). (2) It can build more potential bridges between dierent domains via users, tags, categories, and words, which cooperates well with the inter-CL tasks and the online multi-channel matching in CDR (see Sec. 3.5 and Sec. 4). 3.3.1 GNN-based Aggregator. Inspired by the great successes of GNN, we adopt GAT [34] as the aggregator on the diversied preference network for simplicity and universality. Precisely, we randomly initialize𝒆for all heterogeneous nodes. For a node𝑒and its neighbor𝑒∈ 𝑁(𝑁is the neighbor set of𝑒after a weighted sampling), we have𝑒’s node representation𝒆at the𝑥-th layer as: 𝑾is the weighting matrix,𝜎is the sigmoid function.𝛼represents the attention between 𝑒and 𝑒in 𝑥-th layer noted as: where𝑓 (·)indicates a LeakyReLU activation and||indicates the concatenation.𝒘is the weighting vector. Note that the𝑁is a dynamic neighbor set which is randomly generated based on the edge weight in Sec. 3.2. We conduct a two-layer GAT to generate the aggregated node representations𝒆for all nodes (𝒆and𝒆for the source and target domains). It is also not dicult to conduct other GNN models such as LightGCN [12] in this module. 3.3.2 Neighbor-similarity Based Optimization. In practical CDR scenarios, users often have fewer historical behaviors on items in (cold-start) target domains. Conventional embedding-based matching methods such like Matrix factorization (MF) [18] cannot get sucient supervised information from the sparse user-item interactions, and thus cannot learn reliable user and item representations for matching. To capture additional information from behavior, session, taxonomy, semantics, and data source aspects, we conduct the neighbor-similarity based loss [20] on the diversied preference network. As shown in Fig. 2, this loss projects all nodes into the same latent space, making all nodes similar with their neighbors. It regards all types of edges as unsupervised information to guide the training besides user-item interactions. Formally, the neighborsimilarity based loss 𝐿is dened as follows: 𝐿= −(− log(𝜎 (𝒆𝒆)) + log(𝜎 (𝒆𝒆))).(3) 𝒆is the𝑖-th aggregated node representation, and𝑒is a sampled neighbor of 𝑒. 𝑒is a randomly selected negative sample of 𝑒. We choose the neighbor-similarity based loss for the following advantages: (1)𝐿makes full use of all types of interactions between heterogeneous objects in matching, which contain signicant information from user behaviors (U-I edges), sessions (I-I edges), item taxonomies (T-I and C-I edges), data sources (M-I edges) and semantics (W-I edges). It helps to capture user diverse preferences to balance accuracy and diversity in matching. If we only consider U-I edges, this loss will degrade into the classical MF. (2) CDR in matching should deal with long-tail items.𝐿brings in additional information for long-tail items that can benet cold-start domains. (3) We conduct a cross-domain multi-channel matching strategy in online for diversity. This embedding-based retrieval strategy also depends on heterogeneous node embeddings optimized by𝐿to retrieve similar items in the (cold-start) target domain (see Sec. 4 for more details). The𝐿loss exactly ts for the online multi-channel matching, and also well cooperates with the diversied preference network and the inter-CL losses. We cannot conduct complicated user-item interaction calculations in Eq. (3), since we rely on the fast embedding-based retrieval in matching for eciency. Contrastive learning is a widely-used SSL method that can make full use of unlabelled data via its pair-wise training. In CCDR, we conduct two types of CL tasks. Theintra-domain contrastive learning (intra-CL)is conducted inside the target domain to learn better node representations, while theinter-domain contrastive learning (inter-CL)is adopted across the source and target domains to guide a better knowledge transfer. Figure 2: The neighbor-similarity loss and the intra-CL loss. In intra-CL, we conduct a sub-graph based data augmentation for each node aggregation, which could be regarded as a dynamic node/edge dropout in classical graph augmentation [37]. Precisely, for a node𝑒, we sample two neighbor set𝑁and𝑁to conduct the GNN aggregation, and receive two node representations𝒆and 𝒆.𝒆is regarded as the positive instance of𝒆in intra-CL, with a dierent sub-graph sampling focusing on dierent neighbors of𝑒. Similar to [2], we randomly sample from other examples𝒆in the same batch𝐵of𝑒to get the negative samples𝑒. We do not use all examples in𝐵as negative samples for eciency. In this case, the popularity bias is partially solved [36]. Formally, we follow the InfoNCE [24] to dene the intra-CL loss 𝐿as follows: 𝐿= −logexp(sim(𝒆, 𝒆)/𝜏)Íexp(sim(𝒆, 𝒆)/𝜏).(4) 𝑆indicates the negative samples of𝑒in𝐵.𝜏is the temperature. sim(𝒆, 𝒆)measures the similarity between𝒆and𝒆, which is calculated as their cosine similarity. With the intra-CL loss, longtail nodes can also get training opportunities via SSL. The inter-CL aims to improve the knowledge transfer across dierent domains via various types of nodes and edges in the diversied preference network. Precisely, we design three inter-domain CL tasks via aligned users, taxonomies, and neighbors as in Fig. 3. 3.5.1 User-based Inter-CL. Most conventional CDR methods [23] take aligned users as their dominating mapping seeds across domains. We follow this idea and conduct a user-based inter-CL task. ttTaxonomy-based inter-CL: e eeNeighbor-based inter-CL: Figure 3: Three inter-CL tasks across dierent domains. Each user𝑢has two user representations𝒖and𝒖in the source and target domains learned in Sec. 3.3. Although users may have dierent preferences and behavior patterns in two domains, it is still natural that the source-domain representation𝒖should be more similar with its target-domain𝒖than any other representations 𝒖. We dene the user-based inter-CL loss 𝐿as follows: 𝐿= −logexp(sim(𝒖, 𝒖)/𝜏)Íexp(sim(𝒖, 𝒖)/𝜏).(5) 𝑆is the sampled negative set collecting from all other users except 𝑢. sim(·, ·) indicates the cosine similarity. 3.5.2 Taxonomy-based inter-CL. Diering from some classical CDR methods [17], CCDR builds a diversied preference network that introduces more bridges across dierent domains. We assume that the same tag/category/word in dierent domains should have the same meanings. Hence, we design a taxonomy-based inter-CL similar to the user-based CL. we take the aggregated node representation pair (𝒕,𝒕) of the same taxonomy𝑡in two domains as the positive pair, where 𝑡could be tags, categories, and words. We have: 𝑆is the sampled negative set of𝑡from all other taxonomies with the same type. We can set dierent temperatures for taxonomies and users if we want to sharpen the dierences of some types.𝐿 functions as a supplement to the original user-based mapping. 3.5.3 Neighbor-based inter-CL. Besides the explicit alignments of users and taxonomies across domains, there are also some essential objects such as items that do not have explicit mapping. We aim to bring in more implicit cross-domain knowledge transfer paths between unaligned nodes in two domains. We suppose that similar nodes in dierent domains should have similar neighbors (e.g., similar items may have similar users, taxonomies, and producers). Hence, we propose a neighbor-based inter-CL, which builds indirect (multi-hop) connections between objects in dierent domains. Precisely, we dene𝐸as the overall aligned node set (including users, tags, categories, and words). The neighbor-based inter-CL loss𝐿is formalized with all aligned nodes𝑒∈ 𝐸and𝑒’s neighbor set 𝑁in the target domain as follows: 𝐿= −logÍexp(sim(𝒆, 𝒆)/𝜏).(7) In𝐿, for an aligned node’s representation𝒆in the source domain, its target-domain neighbor’s representation𝒆is the positive instance, while other target-domain representations𝒆are negative. It is reasonable since related objects should be connected in the diversied preference network and learned to be similar under the neighbor-similarity based loss in Eq. (3). It is also convenient to extend the current positive samples𝑒∈ 𝑁to multi-hop neighbors for better generalization and diversity in CDR. This neighbor-based inter-CL greatly multiplies the diversied knowledge transfer paths between two domains, especially for the cold-start items. For example, through the𝑡𝑎𝑔→ 𝑡𝑎𝑔→ 𝑖𝑡𝑒𝑚 path, the cold-start𝑖𝑡𝑒𝑚’s representation in the target domain can be directly inuenced by fully-trained representations in the source domain. Moreover, the similarities between dierent types of source-domain node representations and the target-domain item representations are directly used in the online multi-channel matching for diversied retrieval, which will be discussed in Sec. 4. Finally, we combine all three CL losses from aligned user, taxonomy, and neighbor aspects to form the inter-CL loss 𝐿as: Following classical CL-based recommendation models [45], we also conduct a multi-task optimization combining the source-domain matching loss𝐿, the target-domain matching loss𝐿, the intraCL loss 𝐿, and the inter-CL loss 𝐿as follows: 𝜆, 𝜆, 𝜆, 𝜆are loss weights set as 1.0,1.0,1.5,0.6 according to the grid search (see Sec. 5.7 for more details). We have deployed CCDR on the cold-start matching module in a well-known recommendation system named WeChat Top Stories. A good CDR-based cold-start matching module should have the following key characteristics: (1) making full use of user behaviors and item features in the source and target domains, (2) capturing user diverse preferences from dierent aspects, and (3) balancing accuracy, diversity and eciency. To achieve these, we propose a new cross-domain multi-channel matching in online. Specically, we conduct six channels including user, item, tag, category, media, and word channels to retrieve items in the target domains via node representations learned by Eq. (9). We rely on the user historical behavior sequence𝑠𝑒𝑞 = {𝑑, 𝑑, · · · , 𝑑}to capture user’s interests, where𝑑is the𝑖-th clicked item and𝑛is the max length. In the item channel, we directly use the node representations of all items in𝑠𝑒𝑞to retrieve similar items in the target domain. Formally, we dene the score𝑠of the𝑖-th target-domain item¯𝑑 in the item channel as follows: 𝑠=𝑠𝑖𝑚(¯𝒅, 𝒅) × 𝑠𝑎𝑡𝑖𝑠 𝑓× 𝑟𝑒𝑐𝑒𝑛𝑐𝑦× 𝑧(𝑖, 𝑗).(10) 𝑠𝑖𝑚(¯𝒅, 𝒅)is the cosine similarity between the clicked item𝑑in user historical behaviors and the item candidate¯𝑑in the target domain, where¯𝒅and𝒅are aggregated item embeddings trained by Eq. (9).𝑠𝑎𝑡𝑖𝑠 𝑓measures the posterior user satisfaction on𝑑, which is roughly calculated as the complete rate of item contents.𝑟𝑒𝑐𝑒𝑛𝑐𝑦 models the temporal factors of historical items, which decays exponentially from the short term to the long term (𝑟𝑒𝑐𝑒𝑛𝑐𝑦=0.95). For online eciency, each item in𝑠𝑒𝑞only recommends its top 100 nearest items.𝑧(𝑖, 𝑗)equals 1 only if the target-domain item¯𝑑appears in the top 100 nearest items of𝑑, and otherwise𝑧(𝑖, 𝑗) =0. We pre-calculate the similarities and index the top nearest items for all nodes in oine to further accelerate the online matching. To capture user diverse preferences from dierent aspects, we further conduct the tag, category, media and word channels similar to the item channel. Taking the tag channel for instance, we build a historical tag sequence𝑠𝑒𝑞= {𝑇,𝑇, · · · , 𝑇}according to the item sequence{𝑑, 𝑑, · · · , 𝑑}, where𝑇is the tag set of𝑑. All tags in𝑠𝑒𝑞retrieve top 100 nearest items in the target domains as candidates. Similar to Eq. (10), the score of the𝑖-th target-domain item¯𝑑in the tag channel 𝑠is dened as follows: 𝑠=𝑠𝑖𝑚(¯𝒅, 𝒕) × 𝑠𝑎𝑡𝑖𝑠 𝑓× 𝑟𝑒𝑐𝑒𝑛𝑐𝑦× 𝑧(𝑖, 𝑗, 𝑘).(11) 𝑠𝑖𝑚(¯𝒅, 𝒕)is the cosine similarity between¯𝒅and the aggregated tag representation𝒕.𝑧(𝑖, 𝑗, 𝑘)is the tag’s indicator.𝑧(𝑖, 𝑗, 𝑘) =1 only if the tag𝑡belongs to the𝑗-th item𝑑in𝑠𝑒𝑞, and¯𝑑locates in the top 100 nearest items of𝑡. Other category, media and word channels are the same as the tag channel, generating their corresponding scores𝑠,𝑠and𝑠. As for the user channel, we directly depend on the user’s gender-age-location attribute triplet’s (i.e., the user group in Sec. 3.2) node representations to retrieve top nearest items according to the cosine similarity score 𝑠for¯𝑑. Finally, all top items retrieved by six heterogeneous channels are combined and reranked via the aggregated score 𝑠as: It is easy to set and adjust the hyper-parameters of heterogeneous channels’ weights for the practical demands and the preferences of systems. We rank top target-domain items via𝑠, and select top 500 items as the nal output of our multi-channel matching, considering both matching accuracy and memory/computation costs. We conclude the feasibility and advantages of our cross-domain multi-channel matching as follows: (1) These multiple matching channels rely on the similarities between the target-domain items and heterogeneous nodes, which is consistent with the neighborsimilarity based loss and the inter-CL losses. (2) The multi-channel matching makes full use of all heterogeneous information to generate diversied item candidates, which is essential in cold-start matching. (3) We pre-calculate the indexes for the top nearest items of all nodes, which greatly reduces the online computation costs. The online computation complexity of CCDR is𝑂 (log(600𝑛)+600𝑛) (𝑛is the length of user historical behavior). More details of online deployment and eciency are given in Appendix A.1. In this section, we conduct sucient experiments to answer the following research questions: (RQ1): How does CCDR perform against dierent types of competitive baselines on metrics of matching (see Sec. 5.4)? (RQ2): How does CCDR perform in online evaluation on real-world recommendation systems (see Sec. 5.5)? (RQ3): What are the eects of dierent components in CCDR (see Sec. 5.6)? CCDR relies on item-related taxonomy, semantic, and producer information for CDR in matching, while no large-scale public CDR dataset is capable for this setting. Therefore, we build a new CDR dataset CDR-427M extracted from a real-world recommendation system named WeChat Top Stories, which contains a source domain and two target domains. Specically, we randomly select nearly 63 million users, and collect their 427 million behaviors on 3.0 million items. We split these behaviors into the train set and the test set using the chronological order. We also bring in 187 thousand tags, 356 categories, 56 thousand medias, and 207 thousand words as additional item-related information. All data are preprocessed via data masking to protect the user’s privacy. To simulate dierent CDR scenarios, we evaluate on two target domains having dierent cold-start degrees. The rst is a few-shot target domain, where most users only have several behaviors. The second is a strict cold-start domain, which is more challenging since all user behaviors on items in the train set are discarded [27]. Following Sec. 3.2, we build three diversied preference networks for all domains separately via the train set and all item-related information. The statistics of diversied preference networks in three domains are in Table 1. Table 1: Statistics of three domains in CDR-427M. few-shot target domain 2.38M 0.39M 29.8M 10.8M 4.99M We implement several competitive baselines focusing on the matching module and cross-domain recommendation for comparisons. Classical Matching Methods. We implement three competitive matching models as baselines. We do not compare with tree-based models [54], for they cannot be deployed in cold-start domains. All user behaviors of two domains are considered in these models. • Factorization Machine (FM) [Rendle 2010].FM [29] is a classical embedding-based matching model. It captures the feature interactions between users and items for embeddingbased retrieval under the two-tower architecture [4]. • AutoInt [Song et al. 2019].AutoInt [32] is a recent method that utilizes self-attention to model feature interactions. It also adopt the two-tower architecture for matching. • GraphDR+ [Xie et al. 2021a].GraphDR [41] is an eective graph-based matching model. The single-domain model of CCDR could be considered as an enhanced GraphDR with dierences in node aggregation and multi-channel matching specially designed for CDR. We directly conduct the singledomain model of CCDR on the joint network containing both source and target domains, noted as GraphDR+. Table 2: Results of matching-related metrics on CDR-427M. All improvements of CCDR are signicant (t-test with 𝑝 < indicates that these models are based on the same single-domain model (noted as GraphDR+) in CCDR. Cross-domain/Multi-domain Methods. We also implement two representative CDR models and one multi-domain matching model as baselines. We do not compare with CDR models like CoNet [14], since they cannot be directly used in matching for eciency. • EMCDR+ [Man et al. 2017].EMCDR [23] is a classical CDR model that directly learns the embedding mapping of users between two domains. For fair comparisons, we use the same single-domain model and multi-channel matching in CCDR for learning and serving, noted as EMCDR+. • SSCDR+ [Kang et al. 2019].SSCDR [17] is regarded as an enhanced EMCDR, which adopts a semi-supervised loss to learn the mapping of items. We also follow the same settings of EMCDR to get SSCDR+. Since the strict cold-start domain has no user-item behaviors, we use aligned taxonomies to learn cross-domain mappings in EMCDR+ and SSCDR+. • ICAN [Xie et al. 2020b].ICAN [42] is the SOTA model in multi-domain matching, which is the most related work of our task. It highlights the interactions between feature elds in dierent domains for cold-start matching. Knowledge Distillation/Contrastive Learning Methods. We further propose two enhanced versions of the single-domain matching model in CCDR (i.e., GraphDR+) for more challenging comparisons. • Sub-graph CL.We build a sub-graph CL method based on GraphDR+. It considers the intra-CL loss with a sub-graph augmentation in Eq. (4) inspired by [28,37]. It can be viewed as an ablation version of CCDR without the inter-CL. • Cross-domain KD.We further propose a cross-domain knowledge distillation (KD) model. This model also follows the single-domain model of CCDR, learning the cross-domain mapping via the Hint loss [31] between aligned nodes in two domains (i.e., user, tag, category, and word). In the single-domain model of CCDR, the input dimensions of all nodes are 128, and the output dimensions are 100. We conduct a weighted neighbor sampling to select 25 and 10 neighbors for the rst and second layers’ aggregations. The edge weight is proportional to the mutual information between its two nodes to make sure that dierent types of interactions can have comparable frequencies. In online matching, we use the top 200 most recent behaviors. All graph-based models have the same online matching strategy. The batch sizes and the negative sample numbers of the intra-CL, inter-CL, and neighbor-similarity based losses are 4,096 and 10. The temperature𝜏is set to be 1. For all models, we conduct a grid search to select parameters. Parameter analyses of CL loss weights are given in Sec. 5.7. All models share the same experimental settings and multi-domain behaviors for fair comparisons. 5.4.1 Evaluation Protocols. We evaluate on the few-shot and strict cold-start domains separately. All models select top𝑁items from the overall corpora for each test instance. Following classical matching models [33,41,42], we utilize the top𝑁hit rate (HIT@N) as our evaluation metric. To simulate the real-world matching systems, we concentrate on larger𝑁as 50, 100, 200, and 500 (we retrieve top 500 items in online). We should double clarify that CCDR focuses on CDR in matching, which cares whether good items are retrieved, not the specic ranks that should be measured by the ranking module. Hence, HIT@N is much more suitable for matching than ranking metrics such as AUC and NDCG. We also evaluate the diversity via a classical aggregate diversity metric named item coverage [13]. 5.4.2 Experimental Results. From Table 2 we can observe that: (1) CCDR achieves signicant improvements over all baselines on all HIT@N in both two domains, with the signicance level 𝑝 <0.01 (the deviations of CCDR are within±0.0004 in HIT@500). It indicates that CCDR can learn high-quality matching embeddings and well transfer useful knowledge to the target domain via CL. The improvements of CCDR mainly derive from three aspects: (a) The intra-CL enables more sucient and balanced training via SSL with selected negative samples, which successfully alleviates the data sparsity and popularity bias issues. (b) The inter-CL builds interactions across dierent domains via three CL tasks, which multiplies the knowledge transfer via heterogeneous bridges. (c) The diversied preference network, CL losses, and multi-channel matching cooperate well with each other. The similarities used in online matching are directly optimized via losses in Eq. (9). (2) CCDR has large improvement percentages on the challenging strict cold-start domain (55% improvement on HIT@500 over SSCDR+), where users have no behaviors on target items. It is natural since the combination of the diversied preference network and user/taxonomy/neighbor based inter-CL tasks can transfer more diversied preferences via more cross-domain bridges. Moreover, comparing with dierent CCDR models, we nd that both intra-CL and inter-CL are eective, while inter-CL plays a more important role in CDR. We also nd that CCDR has 4.2% and 6.0% improvements on the diversity metric item coverage [13] compared to the best-performing GraphDR+ in two domains. It indicates that CCDR has better performances on the diversity via CL tasks. (3) Among baselines, we nd that ICAN performs better in the few-shot domain, while SSCDR+ performs better in the strict coldstart domain. It is because that ICAN strongly relies on the feature eld interactions between behaviors in dierent domains, which are extremely sparse or even missing in the strict cold-start scenarios. In contrast, SSCDR+ benets from cross-domain mapping. Moreover, classical matching methods such as GraphDR+ perform worse than CCDR. It implies that simply mixing behaviors in two domains may not get good performances, since the unbalanced domain data will confuse the user preference learning in the target domain. (4) Comparing with dierent CDR methods, we observe that the CL-based methods are the most eective compared to knowledge distillation (e.g., cross-domain KD) and embedding mapping (e.g., EMCDR+ and SSCDR+). It is because that (a) contrastive learning can provide a sucient and balanced training via SSL, and (b) CCDR conducts knowledge transfer via not only aligned users, but also taxonomies and neighbors. In this case, the popularity bias and data sparsity issues in the CDR part can be largely alleviated. 5.5.1 Evaluation Protocols. To verify the eectiveness of CCDR in real-world scenarios, we conduct an online A/B test on a wellknown online recommendation system named WeChat Top Stories. Precisely, we deploy CCDR and several competitive baselines in the matching module of a relatively cold-start domain as in Sec. 4, with the ranking module unchanged. The online baseline is the GraphDR+ (target) model trained solely on the target domain. In online evaluation, we focus on the following three online metrics in the target domain: (1) CTR, (2) average user duration per capita, and (3) average share rate per capita. We conduct the A/B test for 8 days, with nearly 6 million users inuenced by our online evaluation. CCDR (inter-CL) CCDR (inter-CL+intra-CL) +14.368% +6.623% +10.401% Table 3: Online A/B tests on WeChat Top Stories. 5.5.2 Experimental Results. Table 3 shows the online improvement percentages of all models. We can nd that: (1) CCDR signicantly outperforms all models in three metrics with the signicance level𝑝 <0.01. Note that all models are based on the same single-domain model in CCDR (i.e., GraphDR+). It reconrms the eectiveness of the intra-domain and inter-domain contrastive learning. We jointly consider multiple behaviors such as click, share and like to build the diversied preference network, and use a neighbor-similarity based loss to learn user diverse preferences. Hence, CCDR has improvements on dierent metrics, which reects user’s real satisfaction more comprehensively. (2) Comparing with the base model that only considers the target domain, we know that the source domain’s information is essential. Looking into the dierences among GraphDR+, Sub-graph CL (i.e., CCDR (intra-CL)), and CCDR (inter-CL), we can nd that both intraCL and inter-CL are eective in online scenarios. Moreover, CCDR models outperform GraphDR+ and Cross-domain KD, which also veries the advantages of inter-CL over simple multi-domain mixing and cross-domain knowledge distillation in knowledge transfer. In this subsection, we further compare CCDR with its several ablation versions to show the eectiveness of dierent CL tasks. Table 4 displays the HIT@N results on both few-shot and strict cold-start domains. We nd that: (1) Both intra-CL and inter-CL are essential in few-shot and cold-start domains. Inter-CL contributes the most to the CDR performances, since it is strongly related to the knowledge transfer task in CDR and ts well with the neighbor-similarity based loss of the single-domain model. (2) The intra-CL task also signicantly improves the matching in CDR, while it just achieves slight improvements on the strict cold-start domain. The power of intra-CL will be multiplied when there are more user behaviors in the target domain. (3) From the second ablation group, we observe that all three inter-CL tasks can provide useful information for CDR. We observe that the user-based inter-CL functions well in the few-shot domain (since it has more user-related interactions), while taxonomy-based inter-CL achieves higher improvements in the cold-start domain. Note that CCDR does not conduct𝐿and use user channel in the strict cold-start domain, since the cold-start user nodes are isolated in the target domain with no behaviors. We further conduct two model analyses on dierent weights of the intra-CL and inter-CL losses. Fig. 4 displays the HIT@500 results of dierent intra- and inter- CL weights (𝜆and𝜆) on both fewshot and strict cold-start domains. We can nd that: (1) as the loss weight increases, the HIT@500 results of both intra-CL and interCL losses rst increase and then decrease. The best parameters are 𝜆=1.5, 𝜆=0.6. Note that the parameter analysis is carried out around the optimal parameter point. (2) The performance trends of two CL loss weights are relatively consistent on both few-shot and strict cold-start domains. Moreover, CCDR models with dierent CL loss weights still outperform all baselines, which veries the robustness and usability of CCDR in real-world scenarios. Figure 4: Results of dierent intra-/inter- CL loss weights. In this work, we propose a novel CCDR framework to deal with CDR in matching. We adopt the intra-CL to alleviate the data sparsity and popularity bias issues in matching, and design three inter-CL tasks to enable more diverse and eective knowledge transfer. CCDR achieves signicant oine and online improvements on dierent scenarios, and is deployed on real-world systems. In the future, we will explore more sophisticated inter-CL tasks to further improve the eectiveness and diversity of knowledge transfer. We will also try to introduce the idea of inter-CL to other cross-domain tasks.