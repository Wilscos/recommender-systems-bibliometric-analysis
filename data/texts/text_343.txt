Recommender systems (RS) are typically built on the interactions among three parties: the system, the users, and the items (Tennenholtz and Kurland, 2019). By collecting useritem interactions, the system aims to predict a user’s preference over items. This type of problem setting has been extensively studied for decades and has seen many successes in various real-world applications, such as content recommendation, online advertising, and e-commerce platforms (Das et al., 2007; Linden et al., 2003; Koren et al., 2009; Schafer et al., 1999; Gopinath and Strickman, 2011). ﬁer” (Das et al., 2007; Li et al., 2010; Linden et al., 2003) that allows the system to query their preference over the candidate items directly. However, for at least two reasons, such We propose a new problem setting to study the sequential interactions between a recommender system and a user. Instead of assuming the user is omniscient, static, and explicit, as the classical practice does, we sketch a more realistic user behavior model, under which the user: 1) rejects recommendations if they are clearly worse than others; 2) updates her utility estimation based on rewards from her accepted recommendations; 3) withholds realized rewards from the system. We formulate the interactions between the system and such an explorative user in a K-armed bandit framework and study the problem of learning the optimal recommendation on the system side. We show that eﬃcient system learning is still possible but is more diﬃcult. In particular, the system can identify the best arm with probability at least 1 − δ within O(1/δ) interactions, and we prove this is tight. Our ﬁnding contrasts the result for the problem of best arm identiﬁcation with ﬁxed conﬁdence, in which the best arm can be identiﬁed with probability 1 − δ within O(log(1/δ)) interactions. This gap illustrates the inevitable cost the system has to pay when it learns from an explorative user’s revealed preferences on its recommendations rather than from the realized rewards. Most previous works have modeled the RS users as an unknown but omniscient “classimodeling assumptions of static and omniscient users appear less realistic in many modern RS applications. First, given the huge size of candidate choices, a typical user is usually not fully aware of her “true preference” but needs to estimate it via the interactions with the RS. For instance, an ordinary user on video-sharing apps like TikTok or review-sharing apps like Yelp does not have pre-determined rewards on all possible choices or know the optimal choice in advance. Instead, she has to consume the recommendations in order to discover the desirable content gradually. Second, in many applications, users simply respond to recommendations with accept/reject decisions rather than reveal their consumed items’ utility. This situation is reﬂected in most practical recommendation systems nowadays. Platforms like TikTok and Yelp can easily collect binary feedback like click/non-click while struggling to evaluate the users’ actual extent of satisfaction (Schnabel et al., 2018). These two limitations challenge the reliability of existing recommendation solutions in utility estimation from user feedback and thus shake the foundation of modern recommender systems. model, which hinges on two assumptions of today’s RS users. Firstly, we believe the users are also learning the items’ utilities via exploration. Their feedback becomes more relevant to the item’s utility only after gaining more experience, e.g., consuming the recommended item. This perspective has been observed and supported in numerous cognitive science (Cohen et al., 2007; Daw et al., 2006), behavior science (Gershman, 2018; Wilson et al., 2014), and marketing science studies (Villas-Boas, 2004). For instance, Zhang and Angela (2013) showed through a multi-armed bandit experiment that humans maintain conﬁdence levels regarding diﬀerent choices and eliminate sub-optimal choices to achieve long-term goals when they are aware of the uncertain environment. These works motivate us to study the recommendation problem under a more realistic user model, where the user keeps reﬁning her assessment about an item after consuming it, and she is willing to explore the uncertainty when deciding on the recommendations. Formally, we model her exploration as being driven by her estimated conﬁdence intervals of each item’s reward: she will only reject an item when its estimated reward is clearly worse than others. the system, whereas the realized user reward of any consumed items is kept only to the user (to improve her own reward estimation). This thus gives rise to an intriguing challenge of learning user’s utility parameters from only the coarse and implicit feedback of “revealed preference” (Richter, 1966). One may naturally wonder why the user does not simply give all her realized rewards to the system since both the system and user are learning the best recommendation for the user. This is certainly an ideal situation but, unfortunately, is highly unrealistic in practice. As we mentioned before, it is widely observed that very few RS users would bother to provide detailed feedback (even not numerical ratings). This observation is also supported by the 90-9-1 Rule for online community engagement, and the “Lazy User Theory” (T´etard and Collan, 2009) in the HCI community, which states that a user will most often choose the solution that will fulﬁll her information needs with the least eﬀort. system learning is even possible, i.e., can the system still discover the user’s real preference? To answer the question, we formulate the interactions between the system and such an explorative user in a K-armed bandit framework and study best arm identiﬁcation (BAI) To address the challenges mentioned above, we introduce a more realistic user behavior Secondly, we assume that only the users’ binary responses of acceptance are revealed to Under this more realistic yet challenging environment, it is unclear whether eﬃcient with ﬁxed conﬁdence. We design an eﬃcient online learning algorithm and prove it obtains an O( probability at least 1 − δ. We also show that this bound is tight by proving a matching lower bound for any successful algorithm. Our results illustrate the inevitable gap between the performance under the standard best arm identiﬁcation setting and our setting, which indicates the intrinsic hardness in learning from an explorative user’s revealed preferences. Our experiments also demonstrate the encouraging performance of our algorithms compared to the state-of-the-art algorithms for BAI applied to our learning setup. paragraphRelated Work with ﬁxed conﬁdence (Garivier and Kaufmann, 2016). Instead of minimizing regret, the system aims to ﬁnd the arm with the highest expected reward with probability 1 −δ, while minimizing the number of pulls T . The tight instance-dependent upper bound for T is known to be O(H log the intrinsic hardness of the problem instance. In our work, the system shares the same goal but under a set of more challenging restrictions posed by learning from an explorative user’s revealed preferences. For example, the system cannot directly observe the realized reward of each pulled arm. We prove that under this new learning setup, the budget upper bound increases from O(log rewards. The dueling bandit problem proposed in (Yue et al., 2012) modeled partialinformation feedback where actions are restricted to noisy comparisons between pairs of arms. Our feedback assumption is more challenging than dueling bandit in two aspects. First, we do not assume the system knows the reference arm to which the user compares when making her decisions. Second, the user’s feedback is evolving over time as she learns from the realized rewards. Hence, none of existing dueling bandit algorithms (Yue and Joachims, 2011; Komiyama et al., 2015; Zoghi et al., 2014) can address our problem. The unobserved reward setting is also studied in inverse reinforcement learning. For instance, Hoiles et al. (Hoiles et al., 2020) used Bayesian revealed preferences to study if there is a utility function that rationalizes observed user behaviors. Their work focused on user behavior modeling itself while we studied system learning and analyzed the outcome induced by this type of user behavior assumption. In such a problem, a system aims to maximize the welfare of a group of users who only care about their short-term utility. Kremer et al. (2014) ﬁrst studied this problem and developed a policy that attains optimal welfare by partially disclosing information to diﬀerent users. Follow-up works extended the setting by allowing users to communicate (Bahar et al., 2015) and introducing incentive-compatibility constraints (Mansour et al., 2016, 2020). Our motivation considerably diﬀers from this line of work in three important aspects. First, incentivized exploration looks at an informationally advantaged principal whereas our system is in an informational disadvantageous position, as it has mere access to the user’s revealed preferences. Second, their setting looks at how to inﬂuence user decisions through signaling with misaligned incentives, whereas we are trying to help a boundedly rational ordinary user to learn their best action in a cooperative environment. Third, the user in our model is an adaptive learning agent rather than a one-time visitor to the system. ) upper bound on the number of recommendations to identify the best arm with The ﬁrst related direction to this work is the problem of best arm identiﬁcation (BAI) There are also previous works that focus on online learning without access to the actual Another remotely related direction is incentivized exploration in the Internet economy. To study the sequential interactions between a recommender system and an explorative user, we adopt a stochastic bandit framework where the time step t is used to index each interaction, and the set of arms [K] = {1, ··· , K} denote the recommendation candidates. At each time step t, the following events happen in order: 3. The system observes the user’s binary decision of acceptance or not, i.e., the revealed realized reward after each acceptance of arm i is drawn independently from a sub-Gaussian distribution with mean µ aims to ﬁnd the best arm while minimizing the total number of recommendations. This renders our problem a best arm identiﬁcation (BAI) problem with ﬁxed conﬁdence but based on partial information about the rewards. Throughout the paper, we assume without loss of generality that µ 2016; Audibert and Bubeck, 2010), we further deﬁne the quantity H = characterizes the hardness of the problem instance. decide whether to accept or reject the recommended arms from the system. To make a decision at time t, we assume the user utilizes the information in all previous interactions by maintaining a conﬁdence interval CI where lcb and ucb stand for the lower/upper conﬁdence bounds respectively, n of acceptances on arm-i up to time t, ˆµ mean reward of arm i at time t, and Γ(t; ρ, α) is a function parameterized by {ρ characterize the user’s conﬁdence level to her reward estimation at time step t. Following the convention rooted in the UCB1 algorithm (Auer et al., 2002a), we consider the (ﬂexible) conﬁdence bound form: where n(t) = We note that the choice of Γ is to ﬂexibly cover possibly varied user types captured by parameters α and ρ represents the user’s intrinsic tendency to explore: a larger α indicates a higher tolerance for the past observations, meaning the user is more willing to accept recommendations in Learning the Optimal Recommendation from Explorative Users ris disclosed to the user afterwards. preference (Richter, 1966). From the user’s perspective, we denote the true reward of each arm i ∈ [K] as µ, and the − µ> 0, ∀i > 1. Following the convention in BAI literature (Carpentier and Locatelli,P As discussed previously, the user cannot choose from the full arm set but can only Algorithm 1 Phase-1 Sweeping Input: K > 0, δ ∈ (0, 1), N Initialization: F = [K],N = 0, n repeat until F is empty or the time step exceeds N repeat until The time step exceeds N Output: number of acceptances for each arm {n a wider range. ρ interaction history and has a bounded range [ρ where the user’s conﬁdence over the system evolves over time. For example, ρ function of the acceptance rate speciﬁc format. Note that for the special case of α = 1, ρ the classic conﬁdence interval deﬁned in UCB1. We remark that parameters α and ρ only to characterize diﬀerent types of users, which provide ﬂexibility in handling diﬀerent real-world scenarios; but they are not introduced for our solution. The decision rule. When an arm i is recommended, we assume the user will reject it if and only if there exists j 6= i such that lcb arm if there is no other arm that is clearly better than the recommended one with a high conﬁdence. The rationale behind our imposed user decision rule is straightforward: ﬁrst, the user should not miss the arm with the highest lower conﬁdence bound as this is arguably the safest choice for the user at the moment; second, if two arms have chained conﬁdence intervals, the user does not have enough information to distinguish which one is better, and hence should not reject either one, i.e., being explorative. With stochastic rewards, we know P[µ of acceptances n(t) grows. Therefore, the system can conﬁdently rule out a sub-optimal arm once it has collected a reasonable number of acceptances. In light of this, we devise a two-phase explore-then-exploit strategy for system learning: the system ﬁrst accumulates a suﬃcient number of acceptances and then examines through the arm set to eliminate sub-optimal ones with a high conﬁdence. system will execute an initialization procedure by sweeping through the arm set F = [K] and then recommend each arm repeatedly until it collects exactly one rejection on each arm there. This initialization stage is similar to the round-robin style pulls in standard bandit algorithms (e.g., UCB1, -Greedy). But the key diﬀerence is that our algorithm will Recommend each item in F once, and remove the rejected ones from F . Recommend i until rejected, then remove it from F . . Our results only rely on the lower and upper bound of ρand are oblivious to its The Phase-1 design is presented in Algorithm 1. Like standard bandit algorithms, the Output: F . initialize by collecting one rejection on each arm whereas standard bandit algorithm will initialize by collecting one pull (i.e., acceptance) on each arm. This is because rejections in our setup are more informative than acceptances to the system. After initialization, Algorithm 1 enters the main loop and do the following: keeps recommending the same arm until it gets rejected and then moves to another arm in F . After each arm gets rejected once, the system starts a new round by resetting F = [K]. This procedure continues until the total number of acceptances exceeds N of Phase-1: the system aims to collect a reasonable number of acceptances while minimizing the number of rejections by not recommending any risky arm. For the ease of analysis, we divide Phase-1 into diﬀerent rounds (indexed by r) by the time steps when the system resets F . We will prove later that there is a tailored choice of N enters Phase-2 with N once and also needs to recommend each arm repeatedly in the candidate set to the user. Now a natural question to ask is, how this design could be practical given the immense size of item pool in a practical recommender system. However, we note that an arm in our model can be viewed as a type/category of items, rather than just literally an individual item. Therefore, Algorithm 1 should not be interpreted as recommending the same item repeatedly to a user, but instead recommending items from the same category or type. This is also the typical interpretation of arms in stochastic bandit literature. Moreover, consuming repeatedly each item in the candidate set is a typical requirement in the stochastic bandit problems, due to the observation noise. More speciﬁcally, under a stochastic reward setting, the realized reward at each time step is randomly drawn from an underlying reward distribution, and repeated interactions are necessary to pin down the distribution. Sweeping. Here, the system executes arm elimination: always recommend the arm with the minimum number of acceptances; and eliminate an arm when it is rejected by the user, until there is only one arm left. We prove that the stopping time of Algorithm 2 is ﬁnite with probability 1, and it outputs the best arm with probability 1 − δ when it terminates. We name our proposed two-phase algorithm Best Arm Identiﬁcation under Revealed preferences, or BAIR for short. Our main result is formalized in the following theorem. Recommend a= arg minnand update n. Remove afrom F if rejected. One might notice that the Phase-1 algorithm 1 needs to recommend each arm at least We now present the design for Phase-2, as shown in Algorithm 2, which follows Phase-1 Next, we analyze BAIR by upper bounding its stopping time given ﬁxed conﬁdence δ. Theorem 1 When Γ is deﬁned as in Eq (1), with probability at least 1 − 2δ, the system makes at most recommendations and successfully identiﬁes the best arm by running Algorithm 1 and 2. Note that the upper bound on the number of rounds above is deterministic while not in expectation. The proof of Theorem 1 requires separate analysis for Phase-1 and Phase-2, which we discuss in the following subsections. At a high level, the ﬁrst two terms in the bound come from the number of acceptances and rejections in Phase-1, and the last term corresponds to the number of acceptances in Phase-2. We decompose the bound in Theorem 1 in Table 1. in terms of α: a smaller α increases the upper bound in Phase-1 but requires less number of recommendations in Phase-2, while a larger α ensures a lighter Phase-1 but would result in a more cumbersome Phase-2. This is expected because, e.g., when facing a highly explorative user (large α), the system can easily accumulate suﬃcient acceptances in Phase-1. However, it will need more comparisons in Phase-2 to identify the best arm for such a highly explorative user. Theoretically, there exists an optimal α which minimizes the total number of recommendations; however, this is not particularly interesting to investigate in this paper, as α is not under the system’s control, but a characterization of the user. 3.1 Upper Bound for Phase-2 We start the analysis for Phase-2 ﬁrst as it will lead to the correct N Speciﬁcally, we prove that when N it is safe for the system to move on to Phase-2. Lemma 1 If Phase-1 terminates with N 2 will output the best arm with probability at least 1 − δ. must terminate with probability 1 − δ within O(log Lemma 2 With probability 1−δ, Algorithm 2 terminates within O(K + steps. The ﬁrst term O(K) in the bound corresponds to the number of rejections in Phase-2, since Phase-2 Elimination incurs at most K − 1 rejections by deﬁnition. The second term Note that there is a clear tradeoﬀ between the upper bounds in Phase-1 and Phase-2 The next Lemma shows that no matter when the system enters Phase-2, Algorithm 2 characterizes the number of acceptances, which matches the tight lower bound of BAI with ﬁxed budget (Carpentier and Locatelli, 2016) in terms of δ with a factor instead of H = also plays a role in the upper bound because when ρ a very wide conﬁdence interval for each arm which requires extra eﬀort for the system to eliminate sub-optimal arms. Combining Lemma 1 and Lemma 2 and take ρ ﬁxed constants, we conclude that Algorithm 2 will terminate and output the best arm with probability 1 − δ within O( N(δ) = O(K conﬁdence, our upper bound matches the lower bound in (Garivier and Kaufmann, 2016) in terms of δ. This implies that once the system has accumulated suﬃcient acceptances in Phase-1, the learning from reveal preferences does not bring extra diﬃculty. However, the bottleneck for the integrated system strategy lies in Phase-1, which we now analyze. 3.2 Upper Bound for Rejections in Phase-1 Recall that a round in Algorithm 1 is a segment of a sequence of interactions indexed by r, in which the candidate arm set F is reset to [K] at the beginning and each arm gets rejected once in the end. We abuse the notation a bit by using [t round that starts from time t N = n(t prove that Algorithm 1 must terminate in a small number of rounds with probability 1 −δ, as shown in the following lemmas. In particular, if we choose N of rejections in Phase-1 can be upper bounded by O(K In the next section, we will show that N in Phase-2. The proof of Lemma 3 depends on the following two lemmas. Lemma 4 (Lattimore and Szepesv´ari, 2020) Let {X random variables with zero mean and unit variance, and ˆµ Lemma 5 Let f(t) = max time step t. Then for any round r denoted by [t where Γ ) acceptances. Next we upper bound the total number of rounds in Phase-1. We Nlog) rounds and thus incurs at most O(KNlog) rejections. = minΓ(t). during our Algorithm 1 — the maximum empirical mean will decrease by at least 2 after each round. This implies that Phase 1 cannot run for too many rounds. Finally, assembling Lemma 1, 2, and 3, we can derive the upper bound for the stopping time of BAIR in Theorem 1. It is worthwhile to compare our upper bound on the number of recommendations in Theorem 1 with the O(log worse due to the leading term O(δ deterioration is inevitable due to our focus on an intrinsically harder setup with only the user’s revealed preferences. As our second main result, the following theorem shows that the dependence of δ in the upper bound of Theorem 1 is tight. Theorem 2 For any algorithm π and 0 < c < on δ such that if π collects less than N it must make mistake about the best arm with probability at least δ. Proof [Proof Sketch] The lower bound N result for BAI in a stochastic bandit setting, as the system in our setting can never ﬁnd the best arm quicker than an BAI algorithm that has access to the realized rewards. To prove diﬀerent best arms; 2). any system interacting with ν or ν sequences of user binary responses with probability at least 2δ, as long as it collects less than N with probability at least 1 −δ, thus making mistakes about the best arm with probability δ on either ν or ν situations. We defer the detailed construction and proof to the appendix. Note that the lower bound δ preferences: any algorithm has to make at least Ω(δ guarantee the identiﬁcation of the best arm for any problem instances in our setup. This is in sharp contrast to the well-known O(log feedback setting. In this section, we empirically study BAIR to support our theoretical analysis. We use simulations on synthetic datasets in comparison with several baseline algorithms. Since we propose a new perspective to model user-system interactions in RS, there is no baseline for direct comparison. However, this also gives us an opportunity to demonstrate how problematic it may be when using a wrong user model for the observed system-user interactions. Lemma 5 shows an interesting property about the user’s empirical reward estimationr ≥ δ/ρ, we construct two problem instances ν and νsuch that: 1). they have acceptances. Therefore, the system is not able to diﬀerentiate between ν and ν Table 2: Comparison between BAIR and three baselines on proposed metrics. (α = 1) 0.005 As we discussed in the introduction, prior works treat users as an unknown but omniscient classiﬁer, and therefore stochastic bandits are the typical choices to learn from user feedback. Moreover, since the users’ responses in our problem setup are not necessarily stochastic, adversarial bandits could be another choice. Therefore, we employ the corresponding stateof-the-art algorithms, Track-and-stop (Garivier and Kaufmann, 2016) (for BAI) and EXP3 (Auer et al., 2002b) (for adversarial bandits), to compare with BAIR. Besides, we also propose a heuristic baseline, uniform exploration, to directly compete with BAIR. The details of these baselines are as follows. Uniform exploration (UNI): The system recommends candidate arms uniformly until the number of recommendations reaches the given threshold T . When the algorithm terminates, it outputs the arm with the maximum number of acceptances; ties are broken arbitrarily. Track-and-stop (T&S): This is the state-of-the-art solution for BAI with ﬁxed conﬁdence (Garivier and Kaufmann, 2016). The expected stopping time of T&S provably matches its information-theoretic lower bound O(log ness of T&S relies on the independent and stationary reward assumptions on each arm, which fail to hold in our setup as user responses are not a simple function of their received rewards. We will investigate how the theoretical optimality of the T&S breaks down under our problem setting. EXP3: To the best of our knowledge, there is no BAI algorithm under an adversarial setting. As a result, we adopt EXP3 (Auer et al., 2002b) for comparison. Given the number of arms K and a time horizon T > K log K, EXP3 is provably a no-regret learning algorithm if taking γ ∼ O( output the arm with the maximum number of acceptances in the end. For diﬀerent conﬁgurations of (δ, K, ∆ instances (µ of ∆ the result for ∆ space limit. The parameters in the user model are set to α = 1, ρ ρ= 1, ρ BAIR with N set of problem instances and calculate the following three metrics. Probability of success: When each algorithm terminates, we examine whether the output arm is the best arm (i.e., success). The probability of success (p) is then given by the empirical frequency of success over all problem instances. We also calculate the value to measure if and how much the probability of success falls below the given conﬁdence level δ, which is presented right after the probability of success. Rejection rate: When each algorithm terminates at step T , we count the total number of rejections #Rej the system receives. The rejection ratio is given by averaged over all problem instances. Stopping time: It is the total number of interactions needed to terminate an algorithm. BAIR and T&S stop by their own termination rules; UNI and EXP3 stop by the input time T , since these two algorithms terminate by a preset time horizon. To make a fair comparison, we set T for UNI/EXP3 as the average stopping time of BAIR under the corresponding problem instance. Hence, this metric is only set to compare BAIR and T&S. The results are reported in Table 2. Based on the comparison results for BAIR and the baselines, we have the following observations. BAIR vs. T&S. As shown in Table 2, T&S enjoys the best performance among three baselines on rejection rate and probability of success, but still does not work well in our problem setting. Given the conﬁdence threshold δ, T&S fails to identify the best arm with probability 1 − δ for K > 2 and δ < 0.05. We also ﬁnd the stopping time of T&S is worse than BAIR in most cases and fails to meet its theoretical lower bound O(log This is expected: our binary user feedback cannot be simply modeled as independent and stationary rewards, which are the fundamental assumptions behind the design of T&S. Since T&S wrongly models user responses, it is easily misinformed by the user’s potentially inaccurate feedback in the early stage. As a result, it is very likely to miss the best arm and spend most of the rest time on a wrong subset. In contrast to T&S, BAIR is aware that the revealed preferences from the early stage are very likely to have a large variance. Therefore, it chooses to make safe recommendations at ﬁrst to help the user gain more experiences . Observing that our conclusion does not vary much under diﬀerent ∆, we present = 2, and results for diﬀerent choice of α can be found in Appendix 4.7. We run (Phase-1 preparation) such that that the user will reveal more accurate feedback later on (Phase-2 elimination). This explains how BAIR achieves the goal more eﬃciently, even with the additional cost in Phase-1. BAIR vs. UNI/EXP3. The other two baselines, UNI and EXP3, exhibit worse performance in both the rejection ratio and the probability of success than BAIR. Given the same time budget, UNI always suﬀers from the largest proportion of rejections because it does not take any measures to eliminate bad arms. As rejections do not update the user’s empirical reward estimation, the given time budget is insuﬃcient for UNI to diﬀerentiate the arms with similar expected rewards, thus causing a low probability of success. EXP3 enjoys a lower rejection rate than UNI, because it pulls those empirically bad arms less. The mandatory exploration in EXP3 helps correct the inaccurate early observations and gives a more competitive probability of success when K gets larger. However, due to the larger variance of EXP3, if the user’s estimated reward for the best arm is low at the beginning, EXP3 tends to overly focus on diﬀerentiating a group of suboptimal arms, which decreases its chance of discovering the best arm. insuﬃcient system exploration when facing an explorative user. These baselines either treat the user as a black-box or assume independent and stationary user feedback, which leads to a worse empirical result in terms of both accuracy and eﬃciency in ﬁnding the best arm. and δ < 0.1, approximately linear in The ﬁrst column in Table 2 also suggests an approximately linear dependency between BAIR’s stopping time and K. Although it is not fully supported by our theory (the leading term in the upper bound result is O(K informative and could be an interesting target for future work. 4.4 Experiments on Diﬀerent Choices of ∆ diﬀerent values of ∆ Figure 1(b). When facing harder problem instances with a smaller ∆ maintained similar performance in terms of our evaluation metrics. Meanwhile, BAIR also consistently outperforms T&S. UNI and EXP3, however, both suﬀered from a clear drop in the probability of success, as a smaller ∆ ﬁnding the best arm and thus incurs extra diﬃculty for UNI and EXP3 to distinguish those near-optimal arms. In practice, it might be too restrictive to assume that the user strictly follows our proposed conﬁdence interval (CI) based behavior model. Thus, it is interesting and also crucial to test the robustness of BAIR under the situation where the user’s behavior might deviate from the CI-based model. To this end, we extended the user model to a stochastic setting To summarize, the fundamental reason for the failure of these baselines lies in the The result in Table 2 supports our theoretical analysis in Theorem 1. When ∆= 0.5 is an environment variable that determines the diﬃculty of each problem instance; but = 0.5) is illustrated in Figure 1(a) while the comparison under ∆= 0.2 is shown in by incorporating “decision randomness”. Speciﬁcally, we assume at each time step, with some constant probability p, the user makes a random decision (accept/reject the recommendation with an equal probability); otherwise, she would follow the CI-based behavior model. We demonstrate that a minor modiﬁcation of BAIR still maintains competitive empirical performance in this new environment, against the three baselines. We use a natural variant of BAIR which adjusted its Phase-2 slightly: the system discards an arm after Figure 1: The comparison between BAIR/T&S/EXP3/UNI for diﬀerent (δ, K, ∆). Table 3: Results under the extended user model with click noise p = 0.1 and m = 2 log Compared to the results in Table 2 (where p = 0), BAIR performs slightly worse in terms of stopping time and probability of success, but still outperforms the other baselines. Its rejection rate becomes worse, because of the noisy feedback in Phase-1. However, the rejection rate is not as crucial as stopping time and probability of success for best arm identiﬁcation. 0.05 its m-th rejection rather than the very ﬁrst rejection. We call this variant BAIR(m); its adjustment is precisely to account for users’ behavior noises. Note that with probability at least 1 − (1 − p) j and t in Phase-2. By union bound, if we choose m such that 1 − (1 − p) (i.e., m > log probability at most δ, and BAIR(m) ﬁnds the best arm with probability 1 − 2δ in this new setting. Although the choice of m incurs additional cost in Phase-2, it does not aﬀect the order of our upper/lower bound result, and thus demonstrates the robustness of our solution. Table 3 shows the comparison under the new noisy user model with p = 0.1. The same comparison with the three baselines is reported in Table 3. Except for the new user model, the environment remains the same as the one used in Table 2 in the main paper. 4.6 Experiments on the Comparisons with Shared Phase-1 As we have discussed, the baseline algorithms fail due to the inadequate preparation to inform the user. One might wonder whether the baseline algorithms can be strengthened by some tailored preparation procedures, e.g., a straightforward plugin of Phase-1 algorithm. However, this idea does not work due to the following reasons. First, the output of Phase-1 (the number of rejections and acceptances on each arm) cannot be utilized by those baseline algorithms as they require the actual rewards, while the Phase-2 of BAIR is built on such simple information accumulated from Phase-1. Although it might be possible to redesign the baseline algorithms to allow them to leverage the output of Phase-1, this requires speciﬁc designs and should be considered a new algorithm, which is beyond the scope of this work. Second, if we simply run the same Phase-1 step for all algorithms without utilizing its output, there is no reason to believe such algorithms can outperform BAIR in Phase-2. This is because BAIR can conﬁdently eliminate any arm after its very ﬁrst rejection in Phase-2, while other algorithms simply do not have such conﬁdence without the information provided 0.01 by Phase-1. Consider EXP3 and T&S: if two arms are both rejected less than once, but one has accumulated more acceptances than another, it is still insuﬃcient to distinguish them with high conﬁdence. However, only BAIR will utilize this fact to keep exploring these two arms. To summarize, Phase-1 and Phase-2 need to work together in BAIR to guarantee good performance; if other algorithms need to take advantage of Phase-1, we need to redesign them to incorporate the information obtained in Phase-1. Otherwise, a ‘prepared’ user after Phase-1 is still opaque to the baseline algorithms, as their response is only the relative preference rather than the actual reward. shared phase-1 exploration, the UNI/EXP3 exhibits even worse success rate performance than BAIR, given the same stopping time in Phase-2. This is expected since many arms will have a very wide conﬁdence interval at the beginning of Phase-2 and thus look equally good to the system for a long time if the output of Phase-1 is not utilized. This actually makes the user’s response even more confusing to the baseline algorithms. For T&S, even though the stopping time has improved when it is small, the success rate suﬀers a signiﬁcant loss due to the same reason. Table 5, 6 demonstrate the performance of BAIR against three baselines under the choices of α = 2.0 and α = 0.8. In the case α = 2.0, where the system serves an overly optimistic user, BAIR keeps outperforming the other three baselines and enjoys an even larger margin when becomes larger — it is because a larger α reduces the total number of recommendations in Phase-1, which happens to be the dominant part in the stopping time when the other hand, a very explorative user with a larger α would generate feedback sequences that deviate further from the independent and stationary assumptions imposed in classical bandit solutions (e.g., our baselines) and lead to worse performance when those baseline methods are applied. stopping time appears not as good as T&S when K becomes large. This is likely to be caused by our conservative choice of the upper bound of N Table 4: Performance of baselines with shared Phase-1. (α = 1, N=) Our additional experiment result in Table 4 also demonstrates our argument. With a When α = 0.8, BAIR’s probability of success still outperforms the baselines’, but its Table 5: Comparison between BAIR and three baselines on proposed metrics. (α = 2.0) success rate), which may not be tight in K. Speciﬁcally, there is an O(K between the upper bound and the lower bound of N choice of N one may adjust N applying binary search within (0, (2K/δ) is achieved on validation datasets. To demonstrate this tradeoﬀ, we report the stopping time and the probability of success under the same setting as Table 6, but with diﬀerent choices of N is insuﬃcient to guarantee 1 − δ probability of success, and the choice of N wastes too much time in Phase-1 especially for a large K and small α. However, there are choices in between that appear to be better tradeoﬀs as they increase the probability of success drastically at a moderate cost of stopping time. However, as we have discussed in Section 5, the optimal choice of N To bring user modeling to a more realistic setting in modern recommender systems, we proposed a new learning problem of best arm identiﬁcation from explorative users’ revealed preferences. We relax the strong assumptions that users are omniscient by modeling users’ learning behavior, and study the learning problem on the system side to infer user’s true 0.005 Table 6: Comparison between BAIR and three baselines on proposed metrics. (α = 0.8) 0.005 Table 7: The stopping time and probability of success of BAIR under diﬀerent choices of 0.01 preferences given only the revealed user feedback. We proved eﬃcient system learning is still possible under this challenging setting by developing a best arm identiﬁcation algorithm with complete analysis, and also disclosed the intrinsic hardness introduced by the new problem setup. Our result illustrates the inevitable cost a recommender system has to pay when it cannot directly learn from a user’s realized utilities. As concluding remarks, we point out some interesting open problems in this direction: The optimal choice of N δ, it does not match the upper bound in Theorem 1 in terms of K. The mismatch comes from the choice of N only indicates a necessary condition of N bound is needed to improve the choice of N experiment results in Table 2 demonstrate that the choice of N guarantees a success probability 1.0 even when δ takes a large value, e.g., 0.1. This implies the stopping time of BAIR could be improved by setting a smaller N ﬁne-tune N search within (0, (2K/δ) Beyond a single user. We note that our problem formulation and solution for the system and a single user also shed light on learning from a population of users. For example, users sometimes learn or calibrate their utility from third-party services that evaluate the quality of items by aggregating users’ feedback across diﬀerent platforms. As a result, users equipped with these services are inclined to exhibit an exploratory pattern and make decisions based on the comparison of conﬁdence intervals. We believe that our problem setting also provides a prototype to study the optimal strategy for the system under this new emerging situation. This work is supported in part by the US National Science Foundation under grants IIS2007492, IIS-1553568 and IIS-1838615. Haifeng Xu is supported by a Google Faculty Research Award.