Imagine you are a teacher attempting to assess a student’s level in a particular subject. If you design a test with only hard questions, and the student fails, this mostly proves that the student does not understand the more advanced material. A more insightful exam would include dierent types of questions varying in diculty to truly understand the student’s weaknesses and strengths from dierent perspectives. In the eld of Recommender Systems (RS), more often than not, we design evaluations to measure an algorithm’s ability to optimize goals in complex scenarios, representative of the real-world challenges the system would most probably face. Nevertheless, this paper posits that testing an algorithm’s ability to address both simple and complex tasks/problems would oer a more detailed view of performance to help identify, at a more granular level, the weaknesses and strengths of solutions when facing dierent scenarios/domains. We believe the RS community would greatly benet from creating a collection of standardized, simple, and targeted experiments, which, much like a suite of “unit tests”, would individually assess an algorithm’s ability to tackle core challenges that make up complex RS tasks. What’s more, these experiments go beyond traditional pass/fail “unit tests”. Running an algorithm against the collection of experiments allows a researcher to empirically analyze in which type of settings an algorithm performs best and to what degree under dierent metrics. Not only do we defend this position, in this paper, we also oer a proposal of how these simple and targeted experiments could be dened and shared and suggest potential next steps to make this project a reality. CCS Concepts: decision making; Reinforcement learning; • Computing methodologies → Simulation tools. Additional Key Words and Phrases: evaluation, recommender systems, reinforcement learning, simulation ACM Reference Format: Andrea Barraza-Urbina. 2021. Towards Creating a Standardized Collection of Simple and Targeted Experiments to Analyze Core Aspects of the Recommender Systems Problem. In SimuRec’21: Workshop on Simulation Methods for Recommender Systems at ACM RecSys’21, October 2nd, 2021, Online, Worldwide. ACM, New York, NY, USA, 6 pages. Recommender Systems (RS) oer a mix of services that aim to imitate, support and/or augment the social process of creating and sharing recommendations [ think about the everyday situations where people request and oer recommendations. For instance, when faced with a decision, we usually turn to people we trust, who know us best, to seek personalized advice. Also, we often feel the need to proactively share our opinions about a positive or negative experience to inform others. RS enable these processes at a large scale: they aim to learn from the mass of available user opinions and information sources to oer customized suggestions to help users discover and make decisions about items. By reecting on the numerous ways to exploit a collection of user opinions, it is no surprise that a broad spectrum of RS services exists for various use cases and application domains, such as movies, news, fashion, restaurants, education, health and lifestyle, among many others. • Information systems → Test collections;Recommender systems;• Theory of computation → Sequential Evaluating a RS solution/algorithm can be rather challenging. At a high level, an evaluation consists of one or more experiments. In each, we assess the RS’s ability to address some class of problems/tasks quantied by a set of performance measures. In many ways, an evaluation is similar to a school exam, aiming to determine a student’s level in a particular subject. For a student’s performance on the exam to represent their actual knowledge, an exam generally includes a comprehensive set of dierent types of questions. Not only do questions need to cover the relevant topics, but they also need to vary in diculty to truly understand the student’s weaknesses and strengths from dierent perspectives. In RS, more often than not, we test algorithms on their ability to optimize goals in complex scenarios, representative of the real-world challenges the system would most probably face. In short, we tend to create evaluations/exams for RS with only “hard” tasks/questions. For instance, a “hard” task might be for the RS to optimize several metrics (such as accuracy, diversity, coverage [8]), in a complex simulation, where the system would have to oer item suggestions from an extensive and dynamic catalogue to a frequently changing pool of users on dierent devices. In contrast, a “simple” task might be for the RS to oer suggestions to a small and static set of known users from a small and static item catalogue to increase rating prediction accuracy. This paper posits that testing an algorithm’s ability to address both simple and complex tasks/problems would oer a more detailed view of performance to help identify, at a more granular level, the weaknesses and strengths of solutions when facing dierent scenarios/domains. The ideas presented in this paper are heavily inspired by the work of Osband et al. in [11], where the authors introduce a “Behaviour Suite for Reinforcement Learning” (more in the next section). In fact, our main argument is that the RS community would benet from the creation of our own “Behaviour Suite”: a collection of standardized, simple, and targeted experiments, which, much like a suite of “unit tests”, would individually assess an algorithm’s ability to tackle core challenges that make up complex RS tasks. What’s more, these experiments go beyond traditional pass/fail “unit tests”. Running an algorithm against the collection of experiments allows a researcher to empirically analyze in which type of settings an algorithm performs best and to what degree under dierent metrics. The following sections will provide more details on the proposed approach and highlight arguments in favour and against it. Finally, we lay out potential next steps to make the presented ideas a reality. 2 PROPOSED APPROACH The “Behaviour Suite for Reinforcement Learning” (or simplybsuite) is an open source library of “carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents” [11].bsuiteaims to play a role similar to that of the ‘MNIST’ dataset which “oers a clean, sanitised, test of image recognition as a stepping stone to advanced computer vision” [11]. The authors highlight that a collection of clear experiments, where each embodies a fundamental issue of the Reinforcement Learning (RL) problem, would be a powerful driver for progress. Among ve key qualities, experiments inbsuitemust be targeted, in that performance on the task corresponds to a single key issue in RL, and simple, in that it “it strips away confounding/confusing factors in research”. So far, we have compared designing an evaluation for a RS approach to a teacher creating a school exam. In this case, bsuite-type experiments would be analogous to what we have called “simple” questions in the exam. Moving forward, we develop the general idea of creating simple and targeted experiments for RS from a RL point of view. For this, we rst briey introduce RL and RL-based RS. Though we focus on a RL perspective, the underlying messages should also be relevant to experiments based on other paradigms used to represent the RS problem, such as Supervised Learning. Reinforcement Learning (RL) is a branch of Machine Learning inspired by how humans and animals learn from trial-and-error experiences. In RL, a goal-oriented agent must learn to perform a task despite uncertainty about its environment. The agent does not receive examples or instructions of the correct actions to perform in dierent situations. Instead, in response to actions, the agent obtains feedback from the environment in the form of positive or negative reward signals . Positive rewards are meant to reinforce actions/behaviour and negative rewards are meant to discourage actions. The interaction between the agent and its environment is formalized by the RL Framework in Figure 1, an “abstract and exible” framework that can be used to represent dierent problems where a decision-making agent needs to learn from interaction [13]. For us, the relevant aspects of the framework are: • The agent is the learner or decision-maker. •The environment characterizes the agent’s application domain and the task/goals the agent is trying to achieve. •The agent interacts with the environment by performing actions in a continuous sequence of discrete time steps. • The state represents all the information about the environment available to the agent at a specic time step. •A reward is a numeric score that is used to motivate the agent to learn how to perform actions in dierent states/situations. •From the agent’s perspective, the environment has a simple interface reduced to the information found in states, actions and rewards. Conventionally, the RS problem has been formalized as a Supervised Learning task (Batch Learning). However, recent works have proposed that RL can be a more appropriate paradigm to frame the “modern” and Interactive RS problem (more details in [ focused on learning over time how to perform actions (e.g., oer item suggestions), in dierent states (e.g., to dierent users), using interactive feedback in the form of reward signals (e.g., user ratings). The RS problem/task is characterized by the environment. Although, at rst glance, the environment could be confused with a simulation, in [ more details on why datasets and simulations require additional assumptions to specify a RS task fully. To simplify, the reader can interpret the environment as a special type of simulation that requires no additional assumptions to generate a state and a reward in response to an agent’s action. Now that we have provided the necessary background, we can continue dening what a simple and targeted experiment would be. In bsuite, an experiment has three components: (1) Environments reminder, a targeted task corresponds to a single key issue in RL. (2) Interaction the agent and environment interact for a 100 time steps. Another relevant parameter is the number of episodes, where an episode is a sequence of interactions starting from an initial time step and ending on a nal time step. : A xed strategy dening how the agent-environment interact. For instance, we can dene that Due to the stochastic nature of both the agent and environment, it is helpful to measure the agent’s expected performance when interacting with the environment over several episodes. Interesting examples of interaction can be found in [15](e.g., interactions with training and testing phases). For a concrete example in RS, see [4]. (3) Analysis: “a xed procedure that maps agent behaviour to results and plots” [11]. Having an explicit analysis component allows to have a shared understanding of what it means to solve a task and share benchmarks. One example experiment included inbsuiteis called Mountain Car, a classic RL task where the agent needs to learn to drive an underpowered car up a hill. To solve the task, the agent must drive back and forth to gain the momentum needed to reach the top of the hill (see [15, Fig. 2]). Among challenges, the task represents the issue of delayed reward or credit assignment [11,15]. In Mountain Car, the environment provides the agent with constant negative rewards at each time step and only generates a positive reward if the car reaches the top of the hill. This reward scheme makes it dicult for the agent to learn which sequence of actions actually lead to the goal [15]. Withbsuite, a researcher is in a better position to analyze the weaknesses and strengths of a RL solution in relation to key challenges of a RL problem. For example, [11, Fig. 5] presents a ‘radar plot’ summarizing the agent’s performance on several experiments. Each dimension in the plot is a single score quantifying the agent’s ability to address a core challenge. In RS, we believe the community has mainly focused on designing evaluations with complex experiments, where in each, the RS is faced with several core challenges simultaneously. For this, the community has created environments/simulations [7,9,12] and datasets [6] to share the complexities of specic RS problems/use cases. Moreover, every year, dierent competitions are hosted to encourage the community to solve real-world tasks or challenges (such as the ACM RecSys Challenge [2]). All these eorts are essential to drive progress in the eld. However, we believe an eort is also required to create simple and targeted experiments representative of core challenges in the eld, such as user/item cold start, high churn, delayed feedback, privacy, security, trust, fairness, multiple stakeholders, and others. These simple experiments would serve as complementary tools to existing evaluations. Although they would play a dierent role than complex experiments, they can have several advantages, as discussed in the following section. 3 DISCUSSION AND NEXT STEPS Creating a shared collection of simple and targeted experiments can have several advantages for the RS community. We highlight the following: •A clear and standardized methodology to evaluate a RS algorithm from dierent perspectives/dimensions: The approach encourages a standardized evaluation methodology, much more specic than just sharing an environment/simulation/dataset. This is important to compare and reproduce evaluations across multiple works. •Deeper understanding of the RS problem: Absuitefor RS would help the community to have a shared, clear and comprehensive understanding of the core challenges that, as a community, we should be tackling. It would also help identify which essential problems might not be thoroughly dened. •Deeper understanding of the state of the art: By sharing experiment results in a consistent and comparable way, the community can better understand the state of the art in solving the key challenges of the eld. It would also be easier to understand which challenges are better addressed compared to others. •Deeper understanding of algorithm properties and the type of tasks they can solve: All RS solutions are not well suited for all RS tasks [1,5]. The challenge of matching RS techniques to RS problems was explored by Burke and Ramezani in [5]. Their approach was to analyze the underlying properties of dierent application domains (such as, news, e-commerce, music, and others) to provide a matching between domains and RS technologies (e.g., a Collaborative Filtering approach would be better suited for a movie RS task). With a easier to empirically analyze in which type of settings an algorithm performs best by running the algorithm against the collection of experiments. This process would not have to depend on the type of domain, as it might be the case that dierent RS tasks in the same domain might have dierent underlying core challenges. •“Unit Testing” for faster and more informed development of RS solutions: diagnose faster problems during the development of a RS solution. The fact that the researcher would have these experiments readily available, would mean faster development as well. •Shared Benchmarks: Having a collection of standardized experiments means we can have shared benchmarks. Among advantages, shared benchmarks can help to identify the best algorithms for a specic core challenge. Arguments against the approach could include: •Would using a comprehensive set of metrics not be enough to achieve the same level of analysis as using a collection of simple experiments? Multiple metrics exist for dierent RS goals [ component of an experiment. We believe that using a comprehensive set of metrics as part of a collection of diverse experiments would provide more information regarding an agent’s performance. •Designing a targeted experiment for a single core challenge may prove to be too hard: Even the most basic RS task might require for the solution to address several challenges. Even in [ associated with two or three key RL issues. The denition of ‘targeted’ might need to be revisited and adapted to what it would mean for the RS community. Making the proposed approach a reality would entail several challenges. We highlight the following: •Community eort to create experiments: Designing experiments to represent an algorithm’s ability to address a core challenge can prove to be an intricate task and would require the engagement of several experts from the many “sub-elds” of RS. Finding incentives to motivate the community would prove challenging if the project does not gain support from community leaders. A possible solution would be to form a small team to create an initial version of the library, such as version of the library they would be encouraged to participate in future versions of the project. •Dening specic criteria experiments have to meet to be added to the library: To address this challenge, expects to form a committee that can review and select experiments that have been proposed by the community. The committee is expected to meet annually during the NeurIPS conference. •Practical/technical asp ects of developing and sharing experiments: in Python and mostly maintained by researchers at DeepMind. It needs to be determined who would adopt ownership and general maintenance responsibilities of a future codebase. Given the discussed challenges, we propose the following next steps: •Rene the details of the proposed approach and dene specic requirements for an initial version of a library for RS. The initial version should include a small collection of experiments and example analysis reports (view appendices in [11]). • Dene incentive mechanisms to motivate the RS community to participate in the project. • Dene eective community outreach strategies to raise awareness about the project. • Find sponsors for the project willing to take ownership and develop the rst version. • Form a committee that can review and select experiments submitted by the community. ACKNOWLEDGMENTS Special thanks to Brian Tanner for introducing the author tobsuiteand for the interesting discussions. This publication has emanated from research conducted with the nancial support of Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289_P2, co-funded by the European Regional Development Fund. For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.