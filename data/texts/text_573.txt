Ritvik.Vij.cs517@cse.iitd.ac.inrohit.raj21@alumni.iitd.ac.in A good data visualization is not only a distortion-free graphical representation of data but also a way to reveal underlying statistical properties of the data. Despite its common use across various stages of data analysis, selecting a good visualization often is a manual process involving many iterations. Recently there has been interest in reducing this eort by developing models that can recommend visualizations, but they are of limited use since they require large training samples (data and visualization pairs) and focus primarily on the design aspects rather than on assessing the eectiveness of the selected visualization. In this paper, we present VizAI, a generative-discriminative framework that rst generates various statistical properties of the data from a number of alternative visualizations of the data. It is linked to a discriminative model that selects the visualization that best matches the true statistics of the data being visualized. VizAI can easily be trained with minimal supervision and adapts to settings with varying degrees of supervision easily. Using crowdsourced judgements and a large repository of publicly available visualizations, we demonstrate that VizAI outperforms the state of the art methods that learn to recommend visualizations. • Computing methodologies →Machine learning;• Humancentered computing → Visualization toolkits;Visualization theory, concepts and paradigms. ACM Reference Format: Ritvik Vij, Rohit Raj, Madhur Singhal, Manish Tanwar, and Srikanta Bedathur. 2022. VizAI : Selecting Accurate Visualizations of Numerical Data . In 5th Joint International Conference on Data Science & Management of Data (9th ACM IKDD CODS and 27th COMAD) (CODS-COMAD 2022), January 8–10, 2022, Bangalore, India. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3493700.3493717 As more and more sectors of society ranging from government to businesses move to adopt data-enabled approach to decision making, it is not only imperative to have access to large volumes of data, but also to have tools that help get actionable insights from data as quickly as possible. Most users across these domains are not experts, as a result, eectively visualizing the data has become a crucial tool for obtaining insights. But generating eective visualizations is not an easy task, and requires not only a good understanding of the domain but also an expertise with the visualization tools. It has led to a growing interest in developing methods that can automate the process of visualizing datasets itself [1, 5, 11, 17]. Current approaches taken towards this can be broadly classied into (a) those using rule-based generation of visualizations [1,17] and (b) those based on machine models learnt on a large pool of existing visualizations [5,11]. Unfortunately, both these approaches suer from serious limitations: on one hand, the rule-based methods have the obvious limitation that they can not easily adapt to new data or visualization paradigms. On the other hand, the machine learnt models rely on the availability of a large volume of prior data and its visualization to train on, making them hard to easily adapt to new visualizations. For example, the latest VizML system [11] uses more than 2.3 million data-visualization pairs to train, making it harder to replicate and highly dependent on the quality of visualizations chosen by the crowd. Apart from this, they also are aected by the (un)availability of certain visualizations in the training corpus. Further, neither approach considers explicitly quantifying how well the generated / recommended plot performs in terms it capturing the underlying data. In this paper, we present VizAI, a machine learnt visualization recommendation framework that not only considers the preference of the user to a specic type(s) of chart, but, more importantly, also takes into account how well the specic visualization performs in representing the underlying data accurately. In other words, VizAI explicitly builds on two of Tufte’s canons for good visualization [22], viz., (a) “a good visualization should be a distortion-free representation of the underlying data” and (b) “be closely integrated with the statistical description of the dataset”. VizAI automatically generates Figure 1: Example of Visualization Recommendations by VizAI. All three visualizations represent the same data. VizAI selected the Density visualization (highlighted in red box), and in our crowdsourcing evaluation it had a perfect agreement among all judges as the best visualization choice. a number of alternative visualizations for the given dataset, evaluates them on how well they (statistically) represent the underlying data and then selects the best performing one. The evaluation of the visualization itself is done using a deep convolutional neural network, based on an ensemble of standard ConvNets, that extracts features from the visualization which can be used to predict the aggregate statistics of the data. For example, in Figure 1 the underlying data was plotted using scatter, line and density plots (we used Chart Studio by Plotly for plotting these, although our work can be easily used with any other plotting software). Of these, VizAI ranked the density plot as the best and this was further conrmed by our crowdsourcing evaluation (details are in Section 6). Note that our focus in this paper is on generating visualizations of numerical datasets rather than datasets with varied data types, such as textual, categorical, temporal, etc., across data elds. Often these data types are transformed as part of data preprocessing into an appropriate numerical representation before their visualization [5, 17]. VizAI can be adapted to incorporate these preprocessing steps as well. With numerical data, commonly used visualizations are line, scatter, area, bar and pie charts, and more sophisticated settings require density plots. Apart from developing a visualization recommendation framework, our work also goes towards developing a systematic performance evaluation measure of various visualizations. In fact, it is an important topic among visualization researchers [2]. We believe that our approach of extracting statistical features of the underlying data mimics the human perception of these visualizations, and oers a framework that can be extended to take into account aspects like choice of colorschemes for the plots and so on. In summary, we make the following key contributions in this paper: (1)We present a novel framework for automatically evaluating the “goodness” of a visualization in terms of the accuracy of extracting various statistics of the underlying data, just from the visualization alone. Our approach does not rely on axis or key labels in the plot, or any other numerical/textual content that may be embedded in the plot. (2)Based on the above, we present VizAI, an intelligent visualization recommendation system based on generative-discriminative framework. It uses a deep convolutional neural network for generating various statistical features from the plot, and a discriminator that selects the plot with the lowest error in predicting aggregate statistics of the underlying data. (3)We conduct a detailed evaluation, using real-world (from Plot.ly) visualizations along with a visualization learning and benchmarking repository, VizNet [12] and the results show that VizAI oers high quality recommendations nearly as good as human evaluators. The rest of the paper is organized as follows: in Section 2, we discuss work related to the ideas pursued in this paper. These range from studies on human perception of visualizations, machine perception of visualizations, and various automated visualization recommendation systems. Next, in Section 3, we present our key hypothesis regarding how to evaluate the quality of visualizations, and formulate our problem statement. Various aspects of VizAI system, including the statistical features and the CNN for extracting these features from visualizations, are described in detail in Section 4. It is followed by the details of our experimental framework in Section 6, and the results over real-world datasets in Section 7. We nally conclude and outline future directions of research in Section 8. Our work focuses on utilizing deep convolutional neural networks trained to emulate human perception of visualizations for automatic selection of best visualization. In this section we review prior work in each of these areas. Early works experimentally demonstrated that aggregate statistics of data, means and correlations respectively could be perceived with high accuracy and compared across classes by humans from data visualizations [7,9]. Subsequent analysis of chart perception through extensive human studies found that both individual features as well as aggregate of data could be inferred from good visualizations [14]. They varied the number of columns and the entropy of data, showing that for a small number of columns humans can perceive dierences and even individual values of aggregates like averages with low error rates. Using a large-scale crowdsourced study, [21] demonstrated that human performance (accuracy as well as time taken) in tasks like nding aggregates (sum for example) of data as well as extremums is highly aected by the type of visualization (scatter, bar, table etc). Our work builds on these insights to model the perception of aggregates and extremums to help in identifying the most appropriate visualization form for the given data. Deep CNNs are extremely popular for various learning tasks over images [10,20]. Considering data visualizations as images, they have been applied for tasks such as visual QA, focus tracking etc. For example, [13] use a combination of CNN and OCR output for question answering on visualizations. As a solution to the problem of automatically learning visual importance i.e., the places in an infographic a user is likely to focus on, [4] used a fully convolutional network for creating importance maps. Recently [8] did an evaluation of CNNs as applied to visualizations. They tried to regress elementary quantitative features like position, length and area from the images of visualizations presented to the network. They suggest that though CNNs can regress quantities for well constrained problems, they would require extensive training to generalize across tasks. There have been many recent eorts towards automating the visualization recommendation process for a given dataset. We briey survey four of the most recent works that have shown signicant promise in this direction. Data2Vis [5]tackles the visualization generation problem as a language translation problem where data specications are mapped to visualization specications. It uses an LSTM based neural translation model to learn a mapping between JSON encoded data and a Vega-lite visualization specication. The model is trained using 4,300 Vega-lite examples. Their model was in an elementary way able to learn the vocabulary and syntax for a valid visualization specication, appropriate transformations (count, bins, mean) and how to use common data selection patterns that occur within data visualizations. VizML [11]formulates visualization recommendation as making design choices. It learns these choices from a large corpus of one million dataset-visualization pairs collected from Plot.ly public studio of visualizations. They use 841 hand-coded features to train their recommendation networks to make encoding-level and visualization-level design choices. VizML veried that certain steps in a visualization generation pipeline can be learned well using neural networks. DeepEye [17]combines rule based visualization generation with models trained to 1) classify a visualization ‘good’ or ‘bad’ using a decision tree classier and 2) rank lists of visualizations using a ranking neural network. The DeepEye corpus consists visualizations drawn from 42 public datasets. 100 students annotated these visualizations as good/bad. These annotations, combined with 14 features for each column pair and rule based heuristics are used to train their models. Draco [18]tackles the visualization recommendation in a more formal framework. It uses Answer Set Programming (ASP), a contraint logic programming to specify and generate visualizations with soft weights for constraints learnt from training data. The visualization of a dataset is a collection of visual elements used to communicate the data in a distortion-free manner. While there are many aspects of visually communicating the contents of a dataset, including the choice of colors, domain-specic imagery such as maps, overlays, etc., our work is based on a fairly simple and fundamental premise that one should be able to derive the core statistical features of the underlying dataset accurately from the visualization [22,23]. Since nding the best visualization is a normative question, we make the following assumptions and later validate them through user studies: •Humans prefer visualizations from which they can perceive features about the underlying data accurately. •The modern neural network based models, trained to predict data features from plots, naturally emulate human perception to some extent. That is, if the model makes larger errors in estimating statistical features from one visualization over from another visualization of the same data, we expect that humans when asked to perceive the same statistics end up making corresponding scale of errors. With these assumptions our problem of nding the best visualization is reduced to the task of designing and training a model which satises the above criteria and nding a set of features. VizAI consists of the following three phases: (1) True Statistics Computation and Candidate Generation: We compute a set of statistics from the underlying data tables and use them as features that need to be estimated by our model from the given visualizations. We keep these statistics to be easily computable and, more importantly, easily interpretable by our users. Next, we programmatically generate a number of visualizations of each data table using a visualization generator such as Plotly, gnuplot, Vegalite etc. (2) Statistical-Feature Regression:This is the core model building stage where visualizations are input as images to a supervised Convolution Neural Network (CNN) regression model that extracts (regresses) the statistical features from these images, and trains to minimize the loss between the true statistics and the extracted statistical features. Note that during training, we optimize the statistics regression for each visualization type separately. Thus, adding another visualization type has no impact on the models built on earlier visualization types. (3) Visualization Selection:Finally, we compute the loss between the true statistics computed in the rst stage and the extracted statistics by the model trained the second stage. The visualization that is considered to be performing best in terms of this loss is recommended for the given data table. The overall model architecture is depicted in Figure 2. In the rest of the section, we discuss each of these parts of VizAI in detail. Figure 2: Architecture of the Proposed VizAI Model. The components within the dotted box are trained end-to-end – note that VizAI does not rely on human labele d data during model training. Table 1: Aggregation metrics used to get data features. We also use the correlation feature along with this. We tried several approaches of selecting a set of statistics as features to extract from data tables. We initially experimented with table embeddings[6] and other neural feature extraction approaches but their major disadvantage was that these features are not understood or interpretable by end users. Since understanding the various aspects of why a particular visualization is good is relevant for data analysts, for us the interpretability is an essential requirement[16]. Thus nally we settled on a set of simple and commonly used features that most people working with data have some understanding of. These are listed in 1. Note that these are consistent with the recommendation of focusing on extremes, medians and relationships [23]. Note that these core statistics can be further enriched as required by specic application settings. Although the data tables we currently experiment with consist of only two columns to be visualized, in a general setting we will have to deal with tables with variable number columns. If only individual column-level statistics are used as features, then each data table can result in dierent feature dimensionality. To overcome this, we introduce the use of 2-stage feature aggregation. First, column wise statistics (mentioned in the rst column of 1) are calculated for each column. Then a data-table level aggregation is carried out using the metrics mentioned in the second column of 1. Thus we use two levels of features, e.g., ‘max of mean’ is the maximum of the mean values of each column. Along with this, we also use Pearson’s correlation coecient,𝑟, between any two columns X and Y of our table, written as: where𝑥and𝑦are the𝑖row for each column and¯𝑥and¯𝑦represents the mean of each column. The generated correlation coecient in unique for each table as we have only 2 columns in current setup of tables in the dataset. This can be easily modied to performing a table level aggregation of the correlation coecient to accommodate tables with variable number of columns. As a result, in the current setup each data-table is represented using the same 26 statistical features which are used by the next stage of model building. Visualization Candidate Generation.Next, we generate a number of dierent visualizations for each data table and feed them to a convolutional neural network (CNN) layer coupled with a regressor that extracts statistics from each of these visualizations. As we already mentioned we make use of Plot.ly Chart Studiofor generating visualizations of the table to keep our visualizations consistent with the publicly available datasets from VizNet repository (details in Section 5). These visualizations are input to the CNN + regressor model during training, and also as candidate visualizations to select from during the test/inference. The visualizations are given as image input to the CNN model that generates their embeddings. We train a multivariate regressor to extract various statistical features of the underlying data. Figure 3: Heatmaps illustrating statistics extraction error (loss) on test data of dierent CNNs trained on Plotly data (darker colors indicate higher error). We experimented with multiple state of the art CNN’s in our experiments such as AlexNet [15], VGG19 [25] along with ResNet18 [10] and ResNet50 [10]. As the input images as well as the output labels are quite dierent from the standard datasets that were used to train these CNNs, simply training the regressor yielded quite poor results. Instead, we simply ne-tune the nal classier layers of CNNs and train the regression layer for our training dataset. Somewhat surprisingly we found that larger models (in terms of number of parameters) do not correspond to improved performance in our setting. In Figure 3, we illustrate the performance of dierent CNNs in terms of their error in extracting various statistics for the dataset we collected from Plotly (details in Section 5). It is immediately clear that larger models such as ResNet18 and ResNet50 give huge losses on the test dataset for many statistical features. It may perhaps be due to the issue of over-tting because of the number of free parameters. On the other hand, AlexNet and VGG19 show consistently better performance across all the statistics we focus on. In our experiments we settled on using AlexNet and VGG19 which were our top-2 best performing CNNs. Additionally, we also experimented with an AdaBoost ensemble of these two models, which we represent as (AlexNet+VGG19). The output embedding from the convolutional layers passes through a fully connected regression layer trained to output the values of statistical features for the given visualization. For the regressor, we used weighted smooth L1 loss between the predicted vector and the features extracted from the data table. The model is trained in a supervised fashion (using the true statistics computed from the previous phase) to predict the statistical features of each visualization. At the time of evaluation, we take a data table as our input, extract it’s data features for use as true values and produce dierent types of plot images for it. The regression model is run on each of these to obtain predicted feature values. Based on how close these values are to the actual features of the data, each plot is assigned a score. The best plots is chosen by our model using this score. This not only helps in selecting the best plot for some given data, but also helps in realizing how good a visualization is by analyzing the score. In our experiments, we consider two such types of scoring functions. • Normalised L1 Loss- For our setup, we dene the well known normalised L1 loss as whereˆ𝑦is the features predicted by our CNN Encoder,𝑦is the true value of all the feature and𝑡is the average value of the 𝑖feature from the training dataset. • Top-K Closest Loss- We dene the Top-K Closest loss as the cumulative L1 loss for only the Top-K Statistical features for each plot type prediction closest to true values. 𝑙(ˆ𝑦, 𝑦) = min(|ˆ𝑦− 𝑦|𝑡) ∀𝑡𝑢𝑝 ∈ {1, 2..., 26}(2) Evidently, this will be nothing but the sum of normalised L1 Loss for the k features whose prediction is closest to true feature values relative to the true value. We will use Normalised L1 Loss, Top-5 and Top-10 Closest Loss in our experiments. Based on the literature survey done in section 2.3, we identify two major limitations of the existing recommendation systems and discuss how VizAI can solve these limitations : •Rule-based recommendation systems like [1,17] have obvious limitations when it comes to adapting to new data tables and new types of visualizations. Also machine learning based models such as [5,11] require complete re-training of the complete dataset in such a setup. VizAI is highly exible and only requires additional training on new data to adapt to the new setup. •None of these works provide quantication on how well the recommended plots are able to depict the data they were created on. Based on our previous studies, we know the importance of such a quantication, especially for visualisations of data in the numeric domain. Overcoming this limitation, VizAI tries to generate a score for each visualization type based on it’s ability to depict data statistics easily. As we mentioned earlier, nding the best visualization for a data table is a normative question, user studies are necessary to evaluate the overall performance of recommendations. For this purpose, we conduct experiments using publicly available Plotly datavisualization pairs and a subset from VizNet collection to carefully examine the performance of various choices of statistical-feature regression models, and the loss models used in the visualization selection. We generate plots using the Chart Studio Python API for all of these datasets to maintain uniformity. Plot.ly[19] is an online service used by a large number of people to generate various types of plots of data. We wrote our own crawler which extracted data-visualization pair from the Plot.ly raw feed. We extracted data-visualization pairs of the following plot types: (i) scatter, (ii) line, (iii) density. We ltered out data tables with categorical and text columns. For line and scatter plots with more than one dataseries in the same plot, we separated up to 5 of them and treated them as dierent data-visualization pairs. After the preprocessing, the dataset had a total of 24547 datavisualization pairs, the distribution of the data is shown in Table 2. We used min-max normalization within a column to keep the range of x and y in [0,1]. Each visualization type was split in 80:10:10 ratio into training, validation and test respectively. As we described in Section 4, our statistics-extraction models based on VGG19, AlexNet, and their ensemble were trained, validated and tested on this dataset. Figure 3 shows the statistical feature extraction loss we observed on Plotly test data. Note that we make use of only the data tables in this stage to compute the feature extraction loss. VizNet [12] is a large-scale repository of data as used in practice, compiled from the web, open data repositories, and online visualization platforms. VizNet repository contains more than 23 visualization types, and a set of unique data tables and corresponding visualization to represent the data table – thus, the visualization type used is considered the gold label for the data table. We extract 2,042 data table and visualization pairs from the plot.ly fraction of VizNet to build our test dataset. We extract only those instances where (i) the size of data table (i.e., the number of rows)≥5, and, (ii) the gold labels are either of scatter or line plots as VizNet does not contain density plots as gold label. For each data table extracted, we generate 3 plots of type line, scatter and density. We conducted several experiments and crowd-sourced judgments on this data to validate the performance of our model. Figure 4: Screenshot of a sample question from the CSI task. For evaluation, we employed a Turkle[24] instance running on a university server, oering features similar to Amazon Mechanical Turk (AMT) platform. Since the evaluation required expert users who could understand various statistical properties of the data we are interested in inferring, we did not run these experiments directly on AMT. Participants in our evaluation were CS graduate students and faculty members in our university. We dened the following three tasks designed to study our hypothesis about whether statistical features of the underlying data tables are reproduced by the recommended visualizations: (1) Complex Statistics Inference (CSI):Given the mean and standard deviation values of X and Y columns of some data table, the users must select the plot which represents them the best. Similar to the above task, all plots given as choices in every task depict the same data whose statistics are shown to the users (but not the data table itself). The user has to choose on which plots it is easiest, doable and impossible for this task as shown in Figure 4. (2) Fraction Above Threshold (FT):We provide the users with a single plot per question and ask them to estimate the fraction of points in the given plot which have y (or x) value>0.5. While Answering these questions, It might be trivial to count points for some plots and answer the question so we instructed evaluators to estimate the fraction by looking at the plot without counting the number of points one by one. We assign points to each plot type for each data table on the basis of answers given by users in the crowdsourcing experiment for VizNet data as shown in Table 3. For CSI task, the points are awarded based on the choice indicating the ease of statistics inference from the plot. In the FT task, where the evaluators have to estimate the fraction, the points are awarded based on the relative error made by them from the true value of the statistic – upto 20% error we award 2 points, upto 40% error we award 1 point, and 0 points are awarded for errors beyond 40%. The cumulative points are calculated for each plot type for each data table and the plot type with most number of points is chosen as the overall preferred choice of plot type. Figure 5: Screenshot of a sample question from the FT task. Table 3: Scheme of awarding points to individual datavisualization pairs based on the crowdsourced tasks. For SI tasks, the points are awarded for each user choice. For FT tasks, the points are awarded based on the error in estimation made by users –ˆ𝑦 represents fraction predicted by user and 𝑦 represents true fraction. As already mentioned, the statistics extraction model consisting of CNN followed by a fully connected regressor was trained and tested using Plotly dataset. Figure 3 presents the heatmaps of statistics extraction error with dierent choices of CNNs including our AlexNet+VGG19 ensemble. In Figure 6 we show the loss in the Skew column aggregation over each of the table level aggregations given in Table 1. Similarly, Figure 7 plots the loss in estimating Pearson’s correlation coecient between X and Y columns of the data table from the visualization alone. As one can observe, the larger CNN models (ResNet18 and ResNet50) perform much poorer than smaller models, and our (VGG19+AlexNet) ensemble further improves on statistics extraction accuracy. We now turn our attention to evaluating the performance of VizAI on real-world data from VizNet [12] based on a crowd-sourced experiment. We selected samples from VizNet which are 2-Dimensional and have gold labels as either line or scatter plots (note that VizNet does not have density plots). We generated the same three plots for each sample for the crowd sourcing experiment. We removed the labels of the data and just treated the two columns as X and Y. We asked each participants in the crowdsourcing to complete a total of 245 SI and FT (both for Y and X axis) tasks, on a randomly Figure 6: Heatmap of loss observed on Skew column aggregate statistic under various extraction models. Figure 7: Variation in loss in estimating Pearson’s correlation between the two columns of the data-table under various models. selected 35 tables from the test set. We aggregate points based on the scheme described earlier (Table 3) for CSI and FT tasks. The plot type with maximum points is chosen as the preferred plot type. The resulting count of preferred plot type for each visualization type is summarized in Table 5. Note that the sum of these counts is greater than 35 due to ties in the points between two or more plot types. It is interesting to observe that although VizNet did not have density plots, there were a signicant instances where crowd evaluators shown preference to density plots over line chart. In our subsequent evaluation, we used our crowd sourced plot types as gold labels. We experiment using all combinations of statistics extraction models (AlexNet, VGG19 or (VGG19+AlexNet) ensemble) and the visualization selection (L1-loss, Top-5 Closest and Top-10 Closest loss) models. The results are summarized in Table 4. From these results, we can evidently see that the use of the (VGG19+AlexNet) ensemble, followed by the use of Top-5 Closest loss in the visualization selection performs the best in terms of accuracy and F1-scores of individual plot types as well as aggregated weighted F1-score using the counts from Table 5. As we observed deviations in crowdsourced gold labels we collected from the gold labels given in VizNet dataset, we felt it prudent to investigate this issue further. We rst evaluate our best performing combination – (VGG19+AlexNet) with Top-5 closest loss – when using the gold labels as given in VizNet. Table 6 shows that the use of VizNet gold label results in much worse performance. However, Table 4: Performance of dierent Statistics Extraction and Visualization Selection models on VizNet. The bold-faced values represent the best performing combination. Table 5: This table shows the number of times each plot type is chosen as the preferred user plot type after assigning points in the VizNet data Table 6: Performance comparison of VizAI vs plot type labels given in the original VizNet dataset we believe that this is indicative of the data quality issues one would expect when crawling at large (as observed recently in [3]). We further highlight this issue with an example drawn from our crowdsourcing experiment shown in Figure 8. While VizNet gold label was the line plot, two evaluators for this task clearly preferred the density plot (and one evaluator chose the scatter plot). In this work we presented VizAI, a visualization recommendation system based upon machine prediction of statistics of the data from dierent plot images. Our experiments demonstrate that our model makes intelligent visualization predictions which are highly favored by end users. The results of our crowd-sourced human study reveals interesting aspects behind human perception of visualizations. From a practical perspective, the approach taken by VizAI can easily adapt to new visualization paradigms without signicant cost compared to VizML which requires complete retraining to accommodate for the new plot type. Further, it is possible to incorporate domain-specic details like focus on the quality of certain statistics, color preferences, dierent visualization libraries, etc., into VizAI. Figure 8: VizNet Gold Label choice is highlighted in red box, VizAI’s choice is highlighted in the black box and the crowdsourcing users 1,2 and 3’s choice are highlighted in the blue, yellow and green boxes respectively. VizNet Gold Label choice is unreliable as the line plot gives us no idea about the distribution of the points and hence the underlying statistics of the data. A limitation of our work is that we make use of an algorithmic system, viz., deep convolutional neural network (refer Section 4), as a proxy for human visual perception to determine the extent to which a visualization is representing various data statistics. However, it is well-known in visualization literature that human visual perception is highly context dependent (e.g., it is subject to optical illusions), and varies greatly from person to person. Naturally, CNNs are not expected to fully mimic the human perception of visualizations. We believe that further research is required, especially in complex visualizations of semantically rich data. Despite this, VizAI oers a unique and exible approach to recommending visualizations for a dataset. We plan to pursue research in extending VizAI along following three directions: (1)Incorporation of more plot variables such as - which columns to plot, what goes on which axis and more plot types into our prediction framework. For simplicity and easy testing, we chose only to predict plot types but for usability purposes it would be desirable to have multiple decision variables inferred automatically. (2)Applications of our model, which predicts human perceived data features from plot images and compares them with actual data features, in dierent tasks like identifying misleading visualizations (for detecting fake news for example) and helping analysts to interpret plot visualizations. (3)More robust scale and data statistic invariance in our feature prediction model. Right now we use a simple normalization procedure to deal with dierent scales in our input data features. An approach based on learning scale invariant models should improve performance and make the model more adaptable.