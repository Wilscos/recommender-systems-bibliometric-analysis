Key Laboratory of Intelligent Computing and Signal Processing, Ministry of Education, Anhui Province 230601, P.R. Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui Province 230601, P.R. China Distributed document representation is one of the basic problems in natural language processing. Currently distributed document representation methods mainly consider the context information of words or sentences. These methods do not take into account the coherence of the document as a whole, e.g., a relation between the paper title and abstract, headline and description, or adjacent bodies in the document. The coherence shows whether a document is meaningful, both logically and syntactically, especially in scientic documents (papers or patents, etc.). In this paper, we propose a coupled text pair embedding (CTPE) model to learn the representation of scientic documents, which maintains the coherence of the document with coupled text pairs formed by segmenting the document. First, we divide the document into two parts (e.g., title and abstract, etc) which construct a coupled text pair. Then, we adopt negative sampling to construct uncoupled text pairs whose two parts are from dierent documents. Finally, we train the model to judge whether the text pair is coupled or uncoupled and use the obtained embedding of coupled text pairs as the embedding of documents. We perform experiments on three datasets for one information retrieval task and two recommendation tasks. The experimental results verify the eectiveness of the proposed CTPE model. • Information systems → Document representation;• Computing methodologies → Natural language processing; Information extraction; • Applied computing → Document searching. document representation learning, coherence, neural networks, information retrieval, scientic documents Distributed document representation aims some basic problems such as retrieval, classication, and inference to support other downstream tasks. Due to the semantic complexity of the documentation, learning document representation is still a dicult task, not well solved at present. For a document composed of multiple bodies, there should be a logical relationship and smoothness between its adjacent bodies. Therefore, there is a relation between the adjacent bodies in the document, which shows whether a document is meaningful, both logically and syntactically [19]. This relation is very important to the scientic document, without this relation the document will become meaningless. It is very necessary to maintain this relation in distributed document representation. This relation is called coherence. Current distributed document representation methods are based on word embedding or sentence embedding. Yurochkin et al.[36] treat document representation as a hierarchical optimal transport based on the word mover’s distance [16], word embedding, and topic. Chen et al.[6] build the document embedding by averaging the sentence embedding and training through discriminator that determines whether a sentence belongs to a document. However, these methods based on the context information of words or sentences do not take into account the coherence of the document as a whole. The embedding of words and sentences can easily maintain coherence because there is the context (adjacent words or sentences) around the words and sentences. The context of the document exists inside the document, not around it. Maintaining the coherence of the document as a whole is a challenge. In this paper, we propose a coupled text pair embedding (CTPE) model to learn distributed document representation by maintaining the coherence of the scientic document. Specically, we rst divide the scientic document into two parts (such as the title and abstract) to form a coupled text pair. We use some common methods to obtain word or sentence embedding for all coupled text pairs (such as word embedding for short documents and sentence embedding for long documents). Then, we adopt negative sampling to construct uncoupled text pairs whose two parts are randomly derived from dierent documents. Our model maintains the coherence of the document by training to determine whether the text pair is coupled or uncoupled. We input the word embedding or sentence embedding of the text pair into the model and nally obtain two vector representations of the coupled text pair as the document representation. This paper further proposes a method for calculating the similarity of the coupled text pairs to calculate the similarity between documents represented by the paired embeddings. The main contributions of this paper are summarized as follows: •We introduce coherence into representation of scientic documents for the rst time, which makes a document meaningful both logically and syntactically, based on two adjacent bodies in the document. •We propose a novel form of embedding and a general model, called coupled text pair embedding (CTPE), to maintain document coherence and represent documents. •We propose a document similarity calculation method for computing documents represented by coupled text pair embedding. •We perform experiments through information retrieval and recommendation to show that our model is competitive to the state-of-the-art methods based on context information of words or sentences. Current distributed document representation learning methods are not specically used for scientic documents. In the following we describe the main distributed document representation learning methods. The word-level method uses words as calculation objects to represent the document. Early distributed document representation methods are mainly algebraic or probabilistic models such as TFIDF [28] and topic model [2,34]. These models treat the document as bag of words which neglect other useful information such as context information of words. With the development of neural networks, it is easier to obtain context information about words, such as word2vec [23], lifelong domain word embedding [33] and XLNet [35], etc. On this basis, Le and Mikolov [18] and Luo et al. [22] predict a word embedding through other word embedding and document embedding while learning learn document embedding. Arora et al.[1], Chen [7] and Schmidt [29] treat the average word embedding as document embedding. Hansen et al.[12] weight word embedding to obtain document embedding. Kusner et al.[16] and Wu et al.[31] get the distance between dierent documents by calculating word mover’s distance. There are some methods that base on labels of specic tasks. Xiao et al.[32] propose a document representation model base on label semantic information for multi-label text classication. Cohan et al.[8] use citations between documents to train the transformer model and obtain representations of scientic documents. The sentence-level method uses sentences as calculation objects to represent the document. In addition to word embedding, sentence embedding [13,21] can also be obtained from the context information of the sentence. The average of a sentence embedding or sentence embedding that treats a document as a sentence can be used to represent document embedding. Li and Hovy [19] maintain coherence in sentence embedding based on sentence order, but does not form document embedding. Chen et al.[6] build the document embedding by averaging the sentence embedding and training through discriminator that determines whether a sentence belongs to a document. These methods based on the context information of words or sentences do not take into account the coherence of the document as a whole. In this section, we formulate the problem of distributed document representation and introduce the structure and technical details of coupled text pair embedding (CTPE) model for distributed document representation. Firse, we dene our term in a formal way. For a documents set D = {𝑑, 𝑑, ...,𝑑}, its coupled text pairs set is dened asS = {𝑠(𝑑)|𝑠 (𝑑) = (𝑓, 𝑏), 𝑑∈ D,𝑖 ∈ [1, |D|]}and its uncoupled text pairs set is dened asS= {𝑠|𝑠= (𝑓, 𝑏),1⩽ 𝑖 ≠ 𝑗 ⩽ |D|}, where𝑓and𝑏denote the former and latter part of the text of document𝑑, respectively. Document𝑑is the text we want to embed. According to the data available to us, documents can be the full text, the title and the abstract only, or the headline and bodies, etc. In order to get(𝑓, 𝑏)based on𝑑, we dene a parameter𝑝𝑜𝑠 for the segmentation position, which determines the split point that divides the document into two bodies. The left side of𝑝𝑜𝑠is 𝑓, and the right side is𝑏. For example,𝑝𝑜𝑠is generally dened as the position between the title and abstract if the document only includes the title and the abstract. We dene our problem in a formal way. Given a documents set Dand its coupled text pairs setS. Our goal is to learn distributed representation𝑔[𝑠(𝑑)] = 𝑣= (𝑣, 𝑣)for document𝑑, where𝑣 and 𝑣denote embedding of 𝑓and 𝑏, respectively. In the next subsection, we will describe the proposed coupled text pair embedding (CTPE) model for distributed document representation. Figure 1 shows the overall architecture of CTPE. We introduce coherence into the document representation with the three parts of text pair construction,training model, andloss function, and propose a novel embedding form with the fourth partdistributed document representation. Text pair construction.We propose a representation of text pairs so that we model learns to determine which document is more consistent because coherence is the relation between the two bodies in the document. We rst divide each document𝑑into former part text𝑓and latter part text𝑏to form a coupled text pair. The𝑓and𝑏of the same document are used as a positive sample(𝑓, 𝑏) ∈ S. Then we randomly select the document𝑑= 𝑟𝑎𝑛𝑑𝑜𝑚(D)from the documents setDby negative sampling. We construct uncoupled text pairs(𝑓, 𝑏)and(𝑓, 𝑏)through𝑠 (𝑑) and𝑠 (𝑑). We randomly choose(𝑓, 𝑏)or(𝑓, 𝑏)as a negative sample(𝑓, 𝑏) = 𝑟𝑎𝑛𝑑𝑜𝑚 [(𝑓, 𝑏), (𝑓, 𝑏)] ∈ Swhose𝑓and𝑏 come from dierent documents. We adopt negative sampling to obtain coupled and uncoupled text pairs setsSandSin each epoch of training. Training model.We introduce a training model that learn the relation between the𝑓and𝑏to determine which document is more Figure 1: The overall architecture of CTPE. The positive sample takes 𝑓 𝑓and 𝑏as example. 𝑓, 𝑏, 𝑓and 𝑏correspond to four CNN models (former part, latter part, former part and latter part) respectively, and then each CNN model has four parallel convolutional layers. consistent. The embedding model aims to represent text pairs in a distributed form, such as word embedding, token embedding, sentence embedding, paragraph embedding model, or so on. We can choose dierent embedding models for documents of dierent lengths. We compare dierent embedding models in experiments. The embedding model converts text consisting of𝑙words, tokens, sentences, or paragraphs into𝑙vectors of dimension𝑑𝑖𝑚. The𝑑𝑖𝑚∗𝑙 dimension matrix after the embedding model is input to the CNN model to learn the relation between𝑓and𝑏. The structure of the CNN model is composed of input layer, convolution layer, pooling layer, output layer. The number of channels of the CNN model is𝑛, and the sizes of the convolution kernels are𝑛= {𝑛, 𝑛, 𝑛, 𝑛}. The right side of Figure 1 shows the details of the CNN model. The implementation process training model is as follows. Since the length of𝑓and𝑏are dierent, the maximum word length of the𝑓and𝑏is limited to𝑙⩾ 𝑙. First, for any𝑓and 𝑏with an input length of𝑙𝑒𝑛 ⩽ 𝑙, a matrix composed of𝑙 vector of𝑑𝑖𝑚dimension is obtained from the embedding model, which is input to the input layer of the CNN model. Then, through the convolution operation of the rectied linear unit (ReLU) and the convolution lter that stride is 1, four convolutional layers can be obtained, and their sizes are respectively𝑛∗ (𝑙 − 𝑛+1), convolutional layer is processed into a one-dimensional vector of length𝑛by the max pool operation. Finally, a text vector of length 4∗𝑛is obtained by cascading operation. The(𝑓, 𝑏)and the(𝑓, 𝑏) form vector pairs(𝑣, 𝑣)and(𝑣, 𝑣)with the embedding model and CNN model. In order to reduce the training time and space complexity, positive and negative samples usually share the same model parameters. Loss function.In order to maintain coherence in the document representation, the loss function aims to make the distance between the positive and negative samples larger, that is, the similarity and 𝑏as example, and the negative sample takes between the𝑓and𝑏of the same document must be greater than the similarity between the𝑓and𝑏of the dierent documents. We assume a minimum similarity distance𝑀, the dierence in similarity between the positive and negative samples must be at least greater than𝑀. If the dierence of similarity between the positive and negative samples is greater than𝑀, the similarity is recorded as the loss value, otherwise is not included in the loss value. For positive sample(𝑓, 𝑏), negative sample(𝑓, 𝑏), the loss function 𝐿 is dened as: where𝑐𝑜𝑠denotes the cosine similarity of two vectors.𝑀represents the minimum similarity distance (called margin). This means that ifdiis greater than𝑀, then it is no longer necessary to modify the weight, i.e.,𝐿 =0. The small batch gradient is reduced using the adaptive moment estimation (Adam) algorithm and the learning rate is set to 𝑙𝑛. Distributed do cument representation.After training with the loss function, we need to obtain a distributed representation of the document with the model. As shown in Figure 2, we calculate the vector pair(𝑣, 𝑣)of a document𝑑through the trained model of the previous section.𝑣= (𝑣, 𝑣)is dened as the distributed representation of document𝑑.𝑣is composed of a pair of vectors, which is dierent from a single embedded vector. The overall procedure from document to distributed representation is summarized in Algorithm 1. Figure 2: Distributed document representation of CTPE. Algorithm 1: Coupled text pair embedding (CTPE) , 𝑏) = 𝑝 (𝑑) We propose a novel similarity calculation method because of the special document representation. The general similarity calculation method only calculates the similarity without coherence. The calculation of similarity occurs between𝑓in one document and𝑏 Figure 3: Similarity calculation of documents in CTPE. in another document, because coherence is the relation between the adjacent bodies. As shown in Figure 3, for any two documents 𝑑and 𝑑, the similarity 𝑠𝑖𝑚(𝑑, 𝑑) between them is dened as: 𝑠𝑖𝑚(·, ·)means the coherence between dierent bodies that come from dierent documents relative to the same document.𝑠𝑖𝑚(·, ·)is dierent from the general similarity and may not satisfy𝑠𝑖𝑚(𝑑, 𝑑) = 𝑠𝑖𝑚(𝑑, 𝑑) =2 because it does not measure whether two documents are "exactly the same". Finally, we obtain a distributed representation of the document, and a similarity calculation method for this distributed representation. Through these two operations, we implement downstream tasks. In this section, we evaluate the performance and eectiveness of our model. We conduct experiments compared with 24 comparison methods on arXiv (papers), DBLP (papers) and USPTO (patents) datasets. Our code will be released on GitHub. We use the arXiv dataset for the information retrieval task that aims to retrieve similar candidate papers for a paper (consisting of title and abstract). We use the DBLP and USPTO datasets for recommendation tasks (citation recommendation and patent recommendation). Citation recommendation task aims to recommend appropriate candidate citations for a paper (consisting of title and abstract). Patent recommendation task aims to recommend similar candidate patents for a patent (consisting of title, abstract, and claim). The details of datasets are described below. arXiv.This dataset comes from the public data source of arxiv, which contains a total of 1,180,081 papers. All papers contain titles, abstracts, authors, publication time, subject. For the test paper to be queried, candidate paper with the same subject as it is taken as groundtruth. Evaluation protocol: whether to retrieve papers that are candidates for groundtruth. DBLP.This dataset comes from the public data source of DBLP [30]. We use the v10 version, which includes a total of 3,079,007 papers. We use titles, abstracts, publication times, and citations (groundtruth) in this dataset. Evaluation protocol: whether the groundtruth citation can be recommended. Table 1: Datasets. The labels in the table refer to the range of the number of groundtruth candidate documents for the test document. USPTO.This dataset comes from the public data source of the PatentsView database(a total of 6,424,534 patents). We extracte the title, abstract, year, rst paragraph of the claim, and the citation (groundtruth) cited by examiner from the dataset. Evaluation protocol: whether the groundtruth citation can be recommended. In order to eectively evaluate the task, we perform experiments using cleaned subsets of arXiv, DBLP, and USPTO that are released on GitHubin detail. Each dataset contains 20,000 candidate documents and 1000 test documents (papers or patents) randomly selected. Table 1 describes these datasets in detail. The number of candidate and settings for DBLP and USPTO are similar to [5] and [14]. In order to conviniently evaluate the performance of dierent comparison methods, we use the same text preprocessing method for all data. The text preprocessing method is divided into ve steps: 1. Convert all text to lowercase; 2. Remove HTML labels; 3. Restore HTML escape characters; 4. Split text with punctuation; 5. Remove tokens without letters. For sentence embedding, we use the punctuation at the end of the sentence to segment the document into sentences before text preprocessing. We compare our model with the following three types of unsupervised distributed document representation methods on retrieval and recommendation tasks. Random embedding model. We generate random embedding (based on uniform distribution) of documents and words as baseline methods, respectively called doc2vec(ran) and word2vec(ran). The avg-word2vec(ran) use average word embeddings as a document embedding. We use randomly generated embedding to show the diculty of the task. Word-level methods. Algebraic or probabilistic model: TF-IDF [28], LSA [10] and LDA [2]. Word embedding based model: GloVe [24], word2vec [23], doc2vec [18], WMD [16] and Doc2VecC [7]. Since the corpus of datasets is a subset of the public data source, we train word2vec and GloVe on the experimental dataset and the public data source, called avgword2vec and avg-word2vec(full) and avg-GloVe and avg-GloVe(full). Deep language model: ELMo [25], GPT [26], GPT-2 [27], BERT [11], TransXL [9], XLM [17], XLNet [35] and RoBERTa [20]. We adopt pre-trained models without ne-tuning because of unsupervised document representation learning on these methods to verify the eectiveness of CTPE on dierent embedding models. Sentence-level methods. Sentence embedding based model: Skip-thoughts [15]. The average of the sentence embeddings in the document is represented as the document embedding, called avg-Skip-thoughts. We use the methods to nd the𝑡𝑜𝑝𝑁=20 results for three datasets and compare the result of each method with the labels in the datasets. We use the precision (P), recall (R), and F1 score (F) as evaluation metrics. We also employ several popular information retrieval measures [4] including mean averaged precision (MAP), normalized discounted cumulative gain (NDCG), and bpref [3]. These are popular measures for information retrieval [37] and recommendation [5]. The parameters and setup of the comparison methods are described in detail below. random embedding.This is a baseline method for randomly getting text embedding. We generate random embedding of documents and words, respectively called doc2vec(ran) and word2vec(ran). The embedded dimensions of both methods are set to 100. TF-IDF.The distributed representation of the document consists of the TF-IDF of each word in the document. The dimension of the document is the number of unique words in the dataset, and the cosine distance is use to calculate the similarity. LSA.We set the number of topics and iterations to 100. Other parameters are consistent with gensim’s default settings. LDA.We set the number of topics and iterations to 100. Other parameters are consistent with gensim’s default settings. word2vec.We use the average word embedding of word2vec as the document embedding, and the dimension is set to 100. The window size is set to 10, the maximum number of iterations is set to 100, and the strategy is set to skip-gram. GloVe.We use the average word embedding of GloVe as the document embedding, and the dimension is set to 100. Since the experimental dataset is a subset of all the data, we train the word embedding on the experimental data set and all the data, called avg-GloVe and avg-GloVe(full). The window size is set to 10 and the maximum number of iterations is set to 100. doc2vec.The dimension is set to 100. The window size is set to 10. The number of iterations is set to 50. The strategy is set to PV-DM. WMD.The WMD is a distance measurement algorithm based on word embedding. We evaluate the WMD based on GloVe and word2vec, including WMD-word2vec, WMD-word2vec(full), WMDGloVe, WMD-GloVe(full). Doc2VecC.Doc2VecC is based on word2vec, where the settings for the word2vec part are consistent with the word2vec model above. The sentence sample is set to 0.1. Skip-thoughts.Skip-thoughts is a sentence embedding model. We use the generic model trained on a much bigger book corpus to encode the sentences. A vector of 4800 dimensions, rst 2400 from the uni-skip model, and the last 2400 from the bi-skip model, are generated for each sentences. The EOS token is used. The average of the sentence embeddings in the document is represented as the embedding of the document. ELMo, GPT, GPT-2, BERT, Transformer XL, XLM, XLNet, RoBERTa.The distributed document representation is represented by the average pooling of token embedding (BERT does not include [CLS] and [SEP]). The pre-trained models used in this paper are Original (5.5B), openai-gpt, gpt2-medium, bert-large-uncasedwhole-word-masking, transfo-xl-wt103, xlm-mlm-en-2048, xlnet-large-cased, and roberta.large, respectively. Because these deep language models are context-dependent token embedding, there are two strategies for training text pairs: the former part text and the latter part text are trained separately or together. We take the maximum of the performance of these two strategies. We use Tensorow to implement our model. We set batch size to 200. The parameters in CTPE include𝑙,𝑙,𝑛,𝑛,𝑙𝑛,𝑀and 𝑑𝑖𝑚, which are empirically set to 200, 200, 1024,{1,2,3,5}, 0.001, 0.1, 100, respectively. If the embedding model is a sentence embedding model,𝑙is set to 20. The word embedding dimension of all models is set to 100, and other parameters are set to the default recommendation. All comparison methods use cosine similarity. In order to apply to various unsupervised tasks, there is no verication set and the parameters of dierent tasks are consistent. Our method and all comparison methods use all documents in the dataset for unsupervised training and obtain all document embeddings. Test documents with groundtruth are used to test performance. Our model is trained on the RTX 2080TI. The training time for each result is limited to one day. There is no verication set here. If the loss function value does not decrease for more than 12 hours, the training will be stopped early. The CTPE model uses word2vec(full) as the embedding model. We discuss other embedding models in Section 4.5. For the arXiv and DBLP datasets,𝑝𝑜𝑠is set between the title and abstract. For the USPTO dataset,𝑝𝑜𝑠is set between abstract and claim. We discuss other𝑝𝑜𝑠in section 4.6. Table 2 and Table 3 show the results of 17 methods on 3 datasets. From these results, we have the following observations and analysis. CTPE has the best performance. On the arXiv dataset, our model outperforms Doc2VecC by 2.41% and 1.66% on precision and MAP. On the DBLP dataset, our model outperforms Doc2VecC by 2.29% and 1.27% on precision and MAP. On the USPTO dataset, our model outperforms Doc2VecC by 2.54% and 1.82% on precision and MAP. The CTPE model maintains the coherence of the document as a whole, so it achieves the best performance. Normal documents are coherent, otherwise they don’t make sense. Here are some other results and facts that need to be explained: (1) The performance of avg-word2vec(ran) is better than doc2vec(ran), because doc2vec(ran) is completely random, and avg-word2vec(ran) can uniquely identify a word. (2) The arXiv dataset has lower recall than other datasets. Because the arxiv dataset has more labels (see Table 1), it is more dicult to nd all labels. (3) The word2vecbased approach is better than the GloVe-based approach on all three datasets. (4) The algorithm that uses all the data (full) is mostly better than the algorithm using the experimental dataset, which veries the impact of data quality on the algorithm. (5) The WMD is better than average word embedding in most experiments, but not all, such as WMD-w(full) and WMD-g(full) on arXiv, because the WMD based on word embedding does not learn new information from the document. We conduct an ablation analysis on CTPE to examine the eectiveness of each component. We conduct experiments on three components: (1) We use dierent embedding models in thetraining model. (2) A model called CTPEthat does not use theloss functionto train. (3) Adistributed document representation method (avg) that uses average embedding instead of coupled text pair representation. Table 4 shows the impact of 14 embedding models on the performance of avg, CTPEand CTPE in 3 datasets using F1 metric. We have the following observations and analysis: (1) CTPE improves the performance of various types of embedding models (word and sentence embedding models) eectively. CTPE outperforms avg model on average by 2.6%, 7.9%, and 5.4% on arXiv, DBLP, and USPTO, because the avg model does not consider coherence. (2) The performance of CTPE is signicantly better than CTPE, which proves the eectiveness of the loss function because the loss function guides the training model to determine which documents are more coherent. (3) Our distributed document representation cannot be directly used for general embedding models. More than 90% of avg methods perform better than CTPEmodels, because our similarity calculation method is dependent on coherence. (4) The performance of CTPE-Skip-thoughts is higher than that of avg-Skip-thoughts, which means that CTPE is suitable for super-long documents. CTPE can be based on embedding models of dierent granularity, such as sentence and paragraph embedding, which is several orders of magnitude less complex than CTPE that requires input of all words. The performance of word embedding is stronger than the deep language model because the latter is evaluated by unsupervised document representation without ne-tuning. The setting of the parameter𝑝𝑜𝑠in Section 4.4 is called the meaningful segmentation that divides the document into chapters or paragraphs, retaining complete paragraphs and sentences. We need to analyze how dierent segmentation positions aect the maintenance of coherence, as not all documents are suitable for this split method, such as some documents without meaningful segmentation. We examine the impact of dierent𝑝𝑜𝑠on experimental performance. We divide the document into ve equal parts and evaluate performance at four dierent𝑝𝑜𝑠(20%, 40%, 60%, 80%). Our segmentation is at the token level. The meaningful segmentation positions (all samples average) of arXiv, DBLP, and USPTO are 4.7%, 5.1%, and 40.2%, respectively. Table 5 shows the experimental results of ve positions on all datasets. We have the following observations and analysis. Experiments show that dierent segmentation positions have an impact on the maintaining of coherence because not all adjacent Table 5: Experimental results of CTPE on dierent segmentation positions. The embedding model is based on word2vec(full). The above m and avg denote the meaningful segmentation and the average value of the row. Table 6: Experimental results of CTPE on dierent sampling methods. The embedding model is based on word2vec(full). Metrics include accuracy, recall, and training epochs of the result. bodies in a document have strong coherence. Based on these results, we can draw three valuable suggestions: (1) If the document has meaningful segmentation, using meaningful segmentation can achieve excellent performance because meaningful segmentation outperforms all comparison methods, although not the best. (2) If the document does not have meaningful segmentation, using 20%40% segmentation position can also achieve excellent performance because the performance of these𝑝𝑜𝑠is the best. (3) Finding the best segmentation position will be a meaningful future work. In the process of text pair construction, negative samples are randomly combined from the former part text and latter part text of dierent documents, which makes the number of negative samples far more than positive samples. If the number of positive samples is 𝑛, then the number of negative samples is𝑛(𝑛 −1). Using negative sampling may ignore important negative samples and take longer. We propose TF-IDF sampling to analyze the eciency and performance of dierent sampling methods. In negative sampling, for document𝑑, we choose document𝑑= 𝑟𝑎𝑛𝑑𝑜𝑚(D). In the TF-IDF sampling, for the document𝑑, we select the document 𝑑= 𝑟𝑎𝑛𝑑𝑜𝑚(𝑟𝑎𝑛𝑑𝑜𝑚(𝑇(𝑑, D)), 𝑟𝑎𝑛𝑑𝑜𝑚(D)), where𝑇(𝑑, D) denotes the top 100 documents inDthat are most similar (TF-IDF similarity) to𝑑. This sampling method is based on three considerations: (1) We introduce𝑇(𝑑, D)to reduce the probability of ignoring important negative samples. (2) It is easier to overt by training with only 100 documents instead of all documents. (3) The documents obtained with TF-IDF cannot completely represent the real similar documents. If𝑟𝑎𝑛𝑑𝑜𝑚(D)is not used, the model will overt the results of TF-IDF. Table 6 shows the experimental results of CTPE and CTPE (TF-IDF sampling) on all datasets. We draw a conclusion from these results: CTPE has better performance, but CTPEis faster. The TFIDF sampling allows the model to better distinguish the dierences between similar documents, so it costs less epochs for training, but it is easier to overt the samples sampled by TF-IDF. If we have higher requirements for training speed, CTPEis more suitable, otherwise CTPE is more suitable. In this paper, we propose a coupled text pair embedding (CTPE) model for distributed document representation, a novel architecture that is able to maintain the coherence of the scientic document for better document representing. We divide the scientic document into two parts and obtain a distributed representation of the document through CNN and embedding model. We use the coupling relationship between the two parts of the document to train the model. In the future, we will focus on the improvement of model structure and the application of supervised tasks (e.g., supervised information retrieval, text classication, or clickbait and fake news identication, etc). We will try more elaborate document encodings (e.g., perhaps splitting documents into more than two parts). There are also two interesting future works that are the precise measure of coherence and automatic selection of𝑝𝑜𝑠. For natural language processing tasks with more supervisory information, we will perform supervised ne-tuning on specic tasks based on the pre-training model of CTPE. In addition, CTPE model is a good choice for some tasks that lack supervisory information. This work was partially supported by National High Technology Research and Development Program (Grant # 2017YFB1401903), the National Natural Science Foundation of China (Grants # 61876001, # 61602003 and # 61673020), the Provincial Natural Science Foundation of Anhui Province (Grants # 1708085QF156), and the Recruitment Project of Anhui University for Academic and Technology Leader.