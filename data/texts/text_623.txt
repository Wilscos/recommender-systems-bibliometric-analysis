Abstract—Real-world content recommendation marketplaces exhibit certain behaviors and are imposed by constraints that are not always apparent in common static ofﬂine data sets. One example that is common in ad marketplaces is swift ad turnover. New ads are introduced and old ads disappear at high rates every day. Another example is ad discontinuity, where existing ads may appear and disappear from the market for non negligible amounts of time due to a variety of reasons (e.g., depletion of budget, pausing by the advertiser, ﬂagging by the system, and more). These behaviors sometimes cause the model’s loss surface to change dramatically over short periods of time. To address these behaviors, fresh models are highly important, and to achieve this (and for several other reasons) incremental training on small chunks of past events is often employed. These behaviors and algorithmic optimizations occasionally cause model parameters to grow uncontrollably large, or diverge. In this work present a systematic method to prevent model parameters from diverging by imposing a carefully chosen set of constraints on the model’s latent vectors. We then devise a method inspired by primal-dual optimization algorithms to fulﬁll these constraints in a manner which both aligns well with incremental model training, and does not require any major modiﬁcations to the underlying model training algorithm. We analyze, demonstrate, and motivate our method on OFF- Media (VZM) native advertising, which is one of VZM’s largest and faster growing businesses, reaching a run-rate of many hundreds of millions USD per year. Finally, we conduct an online experiment which shows a substantial reduction in the number of diverging instances, and a signiﬁcant improvement to both user experience and revenue. The Verizon Media (VZM) native ad marketplace(previously known as Yahoo Gemini native) serves users with native ads that are rendered to resemble the surrounding native content (see Figure 1). In contrast to the search-ads marketplace, users’ intent during page (or site) visits are generally unknown. Launched seven years ago and operating with a yearly runrate of several hundreds of millions of USD, VZM native is one of VZM’s most prominent and fastest growing businesses. With billions of ad views (impressions) daily, and an inventory of hundreds of thousands of active ads, this system performs real-time auctions that take into account user preferences, ad targeting, and budget considerations. yairkoren@verizonmedia.com In order to rank the native ads for the incoming users and their speciﬁc context according to the cost per click (CPC) price type, the expected revenue of each ad is computed as a product of the advertiser’s bid and the predicted click probability (pCTR). The pCTR is calculated using models that are continually updated by OFFSET - a feature enhanced collaborative-ﬁltering (CF) based event prediction latent factor model [2]. In content marketplaces (e.g., articles, videos, ads, etc.), the set of users and content items may change abruptly and dramatically over time. Ad marketplaces are particularly susceptible since both the number of ads and their turnover in the marketplace at any given time is signiﬁcant. Ad marketplaces may easily contain hundreds of thousands and sometimes millions of ads. New ads constantly ﬂow in and old ads similarly ﬂow out. Moreover, existing ads may be turned on or off in a heartbeat (e.g., due to budget being depleted or renewed). This discontinuous behavior is also propagated to the user side since some of the most prominent user features are the users’ historical interactions with the various ads. To address these issues, models must swiftly adapt to the changing trends in the marketplace, and online incremental training is a natural choice. Moreover, space and regulation considerations often require deleting old data. If this old data is valuable for capturing long term user preference patterns, incremental training becomes a requirement. As an example, GDPR regulation requires deleting certain types of data that is older than 30 days whereas we’ve observed that our models require two to three months of training data in order to perform optimally. To address these issues, we employ OFFSET in cycles, where in each cycle the algorithm trains on a chunk of training samples using multiple instances running in parallel, where each instance is conﬁgured with a different setting of hyperparameters, e.g., step size. In particular, we train OFFSET using a variant of the Adagrad [15] algorithm. At the end of each cycle, the best performing model is selected and saved for ad ranking. This best model is also used as an initial point for the next training cycle. The chunk training is performed in near real time fashion where the difference between the time of an event and its usage within a production model used to rank ads may be as low as several minutes. The mechanism is described in detail in [3]. Consequently, we achieve two objectives: we adapt the model and its hyper-parameters to the changing trends, and more importantly, we adapt the ad ranking system to the changing trends by frequently feeding it with a fresh model. Indeed, it is our experience, as well as the experience of others (see [22], [26], [29]), that model freshness is of paramount importance to the performance of content recommendation systems deployed in a changing environment. Moreover, it is our experience that the parameters of the models as well as their optimal hyper-parameters change over time, sometimes signiﬁcantly over short periods. In such settings it is sub optimal to deploy a model trained on older data or with apriori chosen hyper-parameter settings. The optimization algorithms which are often employed when training models are not guaranteed to converge, or even stay in a bounded region, for all hyper-parameter settings. Indeed, we observed that for some hyper-parameter settings, the latent factors of our model grow uncontrollably large, causing the corresponding training instance to fail to produce a meaningful model and subsequently be discarded by our hyper-parameter tuning algorithm. When a large enough percentage of the instances are discarded, the pool of models to choose the best model from is substantially reduced, and consequently the performance of the chosen best model for ad ranking is impaired. Thus, in this work we aim to improve the robustness of the training algorithm to the choice of hyper-parameters. We show, by empirically analyzing the latent factors of the model, that only a few latent factor models are affected by bad hyper-parameter choice, and that it is indeed possible to prevent most instances from diverging without impairing the model’s performance. We then show how to achieve the goal by imposing a carefully chosen bound on the model’s latent vector norms using a mechanism that aligns well with incremental model training. We employ the duality theory in optimization to decouple model training from the bound constraint enforcement, and devise a method based on the well-known idea of primal-dual optimization algorithms such that the only change to the underlying model training algorithm is a different regularizer. We specialize the discussion to OFFSET, but the ideas we present here will likely be useful for any factorization based model. To the best of our knowledge, the study of divergent behavior when training collaborative ﬁltering models is new, and our primal-dual approach for divergence mitigation is novel. The rest of the paper is organized as follows. After reviewing related work in Section II, in Section III we describe details about the OFFSET model, its training algorithm, and how divergence is deﬁned, detected and handled during training. Then, we take a deeper look into the behavior of a diverging model in Section IV from both theoretical and empirical perspectives, and draw conclusions that lead to our method, described in Section V. Finally, we demonstrate our method’s improved performance in Section VI. The divergent behavior we study in this work boils down to failure to keep the iterates of the optimization algorithm that trains the model in a bounded region of space. To the best of our knowledge, studying and mitigating divergent behavior has received little attention in the literature, and thus the body of research devoted the subject is quite modest. It is well known (e.g. Example 1 in [8]), that when the trained loss functions grow faster than a quadratic polynomial, divergent behavior under certain hyper-parameter choices is inevitable, even for convex functions. Several works, such as [7], [14], [25], propose algorithms to mitigate the issue for the stochastic optimization setting. When certain assumptions are met, the proposed algorithms ensure that the parameters of the model indeed stay in a bounded region for any stepsize choice. These algorithms are mainly useful when little resources for ﬁne hyper-parameter tuning are available, however, their usefulness is limited in a scenario where signiﬁcant model optimization is required. Indeed, the numerical experiments in [8] show that models trained with classical algorithms using tuned hyper-parameters often perform better. Moreover, our scenario of interest does not ﬁt into the stochastic setting where the training data is sampled from a stationary distribution. The contrary is true - we operate in a non continuous, constantly changing environment, and thus we train our models in an online fashion. Our proposed primal-dual approach aligns well with online training, without introducing signiﬁcant changes the underlying training algorithm. The above is an important property when using an algorithm which has passed the test of time, and proved to be very reliable for the task at hand. A different but related line of work is the convexiﬁcation of latent factor models, such as [11], [21], [28]. In these bodies of work, the decision variables of the resulting optimization problem are inherently bounded by construction, since the convexiﬁcation procedure itself directly represents a matrix of inner products of the latent factors, instead of the factors themselves, and maintains a low rank of this matrix by imposing a threshold on its nuclear norm. These methods, despite promising to achieve a globally optimal solution in terms of the training loss, are computationally expensive and do not ﬁt the low latency scenario, where the amount of data is vast, and speed is of the essence, due to the importance of the freshness of the deployed model. The VZM native ads platform serves billions of ad impressions to several hundreds of millions of users world wide, using a native ad inventory of hundreds of thousands of active ads. Native ads resemble the surrounding page items, are considered less intrusive to users, and provide a better user experience in general (see Figure 1). The online serving system is comprised of a massive Vespa deployment [1], VZM’s open source elastic search solution, augmented by ads, budget and model training pipelines. The Vespa index is updated continually with ad and budget changes, and periodically (e.g., every 15 minutes) with model updates resulting from each training cycle. The VZM native marketplace serves several ad price-types including CPC (cost-per-click), oCPC (optimizing for conversions), CPM (cost-per-thousand impressions), CPV (cost-per-video-view), and also includes RTB (real-time biding) in its auctions. The algorithm driving Verizon media native models is OFFSET (One-pass Factorization of Feature Sets): a feature enhanced collaborative-ﬁltering (CF)-based ad click-prediction algorithm [2], which resembles a factorization machine [24]. The predicted click-through-rate (pCTR) of a given user u and an ad a according to OFFSET is given by where σ(x) = (1 + e)is the Sigmoid function, and where ν, ν∈ IRdenote the user and ad latent factor vectors respectively, and b ∈ IR denotes the model bias. The product ννreﬂects the afﬁnity score of user u towards ad a, where a higher score translates into a higher pCTR. Both ad and user vectors are constructed using their features’ vectors, which enable dealing with data sparsity and cold start issues (ad CTR is quite low in general). For ads, we use a simple summation between the vectors of their features (e.g., unique creative id, campaign id, advertiser id, ad categories, etc.), all in dimension N. The combination between the different user feature vectors is more complex in order to allow nonlinear dependencies between feature pairs. The user vectors are composed of their K-feature latent vectors v∈ IR(e.g., age, gender, geo, etc.). In particular, o entries are allocated to each pair of user feature types, and s entries are allocated to each feature type vector alone. The dimension of a single feature value vector is therefore d = (K−1)·o+s, whereas the dimension of the combined user vector is N =·o + K ·s. An illustration of this construction is given in Figure 2. The model’s parameters are the individual vectors associated with each feature value, and the model’s bias b. As with factorization machines, the advantage over the standard CF approach is that the model includes only O(K) feature latent vectors (one for each feature value, e.g., 3 for gender - male, female and unknown) instead of hundreds of millions of unique user latent vectors. To learn the model parameters θ, OFFSET minimizes the binary cross-entropy loss (or log-loss) of the training data set T (i.e., past impressions and clicks) using one-pass variant of Adagrad [15]. The loss function is as follows: where L(u, a, y) = −(1 − y) log (1 − pCTR(u, a)) y ∈ {0, 1} is the click indicator (or binary label) for the event involving user u and ad a, θare the latent vectors corresponding to the feature values of user u and ad a, and λ is a global `regularization parameter. For each training event (u, a, y), OFFSET updates its relevant model parameters by the gradient-based step where ∇L(u, a, y) is the gradient of the loss function w.r.t θ, and  is a component-wise product between two vectors. The parameter dependent step size is given by where ηis the initial step-size, α, β ∈ IRare the parameters of our variant of the adaptive gradient (AdaGrad) algorithm [15], and S is the set of training samples observed so far. The OFFSET algorithm uses an online approach where it continually updates its model parameters with each chunk of new training events (e.g., every 15 minutes for the click model). The OFFSET algorithm includes an adaptive online hyperparameter tuning mechanism [3]. This mechanism takes advantage of our parallel hardware architecture to run many instances of OFFSET in parallel in each training cycle, each instance with its own set of hyper-parameters, and strives to tune OFFSET hyper-parameters, such as η, α, and β above, to match the varying marketplace conditions. An instance is deﬁned to be diverging, and consequently is aborted and discarded from consideration by our hyper-parameter tuning algorithm, if for some latent vector v we have where τ is some predeﬁned threshold set based on previous experience, and kvkis the maximum absolute value of any component of v. In our system we use τ = 15, since form our experience, in that case the instance will almost certainly continue grownig uncontrollably large and fail to produce a meaningful model. Finally, we point out that the set of latent vectors in the model changes over time as well, since new feature values appear in the stream of training events, while old feature values disappear from the stream. For example, when a new ad appears which has not been previously encountered, a corresponding latent vector is created and initialized with normally-distributed elements with a zero mean and a small standard deviation. When an ad is no longer encountered by our training algorithm for a predeﬁned period of time, its latent vector is removed from the model. Thus, the dimension of the model parameters vector θ, which can be seen as a concatenation of all of the latent vectors of the model, changes over time as well. As mentioned in a previous section, these changes can be signiﬁcant in quantity, abrupt, and induce a non continuous dynamic to the model. Other components of OFFSET, such as its weighted multivalue feature [5], and similarity weights used for applying “soft” recency and frequency rules[4], are not presented here for the sake of brevity. An integral part of solving the divergence issue is gaining a deeper insight from both empirical and analytical perspectives. We ﬁrst take the analytical perspective by qualitatively inspecting the updates of any training algorithm based on gradient updates with coordinate-wise step sizes, such as AdaGrad [15]. Denoting the composition of the Sigmoid function with the binary cross-entropy loss corresponding for the label y by Φ, the loss in (3) can be equivalently written as Consider a training sample (u, a, y) and look at the gradient of the loss (4) with respect to some vector z corresponding to a feature of the ad a: When training the model, the algorithm will update z according to the rule: where η is some vector of coordinate-wise step sizes which depends on the training algorithm of choice. From the above we conclude that if some component of νis large enough and has the appropriate sign, the gradient update will substantially increase the magnitude of the corresponding component of z. Symmetrically, considering a vector w corresponding to some feature of the user u produces a similar argument if some component of the ad vector νor an overlapping user feature vector turns out to be large enough and of the appropriate sign, the corresponding component of wwill substantially increase in magnitude. Consequently, divergence is contagious: it’s enough for a few vector components to start growing beyond control to ’contaminate’ the entire model. To study the phenomenon empirically, we located diverging instances and plotted kνkfor all the latent vectors ν versus the number of training samples which caused that vector to be updated, just before the training in that cycle was aborted. A typical plot can be seen in Figure 3, where each point is a vector whose x coordinate is its inﬁnity norm, while its y coordinate is the number of training samples which caused that vector to be updated. Its apparent that new vectors which were not updated by many training samples, or were not updated frequently, have small norms. That’s not a surprise - newly added vectors are initialized to have small random elements. Among the more mature vectors, we see only a few vectors with exceptionally large norms, but still most vectors retain a small norm. From these two observations we draw a simple conclusion: (a) since norm growth is contagious, it’s essential to maintain vector norms below a certain threshold to mitigate divergence. (b) Imposing such a threshold without hurting the model’s performance seems to be possible, since only a small fraction of the model’s vectors will be affected. To make sure our vectors have small elements, we impose a constraint on their Euclidean norms, or equivalently, their mean-squared element: Thus, instead of training by minimizing (4), we train by striving to solve the optimization problem minΦ(logit(u, a)), subject to msqr(θ where θis the latent vector corresponding to the feature value v, and Vis the set of latent vectors corresponding to the features of user u and ad a. Recall that divergence is detected and analyzed empirically in terms of Lnorms of the latent vectors, while our constraints impose a limit on their squared Lnorm. There are two primary reasons. First, from a practical perspective, it’s easier to work with the squared Euclidean norm due to its differentiability. Second, norms in a ﬁnite dimensional real space are equivalent up to a constant, and in particular we have Therefore, making the Lnorm smaller potentially also induces a smaller Lnorm, and our numerical experiments in Section VI show that it is indeed the case. And ﬁnally, we formulate using the mean squared element instead of equivalently using the squared Euclidean norm, since there is a good heuristic for discovering the upper bound ρ for that case, which appears later in this section. Observe also that the regularization term has been removed from the loss, since imposing constraints is, in itself, a form of regularization. As a by-product, we also make the hyper-parameter tuning mechanism more efﬁcient, since we eliminate the global regularization coefﬁcient λ in (4) from the set of hyper-parameters to be tuned. In theory, solving (5) can be performed by replacing any optimization algorithm with its projected variant, e.g. gradient projection [18]: after each gradient step, vectors whose mean squared element is above the threshold are divided by a constant to normalize their mean squared element. However, projected algorithms are prohibitive in an incremental training setting where models are not trained from scratch, but instead continuously initialized from an existing model and trained over a new chunk of data. The projection operator will substantially modify the latent vectors having large norms, and it from our experience it takes a prohibitively long time for a new model to converge when initialized from a previous model which does not impose constraints. Consequently, we impose constraints using a considerably less aggressive mechanism, inspired by primal-dual optimization algorithms. The remainder of this section is devoted to describing the algorithm in detail, and to a heuristic for discovering a good upper bound ρ for the formulation (5). Before diving deeper, we ﬁrst introduce a simple approach that demonstrates the idea, although we found it to be noneffective. It’s described for two reasons. First, from a pedagogical perspective, it allows readers unfamiliar with duality to grasp the intuition behind our algorithm. Second, it demonstrates that a more systematic approach may be required, and provides some intuition about where improvements may be made. We attempt to solve (5) by replacing the the global regularization coefﬁcient in the loss (4) with a separate regularization coefﬁcient for each latent vector. Formally, upon receiving the training sample (u, a, y) we perform the following steps: Train Perform a training step using the loss Control If msqr(θ) > ρ, update µby multiplying it with some constant β > 1. Otherwise, divide µby β. If the parameter β, which controls the aggressiveness of the control mechanism, is small enough, the control mechanism will gradually increase the regularization coefﬁcients of the vectors having a mean squared element above the threshold, thus causing the following training steps to reduce their norms. Alternatively, vectors whose mean squared element is below the threshold will have their regularization coefﬁcient driven towards zero. Unfortunately, this simple control mechanism did not work in practice. We were not able to ﬁnd β which dramatically reduces the number of retained instances, without worsening the resulting model’s performance. Our conjecture was that the increase or decrease of the coefﬁcients µshould be somehow related to the severity of the constraint violation, i.e. a larger msqr(θ) − ρ should result in a more aggressive update, and duality provides a systematic way of deriving such update formulas. Duality is a central component of modern optimization theory and practice, and is described in most standard optimization textbooks, such as [9]. To make this paper selfcontained and accessible, we give a short review, and authors who are familiar should skip to the following subsection (Section V-C). Suppose we are given an arbitrary constrained optimization problem We deﬁne the following unconstrained counterpart deﬁned for µ ∈ Rwith µ≥ 0: q(µ) = minL(x, µ) ≡ f(x) +µf(x). Namely, we assign a ’price’ µ≥ 0 paid for the violation of the constraint f(x) ≤ 0. For each vector of prices µ there is an optimal value q(µ). The function L, which serves as the minimization objective of the unconstrained problem is called the Lagrangian associated with problem (6). A well known result is that q(µ) provides a lower-bound on the optimal value of the problem (6). The dual problem associated with (6) is concerned with ﬁnding the ”best” lower bound, namely: The original problem (6) is called the primal problem. A well known result in convex analysis implies that under convexity the optimal values of both problems coincide. Formally, Theorem 1 (Strong duality). Suppose that the functions f, . . . , fin (6) are convex, the optimal value of (6) is ﬁnite, and there exists ˆx such that f(ˆx) < 0 for all j = 1, . . . , m. Then, max{q(µ) : µ ≥ 0} = min{f(x) : f(x) ≤ 0, j = 1, . . . , m} A proof, and extensive material on convex duality in particular and optimization theory in general can be found in, for example, [9]. As a consequence of strong duality, the idea of maximizing the dual and minimizing the primal gave rise to a vast variety of algorithms for convex optimization, called primal-dual methods, which eventually boil down to the idea of iteratively performing a pair of steps: Primal descent Update x by performing a descent step on a variant of the Lagrangian L(x, µ), assuming µ is constant. Dual ascent Update µ by performing an ascent step on a variant of the Lagrangian L(x, µ), assuming x is constant. Prominent examples include the classical dual ascent and augmented Lagrangian methods [6], [16], [20], the primaldual methods by Nesterov [23] and by Zhu and Chan [30]. The latter was improved by Chambolle and Pock in [12], and generalized in [13]. Our problem of interest (5) is non-convex, and even if it were, we are not really minimizing the loss over a given training set, but rather training on an inﬁnite stream of samples. Thus, we cannot employ these algorithms as reliable building blocks with proven convergence guarantees. However, they do provide valuable insight into the design of powerful methods which work well in practice. Our method is a variant of the classical dual ascent method [6]. The Lagrangian corresponding to our constrained formulation (5) is L(θ, µ) =Φ(logit(u, a)) +µ(msqr(θ) − ρ) . Recalling that msqr is the mean-squared element function. Our heuristic performs a primal descent and a dual ascent step for each training sample. 1) Primal descent: We employ our regular training algorithm on the given training sample, where the Lagrangian is the training loss. Since primal descent minimizes over θ and treats µ as a constant, it is equivalent to training with the loss where dis the dimension of the latent vector θ. Consequently, primal ascent boils down to training the model using a modiﬁed regularization term, which assigns a different coefﬁcient to each latent vector. 2) Dual ascent: The partial derivatives of the Lagrangian with respect to the components of µ are An intuitive understanding of how dual ascent works can be obtained by analazying the simplest dual ascent algorithm projected gradient ascent: µ= max (0, µ+ β(msqr(θ where β > 0 is the step size for the ascent step. Here, we perform a gradient ascent step, that is followed by a projection onto the non-negative numbers. If msqr(θ) > ρ, namely, we’re violating our upper bound constraint, then the above update rule will increase µ, and thus the next primal descent step will regularize it more aggressively. If msqr(θ) ≤ ρ, then the above update rule will decrease µtowards zero, and thus the next primal descent step will regularize it less aggressively. Consequently, dual ascent is a kind of a control mechanism, and its step-size β affects its aggressiveness. A major advantage of framing the mechanism to control latent vector norms as a dual optimization problem is the fact that it opens up the entire arsenal of optimization algorithms. Consequently, duality is a systematic framework for deriving update rules for µ: every optimization algorithm implies a different control mechanism. 3) Our ascent algorithm: A well known result, e.g. [10], is that the projected gradient step (7) can be alternatively written using the proximal formulation µ= argmin−β h∇L(x, µ), µ − µi| meaning ”ﬁnd µ ≥ 0 such that the update direction µ −µ aligns with the gradient, while µ stays in close proximity to the previous iterate”. The step-size β balances between the two opposing forces. To achieve better convergence properties by adapting the algorithm to either the objective functions or the constraints, a common practice in optimization, e.g. see [10], [13], [17], is to use a more general measure of proximity instead of the squared Euclidean distance, and update µ using: µ= argmin−βh∇L(x, µ), µ − µi where D is a distance-like function called the Bregman divergence. Algorithms of this form are known as mirror ascent, as in [10], or Bregman gradient method, as in [17]. A rigorous presentation, including the deﬁnition of a Bregman divergence, can be found in [10] and references therein, and an extension to the stochastic optimization setting on unbounded domains can be found in [19]. The squared Euclidean distance is one example of a Bregman divergence. In this work, our divergence of choice is with the convention that 0 ln(0) ≡ 0, for which the generic formula (8) reduces to the multiplicative update rule Note, that this kernel results in a very simple update rule which does not involve any projection steps, and the resulting update formula is known by many names, such as the entropic gradient step [10]. As we shall see in the evaluation section, this choice works well in practice. Intuitively, when msqr(θ) > ρ, the algorithm makes increases µby multiplying it by a factor greater than one. Conversely, when msqr(θ) < ρ, the algorithm decreases µtowards zero by multiplying it by a factor smaller then one. Surprisingly, this approach, which works well in practice, bears similarity to the na¨ıve approach described in Section V-A. 4) Summary: To summarize, we modiﬁed the training algorithm such that for each training sample (u, a, y) it performs the following steps: Primal descent Update model parameters θ by train on the sample (u, a, y) with the loss Dual ascent Update the dual variables αusing the update formula in (9). We would like to stress, again, that the approach above is not an algorithm with provable convergence guarantees, but rather a method inspired by primal-dual algorithms that turned out to be very effective in practice. Moreover, note that we did not specify how the primal descent is employed - we can just keep the same training algorithm whose performance and reliability has been proved for our task at hand, e.g. OFFSET training stays unmodiﬁed in our case. Consequently, the approach is non-intrusive and generic enough for a variety of tasks which are solved by factorization models with their corresponding training algorithm. Our heuristic is based on an analysis of the sigmoid function σ(t) = (1+exp(−t)), and making some assumptions about the distribution of mass between the user and the ad side. In practice, our heuristic turned out to produce satisfactory results, and may serve as the basis for similar heuristics adapted to other factorization machines. We present it here, since a similar line of thought may be useful for deriving similar upper bounds for other factorization-machine based models. The click probabilities are computed by composing the sigmoid function σ(t) onto logit(u, a). Observe that if t /∈ [−12, 12] then σ(t) /∈ [10, 1 − 10]. Therefore, if logit(u, a) lies in the interval [−12, 12], we cover the entire range of plausible click probabilities. Consequently, our heuristic begins from requiring that Using the Cauchy-Schwartz inequality, namely, we conclude that inequality (10) is ensured when Let t > 0 be some parameter describing the ”division of mass” between the user and the ad side, and it will be determined later. Using this parameter, inequality (11) is ensured if we require that Denoting the length of the user and ad vectors by N , equation (12) can be written as: msqr(ν) ≤1N121 + t, msqr(ν) ≤1N12t1 + t Since νis constructed from products of user feature vectors in the overlapping part, we make a rough approximation by discarding the independent part of the user vectors and assuming that the components of the overlapping vectors each contribute roughly a square root of their product, and thus require that for each user feature vector z we have To summarize, equation (13) provides a bound on the mean squared element of an ad vector, whereas equation (14) provides a similar bound for user feature vectors. We would like both bounds to be to the same value ρ, and therefore we solve the equation which is equivalent to a quadratic equation. Having computed t, we substitute it into for example, its left hand side, and obtain the heuristic mean-squared element bound ρ: Figure 4 shows a plot of ρas a function of various model vector length N. Since the entire process above is a heuristic based on simplifying assumptions and unidirectional implications, imposing the bound ρon the mean squared elements is too tight, and the actual bound we use should be larger. Consequently, we used ρis used as a starting point for a simple exhaustive search: we trained ten models with ρ ∈ {kρ: k = 1, . . . , 10}, and selected the smallest k for which models trained with a larger k show no visible log-loss improvement. Our aim was reducing the number of instances discarded by the hyper-parameter tuning mechanism while improving the model’s accuracy. We therefore evaluate the performance of our method along these two criteria. We measured, over the course of a week, the fraction, between 0 and 1, of retained instances in each training cycle. The results are plotted in Figure 5, where the orange line depicts our improved algorithm, whereas the blue line depicts the algorithm deployed in production at the time. It is apparent that the production algorithm often discards over 20 percent of its training instances, whereas our improved algorithm rarely discards more than a few percent. We also measured the capability of our algorithm to actually keep vector norms under control. To that end, we plotted the maximum inﬁnity norm and mean squared element among the model’s latent vectors after each training cycle for both the new and the production model. The results are in Figure 6, where we see a vast improvement over the production model. Moreover, we see that indeed controlling the mean squared element also keeps the inﬁnity norm in check. To back the claim that, except for reducing the number of diverging instances, our method improves model accuracy, we measured the LogLoss lift between a model training with our algorithm and our production algorithm. In Figure 7 we can see that except for a short ”acclimation” period during the ﬁrst day, the new algorithm consistently improves the LogLoss metric. We also conducted an online A/B test comparing our new algorithm to the one deployed in production, in terms of revenue impact measured by CPM (the cost of 1000 ad views), and ﬁtting to user preferences terms of CTR. The results are summarized in Table I. The table shows the relative daily lifts observed on each metric by the new model relatively to the production model, e.g. CPM lift is deﬁned as 100 ×− 1. We see an over-all 0.67% CPM lift and 0.93% CTR lift, which are quite signiﬁcant for an online advertising product. We measure the reliability of the overall lifts using a Bayesian A/B testing methodology similar to [27], since it allows to model the revenue generation process. For each experiment, costs per click are modeled using an exponential distribution with parameter λ, while CTR is modeled using a Bernoulli distribution with parameter p. Consequently, CPM is modeled by CPM = 1000 ×. The lift comparing CTR and CPM of both experiments are deﬁned accordingly. By constructing a posterior distribution of p and λ for both experiments, we use Monte-Carlo simulation to compute two the 95% credible interval [l, u], such that l, u corresponds the 2.5% and 97.5% percentiles, respectively. Looking at Figure 8 and the corresponding credible intervals, it is apparent that the overall lifts are strictly positive. Finally, we show that the multiplicative updates in equation (9) are indeed better than the naive projected gradient ascent algorithm, which stems from using a Euclidean distance measure in the proximal formulation. We plotted in Figure 9 the results of comparing the entropic algorithm to the euclidean algorithm with the best-performing step-size, in terms of LogLoss, we could achieve. We can see that the entropic algorithm is both better at controlling vector norms, and at achieving a better LogLoss.