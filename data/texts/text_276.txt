Click-through rate prediction plays an important role in the ﬁeld of recommender system and many other applications. Existing methods mainly extract user interests from user historical behaviors. However, behavioral sequences only contain users’ directly interacted items, which are limited by the system’s exposure, thus they are often not rich enough to reﬂect all the potential interests. In this paper, we propose a novel method, named Dynamic Sequential Graph Learning (DSGL), to enhance users or items’ representations by utilizing collaborative information from the local sub-graphs associated with users or items. Speciﬁcally, we design the Dynamic Sequential Graph (DSG), i.e., a lightweight ego subgraph with timestamps induced from historical interactions. At every scoring moment, we construct DSGs for the target user and the candidate item respectively. Based on the DSGs, we perform graph convolutional operations iteratively in a bottom-up manner to obtain the ﬁnal representations of the target user and the candidate item. As for the graph convolution, we design a Time-aware Sequential Encoding Layer that leverages the interaction time information as well as temporal dependencies to learn evolutionary user and item dynamics. Besides, we propose a Target-Preference Dual Attention Layer, composed of a preference-aware attention module and a target-aware attention module, to automatically search for parts of behaviors that are relevant to the target and alleviate the noise from unreliable neighbors. Results on real-world CTR prediction benchmarks demonstrate the improvements brought by DSGL. Click-Through Rate (CTR) prediction is critical in many applications such as recommendation, online advertising and web search, and the main goal is to estimate the likelihood of a user clicking at an item (Zhou et al. 2019). Since accurate CTR prediction beneﬁts both business effectiveness and user experience, the topic has drawn the attention of both academic and industry communities. On many online platforms, users interact with items in chronological order, forming the historical interaction sequences. Motivated by deep learning’s expressive power in modeling sequential data, some sequential methods (Hidasi et al. 2015; Wang et al. 2017; Zhou et al. 2018a; Tang and Copyright © 2022, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. Figure 1: Illustration of Dynamic Sequential Graph for CTR prediction. DSG is a lightweight heterogeneous timeevolving ego graph combining the multi-hop connectivity in graphs and the temporal dependency in sequences. In CTR prediction, we construct DSGs for the target user and the candidate item at every scoring moment, respectively. Wang 2018; Guo et al. 2019; Li et al. 2020) utilize the recurrent neural networks or the self-attention networks to model the update of user interest through the historical behaviors, and have gained impressive success in CTR prediction. Despite the progress, the above methods only focus on mining the associations between the candidate item and the target user’s historical behaviors, suffering from some limitations. On one hand, the user behavior sequences are limited by the recommender system’s exposure. Relying only on the one-hop collaborative neighbors of a user is hard to predict his/her emerging or potential interests. On the other hand, when the interactions are sparse, especially for inactive users whose sequences are short with long time intervals, it is hard to learn high-quality representations. To tackle these challenges, we propose a novel method called Dynamic Sequential Graph Learning (DSGL), which enriches the collaborative information explicitly by constructing dynamic sequential graphs and models the sequential evolution of the multi-hop collaborative neighbors. Speciﬁcally, we propose a novel lightweight heterogeneous time-evolving ego graph, namely Dynamic Sequential Graph (DSG), to capture the dynamics of both the target user and the candidate item from their respective multi-hop collaborative neighbors at every scoring moment, as illustrated in Figure 1. Then, we perform graph convolution bottom-up iteratively on the DSGs, i.e., to learn the new representation of a node by aggregating the embeddings of the neighbors in its historical behavior sequence. The graph convolution contains two main components: A Time-Aware Sequence Encoding Layer and A Target-Preference Dual Attention Layer. The Time-Aware Sequence Encoding Layer leverages the sequential dependency and time decay information explicitly in the behavior sequence to capture the evolutionary dynamics. The Target-Preference Dual Attention Layer is composed of two module: An preference-aware attention mechanism that ﬁnds representative behaviors that are similar to the central node’s preference, and a target-aware attention mechanism that searches for parts of behaviors that are relevant to the target node. Finally, inspired by (He et al. 2020), DSGL combines the representations learned at different graph convolution layers with a weighted sum to obtain the ﬁnal embedding for prediction. Our main contributions can be summarized as follows: • We construct lightweight dynamic sequential graphs for CTR prediction. To the best of our knowledge, this work is the ﬁrst dynamic-graph-based CTR prediction method. • We propose a model DSGL that performs graph convolution on DSGs. The graph convolution operation consists of a Time-Aware Sequence Encoding Layer as well as a Target-Preference Dual Attention Layer to capture the evolutionary dynamics for nodes and alleviate the noise brought by the unreliable neighbors. • We conduct extensive experiments on real-world CTR prediction benchmarks. Experimental results demonstrate the effectiveness of our model over strong and state-ofthe-art baselines. We discuss two lines of researchs that are relevant to our work: 1) deep models for CTR prediction, and 2) recent progress of graph neural network-based methods developed for recommendation. Deep Models for CTR Prediction The prediction of Click-Through Rate (CTR) plays an important role in many applications, ranging from web search, personalized recommendation and online advertising. In recent years, deep learning based CTR prediction models have achieved remarkable success in feature interaction modeling (Guo et al. 2021; Lyu et al. 2020). Wide&Deep (Cheng et al. 2016) and DeepFM (Guo et al. 2017) combines the advantage of shallow model and non-linear deep model to learn low-order and high-order feature interactions simultaneously. PNN (Qu et al. 2016) introduces a product layer to capture high-order feature interactions between inter-ﬁeld categories. However, these methods cannot capture the interest behind data clearly. Nowadays, extracting user interest from the historical behavior for better CTR prediction has attracted increasing attention. Recurrent Neural Networks (RNN) are commonly adopted due to their power in modeling sequence (Hidasi et al. 2015; Wu et al. 2017; Hidasi and Karatzoglou 2018). Among them, GRU4Rec (Hidasi et al. 2015) is the ﬁrst RNN-based models that uses Gated Recurrent Units (GRU) (Cho et al. 2014) to capture the dependencies in users’ historical behavior sequence. Besides RNN, Convolutional Neural Networks (CNN) have also been applied to learn sequential patterns using convolutional ﬁlter (Tang and Wang 2018). In order to selectively utilize information from interactions that are truly relevant to the next interaction prediction, attention-based models are increasingly employed (Ying et al. 2018a; Zhou et al. 2018b,a, 2019; Feng et al. 2019). For example, DIN (Zhou et al. 2018b) and DIEN (Zhou et al. 2019) designs an attention mechanism to model the users’ interests from historical behaviors w.r.t. the target item. Above methods only model the user behavior with the evolution of items ignored. Motivated by it, TIEN (Li et al. 2020) deals with the item behavior by proposing an attention mechanism to achieve robust personalized item dynamics. Despite great success has been made by the above CTR prediction methods, they cannot explore the user-item interactions to the fullest extent, since they only consider the one-hop behavior. We are going to solve the problem in this paper by incorporating and exploiting the graph learning to enrich the collaborative information. Graph Neural Network for Recommendation In the last few years, Graph Neural Networks (GNN) have seen a great surge of interest with promising methods (Hamilton, Ying, and Leskovec 2017; Veliˇckovi´c et al. 2017). The effectiveness of GNNs is also proved on recommendation problems. Historically, two classes of GNNbased recommendation have been developed. The ﬁrst class considers the user-item interactions as a static bipartite graph and adapts GCN to the user-item graph, capturing Collaborative Filtering (CF) signals in high-hop neighbors for recommendation (Berg, Kipf, and Welling 2017; Ying et al. 2018b; Wang et al. 2019; He et al. 2020). However, these methods compress the dynamic timeevolving interactions into a static snapshot, disregarding the time-dependent structure, the exact timestamps (or time intervals), and emerging interactions. Thus, they fail to capture the dynamics in users and items. The second class models the interactions as dynamic graphs. One branch of dynamic-graph-based methods (Goyal et al. 2018; Pareja et al. 2020) utilize a sequence of discrete snapshots to model the time-evolving interactions, but they cannot capture the ﬁne-grained temporal and structural information, thus failing to capture the real-time interests. Although some methods (Dai et al. 2016; Kumar, Zhang, and Leskovec 2019) model dynamic graphs in continuous time and update dynamic node embeddings given each interaction, they fail to capture the higher-order temporal neighborhood structures explicitly. Another branch of such methods (Wu et al. 2019; Yu et al. 2020; Song et al. 2019; Chen and Wong 2020) model the temporal dependency of interacted items in behavior sequences as session graphs, and then adopt GNN to capture the complex transitions of items. Although these methods utilize timedependent structure to model dynamic user interests, similar to sequential model in CTR prediction, they cannot capture Figure 2: Framework of the proposed DSGL method. At every scoring moment, DSGs are constructed for the target user u (left) and the candidate item i (right) respectively. Their representations are reﬁned with multiple bottom-up graph convolutions, each of which consists of a Time-Aware Sequence Encoding Layer and a Target-Preference Dual Attention Layer. DSGL gets the ﬁnal representations via layer combination followed by an MLP-based prediction layer. high-order connectivity, thus conﬁning the performance. Our work differs from above methods in that DSGL constructs lightweight ego subgraphs for the target user and candidate item at every ranking moment and combines the advantages of graphs and sequences to capture high-order connectivity and temporal dependency simultaneously. The basic idea of DSGL is to perform graph convolution iteratively on the DSGs to enrich the collaborative information explicitly. In this section, we elaborate the design of Dynamic Sequential Graph Learning (DSGL). The methods consists of two main components: (1) Dynamic Sequential Graph Construction that establishes multi-hop collaborative connections to expand user and item behaviors. (2) Bottom-Up Graph Convolution that reﬁnes the node embedding by aggregating the embeddings of the neighbors in the behavior sequence in a bottom-up manner. Speciﬁcally, the Bottom-Up Graph Convolution consists of two layers: the Time-Aware Sequence Encoding Layer that encodes the behavior sequence with time information and temporal dependency captured and the Target-Preference Dual Attention Layer that activates the related behavior in the sequence to eliminate noisy information. Besides the above components, we also propose an embedding layer that initializes user, item, and time embeddings, a layer combination module that combines the embeddings of multiple layers to get ﬁnal representations, and a prediction layer that outputs the prediction score. The complete framework is demonstrated in Figure 2. Figure 3: An example of an item’s 2-depth dynamic sequential graph. DSG is constructed from bottom to top recursively. The edges are directed representing how messages are passed, and the direction can only be from the past node to the current node. The way we deﬁne a graph is vital for model performance. One can construct a heavy static graph containing all the users and items. However, such treatment neglect timedependent structure and the exact timestamps, which is critical to capture potential or emerging preference. Thus, we design a lightweight heterogeneous time-evolving ego graph, namely DSG, which combines the multi-hop connectivity in graphs and the ﬁne-grained temporal dependency in sequences. The DSGs are induced of historical user-item interaction edges E = {(u, i, t) ∪ (i, u, t)}, each of which represents a user u interacts with an item i associated with an timestamp t ∈ R. For each user-item pair to be scored, the corresponding DSG is constructed in a recursive way, and the process is as follows: • For a user u (or an item i) at time t, we deﬁne the 1depth DSG of user u (or item i) at time t as a set of directed interaction edges before time t in chronological order, denoted by G= {(i, u, τ)|τ < t, (i, u, τ) ∈ E} (or G= {(u, i, τ)|τ < t, (u, i, τ) ∈ E}). • We deﬁne the (k+1)-depth DSG of user u (or item i) at time t as a set of k-depth DSGs that user u (or item i) interacts in chronological order with its 1-depth DSG, G= {G|τ < t, (i, u, τ ) ∈ E}∪G(or G= {G|τ < t, (u, i, τ ) ∈ E} ∪ G). The construction process of DSG is illustrated as Figure 3. We deﬁne the historical behavior sequence of user u (or item i) at time t as a sequence of interacted items (or users) in chronological order, denoted by S= {(i, τ)|τ < t, (i, u, τ) ∈ E} (or S= {(u, τ)|τ < t, (u, i, τ) ∈ E}). In practice, we select the last r interactions before time t at each layer and control the number of layers to 2-3 layers to ensure DSG’s lightweight from the perspective of efﬁciency. Note that the depth of the candidate item’s DSG is set one less than that of the target user so that the structure of each item in the target user’s sequence is the same as that of the candidate item. There are two groups of inputs in the proposed DSGL: the target user’s k-depth DSGs Gand the candidate item’s (k1)-depth DSGs G. Features related to users can be user ID and user proﬁle, e.g. age, gender, country, and so on. For items, features can be item ID and item attributes, such as category, brand, and statistical click-through rate. For DSGs, besides the node features, each interaction is associated with a timestamp. For each ﬁeld of discrete features, we represent it as an embedding matrix, and perform embedding lookups by feature ID to obtain low-dimensional embeddings of each discrete feature. By concatenating all ﬁelds of feature embeddings, we have the node embedding of items and users, denoted by f∈ Rand f∈ R. As for the interaction timestamp in DSG, we compute the time intervals between the interaction time and its parent interaction time as time decays. For example, given a historical behavior sequence Sof user u at the timestamp t, each interaction (u, i, τ) ∈ Scorresponds to a time decay ∆= t−τ. Following (Li et al. 2020), we transform the continuous time decay values to discrete features by mapping them to a series of buckets with the ranges [b, b), [b, b), . . . , [b, b), where the base b is a hyper-parameter. Then by performing the embedding lookup operation, the time decay embedding can be obtained, denoted by f∈ R. The nodes at each layer of DSGs are in time order, which reﬂects the time-varying preference of users as well as the popularity evolution of items. Thus we perform sequence modeling as a part of graph convolution to capture the dynamics. Previous works (Hidasi et al. 2015; Li et al. 2020) usually apply Recurrent Neural Network (RNN) based model to the node feature in the historical sequence recurrently to reﬁne the behavior embeddings. They preserve only the order of behaviors in a sequence, with the impacts of different time decays ignored. However, the time decay feature is critical in recommendation. On the one hand, the time information can reﬂect the drifting of user interests. Users may not be interested in the item that they interacted with far from the current anymore. On the other hand, the time information indicates the varying audience of items. Thus, we propose the Time-Aware Sequence Encoding Layer to capture the ﬁne-grained time information explicitly. For each interaction (u, i, t), we have the historical behavior sequence Sof user u and Sof item i. For sequence S, by feeding each interacted item along with the time decay in the sequence into the embedding layer, the behavior embedding sequence is formed with the combined feature sequence, as {e|(i, τ) ∈ S}, where e= [f; f] ∈ Ris the embedding of item i in the sequence. Similarly, for sequence S, we have the embedding sequence as {e|(u, τ) ∈ S}, where e= [f; f] ∈ R. We take the obtained embedding as the zero-layer of inputs, i.e., x= eand x= e. For ease of notation, we will drop the superscript in the rest of the following two subsections. In this layer, we infer the hidden state of each node in the behavior sequence step by step with the embeddings containing time information as inputs. The encoder can be LSTM (Hochreiter and Schmidhuber 1997) or GRU (Chung et al. 2014), whose gates can utilize time feature to control the information to be propagated with the time decay feature as part of input. Given the behavior sequences Sand S, we represent j-th item’s hidden states and inputs in the sequence Sas hand x, and j-th user’s hidden states and inputs in the sequence Sas hand x. The forward formulas are where H(·, ·) and H(·, ·) represent the encoding functions speciﬁc to user and item, respectively. We obtain the corresponding hidden states sequence of historical behavior sequence Sand Safter the TimeAware Sequence Encoding Layer. For ease of notation, the process can be represented as: f({x|(i, τ) ∈ S}) = {h|(i, τ) ∈ S}; f({x|(u, τ) ∈ S}) = {h|(u, τ) ∈ S}. Target-Preference Dual Attention Layer In practice, with the multi-hop connectivity in the graph, it is almost inevitable to introduce unreliable and noisy neighbors due to unintended interactions, fake similar users, and drifting interests or trends (Zhou et al. 2018a; Li et al. 2020). To eliminate the noise, we propose a Target-Preference Dual Attention Layer that can perceive the core preferences (i.e.,the most representative behavior) meanwhile activate the most related behavior concerning the target. Thus, the attention mechanism considers the following two aspects. Firstly, we propose the preference-aware attention mechanism to perceive the core or main preferences of a user or an item from its behaviors. We computes the attention weights between the central node and its behavior neighbor nodes, which indicates the importance of each behavior neighbor to the central node. In this way, the central node is regarded as the attention query to activate the nodes most similar to it in its behavior sequence, as illustrated in Figure 4(a). Formally, given a user u (or an item i) at time t as the central node, we build the query of the preference-aware attention as: Secondly, we design the target-aware attention mechanism to soft-search parts of the central node’s behavior sequences that are relevant to its target node. In DSGs, we take the upstream node of the central node as the target node, as illustrated in Figure 4(b). We explain this by the following example. We take Anna in Figure 1 as the central node. From Anna’s behaviors, we ﬁnd that she is interested in makeup products most of the time, and browses women’s clothing occasionally. She is connected to Emma due to the co-click Figure 4: An illustration of the Target-Preference Dual Attention. action on the green dress (i.e., the upstream node). By taking the green dress as the query, the behavior related to clothing can be activated and the preference about clothing can be further delivered upward, which can enrich the collaborative information about clothing for the target user Emma. For the target user and the candidate item, as they are the root nodes of their respective DSGs, we take them as the target nodes of each other to attend related behaviors like previous works (Zhou et al. 2018b, 2019). Formally, given a user u (or an item i) at time t as the central node, we build the query of the target-aware attention as: where target(∗, t) denotes the target node of the central node ∗ at time t. We obtain the Target-Preference Dual Attention Layer by summing up the results of preference-aware attention and the target-aware attention. In detail, we apply scaled dotproduct attention as the attentive pooling method following (Vaswani et al. 2017), and the attention function is deﬁned as Attention(Q, K, V ) =softmax(QK)√ where Q, K and V represent the Query, Key and Value, respectively, and d is the dimension of K and Q. We take the behavior neighbors as the Key and Value. Moreover, we adopts the multi-head attention (Vaswani et al. 2017) to capture multiple interests or audience, deﬁned as follows: MultiHead(Q, K, V ) = [head; head; . . . ; head]W head= Attention(QW, KW, V W where weights W, W, Wand Ware trained parameters. Given the behavior hidden states sequence {h|(i, τ) ∈ S} and {h|(u, τ) ∈ S}, we represents the attention process as: Algorithm 1: The algorithm of DSGL. Input: The training set D = {(u, i, t, y)}; Interaction set E; Depth K. Output: Network parameters Θ. 1: Initialize input embedding fand f; 2: for (u, i, t, y) ∈ D do The core idea of graph convolutions is to learning representation for nodes by performing convolution over their neighborhood. Note that we perform graph convolutions in a bottom-up manner to avoid the messages propagated from future. The convolution computation for node u at the k+1th layer, which takes the input feature representation x and {x|i ∈ N} as input and outputs the induced representation x, can be abstracted as: In DSGL, we stack the Time-Aware Sequence Encoding Layer and the Target-Preference Dual Attention, and the graph convolution operation can be represented as: x= f(f({x|i ∈ S})); x= f(f({x|i ∈ S})). Different from traditional GCN models that use the last layer as the ﬁnal node representation, inspired by (He et al. 2020), we combine the embeddings obtained at each layer to form the ﬁnal representation of a user (or an item): where Kand Kdenotes the numbers of DSGL layers for user u and item i, respectively. Model Prediction Given an interaction triplet (u, i, t), with the corresponding DSGs of the target user and the candidate item, we can pre- Table 1: Overall Comparison. The bold value marks the best one in one column, while the underlined value corresponds to the best one among all baselines. MethodLogloss ↓ AUC ↑ Logloss ↓ AUC ↑ dict the possibility of the user interacting with the item as: ˆy = F(u, i, G, G; Φ) = MLP([e; e;ˆx;ˆx]) where MLP(·) represents the MLP layer. Given the real label y ∈ {0, 1} and predicted probability ˆy ∈ {0, 1}, the cross-entropy loss function is adopted, formulated as:X L = − where D is the set of training samples. The algorithm procedure are presented in Algorithm 1. In this section, we ﬁrst describe the experimental settings. Then we compare DSGL with the state-of-the-art methods. Besides, we conduct ablation study on DSGL to justify the essential components . Experimental Setups Datasets We evaluate our methods on two real-world public datasets, i.e., Amazon Datasetand Alimama Dataset. Amazon Clothing Dataset contains 2.8 million logs with 39 hundred users and 23 hundred items and each log contains three ﬁelds, i.e., user ID, item ID, and category. We perform a temporal train-test split, i.e., dividing the dataset into training and testing dataset by a cut timestamp, with the ﬁrst 85% for training and the rest 15% for testing. Alimama Dataset contains 26 million logs with 1.14 million users and 0.84 million items, and each log is composed of 14 feature ﬁelds including user ID, item ID, user age level, occupation and some other information. We use the logs in the ﬁrst 7 days for training, and logs in the last day for testing. Compared Methods The compared methods can be grouped into conventional, sequential and graph-based categories. • Conventional methods: – PNN (Qu et al. 2016) uses a product layer to capture high-order feature interactions between interﬁeld categories. http://snap.stanford.edu/data/amazon/productGraph/ https://tianchi.aliyun.com/dataset/dataDetail?dataId=56 • Sequential methods: – DIN (Zhou et al. 2018b) uses the attention mechanism to activate related user behaviors. – DIEN (Zhou et al. 2019) uses GRU with attentional update gate to model users’ dynamic interest that is relative to the candidate item. – TIEN (Li et al. 2020) leverages GRU with attention mechanism to capture both time-aware user behaviors and item behaviors. • Graph-based methods: – NGCF (Wang et al. 2019) is a GCN-based collaborative ﬁltering method. It explicitly integrates a bipartite graph structure into the embedding learning process to model the high-order connectivity. – LightGCN (He et al. 2020) simpliﬁes the design of GCN to make it appropriate for recommendation by light graph convolution and layer combination. Metrics We adopt two widely used metrics for the CTR prediction task, i.e., AUC and LogLoss. AUC (the area under the ROC curve) measures the probability that a random clicked sample is ranked higher than a random non-clicked sample. LogLoss is the cross-entropy loss on the test dataset. Reproducibility For PNN, DIN, DIEN and TIEN, we use the the open-source implementations. For NGCFand LightGCN, we use the source code provided by the authors. Further implementation details and the codes are provided in the supplementary material. Performance Comparison To demonstrate the overall performance of the proposed model, we compare DSGL with the state-of-the-art recommendation methods. All experiments are repeated 10 times and averaged results are reported in Table 1. We have the following observations. • DSGL consistently outperforms all other baselines on both datasets. The improvements on AUC scores of DSGL over the best baseline model are 1.51% and 3.24%. In our practice, even 1% improvement in AUC is substantial to achieve signiﬁcant online promotion. • The performance of the static-graph-based methods, i.e., LightGCN and NGCF, are not competitive. The reasons are two folds. First, these methods ignore the new interactions in the testing set in the inference phase. Second, since they do not model the temporal dependency of interactions, they cannot capture the evolving interests, degrading the performances compared with sequential models. • All of the sequential models outperform the conventional methods and static-graph-based methods by a large margin, proving the effectiveness of capturing temporal dependency in recommendation. https://github.com/itemevolutionnet/ItemEvolutionNet https://github.com/xiangwang1223/neural graph collaborative ﬁltering https://github.com/kuandeng/LightGCN Since LightGCN and NGCF load the whole graph in memory, causing memory overﬂowing on large-scale graphs with million nodes, so we didn’t report the performance on Alimama Dataset. Table 2: Results of DSGL at different layers and the variant that does not use layer combination (i.e., w/o LC). MethodLogloss ↓ AUC ↑ Logloss ↓ AUC ↑ Table 3: Performance of DSGL with different use of time information. MethodLogloss ↓ AUC ↑ Logloss ↓ AUC ↑ Ablation Study In this section, we perform the ablation studies to show the necessity of the graph structure, and verify the effectiveness of the proposed Time-Aware Sequence Encoding Layer, Target-Preference Dual Attention Layer and Layer Combination. Effectiveness of Graph Structure and Layer Combination. Table 2 shows the results of DSGL at different layers and its variant DSGL w/o LC that use the last layer instead of the combined layer as the ﬁnal representation. We have the following observations: • Focusing on DSGL with layer combination, the performance gradually improves with the increasing of layers. We attribute the improvement to the collaborative information carried by the multi-hop connectivity in the graph structure. • Comparing DSGL and DSGL w/o LC, we ﬁnd that removing the layer combination degrades the performance largely, which demonstrates the effectiveness of layer combination. Effectiveness of the Time-Aware Sequence Encoding Layer. In DSGL, we perform the Time-Aware Sequence Encoding Layer to preserve both the temporal dependency of behaviors and the ﬁne-grained time information. Thus, we design ablation experiments to study how the temporal dependency and time information in DSGL contributes to the ﬁnal performance. To evaluate the role of time informa- Table 4: Performance of DSGL with different settings of attention. MethodLogloss ↓ AUC ↑ Logloss ↓ AUC ↑ tion, we test the removal of time feature (i.e., w/o time). To evaluate the contribution of the behavior order, we test the removal of the sequence encoding module while retaining time information (i.e., w/o Seq ENC) and the removal of the Time-Aware Sequence Encoding Layer (i.e., w/o TASE). The comparison is shown in Table 3. We have the following observations: • DSGL outperforms DSGL w/o TASE by a signiﬁcant margin, demonstrating the efﬁcacy of the Time-Aware Sequence Encoding Layer. • Comparing DSGL w/o time with the default DSGL, we observe that removing the ﬁne-grained time decay information will cause performance degradation. • DSGL outperforms DSGL w/o Seq ENC, conﬁrming the importance of temporal dependency carried by the historical behavior sequence. Effectiveness of the Target-Preference Dual Attention Layer. In DSGL, we propose a Target-Preference Dual Attention Layer that consists of the target-aware attention and the preference-aware attention to eliminate noise from unreliable neighbors. To justify its rationality, we explore different choices here. We test the performance without the proposed attention (i.e., DSGL w/o ATT). We also remove the target-aware attention part (i.e., DSGL w/o TAATT) and the preference-aware attention part (i.e., DSGL w/o PAATT) from the dual attention layer respectively. From the results in Table 4, we have the following observations: • The best setting in all cases is adopting the Target- Preference Dual Attention (i.e., the current design of DSGL). Removing either the target-aware part or the preference-aware part drops the performance, demonstrating the effectiveness of dual attention in activating related neighbors and eliminating the noise. • When the attention mechanism (i.e., DSGL w/o ATT) is removed, the performance degrades largely. In some cases, the performance is even not as good as the best baseline. The observation demonstrates the necessity to introduce the attention mechanism in GNN-based recommendation methods due to the inevitable noise in the multi-hop neighborhood. In this paper, we focus on explicitly incorporating multi-hop collaborative signal into the CTR prediction model while capturing the dynamic evolution. We propose a novel graphbased method, named Dynamic Sequential Graph Learning (DSGL), to enhance users or items’ representations by performing graph convolution over their dynamic sequential graphs. Comprehensive experiments demonstrate that DSGL can consistently outperform the other state-of-art methods. Future directions include modeling long-term dependencies on the dynamic sequential graph as well as sampling reliable neighbors to eliminate noise from the source.