Despite the widespread use of graphs in empirical research, little is known about readers’ ability to process the statistical information they are meant to convey (“visual inference”). We study visual inference within the context of regression discontinuity (RD) designs by measuring how accurately readers identify discontinuities in graphs produced from data generating processes calibrated on 11 published papers from leading economics journals. First, we assess the effects of different graphical representation methods on visual inference using randomized experiments. We ﬁnd that bin widths and ﬁt lines have the largest impacts on whether participants correctly perceive the presence or absence of a discontinuity. Incorporating the experimental results into two decision theoretical criteria adapted from the recent economics literature, we ﬁnd that using small bins with no ﬁt lines to construct RD graphs performs well and recommend it as a starting point to practitioners. Second, we compare visual inference with widely used econometric inference procedures. We ﬁnd that visual inference achieves similar or lower type I error rates and complements econometric inference. Key Words: Graphical Methods; Visual Inference; Regression Discontinuity Design; Expert Prediction; Statistical Decision Theory; Scientiﬁc Communication JEL Code: A11, C10, C40 Zwiers (2020) call the “graphical revolution.” Effective use of graphs conveys a large set of statistical information at once and improves research transparency (Andrews, Gentzkow, and Shapiro, 2020). However, there are different ways to construct a graph with the same data, and the particular construction an analyst chooses has the potential to mislead readers (Schwartzstein and Sunderam, 2021). To understand the best use of graphical evidence, it is important to study readers’ ability to process information from graphs—which we term visual statistical inference or visual inference per Majumder, Hofmann, and Cook (2013)—as well as the sensitivity of visual inference to choices in graph construction. To date, little is known about visual inference for commonly presented graphs in empirical research designs. (RDD or RD design). The popularity of RDD in the modern causal inference toolkit, which began in economics with Angrist and Lavy (1999), makes it an important setting in which to study visual inference. Standard practices in applying RDD today perhaps best embody the spirit of Watson’s quote above, with graphs playing a central role in the presentation of ﬁndings. The key RD graph plots the bivariate relationship between outcome variable Y and running variable X and is meant to display a discontinuity (or lack thereof) in the underlying conditional expectation function (CEF) as X crosses a policy threshold. Inﬂuential practitioner guides by Imbens and Lemieux (2008), Lee and Lemieux (2010), and Cattaneo, Idrobo, and Titiunik (2019b) recommend creating this graph by dividing X into bins, computing the average of Y within each bin, and generating a scatter plot of these Y -averages against the midpoints of the bins. accurately extract the embedded statistical information, where our main criterion is the correct identiﬁcation of the existence or absence of a discontinuity at the policy threshold. Our project has two major components. In the ﬁrst, we build on work pioneered by Eells (1926) and reﬁned by Cleveland and McGill (1984) and conduct a series of randomized experiments to examine how different graphical parameters affect visual inference in RD. We present participants recruited through the Cornell University Johnson College’s Business Simulation Lab with RD graphs produced from data generating processes (DGPs) based on microdata from Graphical analysis is increasingly prevalent in empirical research, a phenomenon Currie, Kleven, and We begin to ﬁll this knowledge gap and study visual inference in the regression discontinuity design We assess the performance of visual inference by studying whether people presented with this graph can 11 published papers that we randomly selected from a list of 110 empirical studies from top economics journals. For each graph, we ask participants to identify the existence or absence of a discontinuity. We randomize respondents into different treatment arms and show participants within each arm graphs produced with particular graphical parameters such as small bin widths and evenly spaced bins. taneo, and Titiunik (2015) propose two popular data-driven bin width selectors: one that minimizes the integrated mean squared error (IMSE) of the bin averages, resulting in fewer, larger bins, and another that mimics the variability of the underlying data (mimicking variance or MV), which leads to more, smaller bins. While both proposals set a graphical parameter to satisfy an econometric criterion, practitioners are left with little basis to choose between them. Moreover, a host of other choices over graphical parameters remains with minimal guidance from the literature, such as including smoothed regression lines in the binned scatter plot, adding a vertical line to indicate the policy threshold, and choosing the axis scales. can assess the advantages and disadvantages of different graphical parameters. We ﬁnd that certain graphical parameters such as bin width and imposing smoothed regression lines create important tradeoffs between type I error (identifying a discontinuity when there is none) and type II error (identifying the absence of a discontinuity when there is one) rates. Relative to MV (small) bins, using IMSE (large) bins tends to increase type I error rates but decrease type II error rates. Similarly, imposing ﬁt lines may also increase type I error rates, echoing the concerns by Cattaneo and Titiunik (2021a,b). two decision theoretical frameworks that build on the recent economics literature (Kline and Walters, 2021 and Andrews and Shapiro, 2021). Our two frameworks use classiﬁcation accuracy and reader conﬁdence in her classiﬁcation, respectively, as metrics to facilitate comparisons of graphical methods. In both frameworks, the method that uses MV bins with no ﬁt lines consistently performs well relative to IMSE bins or imposing ﬁt lines. Bin spacing (equally spaced versus quantile-spaced), axis scaling, and the presence of a vertical line indicating the policy threshold do not appear to matter, implying that researchers can adhere to reasonable personal preferences. we may be concerned that our results are less relevant to academic audiences. To assess whether our ﬁndings generalize across experience levels, we also recruit experts from a pool of seminar attendees and afﬁliates There is limited research on how to choose these parameters in practice. For example, Calonico, Cat- By comparing the rates at which respondents correctly classify discontinuities across treatment arms, we To translate our ﬁndings to recommendations on graphical practices in RDDs, we empirically implement Because only non-experts participate in our randomized experiments on the effects of graphical methods, of the National Bureau of Economic Research (NBER) and the Institute of Labor Economics (IZA) to participate in our study. Although our expert sample is not large enough to conduct the same randomized experiments, we can compare the non-expert and expert results by using the subset of non-experts who saw the same graphs as the experts. We ﬁnd that the two groups perform comparably. parameters that result in the highest rate of visual inference success by non-experts. We ﬁnd that experts only partly anticipate the aforementioned effects of bin widths and ﬁt lines. econometric inference. For visual inference, we use results from the sample of experts who viewed graphs constructed with the best-performing technique from our experiments. For econometric inference, we apply three inﬂuential methods by Imbens and Kalyanaraman (2012), Calonico, Cattaneo, and Titiunik (2014), and Armstrong and Kolesár (2018) (henceforth IK, CCT, and AK) and conduct hypothesis testing at the 5% (asymptotic) level. We ﬁnd that visual inference achieves a type I error rate which, at just below 8%, is lower than the IK and CCT procedures (the CCT type I error rate is not signiﬁcantly higher), but the two econometric procedures enjoy considerably lower type II error rates. Visual inference performs very similarly to the AK procedure, a remarkable result given the minimax optimality property of AK. distribution of visual and econometric tests: while they commit similar type II errors, there does not appear to be a strong association in their type I errors. Second, we assess the performance of a combined visual and econometric inference. One simple way of combining the two inferences mirrors the practice in which a researcher believes a discontinuity exists if and only if a formal test rejects the null of no effect and she sees it with her own eyes. We ﬁnd that the combined IK and visual inference performs similarly to the most recent AK procedure, which may help explain the enduring credibility of the RD design despite formal inference issues in earlier RD papers. At the same time, if many researchers already adopt this practice, then the de facto type I error rate is lower than intended by the econometrician, suggesting a higher-than-intended bar for empirical evidence. we compare the accuracy of their estimates to that by econometric methods. Econometric methods tend to do better on this front. For example, the simple local linear IK estimator yields lower mean squared errors than experts across all 11 DGPs, shedding light on the limit of visual inference. In the spirit of DellaVigna and Pope (2018), we also test whether experts are able to predict the graphical As a second major component of the project, we compare the performances of visual inference and Furthermore, visual and econometric inferences appear to be complimentary. First, we examine the joint Finally, we ask experts to estimate the discontinuity magnitude when they classify a discontinuity, and ﬁll an important gap in our understanding of graphical evidence by evaluating visual inference and graphical representation practices in a widely used quasi-experimental research design. Our endeavor draws from three strands of the statistics literature that study the choice of graphical parameters (e.g. Calonico et al., 2015, Li et al., 2020), their effects on visual inference (e.g. Cleveland and McGill, 1984), and the evaluation of visual inference through comparison with econometric inference (e.g. Majumder et al., 2013). paradigm that can be applied to other important areas (see Section 5 for more discussion). Using 11 DGPs calibrated to RD microdata allows us to carry out a more comprehensive and empirically relevant evaluation of econometric inference procedures than econometric studies that typically rely on two or three DGPs. tual framework, which may extend to future studies of visual inference in other contexts. In particular, we can interpret the average type I (or II) error rate we use as an estimate of the probability that a randomly sampled reader commits such an error when viewing a graph generated from a randomly chosen DGP. We show that these error probabilities are key inputs in a decision theoretical framework similar to that by Kline and Walters (2021), which helps guide the best graphical practice. We also bring together the recent theory literatures on scientiﬁc communication (e.g., Andrews and Shapiro, 2021) and persuasion (e.g., Schwartzstein and Sunderam, 2021) that shed light on the role of statistical graphs. We demonstrate the broad utility of Andrews and Shapiro (2021) by empirically implementing their framework adapted to our experiments to better understand the merits of different RD graphical methods. Relating to the persuasion literature, our paper documents that framing inﬂuences the conclusions drawn from the data (even for experts), sheds light on presentation strategies that limit incorrect inference, and suggests avenues for future studies on elements of visual inference and graphical representation not yet featured in existing persuasion models. forecasts of research results (e.g. Sanders, Mitchell, and Chonaire, 2015). Our ﬁnding that experts only partly anticipate our experimental results underscores the value of empirically evaluating visual inference and providing evidence-based guidance on graphical methods. in Section 3, present results in Section 4, and conclude in Section 5. For readers in a hurry, the takeaway results are in Figures 5 and 8 with corresponding discussions in Sections 4.1 and 4.3. This paper connects a diverse set of literatures and makes the following contributions. First, we begin to Second, we are the ﬁrst within economics to use lab experiments to study empirical methods, a new Third, to guide our study design and to help interpret our empirical results, we propose a general concep- Fourth, we add to the literature on expert judgments (e.g. Camerer and Johnson, 1997) and expert We introduce the conceptual framework in Section 2, describe the design of our experiments and studies In this section, we propose a conceptual framework for evaluating visual inference to guide our study design and aid in the interpretation of our empirical results. First, we show how to aggregate visual inference performances across subjects, who may reach different conclusions even when viewing the same graph, and meaningfully interpret the parameter our aggregate measure corresponds to. Second, we adapt decision theoretical models used by recent economic studies to guide our search for the best graphical practice as informed by the aggregate measures. conveying discontinuity existence and magnitude at the policy threshold. According to Lee and Lemieux (2010), other purposes of RD graphs include i) helping to assess regression speciﬁcations and ii) allowing for the inspection of discontinuities away from the policy cutoff. But ultimately, these other functions are also motivated by inference on the discontinuity at the policy threshold: i) can be viewed as reconciling visual and econometric inferences thereof and ii) informs the reader, under implicit global homogeneity assumptions, whether to believe the existence of a discontinuity at the policy threshold. measures for visual inference. a continuous graph as having a discontinuity and a type II error if she classiﬁes a discontinuous graph as continuous. In Section 2.1, we deﬁne the key population type I and type II error probability parameters of interest and propose estimators we can implement using our experimental data. We then show in Section 2.2 that these parameters are key inputs in a decision theoretical framework that can point to the best graphical practice by weighing the tradeoff of type I and type II error probabilities. In the same section, we also propose an alternative framework for evaluating graphical methods that builds on Andrews and Shapiro (2021)’s work on scientiﬁc communication. In section 2.3, we discuss additional conceptualizations of the role of graphs by drawing from the recent work by Schwartzstein and Sunderam (2021) on model persuasion and related studies. Although RD graphs may serve other purposes, we view their most important function as accurately We focus on binary classiﬁcations of a graph and treat type I and type II errors as the main performance To deﬁne the measures of visual inference performance, we introduce the following notations. First, the vector γ denotes a combination of graphical parameters (see Wilkinson, 2013 for an extensive list). We study ﬁve parameters in this paper, bin width, bin spacing, axis scaling, the use of polynomial ﬁt lines, and inclusion of a vertical line, and each of the ﬁve entries of γ represents the value of a particular parameter. elements: i) the distribution of the running variable X; ii) the conditional expectation function E[ which is continuous at the policy threshold x = 0; iii) the distribution of the error term u where x] + u; and iv) the sample size N. Intuitively, g speciﬁes everything in the probability model except for the discontinuity, including the shape of the conditional expectation function. The discontinuity then results from shifting the right arm of the smooth function E[ ˜Y + d ·1 in Section 3.2). We note that the variable Y can represent the outcome, baseline covariates, or treatment take-up, so this framework applies to all graphs typically included in RD studies, including those from a fuzzy design. Typically, the (g,d) combination is jointly referred to as the “data generating process,” but we separate out the discontinuity level d and call g the DGP for ease of exposition below. note by W—or W (g,d) if we need to emphasize the underlying probability model. Implementing a graphical procedure with parameters γ on dataset W results in an RD graph (γ,W ), which we denote by T or T(γ,g,d). Alternatively, we can think of T as a realization from (γ,g,d) and refer to (γ,g,d) as the graph generating process (GGP). some readers may be more skilled than others at classifying a discontinuity because they have received more training in statistics, have more experience with RD graphs, or otherwise have superior ability. We use φ to capture these human characteristics that affect graph perception. T (γ,g,d) is denoted by ˜p(T (γ,g,d),φ). From casual observation, we know that the same reader may The combination (g,d) denotes the probability model underlying an RD dataset. g encompasses four We think of each (bivariate) RD dataset as a realization from the probability model (g,d), which we de- When presented with the same RD graph, readers may draw different visual inferences. For example, The probability that a reader with characteristics φ reports that a discontinuity exists in RD graph be inﬂuenced by idiosyncratic elements not encapsulated in φ and classify the same graph differently on different days. The probability formulation ˜p allows these factors to affect visual inference. averaging ˜p over both data realizations W and reader characteristics φ leads to the quantity This is the probability that a randomly chosen reader reports a discontinuity in a graph randomly generated from the GGP (γ,g,d). A high value of p indicates a high classiﬁcation error probability when the true discontinuity d is zero (type I error), but a low classiﬁcation error probability when d is nonzero (type II error). Formally, the DGP-speciﬁc or g-speciﬁc type I and type II error probabilities for graphical parameter γ are deﬁned as: Conceptually, we can further average p(γ, g,d) over the space of DGPs, G (discussion of G after Assumption 1 below), to arrive at the overall discontinuity classiﬁcation probability for γ: Correspondingly, the overall type I and type II error probabilities are deﬁned as Consistent with the deﬁnitions in Casella and Berger (2002, p. 382), we call p(γ,g,d) and ¯p(γ,d) power functions as functions of d. We show in Section 2.2 below that the overall power function, ¯p(γ,d), is a key input into a Bayes risk criterion we use to evaluate graphical methods. above. For each GGP (γ, g,d), we generate M different realized graphs and present each to a random participant. That is, participant i is shown one RD graph denoted by T the set {1,...,M}, and is asked to assess the presence of a discontinuity. Let the binary variable R denote participant i’s discontinuity classiﬁcation, which equals one if the participant reports a discontinuity at the policy threshold. Under random sampling, the following assumption holds: We now deﬁne the type I and type II error probabilities we use to gauge reader performance. First, In this paper, we design experiments to estimate the type I and type II error probabilities as deﬁned Proposition 1 in Appendix A.1 states the distribution of ˆp(γ,g,d) and shows the estimator to be unbiased and consistent as M → ∞ for p(γ,g,d) under Assumption 1. deﬁne in Appendix A.2. While the inﬁnite dimensionality of G makes it difﬁcult to theoretically characterize the distribution of DGPs, we think of the data used in empirical RD research as realizations when sampling from G according to this distribution. To that end, we can specify J DGPs that approximate data from existing research and present graphs generated with discontinuity d for each DGP g distinct group of M participants for a total of M ·J participants and visual discontinuity classiﬁcations. Assumption 2. The DGP g the average of discontinuity classiﬁcations across the M ·J classiﬁcations. Proposition 2 in Appendix A.1 states the distribution of ¯p(γ,d) under Assumptions 1 and 2 (given that J = 11 in our experiments, consistency here is a conceptual statement implying that were we to incorporate DGPs from more RD studies, our estimators would be closer in probability to the population parameters of interest). We henceforth refer to ˆp(γ, g,d) when d = 0 and 1 − ˆp(γ,g,d) when d 6= 0 as the DGP- or g-speciﬁc type I and type II error rates, respectively. We refer to ˆ¯p(γ,d) when d = 0 and 1 − type I and type II error rates, respectively. based on a discontinuity estimator, expressions because we directly implement testing procedure, which we set to 5%, the prevailing standard in empirical studies. Because the deﬁnitions of these probabilities and their estimators are similar to the quantities deﬁned above, we omit them here. A natural estimator for p(γ, g,d) is the sample average of discontinuity classiﬁcations: To estimate the overall probability ¯p(γ,d), we need to sample from the DGP space G , which we formally We can also deﬁne the type I and type II error probabilities of an econometric inference procedure In subsequent sections, we empirically trace outˆ¯p(γ,d) as functions of d, which concisely summarize the type I and type II error probabilities of a graphical method. For brevity, we also use the term “power functions,” as opposed to “estimated power functions,” to refer to their empirical estimates. We study visual inference by comparing its power functions functions of various econometric inference procedures. We discuss the calculation of the standard errors on the differences between the visual and econometric power functions when we present our empirical results in Section 4, as its details depend on the design of our experiments. is the fraction of continuous graphs participants incorrectly classify as having a discontinuity, and the type II error rate is the fraction of discontinuous graphs incorrectly classify as being continuous. Our framework allows us to interpret these rates as unbiased and consistent estimates of the probabilities of type I and type II errors a randomly chosen person commits when classifying a graph generated from a representative DGP. As we show in the next section, these probabilities play an important role in our decision theoretical framework that sheds light on best graphical practice. We present two decision theoretical frameworks for evaluating graphical methods. In the ﬁrst framework, we incur a loss from reader i misclassifying graph T where κ and ϕ are the costs of a type I and type II error, respectively. This loss function generalizes the zero-one loss (e.g., p. 20 of Friedman, Hastie, and Tibshirani, 2001) to allow asymmetric loss for the two error types. A similar setup was recently used by Kline and Walters (2021) to formulate an auditor’s decision on which employers to investigate for discrimination (see also Storey, 2003). the expected loss or risk for DGP g: Further integrating R over the distribution of g leads to the average or Bayes risk: In summary, we have deﬁned the type I and type II error rates for visual inference. The type I error rate Averaging Lover i, which encapsulates averaging across both readers and graph realizations, leads to risks. Second, unlike econometric inference, it is hard to theoretically control the type I error probability of visual inference under a pre-speciﬁed threshold, and different graphical methods often lead to type I and type II error tradeoffs. To choose a method in the presence of these tradeoffs, we need to specify the cost parameters κ and ϕ and further average over d according to a prior probability of encountering graphs with different discontinuity levels. These speciﬁcations entail subjective judgement, and we discuss our choices in Section 4.1.1 when we estimate the classical risks with experimental data. Third, we can deﬁne the minimax graphical method as the best performing method under the most adverse DGP. While the vastness of the DGP space makes it difﬁcult to estimate the population maximal risk, the best performing graphical method in terms of the estimated Bayes risk deﬁned above also does very well under the sample maximal risk among our DGPs. munication by Andrews and Shapiro (2021) (henceforth AS), who propose the so-called communication risk and show that reporting certain statistics achieves lower communication risk than others. Unlike the classical risks, which take each reader’s classiﬁcation as given and incorporate it directly into the loss, the AS risk formulation starts from each individual’s optimal decision problem. generated from GGP (γ,g,d) is We use the capital letter D to denote the reader’s perceived discontinuity. More precisely, D is the random variable that represents the discontinuity unknown to the reader, for which she perceives a distribution. The expectation E graph. κ two values of δ represent the classiﬁcation choices, and risk minimization leads to the reader’s discontinuity classiﬁcation R (i.e., R 0|T risk is We make three remarks. First, the type I and type II error probabilities are key inputs into the classical Our second framework for evaluating graphical methods builds on the recent work on scientiﬁc com- Adapted to our context, the (posterior) AS communication risk for reader i when viewing graph T and ϕare the costs the reader incurs for committing a type I and type II error, respectively. The = 1) if she is reasonably certain. Denoting her perceived discontinuity probability by q≡ Pr(D 6= ), the reader classiﬁes a discontinuity when qis above the cutoff ς≡κ/(κ+ ϕ), and her posterior AS which is tent-shaped as a function of q our experimental design (only the ratio of the two parameters matters in determining the optimal graphical parameter, but choosing a value of 1 leads to easily interpretable quantities). The risk simpliﬁes to 1 −β where β highest value of 1 when she is certain that the graph is continuous (q attains the lowest value of 0.5 when she is completely unsure (q preferred under the AS communication risk if it leads to higher reader conﬁdence (this form of the AS risk is also known as the (mis)classiﬁcation risk, see e.g., Kitagawa, Sakaguchi, and Tetenov, 2021). where we make explicit the dependence of β participants, we can interpret the expectation above as ﬁrst averaging over the realizations of graph T for each participant type φ as deﬁned in Section 2.1, and then averaging over the distribution of φ . The ﬁrst average is the AS ex ante communication risk if each reader correctly anticipates the objective distribution of T (the ex ante communication risk corresponds to the integral of the ex post communication risk with respect to the reader’s subjective prior distribution of T , which is hard to ascertain). Because the second average is over the φ-distribution, we can interpret R where the weights reﬂect the composition of our experimental subjects. As before, we can further average the DGP-speciﬁc risk with respect to the distribution of g to deﬁne an overall average communication risk: being correct. We do not directly elicit them from the participants in our experiments but can approximate them with each participant’s graph-speciﬁc choice of a risky or risk-free payment scheme. We discuss the payment schemes in Section 3.3.2 and compare across graphical methods the estimated values of the communication risk in Section 4.1.1. Finally, we offer another lens to view the role of graphs by connecting to the recent literature on persuasion. A particularly relevant study is by Schwartzstein and Sunderam (2021) on “model persuasion” (related ≡ 0.5 + |q−0.5|. βis simply the reader’s perceived probability of being correct: it attains the Averaging over i, we arrive at the AS risk for GGP (γ, g,d): Estimating the AS communication risks requires measures of β, each reader’s perceived probability of works include Eliaz and Spiegler, 2020, Bénabou, Falk, and Tirole, 2018 and Olea et al., 2019). There are two actors in the Schwartzstein and Sunderam (2021) framework, an analyst and a reader. The analyst presents the reader a model of his choosing (in the form of a likelihood function) to persuade the reader to interpret the data in a way that will beneﬁt him. The reader has a default model in mind, but she will adopt the analyst’s model if it ﬁts the data better. authors write “[w]hen social scientists want to build the case for a particular conclusion, they may draw curves through data points in ways that make the conclusion visually compelling,” and they humorously illustrate their point with a comic strip (Figure 1 of the paper shows stylized scatter plots from xkcd.com/ 2048/). We can view the choice of the other graphical parameters through the same lens, where the analyst tries to inﬂuence the reader’s visual perception of how well his model ﬁts the data. ments in the visual inference process. First, in Schwartzstein and Sunderam (2021), the reader chooses between her ex-ante default model and the model supplied by the analyst, but seeing a graph should allow the reader to adopt a model that is neither her default nor the one supplied by the analyst. That is, she may draw her own conclusion upon seeing a graph, a process better captured by the AS framework. Second, Schwartzstein and Sunderam (2021) assume that the analyst and the reader can access the same data, but in our context the reader may only have summary empirical results supplied by the analyst. In fact, RD graphs may be the most disaggregated data a reader has access to, and the bin width dictates the level of granularity. In this sense, we can also view the bin width choice through the lens of how much information to disclose. Andrews and Shapiro (2021): rather than having a disinterested analyst, it gives him a stake in the belief of the reader. Recent studies by Banerjee et al. (2020) and Spiess (2020) combine elements from both papers: they study statistical problems—experimental design and covariate adjustments—using decision theory while accounting for analyst preference. The research referenced here can thus orient future studies on the role and use of statistical graphs. Schwartzstein and Sunderam (2021) ﬁnd a natural application of their theory in the use of ﬁt lines. The However, the formal model persuasion theoretical framework does not yet accommodate certain ele- Although the persuasion framework does not capture all aspects of visual inference, it complements We test the effects of bin width, bin spacing, parametric ﬁt lines, vertical lines at the policy threshold, and y-axis scaling. We discuss each of these treatments in detail below and provide graphical illustrations of each in Figure 1. class of bin width selection algorithms comes from Lee and Lemieux (2010): start with some number of bins, double that number, test whether the additional bins ﬁt the data signiﬁcantly better, and repeat until the test fails to reject the null. Calonico et al. (2015) propose two bin width selection algorithms based on different econometric criteria. The ﬁrst, which is more in line with the convention of the nonparametric regression literature is the bin selector that minimizes the IMSE of the bin-average estimator of the CEF, where the resulting number of bins increases with the sample size N at the rate N bin selector, the MV selector, Calonico et al. (2015) state that they “choose the number of bins so that the binned sample means have an asymptotic (integrated) variability approximately equal to the amount of variability of the raw data.” The resulting number of bins increases with the sample size more quickly, at the rate N/log(N) widths, than the MV algorithm (we describe these algorithms further in Appendix A.3). In addition to these two algorithms, Calonico et al. (2015) provide an interpretation for any given number of bins as the output of a weighted IMSE-optimal algorithm. Applying the Lee and Lemieux (2010) algorithms to our datasets typically leads to bin numbers in between the IMSE and MV selectors that tend to be closer to those of the IMSE. Thus, we restrict our analysis to the visual inference properties of the IMSE and MV bin selectors. resulting bins may contain vastly different numbers of observations, or even none at all. when the distribution of the running variable is far from uniform. As a remedy, Calonico et al. (2015) also propose quantile-spaced bins where each bin contains (approximately) the same number of observations. Both spacings support IMSE and MV bin selectors, and we test each of these combinations. The most studied graphical parameter in RD is the width of each bin in the binned scatter plot. The ﬁrst Although the prevailing approach is to adopt evenly spaced bins, this method has drawbacks in that the Following suggestions by Imbens and Lemieux (2008) and Lee and Lemieux (2010), RD graphs frequently feature parametric ﬁt lines and a vertical line at the treatment threshold. Both papers suggest ﬁt lines improve “visual clarity” by approximating the conditional expectation functions, and the default in the popular rdplot command by Calonico et al. (2015) uses piecewise global quartic regressions on each side of the policy threshold. Of the 11 RDD papers on which we calibrate our DGPs, ten include ﬁt lines. For the six of these papers that generate ﬁt lines using polynomial regressions, we use the same polynomial order as in the source graph, and in the remaining cases, our team unanimously decides on the ﬁt that best matches the original ﬁt line or the data. We could also use a formal data-driven approach to select the polynomial order, but different criteria from Lee and Lemieux (2010) lead to conﬂicting recommendations (for example, for our ﬁrst DGP, the Akaike information criterion selects a ﬁfth-order polynomial, while the Bayesian information criteria and F-test they describe choose a zeroth-order polynomial). We test all combinations of these two treatments except for including the ﬁt line and not having a vertical line, which is used infrequently in practice (our literature review shows that fewer than 10% of papers use this combination). that correlations on scatter plots seem stronger when scales are increased. We use two axis scaling options in our experiments. First is the default output returned by Stata 14. Second, we double that range by recording the range of the y-variable from the default graph, then increasing the bounds by 50% of the original range in each direction, resulting in a graph where the data are condensed along the vertical axis. We do not manipulate the scale of the x-axis because our survey of the literature suggests that this is not a common adjustment. A related decision researchers encounter is the range of the running variable to use in producing graphs: should they use the entire dataset or only a subsample close to the policy threshold? We do not test this margin of adjustment in our experiments due to the difﬁculty in generalizing the ﬁndings from such an exercise. Suppose we ﬁnd that selecting 50% of the observations closest to the threshold improves visual inference, should researchers “chop” the sample they are already planning to use? And after doing so, will it be beneﬁcial to chop again? One could argue for testing the effect of using the full sample versus the subsample falling within the IK or CCT bandwidth, but these bandwidths themselves depend on the full sample—the ﬁrst step in bandwidth calculation is a (semi-)global regression—and it is not even clear how we should deﬁne the “full” sample. In fact, some of the replication data used for our DGP calibration are already subsets of a larger sample. The presence of a vertical line is designed to visually separate observations above and below the cutoff. The motivation for rescaling graph axes comes from Cleveland, Diaconis, and McGill (1982), who note bands around the binned averages or ﬁt lines. However, conﬁdence intervals are too complex to explain to the non-experts in our short tutorial without potentially affecting the way participants think about the classiﬁcation task itself, and therefore we do not experimentally test their effects on visual inference. Because the moderate size of our expert pool makes it unsuited to randomized experiments, we refrain from testing other graphical parameters. We specify data generating processes based on the actual data used in published research. We randomly sample 11 from a set of 110 empirical RD papers published in the American Economic Review, American Economic Journals, Econometrica, Journal of Business and Economic Statistics, Journal of Political Economy, Quarterly Journal of Economics, Review of Economic Studies, and Review of Economics and Statistics that have replication data available to create our DGPs. We refer to these DGPs as DGP1-DGP11. running variable X, the continuous conditional expectation function E[ term u, and the sample size N. 11 papers but normalize it to lie in [−1,1] with 0 representing the treatment cutoff. We also remove the most extreme observations where |X| > 0.99, following Imbens and Kalyanaraman (2012) and Calonico et al. (2014). For two papers which feature semi-discrete running variables, we add small amounts of normal noise to the running variable to match the regularity conditions from Calonico et al. (2015). To create a continuous CEF, we ﬁt global piecewise quintics (still following Imbens and Kalyanaraman, 2012 and Calonico et al., 2014) and vertically shift the right arm. We specify the distribution of the error term u as i.i.d. normal with mean zero and standard deviation σ , which we set as the root mean squared error of the piecewise quintic regression. We use the same number of observations as the original paper minus any observations removed while trimming the data. Plots of the resulting CEFs before we vertically shift their right arm to make them continuous are in Figure 2, and Figure 3 illustrates the construction process. We describe the DGP creation process in full detail in Appendix B. There are other graphical parameters we do not test in our experiments. One is plotting conﬁdence The calibration of each DGP g entails the speciﬁcation of its four components: the distribution of the Because the outcomes from the 11 papers are measured in different units, we need to standardize the discontinuity levels and choose to specify discontinuity levels d as multiples of σ . Alternatively, we could specify d as multiples of the overall standard deviation of the outcome variable net of the discontinuity, a measure that also captures the variation due to the conditional expectation function besides the error term. Using this alternative measure turns out not to make a difference: the variance of the error term, σ dominates the variance of E[Y |X] in all of our DGPs, with the ratio of the two ranging from 8 to 690. d = 0 in order to measure type I error rates. We choose the upper bound |d|= 1.5σ based on our own visual judgment: it represents the point at which we expect every reasonable person to say a graph from any of our 11 DGPs features a discontinuity. The nonzero magnitudes of d are equally spaced on the log scale. We use a log rather than linear scale to generate more graphs with smaller discontinuities, which are harder to detect, in order to better capture the shape of the power functions. in our literature review. The average absolute value of the discontinuity t-statistics in our datasets from piecewise quintic regressions is 5.0 with a standard deviation of 5.3, compared to the observed mean of 3.9 with a standard deviation of 6.4. If we instead compare the distributions of the absolute value of the discontinuity divided by the control magnitude (the left intercept of the CEF), the means are similar, 1.3 in our datasets and 1.9 in the ﬁeld, while our standard deviations are somewhat smaller at 2.2 compared to 7.7. we also need to evaluate how well our DGPs approximate the actual data from the respective studies. To do this, we adapt the lineup protocol from Buja et al. (2009) and Majumder et al. (2013), which uses visual inference to conduct hypothesis testing. In our case, we test the null hypothesis that the original datasets come from the calibrated DGPs. Speciﬁcally, we present one graph of the original data randomly placed among 19 graphs from datasets drawn from the corresponding DGP. The goal is to identify the true dataset by choosing the graph that least resembles the others. If the viewer does not select the original graph, then we cannot reject the null hypothesis. Under the null hypothesis, the probability of identifying the graph produced from the original data (or the type I error probability) among the 19 simulated datasets is 5% (1/20) for a single reader. For our lineup protocol, each graph is a binned scatter plot using the MV bin selector. We present two examples in Figure 4. of our 11 DGPs, which supports the idea that our DGPs approximate the original datasets well. For three As a multiple of σ, d takes on 11 values: 0,±0.1944σ,±0.324σ , ±0.54σ , ±0.9σ , ±1.5σ. We include Our discontinuity magnitudes are similarly distributed to those observed in the main outcome graphs As argued in Section 2, we want our DGPs to be representative. Although we select the papers randomly, Based on visual testing among the authors, we cannot identify the graph from the true data for eight out DGPs, however, there is an obvious difference, as exempliﬁed by DGP3 in the right panel of Figure 4. All three “fail” seemingly because of the misspeciﬁcation of the variance structure of the error term u. Recall that we specify u as being i.i.d. across observations and homoskedastic. But in the right panel of Figure 4, for example, the running variable is time, and there is positive serial correlation in the outcome. As a consequence, the outcome variability in the binned scatter plot is understated when u is assumed to be i.i.d. Nevertheless, we adhere to the i.i.d. speciﬁcation because it is standard in Monte Carlo exercises to evaluate RD estimators and inference procedures. the same issue that has brought forth the warning by Gelman and Imbens (2019) against using high-order global polynomial regressions to estimate RD treatment effects (see Pei et al., Forthcoming for related discussions on the order of local polynomial regressions). We acknowledge this potential drawback of using quintics as some of our graphs indeed feature high variation in the tails. That said, the lineup protocol we adapt offers a novel and transparent method to evaluate our DGP speciﬁcations, and we ﬁnd our inability to distinguish the real data from those drawn from one of our DGPs in a majority of cases reassuring. To further assuage the concerns regarding our DGP speciﬁcation, we carry out a supplemental phase of experiments to gauge the sensitivity of visual inference to alternative DGP speciﬁcations. In Appendix C, we demonstrate the remarkable robustness of our results to using local linear estimates as an alternative to model the CEF (and allowing for heteroskedasticity), which is much less likely to overﬁt. In our randomized experiments, we present non-expert participants with binned scatter plots made from our DGPs and ask them to classify the graphs as having a discontinuity or not. We conduct ﬁve phases of computer-based experiments online through the Cornell University Johnson College’s Business Simulation Lab. Our subject pool consists of current and former Cornell students, Cornell staff, and non-student local residents with an expressed interest in focus groups or surveys. Although these educated laypeople are not the primary audience for academic research, RD graphs are sufﬁciently transparent that they are featured in popular media articles in publications such as The New York Times, The Washington Post, and The Atlantic (Dynarski, 2014; Sides, 2015; Rosen, 2015), suggesting the participants in our sample should be capable of interpreting the graphs. Another caveat of our DGP speciﬁcation is that using global quintic regressions can lead to overﬁtting, Before the experiment, participants watch a video tutorial explaining how the graphs are constructed. do not instruct participants on how to make their decisions, e.g. whether only to look at points near the cutoff or mentally to trace out the CEF. The video contains an attention check with a corresponding question later in the experiment to ensure that subjects pay attention to the instructions. After the video, participants complete a series of interactive example tasks and receive feedback on their answers. As part of the instructions, we tell participants explicitly that all, some, or none of the 11 graphs they classify may feature a discontinuity. erated as described in Section 3.2. Participants see two graphs with zero discontinuities, one each of ±0.1944σ,±0.324σ,±0.54σ,±0.9σ, and one of either 1.5σ or −1.5σ . Participants see one graph from all 11 DGPs in a randomized order. We have up to 88 participants per treatment arm, and every graph we generate is seen by only one participant. For each graph, we ask participants whether they believe there is a discontinuity at x = 0. our experiment in phases, testing only a few treatments in each phase. Table 1 details the timeline of the experiments and lists the graphical parameters we test and hold ﬁxed for various experimental phases. In phase 1, we test both bin width and axis scaling options. In phase 2, we test bin widths and bin spacings. In phase 3, we test imposing ﬁt lines and a vertical line at the treatment threshold. Based on the results from these three phases, in which only bin widths and ﬁt lines have major impacts, phase 4 tests all four combinations of those two treatments together. the AEA RCT registry (Korting et al., 2019a) and the Center for Open Science’s OSF platform (Korting et al., 2019b). The study takes participants approximately 15 minutes to complete. Participants receive a base pay of $3 for being in the experiment. To stimulate participant engagement and elicit participants’ conﬁdence in their response, participants can choose, for each graph they classify, a bonus that is either based on a monetary wager which pays 40 cents if their judgment is correct but nothing In each phase of the experiment, we present participants with a series of RD graphs using data gen- Because running an experiment with 2= 32 treatment arms is infeasible with our resources, we conduct The experiments are programmed in oTree (Chen, Schonger, and Wickens, 2016) and pre-registered at otherwise, or a ﬁxed payment of 20 cents irrespective of their performance. We do not give participants real-time feedback on either the accuracy of their responses or their cumulative earnings but do report total earnings and the ﬁnal tally of correct classiﬁcations after a short exit survey soliciting demographic information and comments at the very end of the experiment. her conﬁdence in discontinuity classiﬁcation and calculate the AS risk in Section 4.1.1. First note that participants will choose a classiﬁcation if and only if their perceived probability that the classiﬁcation is correct (β as deﬁned in Section 2.2) is at least 50%. Denoting the utility function by v, the expected utility under the wager is given by βv($0.4) + (1 −β)v($0), which is equal to β v($0.4) if we normalize v($0) to zero. Expected utility maximizing participants will choose the sure amount whenever β v($0.4) 6 v($0.2), i.e. whenever β ∈ [0.5,v($0.2)/v($0.4)]. A risk neutral subject (with a linear utility function) would therefore only choose the sure amount when they believe their chance of being correct is exactly 0.5 and choose the wager as soon as their conﬁdence exceeds 0.5. If the distribution of β is continuous, then few should choose the risk-free payment scheme. In the data, however, we see a number of people choosing the risk-free payment scheme. this case, the utility function is replaced with a value function for gains and losses with respect to a reference point, and loss aversion is governed through a parameter λ > 1 multiplying outcomes in the loss domain. In the context of our wager, the sure option of 20 cents serves as a natural focal point to participants, so we consider the simple loss aversion framework put forward by Kahneman and Tversky (1979) assuming a ﬁxed reference point of 20 cents. That is, participants would consider the gamble as an opportunity to win 20 cents or lose 20 cents relative to this reference point depending on their answer. zero under the ﬁxed payment option. Participants choose the wager whenever β > λ /(1 + λ ). Therefore, for a given value of λ, a player’s bonus choice indicates the interval in which β falls, which we can use to approximate the communication risk by Andrews and Shapiro (2021). Brown et al. (2021) conduct a metaanalysis of 607 empirical loss-aversion estimates across 150 studies and ﬁnd that the average coefﬁcient λ lies between 1.8 and 2.1. For simplicity, we abstract away from heterogeneity in λ and assume a loss- In the remainder of this section, we analyze a participant’s bonus choice, which we use to estimate An alternative behavioral model that is consistent with the observed bonus choices is loss aversion.In The expected payoff of the wager in this case is given by β v($0.2) −λ(1 −β )v($0.2), compared to aversion parameter of λ = 2 throughout our analysis. In addition to our non-expert experiment, we conduct a study with researchers in economics and related ﬁelds who work on topics that often employ RDDs. We collect data at three technical social science seminars and online by contacting randomly selected members of the NBER in applied microeconomic ﬁelds (aging, children, development, education, health, health care, industrial organization, labor, and public) and IZA fellows and afﬁliates. After removing a total of six responses from participants who completed the survey more than once, did not provide a valid email address for payment, or were not part of our recruited sample, we are left with 143 expert responses. impacts of graphical techniques differ between experts and non-experts? And second, can experts correctly predict which graphing options perform best for our non-expert sample? This second question speaks to experts’ ability to predict which visualization choices are best suited for interpretation by a lay audience. Because the success of a graphical technique ultimately lies in the reader’s correct perception of graphs using it, it is important to understand whether experts’ intuition regarding the relative advantages and drawbacks of alternative representation choices aligns with the evidence we ﬁnd in practice. In related work on experts’ ability to predict non-expert performance, DellaVigna and Pope (2018) ﬁnd that economic experts are better than non-experts at estimating the effect of alternative incentive schemes on performance in a real effort task, but perform similarly to non-experts in terms of a simple ranking of incentive schemes. Participants see a series of RD graphs and are asked to classify them by whether they have a discontinuity. To assess the accuracy of point estimates in addition to binary classiﬁcations of discontinuities, we also ask participants for an estimate of the discontinuity magnitude whenever they report a discontinuity. Due to sample size limitations, we do not randomize graphical treatments in the expert study, and all participants see graphs with equally spaced bins, no ﬁt lines, default axis scaling, and a vertical line at the treatment threshold. All expert graphs use small bins, except for one seminar where participants see large bins. Four randomly selected participants receive a base payment of $450 plus a bonus payment of $50 per correct discontinuity classiﬁcation. The bonus payment does not depend on the accuracy of the magnitude estimate. This expert study allows us to answer two questions. First, how do classiﬁcation accuracy and the Our expert study consists of two parts. The ﬁrst is similar in structure to the non-expert experiment. The second part of the expert study asks about experts’ preferences and their beliefs regarding nonexpert performance across alternative graphical parameters. We present experts with three discontinuity magnitudes: 0, 0.54σ, and 1.5σ. For each magnitude, we present four graphs using the same underlying data, one for each combination of bin width and ﬁt lines. We show graphs from the DGP where visual inference performs most closely to the average over all DGPs and randomize graph treatment order between participants. At each magnitude, we ask the experts to indicate which of the four treatment options they prefer and which of the four treatment options they believe perform best and worst in our non-expert sample. To evaluate the experts’ predictions about non-expert performance, we run an additional non-expert phase 4 in which we test all four treatments simultaneously across subjects. For each combination of graphical parameters in all phases of the experiment, we compute power functions based on participants’ classiﬁcations of graphs as having or not having a discontinuity. Using notation from Section 2, a DGP-speciﬁc power function represents the estimates ˆp(γ, g,d) for graphical parameters γ and DGP g across different levels of discontinuity d. An overall power function represents the estimates discontinuity magnitudes, the power function represents the proportion of graphs with discontinuities that participants classify correctly, which can be interpreted as one minus the type II error rate when the DGP is chosen uniformly randomly from the 11 possibilities. A desirable inference method has a small intercept before quickly rising to achieve a low type II error rate. Plots of the overall power functions for each phase are shown in Figure 5, while Figure A.1 plots the corresponding DGP-speciﬁc power functions. The x-axis in these graphs is the magnitude of the discontinuity divided by the DGP-speciﬁc σ. This normalization facilitates aggregation and comparisons across DGPs, which then have six identical discontinuity magnitudes: 0, 0.1944, 0.324, 0.54, 0.9, and 1.5. Estimated effects on visual inference are in Tables A.1-A.5. Because we effectively adopt stratiﬁed randomization in the design of our experiments as described in Section 3.3.1, where each of the 11 strata is determined by the DGPs seen for every discontinuity magnitude, we obtain these estimates by regressing the participants’ responses on treatment indicators and stratum ﬁxed effects. bins and small MV bins) with the y-axis scaling treatments (the default in Stata 14 and double that range). The intercept of a power function indicates the type I error rate as deﬁned in Section 2. At all other Phase 1 of the experiment tests the four combinations of the bin width treatments (large IMSE-optimal Comparing power functions, large bins have signiﬁcantly higher type I error rates relative to small bins. While both small bin treatments lead to a type I error rate of approximately 5%, the large bins have a type I error rate of around 20% to 25%. The large bins have a type II error advantage over the small bins, which is at least partly driven by its higher type I error rate, but power functions converge as the discontinuity magnitude increases. In contrast to bin widths, axis scaling has little effect on participant perception. Based on this result, we use Stata’s default scaling for all subsequent phases. treatments: even spacing and quantile spacing. Note that large bins and small bins with even spacing appear in both phases 1 and 2. With this design, we can gauge the stability of visual inference across different samples from the non-expert population, and it is encouraging to see the results for these two treatments being virtually identical across phases. Comparing the power functions for these repeated treatments with their new quantile-spaced versions, we see that evenly-spaced and quantile-spaced bins perform very similarly. We conclude that bin spacing has a small or null effect on visual inference for the DGPs we test. polynomial ﬁt lines and the omission of both the vertical line and ﬁt lines. We ﬁnd that the vertical line at the cutoff makes little difference in perception. Fit lines, on the other hand, appear to increase type I error rates in this phase, in line with a common concern that they may be overly suggestive of discontinuities. have the largest impact on visual perceptions of discontinuities. We therefore base our analysis of expert preferences and expert predictions about non-expert performance on the interaction of these two treatments and run a ﬁnal phase of experiments, phase 4, directly comparing the four possible treatment combinations. Interestingly, while the effects of bin width choice are once again robust across phases, the effects of ﬁt lines are more muted in this phase. In particular, the treatment with small bin and ﬁt lines has a type I error rate of only 0.052 in phase 4 but 0.175 in phase 3. This ﬁnding suggests that we cannot conclude that ﬁt lines unequivocally result in an increase in type I errors, but that they do add uncertainty to visual inference. of covariates across treatment arms and the measurement of predictive power of demographic and DGP characteristic variables on visual inference performance. Appendix F examines how large the t-statistics need to be for readers to visually detect a discontinuity. In phase 2, we again test the two bin width treatments, this time interacted with the two bin spacing Phase 3 tests three treatments: the inclusion of a vertical line at the treatment threshold with and without Jointly, these three phases of experiments suggest that the presence of ﬁt lines and the bin width choice We provide additional results in Appendices E and F. Appendix E includes evidence for the balance In this section, we provide a recommendation to practitioners on the choice of graphical method in RDD. We arrive at our recommendation by empirically implementing the two decision theoretical frameworks from Section 2.2, which synthesize our experimental results. is a simple transformation of the type I or type II error probability and is effectively summarized by the power functions presented in Figure 5. To facilitate the risk comparisons across γ, however, we will need to aggregate the information in each power function across d. Formally, as mentioned in Section 2.2, this task requires the speciﬁcation of the type I and type II error cost parameters—κ and ϕ, respectively—and a prior on d that represents the probability of encountering a graph with a certain discontinuity level. Because their choices involve subjective judgement, we present results under different scenarios, and our power functions also allow researchers to estimate the risks with their preferred speciﬁcation of these quantities. For the cost parameters, we normalize ϕ to 1 (only the ratio of the cost parameters matters for comparing risks) and try both the baseline case of κ = 1 and a benchmark used by Kline and Walters (2021), κ = 4. For the prior on d, we adopt the uninformative prior that it is equally likely to encounter a continuous graph as a discontinuous graph, but there is still the question of how to incorporate the type II error rates across different discontinuities. In our main estimates presented in Table 2, we use the type II error rate at 0.324σ, the modal (and median)—and therefore the most likely—discontinuity level among the subset of the 11 papers that report a signiﬁcant discontinuity. In Appendix Table A.6, we also present risk estimates by simply averaging the type II error rates across the ﬁve different nonzero discontinuity levels, which lead to the same recommendation. fact, no risk is statistically signiﬁcantly different from that of the benchmark small bins/no ﬁt lines treatment (in bold) repeated across experimental phases. When penalizing type I errors more in the κ = 4 calculations, the beneﬁt of the low type I error rate of the benchmark treatment is more evident. The benchmark treatment has the lowest risk in two of the four phases (phases 2 and 3) and the second-lowest in the other two, where the differences with the best performer are not statistically signiﬁcant. Further, the large bin treatments in the three phases where they appear (phases 1, 2, and 4) have statistically signiﬁcantly higher classical risks, as does the ﬁt lines treatment in phase 3. First, we estimate the overall classical risk of each graphical method. For a given d, the risk¯R(γ,d) When weighting type I and II errors equally, the classical risks are fairly similar across treatments. In to measure the perceived probability of being correct, β , for each participant-graph combination. As noted in Section 3.3.2, a participant’s bonus scheme choice provides information on β . Assuming a loss aversion parameter of 2, β falls into the interval (2/3,1) if the participant chooses the ($0.4,$0) wager, and it falls into (1/2,2/3) if she chooses the risk-free ($0.2, $0.2) option. For the estimates of the AS risk we present, we simply approximate β by the midpoint of the two intervals, and the resulting averages of β lie between 7/12 and 5/6. We plot these values in Figure A.2 with inference on the differences across treatment arms in Tables A.7 through A.10. We note that the midpoint approximation is coarse and may underrepresent the true curvature in Figure A.2—the average β at the highest and most obvious discontinuity magnitude is likely very close to one as opposed to 5/6. We use it because it is a common way of handling interval data and because it amounts to a transparent linear combination of the fractions of the two bonus scheme choices, leading to a single interpretable quantity. We have also tried alternative functional forms for the underlying distribution of β and reach similar conclusions. for each (γ, d) combination. But we still need to aggregate across d so that we can compare the overall risks across γ. We simply proceed with the same weighting scheme as with our classical risks. treatment (in bold) again performs well. When giving the same weight to the AS risk at zero and nonzero discontinuities, the benchmark treatment has the lowest risk in phase 2 and the second-lowest in phases 1, 3, and 4. With greater weights at zero discontinuity, it has the lowest risk in phases 1 and 2. As with the classical risks, no treatment achieves a statistically signiﬁcantly lower AS risk than small bins/no ﬁt lines, and many are signiﬁcantly outperformed in either weighting scheme. y-axis scaling, and a vertical line at the policy threshold consistently performs well, as measured by both the classical and AS risks, in all phases of our experiment. This particular method, therefore, can serve as a sensible default for generating RD graphs. Of the ﬁve graphical parameters, bin spacing, y-axis scaling, and the presence of the vertical line do not appear to matter much, allowing researchers to use reasonable discretion. The other two parameters are much more important, and the use of small bins and no ﬁt lines is key for good visual inference performance. Finally, we emphasize that our recommendation is not intended as a doctrine that practitioners must abide by. We are limited to the 11 DGPs we test. In addition, as Second, we estimate the AS communication risks per Andrews and Shapiro (2021). To do this, we need Approximating β using participants’ bonus scheme choices allows us to estimate the AS risk¯R(γ,d) We present these results in the bottom half of Table 2, and the repeated benchmark small bins/no ﬁt lines To summarize, we ﬁnd that the graphical treatment of small bins, no ﬁt lines, even spacing, default described in Appendix A.3, there is an ad hoc element in the construction of small bins. In fact, we prefer (quantile-spaced) large bins ourselves in the regression kink design experiments in a previous working paper Korting et al. (2020), where the sample sizes are much larger than for our RD DGPs. In this regard, we view our recommended graphical method as a sensible starting point based on the best evidence we have. An equally important takeaway is the value in documenting the robustness of graphical evidence given our ﬁnding of divergent visual inferences under commonly used methods. Most expert participants (95 out of 143) saw graphs generated with our preferred method as discussed above. We plot in Figure 6 the expert power functions against those of the non-experts who saw the same graphs. When comparing expert and non-expert performances, we use solid (hollow) markers to indicate that the point is (not) statistically signiﬁcantly different from the reference curve, and present plots of the corresponding 95% conﬁdence intervals (created here with the large sample approximation described at the end of Appendix A.1 and by assuming independence between the experts and non-experts) in Figure A.29. The two groups perform similarly, with experts having a slightly higher type I error rate (approximately 8% to the non-expert 5%) and a slightly lower type II error rate. The only statistically signiﬁcant differences are for the experts’ marginally lower type II error rates at the 0.1944 and 0.324 discontinuities. using the large bins and no ﬁt lines treatment. The two groups again perform similarly, and the expert and non-expert power functions are not statistically signiﬁcantly different anywhere. Both groups have type I error rates well above their corresponding small bin rates. That is, experts also do worse when viewing graphs constructed with large bins. 4.2.1 Expert Preferences and Predicting Non-Expert Performance We present experts’ preferences and their beliefs about non-expert performance across the four considered treatments in Figure 7. When asked about graphing options for the main graph of a paper that conveys the treatment effect, most experts report preferring small bins, usually with ﬁt lines. These results hold at all three discontinuity magnitudes considered, including zero. Experts’ predictions about the most effective treatments for non-experts tend to mirror their preferences. By a large margin, experts believe small bins In addition to the aforementioned treatment, we show experts in one seminar pool (48 out of 143) graphs with ﬁt lines to be the most efﬁcacious treatment for non-experts at all discontinuity magnitudes. Conversely, most experts view large bins without ﬁt lines least favorably in the context of non-expert performance. for the effects of bin width choice on non-expert classiﬁcation accuracy. The best- and worst-performing treatments at each discontinuity magnitude have + and - signs, respectively, in Figure 7. The actual power functions are shown in Figure 5 (Figure A.3 shows the power functions based only on DGP9 which was used in the example graphs shown to experts in the second part of the expert study.) While a majority of experts correctly identiﬁes the bin width treatment with lowest type I error rates (i.e. most experts prefer small bins at the zero discontinuity level, either with or without ﬁt lines), there is also signiﬁcant expert support for the large bin with ﬁt lines treatment, even when there is no discontinuity, which exhibits the greatest type I error rate in our sample. In addition, experts fail to predict the type I vs type II error tradeoff presented by the bin width choice: most experts expect large bins to perform worst even at large discontinuities, while we ﬁnd this treatment arm has the lowest type II error rates in those cases. Although in the actual power functions, the effects of bin width are much more pronounced than the effect of ﬁt lines, we ﬁnd more expert disagreement regarding non-expert performance along this dimension. We also ﬁnd expert predictions to be similar whether their own visual inference performance is above or below the median. In this section, we compare the performances of visual inference from our small bin expert sample with various econometric RD procedures. We present both the overall power functions and the difference between visual and econometric inferences for each econometric procedure. For a fair comparison, we base the estimators’ power calculations on their rejection decisions over the same set of datasets underlying the graphs seen by the experts. That is, the estimators “see” the same data as the experts, preventing differences driven by variation in sampling from the same DGP. regression with homoskedastic standard errors. The power function for the corresponding 5% test compared with human performance is presented in the left panel of Figure 8. We again use solid (hollow) markers to indicate that the difference to the comparison power function is (not) statistically signiﬁcant, and provide plots of the differences in Figure A.30. We additionally include in Table 3 type I and II error rates (the latter at both |d| = 0.324σ and averaged across nonzero discontinuities) for visual and econometric inferences. Comparing the expert predictions to our experimental data from phase 4, we ﬁnd substantial discordance As a benchmark, our ﬁrst estimator comes from a correctly speciﬁed model: a global piecewise quintic functions in the left panel of Figure 8. All three procedures build upon local linear regressions but take different approaches to conduct inference. bandwidth selector, the IK bandwidth: where C r is a regularization term to prevent very large bandwidths, and the hats on the various quantities indicate that they are estimated. We refer to the “conventional” (terminology from Calonico et al., 2014) inference procedure practitioners typically implement in conjunction with the IK bandwidth as the “IK” inference procedure. Speciﬁcally, letting τ denote the true RD parameter and B is the asymptotic bias constant, which is a function of the second derivatives of the CEF on both sides of the threshold. Even though the normal distribution is centered around the constant τ + h shrinks at the optimal rate h = O(N bias term error rate than the piecewise quintic estimator, and is signiﬁcantly better than visual inference at detecting discontinuities up to 1.5σ. But with this advantage in type II error rate comes a signiﬁcant disadvantage in type I error rate. When there is truly no discontinuity, the estimator still rejects the null hypothesis in 22.6% of datasets. as implemented in the rdrobust Stata, R, and Python packages. This procedure differs from IK and creates robust conﬁdence intervals by estimating the bias B using another (“pilot”) bandwidth b, creating a biascorrected estimator asymptotics where h/b converges to a positive constant. Calonico et al. (2014) also generalize IK and propose a new class of MSE-optimal bandwidth selectors. Next, we implement the IK, CCT, and AK inference procedures, again plotting the corresponding power Strictly speaking, Imbens and Kalyanaraman (2012) do not study inference but propose an MSE-optimal is a constant determined by the kernel, f (·) is the density of the running variable, µ(0) and ) represent the second derivatives of the CEF and conditional variance on both sides of the threshold, NhB. As seen in the left panel of Figure 8, the IK procedure achieves even lower type II Third, we assess the performance of the inference procedure proposed by Calonico et al. (2014) (CCT) approximately 12.5%, as seen in the left panel of Figure 8, this could be a “small sample” problem. As mentioned in Section 3.4, we effectively have 88 datasets modulo the discontinuity level for the expert study. In a separate Monte Carlo simulation with 1,000 draws, the type I error rate is in line with that of the experts at 7.0%. However, it is possible that the type I error rate of visual inference also decreases over graphs based on these alternative datasets—the realization of the disturbance term can impact both econometric and visual inference—and therefore we keep the comparison based on the datasets the experts saw. The CCT inference procedure achieves lower type II rates, enjoying a signiﬁcant 15 to 20 percentage point advantage over expert visual inference at intermediate discontinuity levels. in the R package RDHonest, which adapts Donoho (1994) and produces asymptotically valid and minimax (near-)optimal conﬁdence intervals over the Taylor class of conditional expectation functions the second derivatives. C estimator—unlike CCT, AK’s bias correction is nonrandom. Using the default rule-of-thumb procedure in the RDHonest package to estimate the tuning parameter, the AK inference has a type I error rate of approximately 6%, and the power function is very close to that of the experts. AK, the comparable performance of visual inference is remarkable. However, it is worth emphasizing that although they have approximately the same average type I error rate, AK offers a theoretical guarantee to control the (asymptotic) type I error rate for all DGPs in the Taylor class while visual inference does not. DGP. In particular, we use the theoretical MSE-optimal bandwidth for IK, this bandwidth and the theoretical asymptotic estimator bias for CCT, and the true second derivative bound for AK (because we specify the X- Finally, we apply the AK inference procedure from Armstrong and Kolesár (2018) and implemented is the support of the running variable on two sides of 0, and Cis a researcher-supplied bound on In Appendix Figure A.5, we present additional power functions where we impose our knowledge of the distribution as the empirical running variable distribution from the published study, we need to estimate its density at 0 in computing the theoretical MSE-optimal bandwidth, and it is the only quantity we estimate). While the AK result is similar to when we use the estimated turning parameter (it is possible that our quintic speciﬁcation is friendly to the rule-of-thumb estimator, which relies on a quartic regression), the IK type I error rate is considerably lower, leaving little for CCT to improve upon. Although the theoretical MSEoptimal bandwidth is still “too large” based on its N type I error rate appears to be the noisy estimates of the constants in the bandwidth formula (similar patterns also emerge from the simulation results in Calonico et al., 2014). rates than visual inference. We can circumvent this tradeoff by adjusting their type I error rate to the level of visual inference: we search for alternative critical t-values such that the resulting type I error rate of the econometric inference procedure is equal to that of visual inference and then use those critical values to conduct inference. For IK, this critical t-value is 2.46, and it is 2.28 for CCT. We present the results for these type-I-error-rate-adjusted inference procedures in the right panel of Figure 8, with the differences between these procedures and expert visual inference in Figure A.31. Despite the extent of their differences in type I error rates relative to visual inference, both econometric procedures’ type II error rates only increase by around 5-10 percentage points from this adjustment and are still signiﬁcantly lower than those of visual inference at moderate discontinuities. estimate the magnitude of the discontinuity. We take several steps to make human and econometric point estimates comparable. Because we ask participants to round their estimates to the nearest hundredth, we round estimators with the same precision. We similarly replace econometric estimates with 0 when the test fails to reject the null hypothesis. averaged over all discontinuity magnitudes. We do this by DGP to prevent scaling issues, as units for each DGP are different and results from one DGP are not directly comparable to those from another. Although there are a handful of DGPs where experts perform similarly to the econometric estimators, their point estimates generally have a greater RMSE than the estimators, and experts overall are worse at estimating magnitudes. Speciﬁcally, IK has a lower RMSE than visual inference for every single DGP, which is driven by the variance component of the MSE in a majority of cases as shown in the right panel of Figure 9, (we The IK and CCT inference procedures at the 5% level exhibit higher type I and lower type II error In addition to asking experts to classify graphs as having or not having a discontinuity, we ask them to Figure 9 presents the root mean squared error (RMSE) of each estimator’s point estimates by DGP, leave the details of the decomposition of their MSE differences to Appendix A.4). In summary, although the average human performs quite well at identifying the existence of discontinuities, her ability to estimate their magnitude is not as strong. 4.3.1 The Complementarity of Visual and Econometric Inferences Thus far, we have studied the “marginal” power functions for visual and econometric inferences. In this section, we use the joint distribution of visual and econometric discontinuity tests to examine their complementarity and explore the performance of a simple combined visual-econometric inference procedure. agree on the same data. For each discontinuity magnitude, we characterize the joint distribution of visual and econometric classiﬁcations in the form of a two-by-two contingency table. We conduct Fisher’s exact test for independence and present one-sided p-values in Table 4 by discontinuity magnitude and for each econometric method. We report one-sided p-values because two-sided p-values are method-dependent due to the ambiguity in classifying contingency tables as extreme in the opposite direction (see for example Agresti, 1992). In principle, we could also report correlations between inferences, but they may be hard to interpret: because of the binary nature of the classiﬁcation variables, the maximum of the standard correlation measure depends on the marginal distributions of the classiﬁcations (for example, if their probabilities of rejecting no discontinuity are not identical, then their correlation cannot be 1), which vary across discontinuity levels and by econometric methods. Although we can follow the proposals by Cohen (1960) and Davenport and El-Sanhurry (1991) and present correlations scaled relative to their maximum as determined by the marginal distributions, we omit them for brevity. visual and econometric classiﬁcations when the true discontinuity is zero. But when the true discontinuity is nonzero (rows two to six), there appears to be strong evidence in support of association (though not reported in Table 4, all correlations are positive in these cases). In other words, type II errors by experts are predictive of type II errors by various econometric inference methods, but this is not true for type I errors, which highlights the complementarity of visual and econometric inferences. combined visual-econometric inference procedure. It infers a discontinuity if and only if both the visual and econometric procedures reject no discontinuity. As a referee suggests, many researchers may already use First, we examine the joint distribution of visual and econometric inferences to see whether they tend to Two patterns emerge from Table 4. The ﬁrst row shows no strong support for an association between Second, we illustrate this complementarity more concretely by studying the performance of a particular this procedure informally when reading or writing RD papers. the AK procedure for comparison. Because of the lack of dependence between visual and econometric classiﬁcations when d = 0, the combined inferences achieve lower type I error rates than either of the individual inference types. On the other hand, the same mechanism pushes type II error rates higher, but the positive associations between the classiﬁcations when d 6= 0 help limit their increase. AK. The bottom panel of Figure 10 presents the difference between the two. Despite the limitations of the IK procedure with a type I error rate above 20% as shown before, the IK-expert hybrid has a type I error rate of 2.6% while not performing statistically signiﬁcantly differently from AK at any of the nonzero discontinuity levels. This ﬁnding helps to explain the enduring credibility of RDDs despite potential issues with the econometric inference method used prior to CCT and AK. It also suggests that the de facto type I error probability may be lower than intended if researchers informally combine different statistical evidence instead of relying on a single econometric inference result, a point that deserves attention in future research. This paper studies visual inference and graphical representation in RD designs via crowdsourcing. Through a series of experiments and studies that recruit both non-expert and expert participants, we provide answers to two sets of questions. First, how do graphical representation techniques affect visual inference and which technique should practitioners use? And second, when presented with well constructed graphs, how does visual inference perform compared to common econometric inference procedures in RDDs? visual inference accuracy. We ﬁnd that generating graphs with the Calonico et al. (2015) IMSE (large) bin selector leads to higher type I error rates but lower type II error rates relative to their MV (small) bin selector. Imposing ﬁt lines can have a similar effect as using large bins, conﬁrming the worries by Cattaneo and Titiunik (2021a,b). Implementing the decision theoretical frameworks that build on Kline and Walters (2021) and Andrews and Shapiro (2021), we recommend the use of small bins and no ﬁt lines as a sensible starting point in practice. Bin spacing, a vertical line at the policy threshold, and y-axis scaling have little effect, implying that researchers can adhere to reasonable preferences. We plot the resulting power functions in the top panel of Figure 10, along with the power function of In fact, the power function of the combined visual-IK inference procedure is fairly close to that of To answer the ﬁrst set of questions, we experimentally assess how ﬁve graphical parameters impact structed with the recommended method. It achieves a lower type I error rate than econometric inference at the 5% level based on the Imbens and Kalyanaraman (2012) and Calonico et al. (2014) methods (the difference between the visual and CCT type I error rates is not statistically signiﬁcant), though the two econometric inference procedures offer considerable type-II-error advantages. The performance of visual inference is very similar to that based on the procedure suggested by Armstrong and Kolesár (2018). Furthermore, visual and econometric inferences appear to be complimentary. Through the analysis of the joint distribution of visual and econometric tests we ﬁnd that, while they commit similar type II errors, there does not appear to be a strong association in their type I errors. eters we are able to experimentally test. As mentioned above, we do not impose ﬁt lines with conﬁdence intervals in our graphs, which researchers sometimes do, due to the difﬁculty in explaining it to non-expert participants. We also do not vary the size and color of the dots. However, these choices may impact inference due to their effect on visual attention and visual complexity as suggested by the literature on the psychological and neurological mechanisms underlying the processing of (visual) information (Hegarty, Canham, and Fabrikant, 2010; Kriz and Hegarty, 2007; Rosenholtz, Li, and Nakano, 2007; Wolfe and Horowitz, 2004). We leave these investigations to future work. equally spaced or quantile spaced—appears immaterial in our experiments, it could be important when the distribution of the running variable is farther from uniform than in our DGPs. On the other hand, the number of DGPs used in Monte Carlo simulations that lead to methodological recommendations is typically far lower than our 11, and those DGPs sometimes bear no semblance to real-world data. In addition, we test the validity of our simulated datasets by adapting the lineup protocol from Majumder et al. (2013) to assess the degree to which our DGPs approximate the original data, and we document the robustness of our experimental results to alternative DGP speciﬁcations. et al., 2020), we reported the results from an eyetracking study, in which we sought to identify eyegaze patterns (e.g., “visual bandwidths”) that robustly predict visual inference success. Had the predictive patterns emerged from the eyetracking study, we would have followed up with additional experiments, in which we instruct a random subset of the participants to focus their visual attention according to our ﬁnding. But we For the second set of questions, we ﬁnd that visual inference performs competitively on graphs con- Our study is subject to several important limitations. The ﬁrst limitation is the restricted set of param- Second, our results are based on a speciﬁc set of DGPs. For example, while the bin spacing choice— And third, the mechanism of RD visual inference remains elusive. In a previous working paper (Korting were not able to identify predictive ocular patterns and could only conclude that the processing of visual signals, as opposed to where in a graph participants looked, drove visual inference success. A next step toward better understanding the mechanism is to systematically study the types of DGPs for which visual inference performs well and poorly. pirical evidence on best practices in data analysis, and our approach can ﬁnd applications in other important areas. We have conducted analogous experiments to study visual inference and graphical representations in RKDs using DGPs based on Card, Lee, and Pei (2009), Card et al. (2015a), and Card et al. (2015b) (Ganong and Jäger, 2018 also discuss RK visual inference, albeit informally). Interested readers can consult our previous working paper (Korting et al., 2020) for our nuanced ﬁndings. Another related topic to study follows the recent work by Cattaneo et al. (2019a), who, among other contributions, propose econometric tests of linearity and monotonicity based on binned scatter plots, which are motivated by studies such as Chetty et al. (2011) and Chetty et al. (2014). One could assess the impact of the graphical parameters on reader perception and compare visual and econometric linearity/monotonicity tests. A third related topic is structural breaks in time series econometrics, in which graphs serve practically the same purpose as those in RDD. Finally, within time series econometrics, studying visual inference for unit root/stationarity analysis may also be promising. Dickey-Fuller (ADF) test for the presence of a unit root in U.S. inﬂation. Upon ﬁnding that the test rejects a unit root at the 10% level but not at the 5% level, Stock and Watson (2011) write “The ADF statistics paint a rather ambiguous picture. . . Clearly, inﬂation in [the ﬁgure] exhibits long-run swings, consistent with the stochastic trend model.” In this case, Stock and Watson (2011) apply visual unit root inference when the test statistic is marginal, which raises the question: can we leverage our eyes to begin with? These limitations notwithstanding, our study answers the call by Leek and Peng (2015) to provide em-