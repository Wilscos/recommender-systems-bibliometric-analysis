Fast𝑘-Nearest Neighbor search over real-valued vector spaces (Knn) is an important algorithmic task for information retrieval and recommendation systems. We present a method for using reduced precision to represent vectors through quantized integer values, enabling both a reduction in the memory overhead of indexing these vectors and faster distance computations at query time. While most traditional quantization techniques focus on minimizing the reconstruction error between a point and its uncompressed counterpart, we focus instead on preserving the behavior of the underlying distance metric. Furthermore, our quantization approach is applied at the implementation level and can be combined with existing Knn algorithms. Our experiments on both open source and proprietary datasets across multiple popular Knn frameworks validate that quantized distance metrics can reduce memory by 60% and improve query throughput by 30%, while incurring only a 2% reduction in recall. Knn search has become increasingly prevalent in machine learning applications and large-scale data systems for information retrieval [14] and recommendation systems targeting images [1], audio [19], video [29], and textual data [27]. The classical form of the problem is the following: given a query vector𝔮 ∈ R, a distance metric𝜙, and a set of vectorsI,where each𝑥 ∈ Iis also inR,nd the set of𝑘vectors inIwith the smallest distance to𝔮. Most methods for improving the performance of Knn focus on pruning the search space through the use of ecient data structures [8,18,20,28]. Some methods also use compressed vector representations that trade a minimal loss of accuracy for orders of magnitude of compression [11]. This property allows these applications to scale to datasets which might not otherwise t in physical memory. Common to all of these approaches, however, is that the computation of distances involves full-precision oating-point operations, often to the detriment of performance and memory overhead. In practice, we observe that Knn corpora found in both industry [20] and open source datasets [4] are dramatically overprovisioned in terms of the range and precision that they can represent. For example, Figure 1 plots the distribution of feature values from a product embedding dataset used by a large e-commerce search engine. The data is highly-structured, with values observed exclusively in the range(−.125, .125). The distribution is consistent Figure 1: Distribution of values in a dataset of 100 million product embeddings derived from a large e-commerce catalog. We observe that the values comprising these embeddings cluster in a very narrow band. across dimensions, with the majority of values (50%) observed in the narrow band ±(.125, .08). This observation suggests a novel vector quantization method which focuses on feature-wise quantization. Given a corpus of vectors, we perform a data-driven analysis to determine the range of values that appear in practice and then quantize both the corpus and the distance function we intend to use into the lowestprecision integer domain that can capture the relative positions of the points in that set. As long as the quantization is distance preserving in the quantized space, the loss in recall is minimized. As a simple example, consider the following one-dimensional vectors (i.e. points){1.23,2.34,3.09,1.4𝑒7}. We can safely map these points into a smaller integer space of{1,2,3,4}without considerable loss of recall in nearest-neighbors. The third point remains the nearest neighbor of the fourth, even after its value is altered by seven orders of magnitude. This approach has two major consequences. First, by using a compact representation (say,int8), it is possible to reduce memory overhead and scale to larger datasets. This has direct implications on storage-specic design decisions and, hence, eciency since an algorithm that stores vectors on disk will present signicantly dierent performance properties than one which stores them in physical memory. Secondly, primitive computations involving integer data types can be more ecient than their oating-point counterparts. This implies that our approach can be combined with existing indexing-based KNN frameworks [18,21] as a mechanism for replacing full-precision vectors with compact integer alternatives, and obtaining reductions in runtime overhead. A large body of work in the Knn literature focuses on non-quantizationbased approximate methods for performing inner product and nearest neighbor search. Locality sensitive hashing [2,26], tree-based methods [7,23], and graph-based methods [12,21] focus on achieving non-exhaustive search by partitioning the search space. Other work [13] focuses on learning short binary codes which can be searched using Hamming distances. The seminal work which describes the application of quantization to Knn is product quantization [16]. In product quantization, the original vector space is decomposed into a Cartesian product of lower dimensional subspaces, and vector quantization is performed in each subspace independently. Vector quantization approximates a vector 𝑥 ∈ Rby nding the closest quantizer in a codebook C whereC ∈ Ris a vector quantization codebook with𝑚codewords, and the𝑗-th columnCrepresents the𝑗-th quantizer. This approach can be extended to𝐾subspaces. Numerous improvements have been proposed to this approach with the aim of reducing subspace dependencies [9,25], including additive quantization [5], composite quantization [30,31] and stacked quantization [22]. Our approach is complementary to these approaches in the sense that one can either replace the original dataset with low-precision quantized vectors or use it after the codebook mapping step for calculating the distance computations at query time. A closely related recent work is that of Guo et al. [10], who proposed an anisotropic vector quantization scheme called ScaNN which, using an insight similar to ours, modies the product quantization objective function to minimize the relative distance between vectors as opposed to the reconstruction distance. We note that our work and ScaNN are complementary. We make the same observations, but pursue them dierently. ScaNN reformulates existing quantization approaches in terms of a new, relative distancepreserving optimization criteria. In this work, we do not make modications to the underlying algorithms; instead, we modify them at the implementation level by utilizing ecient integer computations. We propose a quantization family(Q, 𝜙)which is a combination of quantization functionQ:R→ Zand distance function 𝜙:Z× Z→ Z. Following the notation in [6] we rst introduce the notion of a search problem. Denition 1.Asearch problem 𝑆 (I, Q, 𝑑)consists of a set of𝑛 itemsI ={𝑥, 𝑥, . . . , 𝑥},a set of queriesQand a search function 𝑑:I × Q → {1,2, . . . , 𝑛}such that the search function𝑑retrieves the index of an item in I for a given query 𝔮 ∈ Q. In this work, we focus on the maximum inner product (MIP) search problem, namely the task of retrieving the items having the largest inner product with a query vector𝑞 ∈ R. With this denition in hand, we can formalize the concept of a quantization between search problems, which is a preprocessing step designed to improve search eciency. Denition 2. 𝑆(I, 𝔮, 𝑑)is aquantizationof an original search problem𝑆(I, 𝔮, 𝑑)if there exist functions𝑔:I → Iand ℎ : 𝔮 → 𝔮such that 𝑆is partial distance preserving meaning. if 𝑑(𝑎, 𝔮) < 𝑑(𝑏, 𝔮) then 𝑑(𝑔(𝑎), ℎ(𝔮)) ≤ 𝑑(𝑔(𝑏), ℎ(𝔮)) Distance preservation implies that if the triangle inequality holds in the original distance space, it still holds in the quantized space. Thus, metrics which satisfy this property remain valid for quantized vectors. Using the standard denition of recall (the fraction of nearest neighbors retained by the quantized computation) we note that the loss of recall for these metrics arises solely from the equality relaxation. We discuss the practical consequences of this property further in our experimental evaluations. For MIP, we propose a simple quantization function which ensures that relative distances for nearest neighbors are preserved and accepts some errors for instances which are further apart. For a given bit-width𝐵, we use a clamped linear function with constants to quantize the 𝑖-th dimension of a vector 𝑥 as described below where𝑘,𝑆,𝑆are non-negative normalizing constants set per dimension. This non-negativity property ensures that distances are partially preserved as described in Denition 2. Our goal is to learnQ(𝑥). Towards this end, we assume that the elements of a vector are independent of one another, and vectors are conditionally independent given an element. These are strong assumptions, but as we show in Section 5.1, are borne out in practice. This leads to the following expression for the likelihood of the data set 𝐼. ParameterizingPasN (𝜇, 𝜎)we estimate𝜃for each dimension in𝐼. The values of(𝜇, 𝜎)can then be used to set the constants in Equation 1. Given a budget of 2where𝐵is the number of bits per dimension, we set𝑆= 𝜇− 𝜎,𝑆= 𝜇+ 𝜎, and𝑘= 𝜇. Extending this approach to datasets with high interdimensional variance or a signicant numbers of outliers, would be straightforward if tedious, and require additional normalization constants for both scale and oset. This section describes our simplifying assumptions based on the highly structured nature of the datasets we consider. As noted in Section 1, these properties are commonplace in both the literature and industry. As suggested by Figure 1, interdimensional variance tends to decrease for datasets with a large number of dimensions. Furthermore, many datasets are normalized to the unit ball during preprocessing. Thus, for these low-variance datasets, we assume a constant mean 𝜇 and standard deviation 𝜎 across all dimensions. For datasets with signicantly low intradimensional variance, we relax the denition of𝑆and𝑆to allow for higher precision. Specifically, for low-variance dimensions, even modestly sized values of 𝐵are sucient for covering the range of values which we observe in practice. Thus rather than clampQ(𝑥)to a single standard deviation, we use the absolute maximum value observed in a given dimension and rely on standard techniques to discard outliers. Table 1: Build time and memory for HNSWlib indices with dierent parameters, applied to a 60MM dataset. Quantization reduces memory overhead and improves build time. We focus our evaluation on the Hierarchical Navigable Small World (HNSW) algorithm [21], which has consistently measured as one of the top-performing methods on public benchmarks and remains a popular choice for industry applications. HNSW is a graph traversal algorithm that involves constructing a layered graph from the set of vector embeddings. To measure the scalability of our proposed techniques on a real-world dataset, we focus our evaluation on a collection of 60 million vector representations of products sampled from the catalog of a large e-commerce search engine, using the approach described in [24] to learn the embeddings. We refer to this benchmark as PRODUCT60M. This benchmark also provides 1000 search queries represented by embeddings in the same semantic search space, which we used for recall measurements. In addition, to provide evidence of the generality of our approach, we also consider two additional popular Knn algorithms, namely FAISS [17] and NGT [15], and two public benchmark datasets from Aumüller et al. [3]. In all of our experiments, we x𝑘, the number of nearest neighbors to retrieve, to 100. We analyzed each of these algorithms with and without compressing the embedding vectors and report results relating to (1) memory footprint, (2) indexing time, (3) throughput, and (4) recall. All experiments were run on a single AWS r5n.24xlarge instance with Xeon(R) Platinum CPU 2.50GHz. For measuring indexing time, we used all available CPU cores. For measuring throughput, we used a single thread. We used an open source implementation of HNSW known as HNSWLib. HNSWlib supports onlyfloat32for both indexing and the computation of inner product distance. In order to useint8 for this analysis, we extended the implementation to support both indexing and similarity search on dense embedding vectors represented byint8arrays. HNSW uses three hyper parameters referred to as EFC, M, and EFS. These parameters have an eect on memory, build time, recall, and throughput. The EFC parameter sets the number of closest elements to the query that are stored within each layer of the HNSW graph during construction time, implying that a higher EFC leads to a more accurate search index but at the cost of a longer build time; the M parameter determines the maximum number of outgoing edges in the graph and heavily inuences the memory footprint of the search index; nally, the EFS parameter is analogous to EFC except that it controls the number of neighbors dynamically stored at search time. In this study, we considered a range of values for each hyper parameter when we measured performance. We used two M values, 32 and 48, values from 300 to 700 in increments of 100 for EFC, and 300 to 800 in increments of 50 for EFS. We divide our metrics into two distinct categories: search performance and search quality. For performance, we focused on throughput, the number of queries the algorithm can process per unit of time, and index build time. We measure throughput by evaluating a test query, recording its execution time with no other processes running, and then returning the reciprocal value as a measure of queries per second (QPS). Following the standard in the literature, our primary metric for measuring search quality is recall, dened aswhere𝑆is the set of results returned by an exact Knn search and𝑆is the items retrieved by our approximation algorithm. We built two groups of HNSW indexes for each of the hyper parameter combinations reported above. The rst group was built using the original oating point embedding vectors. The second group was built by compressing those vectors using the method described in Section 4. Table 1 summarizes our observations on build time and memory footprint for the two groups, and shows a noticeable improvement for the compressed indexes. Specically, theint8 indices required approximately half the memory and build time Figure 2: QPS and Recall versus EFS. Quantization results in higher overall QPS at only a slight decrease in recall. of thefloat32indices. We note that the transition fromfloat32 toint8did not produce a linear decrease in memory. This is due to the overhead of HNSW’s indexing graph data structure which consists entirely of native pointers. Figure 2 plots QPS as a function of the HNSW search parameter, EFS. The results were computed for several parameter settings which have a non-trivial eect on throughput. Theint8indexes produce higher throughput than thefloat32indices. This phenomenon is likely due to the reduced overhead of nativeint8operations versus nativefloat32operations. We observed that there is a negative non-linear association between the search parameter EFS and QPS. This is expected, as higher EFS values correspond to longer search times. As noted in Section 3, the primary source of recall loss for distance metrics which obey the triangle inequality is equality relaxation. Figure 2 reports the relationship between HNSW parameters and recall. The originalfloat32indices achieve a higher overall recall than the compressedint8indices, but only by around 2%. Furthermore, as the value of the search parameter EFS increases, so too does recall. This observation suggests that our quantization scheme achieves its goal of preserving relative distances for nearest neighbors at the cost of accepting some aliasing errors for points which are further apart. We also see similar small losses in recall when we evaluated on additional algorithms and datasets, as shown in Tables 2 and 3. Our primary evaluation used HNSWlib along with the PRODUCT60M dataset due to the observation that HNSW oered the best recall-QPS tradeo. However, to provide some evidence for the generality of our approach and its expected performance on other domains and algorithms, we consider two other KNN algorithms and public datasets, specically the FAISS [17] exact search implementation and Neighborhood Graph and Tree (NGT) [15]. In addition to the search dataset described in Section 4 we used two public data sets from the KNN benchmark [3]. The SIFT and Glove100 datasets have dimensions of 128 and 100 respectively, and Table 2: (float32) versus compressed (int8) datasets using FAISS exhaustive nearest neighbor search. Table 3: (float32) versus compressed (int8) data sets using Neighborhood Graph and Tree (NGT). consist of approximately 1 million vectors each. Each data set has its own 1000 vector test set along with 100 true neighbors for each test vector. Table 2 reports the performance of FAISS both before and after quantization. Table 3 reports the same for NGT. In both cases we were able to achieve comparable improvements in runtime and memory consumption at a slightly larger, though still modest, decrease in recall of 2 − 6%. We propose a low precision quantization technique based on the observation that most real-world vector datasets are concentrated in a narrow range that does not require fullfloat32precision. We applied this technique to the HNSW Knn algorithm and obtained a 60% reduction in memory overhead and a 30% increase in throughput at only a 2% reduction in recall. We also provided evidence of the generality of our method by obtaining similar performance improvements on the FAISS exact search and NGT frameworks. For future work, we hope to further demonstrate the generality of our technique by evaluating on additional Knn algorithms and realworld benchmark datasets and extend our evaluation to additional distance metrics.