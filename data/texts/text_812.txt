Attribute value extraction refers to the task of identifying values of an attribute of interest from product information. Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product ranking, retrieval and recommendations. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we introduce MAVE, a new dataset to better facilitate research on product attribute value extraction. MAVE is composed of a curated set of 2.2 million products from Amazon pages, with 3 million attributevalue annotations across 1257 unique categories. MAVE has four main and unique advantages: First, MAVE is the largest product attribute value extraction dataset by the number of attribute-value examples. Second, MAVE includes multi-source representations from the product, which captures the full product information with high attribute coverage. Third, MAVE represents a more diverse set of attributes and values relative to what previous datasets cover. Lastly, MAVE provides a very challenging zero-shot test set, as we empirically illustrate in the experiments. We further propose a novel approach that eectively extracts the attribute value from the multi-source product information. We conduct extensive experiments with several baselines and show that MAVE is an eective dataset for attribute value extraction task. It is also a very challenging task on zero-shot attribute extraction. Data is available at https:// github.com/google-research-datasets/MAVE. • Computing methodologies → Information extraction. attribute value extraction, open tag extraction, zero-shot learning ACM Reference Format: Li Yang, Qifan Wang, Zac Yu, Anand Kulkarni, Sumit Sanghai, Bin Shu, Jon Elsas, Bhargav Kanagal. 2022. MAVE: A Product Dataset for Multi-source Attribute Value Extraction. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM ’22), February 21–25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3488560.3498377 Figure 1: An example of the product prole on an eCommerce platform. It consists of multiple information sources, including a product title, a product description and several other sources. Product attributes are critical product features which form an essential component of e-commerce platforms today. They provide useful details of the product, which help customers to make purchasing decisions and facilitate retailers on many applications, such as product search [1,2], product recommendation [25,50], product representation [34,39] and question answering system [11, 23]. However, for most retailers, product attributes are often noisy and incomplete with a lot of missing values, which has been shown in many previous studies [54,63]. Therefore, it is an important research problem to supplement the product with missing values for attributes of interest. The problem of attribute value extraction is illustrated in Figure 1. It shows the prole of a Laptop product as an example, which consists of a title, a product description and several other information sources. It also shows several attribute values that could be extracted. There has been a lot of interest in this topic, and a plethora of research [4,5,10,12,43,46,61] in this area both in academia and industry. Early works on this problem are rule based approaches [6, 13,51], which utilize domain-specic knowledge to design regular expressions. Several other approaches such as [36,44,58] formulate the attribute value extraction as an instance of named entity recognition (NER) problem [37], and build extraction models to identify the entities/values from the input text. With the recent advance in natural language understanding, sequence tagging [54, 56,59,62] based approaches have been proposed, which achieve promising results. Attribute value extraction approaches rely on a rich dataset to help them learn to extract the attribute values from the product prole. Existing attribute datasets are created from dierent e-commerce platforms, such as Amazon [28,59], AliExpress [54,56], JD [63], etc. However, there are several major limitations: •The scales of current public datasets are not large. In fact, many of the datasets used in previous works are not even publicly available. Moreover, existing datasets, such as the MAE one in Table 1, are very noisy and cannot be used without dense pre-processing and cleanup. Researches have shown that having a large and accurate attribute-value dataset can signicantly improve model performance. •The product proles are relatively simple. Most product context is formed from a single source, i.e., product title or description, resulting in short sequences. In real world applications, products are usually represented by multiple sources, such as title, description, specication, table, review, image, etc, forming much longer sequences. •The attributes and values are not diverse. The lack of the data diversity impedes research in the zero-shot/few-shot attribute value extraction, which has huge potential impact on unseen data. With the rapid expansion of e-commerce, new products with new capabilities are being released constantly which means the model needs to gracefully adapt to new attributes. To address these challenges and advance research on product attributes, we present the Multi-source Attribute Value Extraction (MAVE) dataset. MAVE is created by rst extracting the raw values from Amazon Review pages [38] on a pre-dened set of attributes. We then apply rigorous ltering and post-mapping to only retain high quality extractions. The resulting dataset contains over 2.2 million products distributed into 1257 dierent product categories, with 3 million attribute-value annotations - making MAVE the largest attribute value extraction dataset at the time of writing. Moreover, MAVE provides more complete product proles from multiple information sources, including product title, descriptions, features, specications and key-value pairs, which enhance the attribute coverage. For example, in Figure 1, the value 7 Hours for the attribute Battery Life is from the product specications. Furthermore, MAVE contains a rich and diverse set of attributes and values. In particular, there are 2535 unique attributes with 100K unique values in the MAVE dataset. It is worth pointing out that by leveraging the human rules based matching, MAVE is able to ensure a high quality bar with 97.8% precision (manual check) over 1000 random sampled attribute-value annotations. In this paper, we also propose a novel Multi-source Attribute Value Extraction model via Question Answering (MAVEQA). MAVEQA uses the recent ETC [3] model as the backbone to eectively extract the attribute value from the multiple information sources with the long sequences. We conduct an extensive set of experiments on the MAVE dataset over several state-of-the-art baselines (including the proposed MAVEQA). The experimental results demonstrate the potency of the data. They also show that Table 1: Existing datasets for attribute value extraction. MAVE is a challenging dataset on attribute value extraction for various attributes, especially those zero-shot attributes. Rule-based extraction methods [13,37,51] are among those early works on attribute value extraction. They use a domain-specic seed dictionary or vocabulary to identify key phrases and attributes. Ghani et al. [12] predene a set of attributes to extract the corresponding values. Wong et al. [55] extract attribute-value pairs from the semi-structured text. Shinzato and Sekine [48] design an unsupervised method to extract attribute values from product description. Several rule-based and linguistic approaches [6,35] leverage syntactic structure of sentences to extract dependency relations. Several NER based systems [8,29,36,44] were proposed for extracting product attributes and values. However, these rulebase and domain-specic methods suer from limited coverage and closed world assumptions. With the development of deep neural networks, various neural network methods have been proposed and applied in sequence tagging successfully. Huang et al. [15] are the rst to apply the BiLSTM-CRF model to a sequence tagging task. But it employs heavy feature engineering to extract character-level features. Lample et al. [24] utilize BiLSTM to model both word-level and character-level information rather than hand-crafted features. Chiu and Nichols [7] model character level information using convolutional neural network (CNN), which achieves competitive performance and has been extended in [32] with an end to end LSTM-CNNs-CRF model. Recently, several approaches employ sequence tagging models for attribute value extraction. Kozareva et al. [22] adopt BiLSTMCRF model to tag several product attributes from search queries with hand-crafted features. Furthermore, Zheng et al. [62] develop an end-to-end tagging model utilizing BiLSTM, CRF, and attention mechanism without any dictionary. More recently, Xu et al. [56] adopt only one global set of BIO tags for any attributes to scale up the model, which explicitly models the semantic representations for attribute and product title. Most recently, several approaches [31, 54,59] formulate the problem as a question answering (QA) [16,21] task. Wang et al. [54] propose to jointly encode both the attribute and the product context with cross-attention model, and then decode the value span with a BIO tagging. It is worth mentioning that there are also methods that work on multimodal attribute value extraction [17,28,63], which incorporate the visual features using CNN [30] to boost coverage of the attributes. Relation extraction [40,42,47,49,53,60] is also relevant to attribute value extraction. Relation extraction refers to the task of extracting relational tuples and putting them in a knowledge base. The tuple usually corresponds to a subject, a predicate and an object. Attribute value extraction can be thought of as the problem where the subject is known (the product), and given the attribute (the relation) extract the value. However, relation extraction has traditionally focused on extracting relations from sentences relying on entity linking systems to identify the subject/object and building models to learn the predicates in a sentence [5,26]. Whereas in attribute value extraction [41,45], usually the predicates (the attribute names) rarely occur in the product title or the description, and entity linking is very hard because the domain of all entities/values is unknown. Although product attribute value extraction is an important research problem, which has raised a lot of attention in recent years, as far as we know, there is no universal dataset for evaluating the AVE models. This is one of the motivations of this work, to establish such a benchmark dataset that could serve as a rich resource to drive research in attribute value extraction. However, there are a few datasets that are used in previous AVE approaches. MAE [17] is among the early datasets that are applied to this space. MAE is composed of mixed media data over 2 million product items, obtained by running the Dibot Product API on over 20 million web pages from 1068 dierent commercial websites. The attribute-value pairs are automatically extracted from tables present on product webpages without any verication, which are very noisy - only less than 30% values can be found in about 600K products. OpenTag [62] is one of the pioneer work that models this problem as sequence tagging with LSTM. However, there are only ve attributes in this data with 13K attribute value annotations. MEPAVE [57,63] is a multimodal dataset with textual product descriptions and images. It contains 34K products collected from a JD e-commerce platform. The 87K attribute value annotations are generated by crowd sourcing annotators. AE-110K [56] is a public dataset collected from AliExpress Sports & Entertainment category. It is composed of 110K attribute value annotations obtained from Item Specic in 50K product titles. Despite the long-tailed attribute distribution (7650 of 8906 attributes occur less than 10 times), this dataset has been used in several recent works [19,54]. AdaTag [59] builds a dataset by collecting product proles with multiple source representations from the public web pages at Amazon.com. The 410K attribute value annotations are generated from 333K products in a similar way to the previous work [20,56,62]. Unfortunately, this dataset is not publicly available. There are many other works [18,33] that use their own datasets, which are all small data with limited attributes. We summarize the existing datasets in Table 1. Our MAVE dataset is derived from a public product collection - the Amazon Review Dataset [38]by generating and adding attributevalue annotations to the products. In this section, we describe our Table 2: An example of a product prole with annotations. Each source corresponds to a eld of the product. pipeline for creating MAVE, and also provide detailed statistics from various aspects. The products from the Amazon Review Dataset contain multiple information sources. For each product, we organize the ‘title’, ‘description’, ‘feature’, ‘price’, and ‘brand’ into a structured product prole. We performed the following cleaning of the texts to ensure a high quality of the attribute value annotations. •Remove the html tags, e.g., script and style tags, within the product prole by using BeautifulSoupas well as a few hard-coded rules. •Remove extra whitespaces and invalid text unicode characters. •Remove products with a total number of words less than 20. • Remove products without a title. Table 2 shows an example of one product prole after the preprocessing. Before describing how to generate and associate attribute values to product proles, we rst discuss the structure of the attributes. 3.2.1 Aributes are category specific. Products can be classied into dierent categories, e.g. Shoes, Books, Mobile Phones. For each category, a set of attributes are pre-dened by our human raters. For example, the Shoes category contains attributes like Type, Style, Heel Height, etc.; The Books category has attributes Type, Format, Fiction Form, etc.; The Mobile Phones category has attributes Operating System, Battery Life, Screen Size, etc. It is worth pointing out that certain attributes are more general, which are shared across multiple categories, e.g., Type, Style. But some attributes are more specic and apply to only a few categories, e.g. Battery Life, Fiction Form. Table 3: Overall statistics of the MAVE dataset. Figure 2: Distributions on # sources and # words for positive and negative sets. The inset gure is the percentage in each # words buckets within each sets. Moreover, dierent attributes from dierent categories can share similar meanings, for example, Resolution for Digital Cameras and Rear Camera Resolution for Mobile Phones. The correlation among attributes empowers attribute value extraction models to transfer knowledge from one attribute to another. 3.2.2 Aribute values are category specific. For attributes shared across multiple categories, the denition of attribute values may also vary. For example, the Type for Books could be Fiction or Nonction, but the Type for Shoes could be Sandals, Slippers, etc. Note that some of the attributes have discrete or categorical values, e.g. Style, Type, which are associated with a set of well dened attribute values. On the other side, some attributes have measure, i.e., numerical values with units, e.g. Heel Height, Screen Size. In this section, we present the details of generating attribute value annotations on the product proles. In the following discussion, we refer to an example as a tuple of (product, category, attribute). 3.3.1 Category classification. We rst run a category classier on a product to predict the category. The category vocabulary is dened by human experts, and the category classier is a multi-class classication model trained on an existing large dataset containing category labels. We remove the products with predicted category probability lower than a threshold of 0.5 to ensure a high quality of category classications. Figure 3: Number of positive example distributions on categories and attributes. Inner and outer circles are for categories and attributes, respectively. Empty blocks indicate the category-attribute contains too few examples to show. 3.3.2 Aribute value span extraction. As aforementioned, for each category, a set of attributes are pre-dened. For each attribute in a category, a set of extraction rules are also dened by our human raters to extract value spans in product proles and map to normalized attribute values. We do not apply the extraction rules directly to the products because the extraction rules are not complete - they are designed for extracting specic formats of attribute values, resulting in low coverage extractions. Moreover, the extraction rules are not designed to extract all the attribute value spans in the context. Instead, we rst train an ensemble of ve versions of AVEQA [54] models using a large amount of silver data from the extraction rules as well as human veried gold data. These models vary in terms of random initialization, training data version, pre-post processing, etc. The input of these models is a concatenation of WordPiece tokenized category, attribute and product context. The outputs are token level attribute value spans in the product sources. We conduct inference on the cleaned Amazon product proles using the ve trained models. To ensure a high precision of the extractions, we further apply a simpler set of human extraction rules (created by human raters) to map the predicted spans to normalized attribute values, and remove the extractions that can not be mapped. The nal extractions are aggregated from these ve models into a positive set and a negative set as follows. For each example, •If the normalized attribute values from all ve models are identical, the example is added to the positive set. The union of the extracted spans from all ve models forms the span set of the attribute values. •If no span is extracted from either model, and there is no extracted span from extraction rules, the example with empty value spans is put into the negative set, meaning no value for this attribute. The aggregation ensures a low false positive rate in the positive set and a low false negative rate in the negative set. However, we still generated much more negative examples compared with positive examples. Therefore, we further down sample the negative examples by restricting each category-attribute pair having at most 5K negative examples. The spans of the attribute values are very useful information in attribute value extraction research as well as questions answering systems. In this work, we map the token level attribute value spans to character level spans in the original sources following the method used in the BERT paper for the SQuAD task [9]. If two spans overlap, we keep the one with the smallest begin index. However, in the MAVE dataset, we observe the spans are distributed sparsely with rare overlapping. The resulting (product, category, attribute, value spans) tuples are stored as an example in the MAVE dataset. The statistics of the MAVE dataset is reported in Table 3. The original Amazon Reivew dataset contains 14.7M products. After the processing and ltering as described in Section 3.1 and 3.3, we have 2.2M products with 3M product-attribute annotations in the positive set, and 1.2M products with 1.8M product-attribute annotations in the negative set. In the positive set, the majority of products have one or two attributes, but there are still 100k+ products with more than three attributes. 3.4.1 Multi-source and Sequence length statistics. A product prole contains multiple sources, the distributions on number of sources and total number of words are shown in Figure 2, where a basic white space tokenizer is used to split the texts in all sources into words. It can be seen from the gures that the total number of words in a product is much longer than the typical number of words in a product title or description, which means the dataset contains much richer information from the multi-source product proles. We observe that there is also a small but non-negligible portion of products (36k positives, 18k negatives) with sequence length longer than 512, which can serve as a data source for benchmarking models designed for long sequences. 3.4.2 Aributes statistics. The counts of unique categories, category-attribute pairs can be found in Table 3. The average number of attributes per category is roughly 2, and the average number of categories an attribute shared across is 3. We visualize the number of positive example distributions on categories and attributes in Figure 3. It is clear that the head categories in general contain more attributes than average. The product proles in MAVE dataset are composed of multiple text sources. Existing approaches might not work well on MAVE because: 1) They do not model the multi-source structure of the product, but simply treat product prole as one single text sequence by concatenating the texts from all sources. However, dierent source might carry dierent information, e.g., some attributes might appear more often in product title, while certain attributes are only mentioned in product specications. 2) They are not able to deal with products with very long sequences. Long inputs are trimmed to t into their models. In this work, to address these challenges, we propose a novel approach of Multi-source Attribute Value Extraction via Question Answering (MAVEQA), which eectively and eciently models the products with structure and long proles. Following our previous work AVEQA [54], we formulate the attribute value extraction task as a question answering problem. In particular, each attribute is treated as a question, and we seek the best answer span in the product context that corresponds to the value. The overall model architecture is shown in Figure 4. Essentially, our model consists of three main components, an input layer, a contextual encoder and an output layer. In the following sub-sections, we present each component separately in detail. Dierent from previous models [54,56,59], our model treats each product source as an individual sequence, and assigns a global node/token to each of them. For example, in Figure 4, the product has two sources, i.e., one title and one description. By also regarding the attribute and category as two sources, our model assigns four global tokens to represent these four input sources respectively. The global token can be viewed as a summarization of its corresponding source. In the input layer, every word in each input source is converted into a𝑑-dimensional embedding vector. This embedding is obtained by concatenating the word embedding and the source embedding. Each global token will also be initialized with a global token embedding. Note that all the embeddings are trainable in our approach. In MAVEQA, we adopt the ETC encoder [3] to generate the contextual embeddings for the global and long input tokens. ETC is an extension of the Transformer [52] encoder to better capture the structure and long input, which is a natural t to our task. The encoder in our model is a stack of identical ETC layers. There are four attention mechanisms in each ETC layer: •Global to global attention: each global token attends to all other global tokens, which allows information to ow among global tokens. •Global to long attention: each global token attends to all long tokens in the corresponding source. Each global token is similar to a CLS token in the BERT model [9], which summarizes the corresponding source. •Long to global attention: each long token in a source attends to all global tokens. This attention enables knowledge passing from other sources to the token in this source. •Long to long attention: each long token will only attend to the long tokens within a local radius from the same source. This local attention pattern ensures the computational eciency of the encoder. In the output layer, a single dense layer with sigmoid activation is applied to the output long token embeddings, which computes a probability for every long token in all sources excluding category and attribute (as we are not seeking for values there) to predict whether the token is in an attribute value span. Similarly, the global token embeddings can be used to predict whether the corresponding source contains an attribute value or not. During training, we use sigmoid cross-entropy loss function on both long and global tokens. We adopt the following models as baselines on the MAVE dataset. • OpenTag[62] uses a BiLSTM-Attention-CRF architecture with sequence tagging strategies. OpenTag does not encode the attribute and thus builds one model per attribute. In order to scale up to an arbitrary number of attributes, similar to SUOpenTag model [56], we concatenate category, attribute, and source tokens in the input and make it attribute dependent. This attribute-dependent version of OpenTag is referred to ADOpenTag. Table 4: Number of positive and negative examples in train and eval sets for the selected attributes, all attributes and zero-shot attributes benchmarks. • AVEQA[54] formulates the attribute value extraction as a question answering task. This model jointly encodes both attribute and product context with BERT encoder. It achieves the state-of-the-art results in the AE-110K dataset. • MAVEQAis the new model proposed in this work, which adopts ETC [3] encoder to encode structure and longer input sequences. For all these baselines, we use the same output layer which is a single dense layer with sigmoid activation to predict token scores, and we use the same English uncased WordPiece vocabulary as in BERT [9]. More implementation details are provided in the appendix. We arrange the following setups for benchmark: • Selected AttributesTo demonstrate the baseline model performance on individual attributes, we randomly select a set of attributes. In particular, we randomly select ve attributes from the head ones that contain a large number of attribute-value annotations in the dataset. We also randomly select ve tail attributes that have very few examples in the dataset. • All AttributesIn order to evaluate the models’ capability of scaling up to an arbitrary number of attributes, we also conduct experiments of all baselines on all attributes. • Zero-shot AttributesTo further examine the generalization ability of the models, we randomly select ve attributes and holdout these attributes from training, i.e., none of these attributes are seen during training. We refer them to zeroshot attributes as they are used for evaluating zero-shot extraction. Table 5: Individual results on each selected attribute and average results on all attributes. OpenTag is used for each selected attribute and ADOpenTag is used for all attributes. • Few-shot LearningTo study the impact of dierent number of training examples, we explore the model behavior with only a few training examples, namely few-shot learning. We randomly split the dataset into train:eval:test = 8:1:1. Note that OpenTag trains a single model per attribute on the Selected Attributes. We use its extended version,ADOpenTag, to train another model on all attributes. The number of examples in each set is reported in Table 4. Following previous works [54,56,59], we use Precision, Recall, and F1 score as evaluation metrics. Specically, for each attribute annotation, when the ground truth is No attribute value, the model can predict No value (NN) or some incorrect Value (NV); when the ground truth has attribute Values, the model can predict No value (VN), Correct values (VC) or Wrong values (VW). We deneNN, NV,VN,VC, andVWas the number of examples for the above ve cases. The precision and recall can be computed as: The F1 score is calculated as 2P R/(P + R). To study and compare the baseline model performances on individual attributes, we select ve head attributes and ve tail attributes. Note that an attribute can be shared across multiple categories. It can be seen from Figure 3 that the number of categories containing the attribute and the number of the examples having the attribute are positively correlated. As aforementioned, we randomly select ve head attributes, including Type, Style, Material, Size, and Capacity. It is clear that these attributes have more general meaning which are commonly shared across categories. The tail attributes have a small number of examples and often only appear in 1-2 categories. In order to have a meaningful evaluation on tail attributes, we also constrain the number of evaluation examples to be larger than 15. The tail attributes include Black Tea Variety, Staple Type, Web Pattern, Cabinet Conguration and Power Consumption. Figure 5: Precision, Recall, F1 errors within each bucket of example sequence length for all attributes. The results of the OpenTag, AVEQA, and MAVEQA models on these selected attributes are reported in Table 5. From these comparison results, we can see that the AVEQA and MAVEQA models perform consistently better than OpenTag on all head and tail attributes. The reason is that both AVEQA and MAVEQA use the advanced Transformer architecture which have been shown to outperform BiLSTM-Attention-CRF architecture. Moreover, the knowledge can be transferred among dierent attributes in AVEQA and MAVEQA, since they jointly encode the attribute and the product across all attributes. It also can be seen that AVEQA and MAVEQA achieve similar results on these selected attributes. We found out, by examining the evaluation examples, that the product sequences of these selected attributes are not long, and thus AVEQA and MAVEQA are equally eective when modeling short sequences. We will discuss more on the eect of product sequence length in the next section. Another interesting observation is that the results of MAVEQA on the tail attributes are even better (F1 scores are almost 100%) than those on the head attributes. This is because although tail attributes have relatively small training examples, their attribute-value patterns are much simpler than those in the head attribute. For example, Black Tea Variety is a specic attribute in category Tea & Infusions, which only has three possible values, i.e. Ceylon, Darjeeling and Earl Grey. On the other hand, the attribute Type appears in more than 40 categories with more than 500 dierent values. However, MAVEQA is still able to achieve very high performance on the head attributes. The average results over all attributes are reported in Table 5. We observe similar result patterns to those in the selected attributes. It can be seen that MAVEQA performs slightly better than AVEQA. Our hypothesis is that for those examples with multiple product sources and long sequences, MAVEQA is more eective in capturing the attribute-value relations via modeling the complete structure and long sequence. In contrast, AVEQA simply concatenates and trims the text from dierent sources and treats it as a single sequence. To further investigate along this direction, we divide the evaluation examples into dierent buckets w.r.t. the sequence length of the example, and compute the metrics in each bucket for AVEQA and MAVEQA. The precision, recall and F1 gap/error results are shown in Figure 5. It can be seen that with example sequence length exceeding max sequence length of the models, both precision and recall errors for AVEQA go up, while the errors remain the same for MAVEQA. This observation validates that MAVEQA is indeed more eective on examples with longer sequences. We conduct zero-shot extraction experiments to evaluate the generalization ability of AVEQA and MAVEQA on unseen attributes. The zero-shot extraction results are reported in Table 6. There are several interesting observations from this table. First, it is clear that the overall performance of both methods on these zeroshot attributes are much worse compared with those results in Table 5, which is consistent with our expectation as there are no training examples for the zero-shot attributes. Second, AVEQA achieves better results than MAVEQA on Sp ecial Occasion. Our hypothesis is that AVEQA and MAVEQA are trained with dierent pre-trained models, i.e., AVEQA is initialized with the pre-trained BERT [9] while MAVEQA is initialized from ETC [3]. Previous works [14,27,54] have shown that the pre-trained model is crucial for the ne-tuned model to perform well on zero-shot learning. However, the pre-trained ETC model removes all examples with less than 10 sentences since it focuses on long sequences, which might have eliminated a certain amount of knowledge on attribute Special Occasion, and thus does not perform well. Finally, AVEQA and MAVEQA achieve 0 scores in terms of all metrics on Compatibility, which indicates that it is a truly dicult zero-shot attribute. Both pre-trained models and the training examples of other attributes are lacking information about Compatibility, resulting in 0 extractions. We believe there is deep knowledge and information that are uncovered in zero-shot extraction. The MAVE dataset can serve as a rich resource to drive research in this direction. To study the eectiveness of MAVEQA with few training examples, we conduct few-shot experiments by varying the number of training examples, 𝑘, from {1, 2, 3, 5, 10, 50, 100} on all zero-shot attributes. For each run, we rst randomly separate out 100 examples per attribute, then we randomly select𝑘examples from the 100 examples. We perform ne-tuning for 20K steps over these selected examples initializing from the model trained on all attributes (exclude zero-shot attributes). The F1 scores of MAVEQA on fewshot experiments are shown in Figure 6. It is not surprising to see that the F1 scores increase with more training examples. We can also observe that the F1 scores on most attributes are able to reach a reasonably high value, above 90%, when the number of training examples increases to 100. We notice that the F1 score remains attened for attribute Food Processor Capacity. By analyzing the errors, we found that there are a certain amount of false negatives in the evaluation set, where the extractions are actually correct but the attribute value annotations are missing. Improving the attribute coverage on MAVE is denitely one of the future directions. In this paper, we introduce the Multi-source Attribute Value Extraction (MAVE) dataset – the largest, multi-source, diverse, context-rich, clean dataset. By extracting and verifying attribute values from over 2.2 million multi-source product proles, MAVE provides a large set of 3 million attribute-value annotations. We establish benchmarks on MAVE with several state-of-theart methods, including a novel approach proposed in this work that models the multi-source and long product proles. We also empirically demonstrated the use of this dataset on a very challenging task - zero-shot attribute value extraction, which is not fully exploited by existing datasets. We believe this can serve as a rich resource to drive research in the attribute value extraction domain for years to come and enable the community to build better and more generalized models. MAVE can potentially be used as a pretraining dataset in various downstream tasks, including product embedding learning, attribute grounded questions answering, product search and recommendation, etc.