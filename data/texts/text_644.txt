In this paper, we present GRecX, an open-source TensorFlow framework for benchmarking GNN-based recommendation models in an ecient and unied way. GRecX consists of core libraries for building GNN-based recommendation benchmarks, as well as the implementations of popuplar GNN-based recommendation models. The core libraries provide essential components for building ecient and unied benchmarks, including FastMetrics (ecient metrics computation libraries), VectorSearch (ecient similarity search libraries for dense vectors), BatchEval (ecient mini-batch evaluation libraries), and DataManager (unied dataset management libraries). Especially, to provide a unied benchmark for the fair comparison of dierent complex GNN-based recommendation models, we design a new metric GRMF-X and integrate it into the FastMetrics component. Based on a TensorFlow GNN library tf_geometric, GRecX carefully implements a variety of popular GNN-based recommendation models. We carefully implement these baseline models to reproduce the performance reported in the literature, and our implementations are usually more ecient and friendly. In conclusion, GRecX enables uses to train and benchmark GNN-based recommendation baselines in an ecient and unied way. We conduct experiments with GRecX, and the experimental results show that GRecX allows us to train and benchmark GNN-based recommendation baselines in an ecient and unied way. The source code of GRecX is available at https://github.com/maenzhier/GRecX. Personalized recommendation is an important yet challenging task, which has attracted substantial attention in the past decade. Most traditional approaches consider recommendation as a matching task [16], and can be solved by estimating the matching score based upon semantic representations of users and items [5–7]. Recently, graph representation learning approaches are emerging tools to pursue a meaningful vector representation for each node in graphs, which can eectively model users, items and their corresponding relationships. Graph Neural Networks (GNNs), such as GCN [10], GraphSAGE [3] and GAT [13], have shown impressive performance in aggregating feature information of neighboring nodes. In recommender systems, the interactions between users and items can be represented as a bipartite graph and the goal is to predict new potential edges (i.e., which items could a user be interested in), which 2,32,3 Table 1: An Example of Experimental Settings of Dierent Baselines. NSS denotes negative sampling strategies, where "single" and "multiple" represent the number of negative samples. The comparison is unfair due to dierent settings. Negtive Sampling Strategy single(1) single(1) multiple(500+) can be achieved with GNNs, which are called GNN-based recommendation methods. GNN-based recommendation techniques has attracted researchers and engineers from a variety of elds, and they have been utilized to build various real-world applications such as medicine recommendation [8], micro-video recommendation [2, 15], and social recommendation [1, 17, 18]. Although many existing approaches provide ocial implementations, it is still dicult to build ecient and unied benchmarks due to the following limitations: (1) The evaluation of GNN-based recommendation approaches may involve many computationally expensive operations, which should be optimized to perform ecient benchmarking. However, most ocial implementations usually ignore the problem, and some of the computationally expensive operations are still widely used by these implementations. For example, these implementations usually rely on the matrix multiplication between the user representation matrix and the item representation matrix to perform the topK retrieval of items for users, which may result in large time and space complexity. (2) It is dicult to build a unied benchmark since dierent baselines may adopt dierent loss functions, dierent negative sampling strategies, etc. For example, UltraGCN [12] adopts NGCF [14] and LightGCN [4] as baselines. However, the negative sampling strategy of UltraGCN [12] is dierent from that of NGCF [14] and LightGCN [4]. As shown in Table 1, NGCF and LightGCN adopt a negative sampling strategy that uses only one negative sample, while UltraGCN’s negative sampling strategy samples more than 500 negative samples. It is well known that GNN-based recommendation models can be improved by simply increasing the number of negative samples [8]. As a result, the experimental results reported by UltraGCN [12] cannot verify the eectiveness of the model. In this paper, we present GRecX, an open-source TensorFlow framework for benchmarking GNN-based recommendation models in an ecient and unied way. To address the eciency problem, we develop core libraries to provide essential components for build ecient benchmarks, including FastMetrics (ecient metrics computation libraries), VectorSearch (ecient similarity search libraries for dense vectors), BatchEval (ecient mini-batch evaluation libraries), and DataManager (unied dataset management libraries). To build a unied benchmark for the fair comparison of dierent complex GNN-based recommendation models, we design a new metric named GRMF-X and integrate it into the FastMetrics component. In addition, we also provide ecient implementations of a variety of popular GNN-based recommendation models, which enable us to build a more comprehensive benchmark. We conduct experiments with GRecX, and the experimental results show that GRecX allows us to train and benchmark GNN-based recommendation baselines in an ecient and unied way. All features of GRecX and a collection of examples is provided with the source code, which is available at https://github.com/maenzhier/GRecX. Figure 1 shows the overall framework of GRecX, which mainly consists of core libraries and implementations of popuplar GNN-based recommendation models. In this section, we provide an overview of the framework of GRecX. The core libraries provide essential components including FastMetrics (ecient metrics computation libraries), VectorSearch (ecient similarity search libraries for dense vectors), BatchEval (ecient mini-batch evaluation libraries), and DataManager (unied dataset management libraries). There components enable us to build efcient and unied benchmarks In this section, we will introduce each component of GRecX’s core libraries in detail. 2.1.1 FastMetrics. FastMetrics provides ecient implementations for various recommendation metrics, such as NDCG@N, Precision, and Recall. It is non-trivial to implement ecient metrics computation libraries for recommendation due to the complexity of real-world data. Moreover, for fair comparison, we design a new metric named GRMF-X and integrate it into FastMetrics. GRMF-X is the abbreviation forGainRelative toMFin terms of the metrics X, and it is dened as follows: 𝐺𝑅𝑀 𝐹 −𝑋 (𝑀𝑂𝐷𝐸𝐿, 𝐶𝑇 𝑋 ) =𝑋 _𝑆𝐶𝑂𝑅𝐸 (𝑀𝑂𝐷𝐸𝐿, 𝐶𝑇 𝑋 )𝑋 _𝑆𝐶𝑂𝑅𝐸 (𝑀𝐹, 𝐶𝑇 𝑋 )−1.0 (1) where𝑋 _𝑆𝐶𝑂𝑅𝐸 (𝑀𝑂𝐷𝐸𝐿, 𝐶𝑇 𝑋 )is the evaluated score of model 𝑀𝑂𝐷𝐸𝐿under the metric𝑋and context𝐶𝑇 𝑋, and𝑀𝐹denotes a tuned Matrix Factorization (MF) [11] model. We introduce this naive evaluation metric since it can eectively help us to verify the superiority of GNN-based recommendation models. There are mainly two reasons why our unied benchmark can benet from GRMF-X: (1) We observe that although some research’s experimental results show that their proposed model outperforms all the baselines, the reported performance of the baselines or the proposed models may not be competitive with the simple welltuned MF model. Note that although some research provide the performance of MF, they may employ a MF model that is not well tuned, which usually show poor performance. This shows that the authors do not conduct experiments with well-implemented baselines, and thus the experimental results are not convincing. (2) We also observe that some research do not conduct experiments of dierent baselines under the same context. Here we use context𝐶𝑇 𝑋to denote the some important settings beyond the GNN architectures, such as the negative sampling strategies. For example, as shown in Table 1, NGCF and LightGCN employ a negative sampling strategy that uses only one negative sample, while UltraGCN’s negative sampling strategy samples more than 500 negative samples. It is well known that GNN-based recommendation models can be improved by simply increasing the number of negative samples [8]; therefore, directly adopting the performance of the ocial implementations may result in unfair comparison. In Equation 1, we implicitly constrain that dierent models are compared under the same context𝐶𝑇 𝑋. Note some hyper-parameters such as the learning rate and L2 coecient are not considered as the context. For these hyper-parameters, dierent models may rely on dierent parameter settings to achieve their best performance, and we should carefully tune these hyper-parameters to obtain the best performance. 2.1.2 VectorSearch. Many GNN-based recommendation approaches perform recommendation by similarity based searching, which ranks items based on the similarity scores between the dense vector representation of users and items. Most evaluation metrics require a global scan over all the items, which may result in large time and space complexity. We leverage industrial solutions such as Faiss [9] to build ecient similarity search libraries for dense vectors named VectorSearch, which can perform dense vector searching eciently with millions of candidate items. 2.1.3 BatchEval. The evaluation of GNN-based recommendation models can benet from mini-batch techniques, which can take advantage of GPU’s parallel processing ability to improve the eciency of the evaluation. It requires a lot of tricks to design minibatch implementations on irregular real-world recommendation data. To provide friendly and handy mini-batch solutions, we design BatchEval. The users only need to provide the learned representations or the similarity computation function, and BatchEval can automatically perform ecient mini-batch based evaluation with the provided information. 2.1.4 DataManager. DataManager provide abstract Dataset class as interfaces for users to custom handy dataset APIs. The abstract Dataset class can automatically handle the whole lifecycle of data processing, such as data downloading, data preprocessing, data caching, etc. Usually, users can easily custom their Dataset class by subclassing the abstract Dataset class, providing the download urls of raw dataset, and overriding the preprocessing process. Then, the abstract Dataset class will handle the rest of data processing process. In addition, we already implement several widely-used datasets as Dataset classes, which can be directly used to load these datasets. In this paper, we implement the state-of-the-art GNN-based recommendation algorithms (e.g. NGCF [14], LightGCN [4], UltraGCN [12]) and a basic MF model [11] as baselines. Especially, we carefully implement these baseline models to reproduce the performance reported in the literature, and our implementations are usually more ecient and friendly. We conduct experiments on several benchmark datasets to provide a fair comparison of some popular baselines with GRecX. The complete experimental results are at https://github.com/maenzhier/GRecX. We are still working on some baselines to provide more comprehensive benchmark. Here, we only list partial experimental results on two benchmark datasets to demonstrate the performance of models implemented by GRecX and our new evaluation metrics GRMF-X. We use two datasets: yelp2018 and gowalla. Note that some dataset may have dierent versions, and we use the version used by LightGCN [4]. The statistics of the two datasets is listed in Table 2. In terms of the baselines, we choose a basic model MF [11], and two state-of-the-art GNN-based recommendation models UltraGCN [12] and LightGCN [4]. Here we employ a widely-used metrics NDCG@20 and our new metrics GRMF-X (GRMF-NDCG@20) for evaluation. As mentioned in Section 2.1.1, for other hyper-parameters, we carefully tune these hyper-parameters and report the performance with them. We use BCE and BPR as the ranking losses respectively and experimental results are shown in Table 3 and Table 4. For all comparable models, we set dierent parameters including number of negative samples and dimensionality of representation. In terms of number of negative samples, we set it to 1 and 800 to align with the original experimental settings of NGCF (one negative sample), LightGCN (one negative sample) and UltraGCN (800 negative samples). Note that UltraGCNmeans the UltraGCN model with using one negative sample in the training phase. For the dimensionality of representation setting, we set the dimensionality of learned user/item representations to 64 for all the models. Specically, we nd that NGCF model concatenates the output embedding of each graph convolution layer including input layer to construct the nal users/items’ representations, which are then combined with the CF mechanism for recommendations. Taking NGCF model with three layers as an example, its dimensionality of nal output representations is equal to 256 (3∗64+64), which may be unfair to other models. So, we design MLP+MF model which just replaces all graph convolution layer of NGCF model with an MLP layer. And we also show results of MF model with 64- and 256-dimensional representations. In this paper, we implement a simple well-tuned MF model as a important baseline. And we introduce a naive evaluation metric GRMF-X for verifying the superiority of GNN-based recommendation models eectively. The results for recommendations on two datasets in terms of the BCE loss and BPR loss are reported in Table 4 and 3, respectively. Note that the experimental results are preliminary and will be updated continuously. In addition, compared with results of UltraGCNin table 4, results of LightGCN with a same negative sampling startegy in table 3 shows better performances. Interestingly, compared with results of NGCF in table 3 and table 4, MF model (64- and 256-dimensional representations) and MLP+MF model achieve better results on two datasets in most cases. In this paper, we present GRecX, an open-source TensorFlow framework for benchmarking GNN-based recommendation models in an ecient and unied way. GRecX consists of core libraries for building GNN-based recommendation benchmarks, as well as the implementations of popuplar GNN-based recommendation models. With GRecX, we can eciently perform fair comparison between dierent GNN-based recommendation models in a unied benchmark. In the future, we will integrate more baselines into GRecX and further improve the performance of both the core libraries and implementations.