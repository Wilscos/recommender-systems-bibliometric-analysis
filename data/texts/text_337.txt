We propose the use of probabilistic programming techniques to tackle the malicious user identiﬁcation problem in a recommendation algorithm. Probabilistic programming provides numerous advantages over other techniques, including but not limited to providing a disentangled representation of how malicious users acted under a structured model, as well as allowing for the quantiﬁcation of damage caused by malicious users. We show experiments in malicious user identiﬁcation using a model of regular and malicious users interacting with a simple recommendation algorithm, and provide a novel simulation-based measure for quantifying the effects of a user or group of users on its dynamics. In 1993 a famous New Yorker cartoon of a computerbrowsing canine dryly proclaimed, “on the Internet nobody knows you’re a dog.” With hindsight this ur-meme has proven prescient with respect to the problem of authenticity on the Internet. That any one Internet user can have identities that are both multitudinous and mutable formed an important part of the network’s promise as a medium for communication, self-expression and empowerment. But ﬂexible identities also carry with them the risk of deception, with Internet-facilitated fraud coming to cast a dark shadow over the luminous future originally envisaged by techno-optimists (Friedman & Resnick, 2001). Stakes increase dramatically when the problem of authenticity meets the power of ranking algorithms, which are responsible for fulﬁlling and deﬁning information retrieval needs. Tricking an algorithm into honoring at face value those features coming from a certain set of (malicious) users Department of Engineering Science, University of Oxford scie.nz, work done while at FacebookFacebook Inc.. Correspondence to: Andrew Gambardella <gambs@robots.ox.ac.uk>. ICML workshop on Socially Responsible Machine Learning,38 International Conference on Machine Learning, 2021. Copyright 2021 by the author(s). can result in disaster, with unsuspecting users being recommended content, the consumption of which serves purely to enrich a set of attackers rather than to fulﬁll users’ information needs. Ideally speaking, a good recommendations system should be able to identify and remove malicious users before they can disrupt the ranking system by a signiﬁcant margin. However, to eliminate the risk of false positives a resilient ranking system can use as much data as possible. So we have to adjust the tradeoff between false positives and the damage a set of malicious users can cause to a ranking system. Bearing these limitations in mind, as a ﬁrst approximation, it seems reasonable, for the sake of greater analytical clarity, to divide the user base of an online social network into the vast majority of organic users and a minority of attack proﬁles. Seen in this way, discussions of inauthentic ampliﬁcation and coordinated inauthentic behavior ﬁt in with earlier analyses of “shilling” in recommender systems (Si & Li, 2020). Here, we study a popular class of shilling attacks known as proﬁle injection attacks (Williams et al., 2007), in which attackers add bogus accounts to a recommendation system, and attempt to push the ratings of a certain subset of products upward and others downward, while obfuscating their intentions (Ricci et al., 2015). In our case we are interested in asking the deceptively simple question, how would ranking outcomes differ in the absence of malicious users. We deﬁne malicious users here as those users misrepresenting either their motives or their identity for strategic gain involving the promotion or demotion of units of content. The question is difﬁcult to answer because of the multitudinous feedback loops potentially at work in recommender systems, which mean that simply counting the effects directly attributable to known malicious users is insufﬁcient for giving an accurate picture of ranking outcomes in the putative counterfactual universe in which no malicious users existed. Probabilistic programming (van de Meent et al., 2018) has emerged as a principled means of dealing with complex causal scenarios not unlike the issue discussed here, being used in domains as diverse as lion behavior interpretation (Dhir et al., 2017), spacecraft trajectories (Acciarini et al., 2020) and high-energy physics (Baydin et al., 2019a;b). It is our contention that probabilistic programming, and simulation-based inference (Cranmer et al., 2020) in general, can be used credibly to estimate the difference between realized outcomes and the counter-factual scenario which excludes malicious behavior. We support our assertion by providing: 1.A proof-of-concept detection algorithm, validating that malicious user identiﬁcation using simulation-based inference techniques is possible using data from the model. This is a necessary but insufﬁcient condition for the computation of a counterfactual scenario. 2.A simulation-based counterfactual measure of inﬂuence in a ranking algorithm, grounded in an information theoretical view of the joint probability distributions of ranking outcomes in the presence, and absence, of malicious users. 3.An illustration of how the measure could be applied – we show that malicious users acting in coordination have greater impact on social network dynamics than those acting independently. The misuse of recommendation algorithms for adversarial gain dates back to the Internet’s ﬁrst growth spurt as a communication platform, and is inherently intertwined with the history of digital spam, deﬁned as: “the attempt to abuse of, or manipulate, a technosocial system by producing and injecting unsolicited, and/or undesired content aimed at steering the behavior of humans or the system itself, at the direct or indirect, immediate or long-term advantage of the spammer(s).” (Ferrara, 2019) Recommendation algorithms have been a key vector for the ampliﬁcation of spam since the 1990s, when automated – rather than curated – information retrieval ﬁrst became feasible in a consumer setting, thanks to Page et al.’s (1999) nowfamous development of the PageRank algorithm. Compared to earlier proposals, PageRank notably provided a mechanism which enforced algorithmic resiliency – a recursive deﬁnition of popularity which protected against simplistic attempts at faking site popularity for monetary gain through the construction of hyperlinking rings (“spamdexing”). PageRank became the algorithmic foundation for Google, the dominant search engine of the past two decades. Nonetheless, in what would become a common pattern in the Internet industry, its original mechanisms proved only partially effective against adaptive adversaries. The rise of ranking also gave birth to an entire industry, search engine optimization (SEO), dedicated to improving results against the ranking algorithm, sometimes using adversarial “black hat” methods (Malaga, 2010). This evolution, in turn, led to subsequent changes to Google’s algorithms to improve their resiliency against adversaries (McCullagh, 2011). Adversarial attacks against recommendation algorithms have become increasingly prominent recently, given the importance of social media in shaping contentious news cycles rife with misinformation, in particular during the course of events such as the 2016 U.S. general elections (Allcott & Gentzkow, 2017), or the 2018 Brazilian general elections (Machado et al., 2019). Automated posting and engagement, via “social bots” has been recognized as a particularly important factor in the spread of disinformation on social media (Ferrara et al., 2016; Arnaudo, 2017; Shao et al., 2017; Cresci, 2020). The issue of automation is intertwined with that of inauthenticity, with “coordinated inauthentic behavior” (CIB) emerging as a distinct concept both among academics (Giglietto et al., 2020) and industry practitioners (Weedon et al., 2017). The concept of CIB relies on the existence of “inauthentic accounts,” distinguishable from authentic users. The notion of authenticity carries with it a great deal of complexity on the Internet. Our analysis as a result is scoped to those platforms (such as Facebook or Twitter) which rely on the explicit expectation of online identities consistent with ofﬂine personas. Even CIB itself should not be seen simply through the binary view of malicious attackers and organic users, as attackers may act to catalyze existing grievances and ideologies present among audiences ripe for manipulation (Starbird et al., 2019). To ﬁght vote spam in user–item interactions, Bian et al. (2008) proposed training ranking models using a method based on simulated voting spam at training time. Alternatively, Bhattacharjee & Goel (2007) have proposed creating incentives for power users (“connoisseurs”) to counteract the inﬂuence of spammers. More recently, Basat et al. (2017) have proposed introducing noise into the ranking function to account for distorted incentives leading to the production of low-quality content (e.g., through link farming). Our examination is meant to provide a minimal example of a recommender system. We choose movie ranking as our setting, given the canonical nature of the task, e.g., IMDbdata having a long history of use in the study of recommender systems. This is admittedly a “toy” model, which does not account for more complex designs (i.e., personalization), or for the many issues that intervene in the deployment of rechttps://www.imdb.com/ ommender systems in the real world (model update cycles, A/B testing, site outages, etc.). Formulating and studying such a model allows us to focus on the derivation of core concepts such as the inﬂuence metric (Section 4.2) in the framework of probabilistic programming. We created a model that represents several malicious users attempting to game a recommendation algorithm modeled as a simple ranking algorithm (without loss of generality, users rating movies and being recommended new movies to watch based on the current mean rating), provided in the supplementary material. In this model, multiple users (some malicious and some organic) are rating items, which are then ranked and suggested to other users based on their ranking. User tastes are modeled by real-valued variablesν∈ [0, 1], which determine which movies they would naturally like. Similarly, movies have taste featuresµ∈ [0, 1]which denote something akin to their genre and in our model are left ﬁxed. We model the rating functionRate(υ, µ)so that useriwill rate moviejhigher the closer user taste νis to movie tasteµ. The resulting ratingsρin each user–movie pair constitute the elements of the global rating matrix R. In this model the main latent variables we would like to infer are the binary variablesβ, denoting whether a given useriis malicious or not, andτ, denoting the target movie which useriwould like to boost, if useriis malicious, i.e., ifβ= 1. Probabilistic programming will allow us to condition this model on a given rating matrix (for instance, one that represents real-world movie ratings), and then ﬁnd empirical distributions over the latent variables in the simulator (µ, ν, β, τ) consistent with the given rating matrixR. In summary, for the purposes of identifying malicious users and what they are trying to boost, we will obtain the posterior distributionp(β, τ |R), while leavingµandνas nuisance variables. We implemented our model in PyProb (Baydin et al., 2019b), a lightweight probabilistic programming library for stochastic simulators. We obtain our posteriors using weighted importance sampling (Kitagawa, 1996) which gives a posterior in the form of weighted traces drawn from a proposal distribution, which is in our case the unmodiﬁed stochastic simulator. As our posterior is given to us in the form of simulations conditioned on observed data, it is by nature completely disentangled and interpretable, and will tell us the goals of each of the malicious users (τ) in addition to their identities (β). Crucially, this Bayesian approach also gives us principled uncertainty estimates associated with all our predictions. We found that obtaining a posterior over the identities of malicious users, as well as their targets, was non-trivial, with different inference engine families behaving considerably differently. Markov-chain Monte Carlo (MCMC) (Metropolis et al., 1953; Hastings, 1970; Wingate et al., 2011), despite being the gold standard to converge to the correct posterior given enough samples, performed extremely poorly. We suspect that this is due to the variables of interest (the malicious users and their targets) making up only a small portion of the latent variables in the model, as well as being discrete whereas the others are continuous. We observed that the model as it is currently formulated was not a good ﬁt for the single-site MCMC (Wingate et al., 2011; van de Meent et al., 2018) inference engine implemented in PyProb, mainly because generating proposals where a single maliciousness latentβis ﬂipped lead to very low acceptance probabilities due to the very abrupt nature of the resulting change in the rating matrix (which needs to be compensated by corresponding changes in some user taste latentsυthat cannot be achieved in a single-site MCMC algorithm), leading to slow mixing and poor sample efﬁciency. We found that weighted importance sampling (IS) (Kitagawa, 1996) performed better in practice than MCMC, and used it to obtain posteriors conditioned on observed ratings matrices. When using IS with 100,000 executions of our model in which malicious users do not attempt to disguise their activities (i.e., difﬁcultyα = 0), the mean of rating matrices in the posterior, i.e., the posterior predictive p(R|R), appears to be a noisy version of the observed ground truth rating matrixR, showing that the inference scheme sampled a posterior distribution over simulation runs in which the observed rating matrix is likely. Much more interesting are scenarios in which malicious users attempt to disguise their activities through obfuscated attacks. We model this obfuscation by setting the difﬁculty hyperparameterα = 0.3. This obfuscation introduces a signiﬁcant amount of uncertainty into our results, which is reﬂected in the detection of malicious users. Whereas in the unambiguous case IS produced an empirical posterior over malicious users and malicious target that matched with the ground truth nearly exactly, in the ambiguous case we see signiﬁcant uncertainty in the empirical posterior. The results show that the most probable (0.85) explanation of the observed rating matrix is that there are no malicious users, although we also see non-negligible (0.15) probability attached to the scenario where there is one malicious user (which we know to be the ground truth for the observed rating matrix). We also see that the mode of the posterior distribution for the malicious users matches the ground truth Figure 1.Measuring the effect on the rating matrix as the standard deviation of the distribution from which we drawτis increased. Lower values of standard deviation (i.e., more coordination between malicious actors) results in a higher impact to the dynamics of the ratings in the matrix. Results averaged over 10 different seeds, with one standard deviation bounds shown. (user 4), albeit with much lower probability (0.15) compared with the unambiguous case (1.0). We also see that while there is probability mass associated with the ground truth value of the malicious target, this probability is quite low, showing that the IS inference scheme has signiﬁcant uncertainty in the identiﬁcation of the malicious target, given the experimental setup and the number of traces (100,000) that we ran during inference. In addition to giving disentangled and interpretable explanations of the behaviour of users interacting with a ranking algorithm, simulation-based inference also gives us the ability to measure the effects of users on the model’s dynamics. We can quantify the amount of impact that users imposed on the dynamics of an observed rating matrix by using a slightly modiﬁed model, which allows us to disarm users by nullifying the effect of their ratings. Comparing the dynamics reﬂected in the distributions both with and without disarmed users gives us a counterfactual-based method of quantifying their impact on the dynamics of the simulator. Given a set of disarmed usersγ, we ﬁnd the distance between the posterior-predictive distributionsp(R|R) andp(R|R, γ)given observed dataR, or between the prior-predictivep(R)andp(R|γ)in the generic case without an observed R. Our chosen metric for measuring the impact that a disarmed user or group of users has caused is the average JS distance (Endres & Schindelin, 2003), the square root of the symmetric JS divergence (Dagan et al., 1997), computed as the average of JS distances between the counterfactual and real probability distributions over ratings, per entry in the rating matrix. In a distribution over rating matricesp(R|·), each entry in the matrix is a probability distribution (in the empirical case, a histogram) over ratings for each user–movie pair over a large number of simulator executions. Our measure for impact is then the average JS distance for each of these histograms, between the realized and counterfactual rating matrices. The JS distance is a valid metric between probability distributions and always normalised to[0, 1], making it an attractive choice to measure distances between distributions over matrix entries. We show results using our inﬂuence measure between counterfactual and realised ratings matrices while varying the amount of coordination between malicious users in Figure 1. When malicious targets are drawn from a distribution with a low standard deviation, malicious actors act in a more coordinated fashion (as they are more likely to target the same movie), which leads to a higher average distance between the counterfactual and realized ratings distributions. As the malicious target standard deviation increases, malicious users act against each others’ interests, leading to a lower overall impact on the dynamics of the model. We have suggested the use of probabilistic programming techniques to both discover and measure the inﬂuence of malicious users interacting with a ranking algorithm. We base our choice of this method on its conceptual advantages in modeling the full universe of possibilities deriving from the complex interactions between users, content and ranking algorithms. The use of simulation-based approaches in this setting has been limited by practical concerns, until recent advances in numerical computation – coupled with the emergence of high-quality libraries for probabilistic programming. Probabilistic programming techniques also have some important additional advantages over other methods. They provide for interpretable explanations as to, e.g., why a user would be classiﬁed as malicious, and provide measures of conﬁdence in their predictions. Furthermore, generative modeling of the entities and processes involved in this setting allows us to make precise deﬁnitions of the core concepts and to quantify key aspects such as user inﬂuence and malicious user damage.