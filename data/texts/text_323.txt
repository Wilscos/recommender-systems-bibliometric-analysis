A central goal in deep learning is to learn compact (or sparse) representations of features at every layer of a neural network that are useful in a variety of machine learning tasks. For example, in unsupervised representation learning problems [ dimensional embeddings of input features that are capable of reconstructing the original data [ area of network pruning [ (that are known to be heavily over-parameterized [ in resource-constrained environments (e.g., over mobile devices) without compromising on their accuracy. From this uniﬁed view of representation learning and network pruning, the generic problem of “learning compact representations” has applications in several machine learning use-cases such as dimensionality reduction, graph representation learning, matrix factorization, and image classiﬁcation. These authors contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of ﬁne-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and reﬁnes weights during training in a single stage, and does not require any ﬁne-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be reﬁned during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over a diverse range of applications in representation learning and structured pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classiﬁcation. We also theoretically show that the learning objective of DAM is directly related to minimizing theLnorm of the masking layer. All of our codes and datasets are available https://github.com/jayroxis/dam-pytorch. ]. Similarly, in supervised learning problems, there is a growing body of work in the A theoretically appealing approach for learning compact representations is to introduce regularization penalties in the learning objective of deep learning that enforce θ. However, directly minimizing the possible subsets of weights in for enforcing sparsity is to use a continuous approximation of the e.g.,L-based regularization (or Lasso [ capable of pruning individual weights and thus reducing storage requirements, they do not offer any direct gains in inference speed since the number of features generated at the hidden layers can still be large even though the network connectivity is sparse [4]. Instead, we are interested in the area of structured network pruning for learning compact representations, where the sparsity is induced at the level of neurons by pruning features (or channels) instead of individual weights. While there is a growing body of work in structured network pruning [ art (SOTA) methods in this area (e.g., ChipNet [ [32]) can be described as training a learnable vector of mask parameters, with the features extracted at a hidden layer, the pruned outputs of this layer, parameters is generally enforced using different approximations of theL of “crispness” loss in ChipNet). Despite recent progress in this area, current SOTA in structured pruning suffer from two key limitations. First, since most methods do not explicitly minimize theLnorm of instabilities. In particular, most SOTA methods [ thresholding techniques to set small non-zero weights to zero leading to large drops in accuracy during the training process, as evidenced by our results in this paper. Second, once the pruning process is complete and we have converged at a compact network, most SOTA methods still need an additional step of ﬁne-tuning the network in order to achieve reasonable accuracy. This is not only resource-intensive but there is also an on-going discussion on whether and how we should ﬁne-tune [ [26] or train from scratch [33], making it difﬁcult to prefer one approach over another. At the core of these limitations is the lack of a systematic approach for structured network pruning that jointly prunes and reﬁnes weights during training in a single stage, and does not require any ﬁne-tuning upon convergence to achieve SOTA performance. Notice that in existing methods for structured pruning, allowing every neuron parameter increasing the complexity of the problem. Instead, we ask the question: “Can we leverage the intrinsic symmetry of neural networks to design a pruning mask using the least number of free parameters?” We present a simple solution to this question by proposing a new single-stage structured pruning method that learns compact representations while training and does not require ﬁne-tuning, termed DiscriminAtive Masking (DAM). The basic idea of DAM is to use a monotonically increasing gate function position) of the neuron in the layer, i.e., training (see Figure 1). At the start of training, the gate function admits non-zero values for all neurons in the layer, allowing all neurons to be unmasked (or active). As the training progresses, the gate function gradually shifts from “left” to “right” as a result of updating convergence, only a subset of neurons (on the extreme right) are active while all others are masked out. The key intuition behind DAM is to discriminatively prefer some of the neurons (on the right) to be reﬁned (or re-adapted) during the training process for capturing useful features, while gradually masking out (or pruning) neurons on the left. This preferential pruning of neurons using a very simple gate function helps in regulating the number of features transmitted to the next layer. Contributions: various applications, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classiﬁcation, achieving SOTA performance for This is somewhat analogous to how a physical dam regulates the ﬂow of water by shifting a movable gate. 30,32,49,16], the basic structure of most state-of-theg ∈ R, g = [g, g, ..., g], which when multiplied norm ofg(e.g., use of Lasso in NetSlim and use gonly increases the number of learnable (or free) parameters in the learning objective, Gfor masking every neuron in a layer that only depends on the indexj = 1, 2, ..., n(or We show that our proposed DAM approach has remarkably good performance over structured pruning. This shows the versatility of DAM in learning compact representations on diverse problems and network choices, in contrast to baseline methods for structured pruning that are only developed and tested for the problem of image classiﬁcation [ does not require ﬁne-tuning (and hence has lower training time), and does not suffer from training instabilities. We also theoretically show that the learning objective of DAM is directly related to minimizing the for enforcing in a network architecture. The simplicity and effectiveness of DAM provides unique insights into the problem of learning compact representations and its relationship with preferential treatment of neurons for pruning and reﬁning, opening novel avenues of systematic research in this rapidly growing area. Learning with Sparsity: in learned parameters [ in more efﬁcient data representations but also promote better generalizability [ between compactness and generalization has also been recently explored in the context of deep learning [56, 1], motivating our problem of learning compact representations. Unstructured Pruning: network pruning [ widely used technique in this ﬁeld is referred to as the Lottery Ticket Hypothesis (LTH) [ suggests that a certain subset of weights in a network may be initialized in such a way that they are likely “winning tickets”, and the goal of network pruning is then to simply uncover such winning tickets from the initialization of an over-parameterized network using magnitude-based pruning. Similar to our work, there exists a line of work in the area of dynamic sparse training for unstructured pruning [ However, while unstructured pruning methods can show very large gains in terms of pruning ratio, they are not directly useful for learning compact representations, since the pruned weights, even though sparse, may not be arranged in a fashion conducive to the goal of reducing the number of features. Structured Network Pruning: substructures (e.g., neurons or convolutional channels) that is directly relevant to the goal of learning compact representations. Most structured pruning SOTA methods need a three-stage process [ training, pruning and ﬁnetuning, in order to get highly compressed models. Early exploration on the Lnorm based ﬁlter pruning can be traced back to [ which uses a sparsity-inducing regularization based on development in the area of network pruning is ChipNet [ loss to the slimming can only work if the underlying architecture has batch normalization layers. It also has training instabilities due to soft-thresholding inherent to Lasso. ChipNet also iterates between softand hard-pruning, which is done after the training phase. Further, both these approaches require ﬁnetuning or retraining, which can be resource intensive. Faster pruning schemes have also been explored by previous works. Bai et al [ structured pruning algorithm utilizing ﬁlter cross distillation. The SNIP [ performing single-stage pruning. However, SNIP falls in the category of unstructured pruning. To the best of our knowledge, no existing structured pruning method has been demonstrated to achieve SOTA performance without ﬁnetuning. For example, while the method proposed in [ potentially used as a single-stage structured pruning method with the help of group sparsity, the empirical analysis of the paper only focused on unstructured pruning. Deep Representation Learning: Leveraging the power of deep neural networks, high-dimensional data can be encoded into a low-dimensional representation [ representation learning. A good representation extracts useful information by capturing the underlying explanatory factors of the observed input. The existence of noises, spurious patterns and complicated correlations among features make it a challenging problem. Previous work [ Autoencoder architecture can be used for denoising images. To disentangle correlated features, VAE Lnorm of the discriminative mask, providing a new differentiable approximation Lsparsity. Our approach is also easy to implement and can be applied to any layer 57,15,37,11,31] that gradually prunes the model to the target sparsity during training. Lregularization, and achieves SOTA performance on benchmark tasks. However, network be closer to the isotropic Gaussian generative factors. Driven by the idea of the autoencoder, the deep representation learning has been found remarkably successful in various of applications, e.g., graph representation learning [24, 7, 9, 36, 52, 39] and recommendation system [55]. However, most existing works in representation learning treat the embedding dimension as a hyperparameter, which can be crucial and difﬁcult to choose properly. Small embedding dimensions cannot sufﬁciently represent the important information in the data that leads to bad representations. On the other hand, large embedding dimensions allow some level of redundancy in the learned representations, even picking up spurious patterns that can degrade model performances. Problem Statement: F(x) = Ψ the learned features at layer the features learned at every hidden layer following manner, compact representations can then be framed as minimizing the following learning objective: whereθare the learnable parameters of the network andλis the trade-off parameter for enforcing formulation provides a uniﬁed view of both representation learning and structured network pruning, which differ in the choice of loss function (typically, reconstruction error for representation learning and cross-entropy for structured pruning), and the number of layers that are pruned (in representation learning, we only aim to prune the bottleneck layer while in structured pruning, we aim to prune all of the layers in the network, see Figure 2). Further, notice that we only enforce gate outputs g DiscriminAtive Masking (DAM) introduce the concept of an ‘ordering’ among the neurons at any layer (or preferentially) focus on pruning neurons lower in the order, while retaining and reﬁning the features learned at neurons higher in the order. Speciﬁcally, let us assign every neuron iwith an “order number,” simple gate function the neuron, µ whereα during training, while parameter optimized during training to control (or regulate) the process of masking in DAM. Figures 2(c) and (d) show examples of this gate function (in red) at two different values of a single learnable parameter the design of the gate function, in contrast to SOTA methods in structured pruning [ a different masking parameter is trained for every neuron in the network. We hypothesize this simplicity of the gate function, in conjunction with the preferential pruning property of DAM, to result in remarkable gains in learning compact representations, as evidenced later in our experiments. Further, note that while we can use any monotonically increasing function to implement our gates, we found ReLU-tanh to perform best in practice. Neuron Ordering in DAM for DAM to work. Note that the order numbers only have to be assigned during the initialization step, and do not change during the training process. Further, note that every neuron is initialized with weights that are independent and identically distributed (because of the i.i.d nature of commonly Note the parameter set θ includes the learnable parameter of DAM layers β. β-VAE [19] encourages the posterior distribution over the generative factorsq(z|x)to Ψ...ΨΨx, wherexare the input features to the network andh= Ψ(h)are , while the hidden features hare allowed to be non-sparse. , as follows: g= ReLU [tanh (α(µ+ β))] = max [tanh (α(µ+ β is a constant scalar parameter (termed as the steepness parameter) that is not optimized Figure 2: Illustration of the problems of representation learning (a) and structured network pruning (b). Blue blocks are layers in neural networks while red blocks show non-zero values in the mask layers. Figure (c) and (d) show two states of the gate function for prunes 20% of the neurons while β=-6 in (d) prunes 60% of the neurons. used initialization methods in deep learning [ to each other at initialization and any neuron has an equally good chance of reﬁning themselves to capture useful features later in the training process as any other neuron. Hence, any random ordering of the neurons in the DAM approach would result in a similar pruning process (we empirically demonstrate the permutation invariance of choice of nis the total number of neurons at layer i. How Does Reducing β our gate function β. Figure 2 (c) shows an example of number of neurons have zero gate values (white). As we reduce that the gate moves towards the ’right’, resulting in more excessive pruning of neurons. In general, the number of non-zero values of the gate function (and hence, its L as follows: where1 gradients, we use a continuous approximation of Equation (3) (by dropping the ceiling operator) for regularization as to minimizing β Learning Objective of DAM: explicitly minimize number of neurons proportional to layers with large directly minimize the sum of β Parameter Speciﬁcation and Implementation Details: andk = 5 theβ’s were frozen for the duration of cold-start), so as to allow the leftmost neurons to undergo some epochs of reﬁnement before beginning the pruning process. We also set the initial value of 1, which can also be thought of as another form of cold-starting (since pruning of a layer only starts when β Problem Setup: generated dimensionality reduction problems. The general form of the problem is expressed by (5). µ= kj/n, wherekis a constant parameter that determines the domain size ofµ, and g, let us understand howgbehaves as we change the only learnable parameter, k=1(g> 0) =(1 − 1(µ≤ β)) = dn(1 + β/k)e, for β indicates the identity operator andd·eis the ceiling operator. In order to back-propagate kgk≈ n(1 + β/k). Hence, minimizingLnorm ofgis directly proportional with a scaling factor of n/k. . In our structured network pruning experiments, we used a cold-start of 20 epochs (i.e., becomes less than zero). We evaluate the effectiveness of DAM in recovering the embeddings of synthetically Suppose Ω ∈ R Xcan be expressed using a transformation d > r. Let G = diag(g), we formulated the dimensionality reduction problem as: TheF, G, F a isotropic normal distribution, i.e., Fwith randomly generated parameters dimensions needed in the encoded representation for the decoder to reconstruct X is r. Linear DR: We test DAM for removing linear correlations in a given rank-deﬁcient matrix. In this case,Ψis a matrix representing a linear projection from encoder and decoder and train the DAM to ﬁnd the full-rank representation of X. Nonlinear DR: To test the ability of DAM for disentangling nonlinearly correlated dimensions, we present two cases: (i) is a nonlinear transformation expressed by a neural network. We use a deep neural network as the encoder with sufﬁcient complexity. Figure 3: Bottleneck dimension (of nonzero entries) vs number of epochs during training. Observation: different synthetic data with varying sizes of underlying factors ( mean and standard deviation of ﬁve different runs. We can see that DAM consistently uncovers the exact dimension of the underlying factors DAM results based on its hyper-parameters (e.g., learning rate and λ) in Appendix C. Theoretical Analysis of DAM for linear DR: optimal solution for the linear DR case (see Appendix B for details). Problem Setup: (IGMC [55 the ratings are interpreted as links between users and items. In particular, IGMC generates 128-dimensional embeddings to represent the enclosing subgraph and further avoids overﬁtting using a dropout layer with a dropout rate of 0.5. We replaced the dropout layer with a DAM layer in the IGMC to reduce the dimension of the learned representations. We train our IGMC-DAM model for 100 epochs under the same training conﬁgurations. and all of its r dimensions are independent from each other. A d-dimensional data are trained end-to-end using gradient descent. In our experiments,Ωis sampled from Figure 3 shows the convergence of bottleneck dimensions over the training epochs for We consider the state-of-the-art method for recommendation system problems ]), which transforms the rating matrix completion task to a link prediction problem, where Table 1: Results of IGMC with and w.o. DAM on recommendation system tasks. Observation: tations without any increase in errors, demonstrating that DAM is able to boost the performance of IGMC by learning compact representations over the target graph. Problem Setup: learning compact graph representations. Our goal is to learn low-dimensional embeddings for each node in a graph that captures the structure of interaction among nodes. A simple graph autoencoder, e.g., GAE [ as a decoder. The encoder calculates the embedding matrix the adjacency matrix withZ = GCN(X, A) model. To reduce the dimension of the learned representation, we add a DAM layer in a GAE after the encoder (GAE-DAM). Figure 4: Link prediction performance for Cora, CiteSeer and PubMed Dataset. Competing models are GAE, GAE-DAM. Observation: information in compact latent embeddings. DAM improves the link prediction performance of the simple GAE for Cora, CiteSeer and PubMed dataset by learning compact representations. Problem Setup: auto-encoders on the MNIST dataset, and also compare the effectiveness of DAM (that directly enforces to Lasso). We vary the trade-off parameter between DAM and L Observation: higherF 1 see that DAM shows a near-linear descending trend of bottleneck dimensions as evidence presented in Appendix E1). In contrast, the λvalues since the weights come close to zero but require some thresholding to be pruned. This shows that DAM is amenable to learning highly compact representations in contrast to L Table 1 shows that DAM successfully reduces the dimensions of the learned represen- Following the previous experiment, we further explore the effectiveness of DAM in 24], uses a graph convolutional network (GCN) [23] as an encoder and an inner product Figure 4 shows that DAM based GAE method is able to learn meaningful structural We further evaluate DAM on the problem of dimensionality reduction using simple Lsparsity) with an ablation of our approach that instead minimizes theLnorm (similar Figure 5 shows that DAM is able to achieve lower reconstruction error as well as scores over the same bottleneck dimensions thanLbased method. In Figure 5 (c), we can Evaluation Setup pruning. We compared the performance of our proposed DAM with two SOTA structured pruning algorithms, Network Slimming (Net-Slim) [ that is widely used as a strong baseline, while ChipNet is the latest SOTA representing a very recent development in the ﬁeld. ChipNet and Net-Slim are both pretrained for 200 epochs, while the DAM performs single-stage training and pruning in the same number of epochs. Since, our DAM approach does not require additional ﬁne-tuning, we imposed a limited budget of 50 epochs for ﬁne-tuning the SOTA methods. Note that ChipNet also has a pruning stage which involves 20 additional epochs. We evaluate the performance of these methods on the PreResnet-164 architecture on benchmark computer vision datasets, CIFAR-10 and CIFAR-100. Additional evaluation setup details are in Appendix E2. Performance and Running time comparison different network pruning methods for various pruning ratios. We observe that for both datasets, DAM is able to outperform Net-Slim by a signiﬁcant margin specially when the models are sparse. Additionally, DAM is able to achieve similar or slightly better performance than ChipNet with no additional ﬁne-tuning. We also compare the total running time for pruning across the different models (divided into the three categories) in Figure 6c. We can see that the training time of the three methods are almost the same, with ChipNet being slightly small. However, the running time for the pruning and ﬁne-tuning stages for DAM are both zero. Net-Slim also does not involve additional pruning after training. However, ChipNet involves 20 epochs of pruning which is signiﬁcant and is almost comparable to its pretraining time. Finally, comparing the total running time taken by each of the structured pruning methods, we observe that DAM is signiﬁcantly faster than the current SOTA counterparts owing to its single-stage pruning approach. Figure 6: Performance and running time comparison of DAM with state-of-the-art structured pruning methods on PreResNet-164 for various parameter pruning ratios. Stability Analysis pruning through visualization of the training dynamics. We observe that the training cross-entropy (CE) loss and the validation CE loss are very similar to what we expect from a Preresnet model trained on benchmark vision datasets with learning rate changes at 100 and 150 epochs, respectively. We further notice that the total training loss for the DAM is also very stable and does not show any sudden variations, leading to steady convergence. Finally, from the convergence plot for (green-dashed line), we can see that our pruning does not involve any sudden steps and happens in a continuous and steady rate throughout the training stage. The training dynamics of the ChipNet (blue shaded region in the middle panel) is exactly the same as training vanilla vision models since it does not involve sparsity constraints. However, as we move to the pruning stage, we notice a sharp rise in both training and validation losses (white region). This is due to the use of a different optimizer AdamW as opposed to SGD which was used in the training stage. Further, a very interesting phenomenon happens in the ﬁne-tuning stage (red shaded region) where the training and validation losses increases for the ﬁrst 25 epochs and then takes a sharp decrease once the learning rate is adjusted. This suggests that after the pruning stage, ﬁnetuning the model with large learning rate forces the model to make large updates in the initial epochs (akin to training from scratch), which further drives the model out of the local minima. Once the learning rate is adjusted, the model is able to slowly progress towards a new local optima and ultimately converges to the ﬁne-tuned minima. Finally, both the training and ﬁne-tuning dynamics of the Net-Slim (rightmost panel) appears to be highly unstable. This suggests that the pre-training with sparsity loss is not stable and can lead to large ﬂuctuations in the training loss. Also, note that Net-Slim does not have a validation set as it instead uses the test set to choose the best model. We have refrained from using the best model by looking at the performance on the test set, and instead evaluate the model from the last epoch. Figure 7: Stability analysis for DAM (left), ChipNet (middle) and Network Slimming (right) through visualization of the loss curves. Blue, white and red shades denote the training, pruning and ﬁnetuning stages respectively. Permutation Invariance DAM, Table 2 provides results across ﬁve randomly permuted values of in the network, while using the same initialization as previous experiments. We can see that DAM is invariant to permutations of neuron order during initialization, validating our simple choice of assigning neuron ordering based on their index values. Table 2: Permutation Invariance results on PreResNet-164 on CIFAR-10. RSD denotes the relative standard deviation. Additional Results: for more networks such as VGG-19, PreResnet-20, PreResNet-56 and PreResnet-110 on CIFAR datasets and LeNet-5 on MNIST in Appendix E3. To demonstrate decorrelated kernels obtained after pruning using DAMs we use CKA similarity [ extreme values of architectures by visualizing the pruned architectures in Appendix E4. The effectiveness of DAM in learning compact representations provides a novel perspective into structured network pruning and raises several open questions about its relationships with other concepts in this area. In the following, we list some of these connections and describe the limitations and future extensions of our work. Connections with Lottery Ticket Hypothesis masking, i.e., we preferentially favor some neurons to be reﬁned while favoring some other neurons to be pruned. This has interesting connections with the lottery ticket hypothesis (LTH) [ subsets of initialized weights in the network are hypothesized to contain “winning tickets” that are uncovered after a few epochs of training. While there is some open discussion on whether the winning tickets exist even before initialization [ [43,14]. Our results attempts to throw light on the question: “can we ﬁnd winning tickets if we discriminatively search for it in a certain subregion of the network?” Further, we are able to show that the results of DAM are invariant to random permutation of the neuron indices at initialization, since all neurons receive identically distributed weights. Along these lines, we can also explore if Parameters Pruned 64.01% 63.40% 63.97% 63.64% 63.37% 0.0048 λ, we demonstrate that DAM is able to pruning entire bottleneck layers for ResNet there are certain ordering of neurons (e.g., in accordance with their likelihood of containing winning tickets revealed through LTH) that can perform better than random ordering in DAM. Connections with Dropout: ization technique of Dropout [ “co-adaptation” patterns in the learned features so as to avoid overﬁtting. Essentially, by randomly dropping a neuron at some epoch of training, the other neurons are forced to pick up the learned features of the dropped neuron, thus resulting in the learning of robust features. This is similar to the “re-adaptation” of weights at the active neurons at some epoch of DAM training, while the mask value of the neurons in the transitioning zone (when the gate function. This motivates us to postulate the weight re-adaptation hypothesis as a potential reason behind the effectiveness of DAM in learning compact representations, which requires further theoretical justiﬁcations. Budget-aware Variants of DAM would be to make it budget-aware, i.e., to stop the training process once we arrive at a target level of sparsity. Note that there is a direct correspondence between the value of the learnable gate offset parameter a simple budget-aware extension of our current DAM formulation would be to start from a large value of lambda (to provide sufﬁcient scope for aggressive pruning) and keep monitoring the level of sparsity at every layer during the training process. As soon as the target sparsity mark is achieved, we can freeze architecture becomes immutable if β DAM Variants for Pre-trained Networks: DAM formulation is that all neurons are symmetrically invariant to each at initialization, and hence we can simply use random neuron ordering for discriminative masking. While this assumption is valid for “training from scratch”, this may not hold for neurons in a pre-trained network. Future extensions of DAM can include advanced ways of ordering neurons that do not rely on the above assumption and hence can even be used with pre-trained networks. For example, we can order neurons based on the magnitudes of weights of every neuron or the value of mutual information between neuron activations and outputs. This would open novel avenues of research in structured network pruning by ﬁnding effective neuron orderings for different initializations of network weights. β(that is minimized at every epoch) and the resultingLsparsity (see Equation 3). Hence, β’s at every layer thus essentially halting the pruning process (note that the network