Higher-order thinking has become one of the essential skills for the 21century. The best way to develop and strengthen these abilities is through practical hands-on courses [1, 2]. One commonly used learning method for training problem-solving or various IT skills (e.g., programming) is puzzle-based learning. Michalewicz et al. [3] introduced a game-based learning method that uses puzzles as a metaphor for getting students to think about how to frame and solve unstructured problems. In IT education, the puzzle-based learning approach has been prevalent for many years [4, 5, 6]. Even programming courses consist of basic concepts such as recursion with assignments like “Write a program to calculate the factorial of a given number.” Multiple studies conﬁrmed the usefulness of puzzle-based learning also for cybersecurity education [7, 8, 9]. However, while hands-on training produces a tangible output in many learning areas, e.g., a code that can be checked, analyzed, and evaluated, cybersecurity training is process-oriented. Puzzles are tasks like “search for a vulnerability on server X” that are diﬃcult to track. Tutors have only a limited view of what trainees are doing in the computer network and how they deal with the task, making the post-training evaluation challenging. This paper presents results of cooperation with cybersecurity education experts that led to the design of a visualization tool supporting the follow-up learning analysis of the training sessions. Regardless of the education subject, tutors make intensive eﬀorts to create, organize, and continually improve these socalled blended courses. Trainees’ assessment, which usually follows the training session, is integral to the teaching process. The focus lies on comparing individual trainees and analyzing their progress or discovering weaknesses in the training design. We contribute to the state of the art of applying visualizations in education practice with: (a) a user requirement deﬁnition on support tools for tutors of the hands-on puzzle-based learning activities (in the cybersecurity education context); (b) design and implementation of the visualization tool for the post hoc analysis of data from the training session; and (c) an evaluation with domain experts resulting in design recommendations for future work. Assessing the eﬀectiveness of game-based learning poses a signiﬁcant challenge in the learning analytics research domain. Loh [10] distinguishes between ”assessment for learning” and ”assessment of learning.” The former is designed to assess a learner’s understanding at the course end. The latter is more helpful to educators because it helps them to improve the learning processes. This paper deals with educators’ insight into the learning process. A considerable eﬀort has been made in the past to conceptualize data mining and digital assessment for serious games so that generic learning analytics principles can be researched and applied regardless of the speciﬁc game content [11, 12, 13]. Our solution deals with event logs and the score-based assessment that represent broadly accepted types of telemetry and evaluation data for serious games. Our work lies at the intersection of education, visualization, and HCI research. According to the classiﬁcation provided in [14], this paper addresses visual data analysis tasks of organizing participants (referred to as tutors). Using information technologies in blended courses enables us to collect metadata produced by learners. Tutors can use them for a post hoc analysis of learners’ progression and content revision. Nevertheless, the design and deployment of eﬃcient support tools remain a challenging problem [15]. There are general tools that could be used for speciﬁc post-training tasks, e.g., comparing scorebased assessment settings via the LineUp application [16]. Our tool aims to reﬂect the well-deﬁned requirements of training designers and tutors, providing them with a domain-speciﬁc comprehensible analytical dashboard. The purpose of the post-training learning analysis is to understand and optimize learning processes. Previous works [17, 18, 19, 20, 21] address using visual dashboards for learning analysis and conﬁrm the need for insight exceeding simple summative feedback [22]. Apart from focusing on the learning process, learning analytics in higher education also provide valuable teaching or research resources [23]. Analytical tools can support decision-making and improve pedagogical approaches. Most of these learning analysis tools focus on the high-level perspective evaluation of students’ performance. Existing surveys overview and analyze learning dashboards either for tutors [24, 25, 26] or students [27]. Most of them are related to the uptake of massive online open courses. These tools focus on visualizing learning activity, tracking speciﬁc learning goals, and providing a high-level perspective on learners’ progress. Moodleboard [28] is a decision support tool for pedagogical engineers and administrators providing both course statistics and detection of ﬂaws or misuses for an open-source learning management system Moodle. LISSA [29] aims at improving student-advisor dialogue during face-to-face consultations. The tool provides an overview of study progress or peer comparison among multiple students. SAM [30] is a general-purpose web-based environment visualizing learners’ activities, improving awareness, and supporting self-reﬂection. Such high-level tools represent domain-independent systems to gather, process, and report the collected and derived data while overlooking disciplinary knowledge practices. In contrast, tools for lower-level data analysis from practical courses often require considering insight from domain experts because the input data driving the analytical tools are domainspeciﬁc. Examples can be found for math [31], where the system tackles the understanding of selected math functions, programming tools [32] that utilize compilation processes and software quality metrics for assessment, or penetration testing [33] based on knowledge graphs. Figure 1 categorizes these tools in two axes: x-axis – single or multiple training sessions; y-axis – data speciﬁcity, i.e., from the domain-speciﬁc data to derived data and metadata. We propose the Training Analysis Tool (TAT) – a dashboardlike tool for tutors providing data-driven insight into a training session through several linked visualizations. The TAT supports tutors in low-level learning analytics tasks such as inspection and comparison of trainees or identifying training design ﬂaws based on the data from single training sessions. The puzzle-based learning in the cybersecurity domain is primarily represented by Capture the Flag (CTF) games [34, 35, 36]. CTF training scenarios serve as puzzle-based templates structuring the content into levels focused on solving cybersecurity tasks, e.g., scan the network, identify a server, ﬁnd the server vulnerability, exploit it, and gain the root privileges. CTF games can be organized in diverse ways. Very popular are unsupervised online games when a trainee can access the game or interrupt it anytime. Tutored (or supervised) training sessions for small groups are often practiced in a formal cybersecurity education or professional training. The supervised training sessions share the principles of blended courses popular in primary and secondary education. CTF games contain a short background story, task assignments, their evaluation, hints, and solutions for each level. A typical scenario consists of up to ten levels. Finding a level solution is necessary to proceed to the next one. Training scenarios use multiple gamiﬁcation characteristics such as scoring, level-based approach, or scoreboards. Trainees are penalized when taking hints or solutions and reach score points for successful solutions. Hands-on cybersecurity training is often organized in socalled cyber ranges. The KYPO Cyber Range Platform(hereafter referred to as KYPO CRP)that we use for development and evaluation is a cloud-based environment providing features for the virtualization of computer systems and networks [37]. It serves as a platform for practical training of various cybersecurity skills in university courses as well as for the training of practitioners from institutions outside. The KYPO Cyber Range allows us to create so-called sandboxes – isolated computer networks consisting of multiple virtual machines for several dozens of trainees (the exact number depends on the cloud capacity and resource requirements). The web portal provides a user interface for the management of sandboxes, users, training scenarios, and organizing training sessions. A typical training session is organized for 15–20 participants in the IT classroom. Trainees log in to the web portal and launch a training scenario consisting of a sequence of cybersecurity puzzles. Trainees solve the puzzles individually in their private sandboxes without aﬀecting others’ work. A successful solution of the puzzle yields a short string (called ﬂag). Entering the ﬂag in the web portal opens the next level. Trainees who are struggling can use hints speciﬁc for each level. When helpless, they can see the correct solution (a list of steps leading to the ﬂag). Time for solving all the levels is usually limited to the class length (one or two hours). Tutors walk around and help trainees either on request or when they realize that someone signiﬁcantly lacks behind (typically by quick peek on their displays or asking them directly). In the end, the scoreboard shows individual scores, and tutors hold a short debrieﬁng to present correct solutions. Figure 2 illustrates the principal elements and actions of the whole workﬂow. There are two broad use cases for the post-training analysis: (a) a comparison of trainees and (b) training scenario improvements. The former is essential when the CTF games are part of the competitions or exams. The rank or grade is then based on the ﬁnal score and time. However, the tutor cannot understand the subtle diﬀerence in the trainee’s behavior or expose cheating. Likewise, training scenario improvements were usually based on error-prone manual processing of the logged data and anecdotal evidence from training sessions, making revisions ineﬃcient. Hands-on CTF games provide two datasets available for visual analysis: a training scenario and timestamped trainees’ events recorded during the training session. The KYPO CRP provides REST API to access these data on-demand in JSON format. The training scenario contains attributes related to the content. Namely, a background story, puzzle assignments, hints, hint penalties, solutions, solution penalties, correct ﬂags, ﬂag score points, and level time limits. These attributes do not change during the training session. However, tutors might edit them afterward based on trainees’ feedback or outcomes from training session analysis. Typical changes include ﬁxing typos and improving the clarity of puzzle assignments, or adjusting level duration estimate, score, and penalty points. The trainees’ events are automatically collected when trainees interact with the web portal. Example events are: training started, training ended, level started, level ended, correct ﬂag entered, incorrect ﬂag entered, hint taken, solution taken. Each event contains a standard set of attributes (timestamp, event type, training description ID, training session ID, user ID). Three event types (an incorrect ﬂag entered, a hint used, a solution displayed) contain speciﬁc attributes – an incorrect ﬂag string and penalty points. Although the input data is domain-speciﬁc, we can ﬁnd similarities also in other forms of puzzle-based gaming. Data types are either integers (score and penalty points, level duration estimate – representing minutes) or text strings (plain-text for ﬂags, markdown markup for all the rest). We closely collaborate with domain experts (cybersecurity educators) from our university who represent target users. They provided initial requirements, gave us feedback on proposed designs, and participated in both evaluations. Our goal was to improve the workﬂow of tutors and organizers of hands-on cybersecurity training sessions through the design and deployment of the Training Analysis Tool that processes data from the KYPO Cyber Range. In this project, we applied the user-centered approach guided by the design study methodology framework [38], reﬂecting its core stages: discover, design, implement, deploy. Our iterative process has four phases. Each phase reﬂects one or more of these stages: Problem characterization (discover): We conducted semistructured interviews with three domain experts from the university cybersecurity team. All of them partake in educational activities as seminar tutors or lecturers, and they also participated later on in the evaluation. Each interview lasted about an hour. We also did four ﬁeld observations during training sessions to gather user requirements and complement our notes, each lasting up to two hours. From these data, we elicited functional requirements and design decisions for both tools. Early prototype and formative evaluation (design, implement, deploy): We created the early prototype and performed a qualitative formative evaluation with ﬁve collaborating cybersecurity educators and one student familiar with the CTF games. Late prototype and summative evaluation (design, implement, deploy): We added new features and redesigned the user interface based on received feedback. A qualitative summative evaluation with eight participants served us for the validation of the ﬁnal designs. Final deployment (implement, deploy): The last phase includes the integration of TAT into the KYPO CRP. We also plan to collect further feedback from its routine usage. Unfortunately, due to the COVID-19 pandemic, the number of training sessions has been severely limited. Post-training session evaluation provides many opportunities for tutors to perform a detailed analysis of a training scenario and assessment of the trainees. The interviews and ﬁeld observations revealed that tutors struggle with analyzing the training data from the individual sessions. They expressed the need for an overview of the data collected during the training session, which enables them to: analyze trainees’ behavior, compare their performance, and revise the content and conﬁguration of the training scenario. We organized the requirements into the four main categories: R1 – Trainee behavior analysis: Tutors should examine trainees’ behavior and identify outliers – e.g., those who are extremely slow/fast or gave up the training. They should assess the trainees by comparing their results (e.g., ﬁnal time and score, taken hints, number of entered incorrect ﬂags). It is also relevant when the training session is a part of some competition. Further, reviewing the trainees’ actions, such as many partially correct ﬂags submitted by several trainees, can point out ﬂaws in the puzzle assignment. R2 – Assessment revision: Correctly set scores and penalties are crucial for the gameplay and trainees’ motivation to complete the training. Setting the penalties for hints too small, for instance, can demotivate trainees in attempting to ﬁnd the solution by themselves. Instead, they could take all hints immediately, which would even result in a better ﬁnal score. Therefore, the tutors should be able to review the assessment criteria of the training session. R3 – Timing revision: Proper estimation of time requirements for cybersecurity puzzles is tricky. Short time allocated for a challenging puzzle can delay the whole session, put unnecessary pressure on trainees to take hints early, or force tutors to intervene prematurely. During the interviews, even the most experienced tutors admitted that they do not have a proper ﬁrst estimate of mapping puzzle diﬃculty to time limits. Therefore, tutors should be able to review the time limits of the training session. R4 – Training content revision: Tutors should be able to analyze problematic parts of the training content to improve its quality iteratively. The trouble can be hidden either in individual puzzles (e.g., unclear puzzle assignment, useless hint) or their interconnection (e.g., the unbalanced diﬃculty of two successive levels). The main goal of the Training Analysis Tool (TAT) is to display data from a single training session in the context of the corresponding training scenario (e.g., puzzle assignments, scoring, timing). The tool is designed as a dashboard combining several linked views. Its design follows principles formulated by Oslejsek et al. [14]: • Analyze the impact of tutor’s supervision: The tool consists of temporal views of trainees’ actions and the score development at various levels of detail. Tutors can analyze the impact of both individual and class-wide interventions by focusing on the time of intervention. • Analyze quality of training exercise: All views display the score and time limits that form the primary assessment criteria and delimit the training session’s diﬃculty. These visual artifacts help tutors to analyze the quality of training. Moreover, predeﬁned parameters (penalties, time limits, tasks) are available in the dashboard together with runtime data, enabling tutors to reveal possible weaknesses in training scenarios by comparing expected versus actual development. • Analyze behavior analysis of trainees: The training session is captured from several perspectives: temporal view on trainees’ activities, a static preview of ﬁnal results, and detailed dynamic score development. By combining these coordinated views, tutors can interactively analyze individual trainees’ behavior, compare them mutually or concerning expected behavior, and visually identify outliers. The early prototype of the Training Analysis Tool (TAT) (Fig. 3) is a web application consisting of three interactive visualizations: time-score overview, training overview, and individual training walkthrough. The former two are based on visualizations proposed by [39] for player-centered reﬂection and CTF game results. Since their input data is similar (timestamped events), we used its core design principles and visual encoding, but our visualizations provide extended interaction capabilities. We further elaborate on the design of individual TAT components in detail. All three visualizations of the early prototype use a ﬁxed color scheme. The colors were meant to distinguish individual levels of training and were selected in diﬀerent intensities to be distinguishable for people with the most common forms of color vision deﬁciencies. Total duration and the ﬁnal score are two main factors used for measuring the performance of the trainees. The time-score overview (Fig. 3, top-left) helps identify the correlations between these factors, providing a view on the score distribution, pinpoint the outliers, or allocate clusters. Using simple standard statistical views, such as boxplots, would be inconvenient because we need to put in the context multiple metrics (average, and estimate times, ﬁnal scores). Therefore, the visualization combines bar charts with scatter plots to incorporate time and score data into a single view. The top bar shows the total time (x-axis) and each trainee’s ﬁnal score (y-axis). The smaller bars below represent individual levels (i.e., tasks). Each bar’s length expresses the maximum time for the given level (i.e., the time of the slowest trainee). The average time is on the border of two color shades. Although the scoring span can diﬀer in each level, the bars have ﬁxed heights. The vertical space is suﬃcient to display and analyze achieved score distribution regardless of the scoring span. The maximal level or game score is on the y-axis, and the exact score numbers are provided on-demand as tooltips of individual dots together with a trainee’s name. Hovering the mouse cursor over the dot highlights the corresponding results of the trainee in the remaining levels highlight and the exact time and the achieved score for the level display. A mouse click on the dot highlights the corresponding data in the training overview and displays detailed score development in the individual training path at the bottom. Dot clusters can visually indicate the correlations between time and score, which is particularly helpful when the tutor aims to identify the training design issues such as a level diﬃculty compared to its duration. The tutors can use it to analyze the results of individual trainees and put them in the context of the training group (R1) or to review score-based assessment (R2). Bar charts also help the tutors review time requirements (R3). Dot clusters may help in the identiﬁcation of problematic levels in the training scenario (R4). The training overview (Fig. 3, top-right) provides a detailed yet compact and uncluttered view of the trainees’ progressions and activities. It is based on a stacked bar chart where each row corresponds to one trainee. Segments represent training levels and encompass related game events as glyphs. A user can ﬁlter the data based on the level duration and zoom the view to unfold the aggregated events (numbered circles) performed quickly. The visualization shows the relative time of the training. The stacked bars are aligned to the left, so it is possible to compare the time requirements regardless of the delays caused by individual trainees’ various starting times (R3). Level labels above the bars support sorting by the duration of the corresponding levels. The related vertical lines indicate the expected level duration. When sorted, they also reveal the deviation of the actual and estimated time for each trainee. The glyphs indicate events. In this view, they help the tutors to recognize possible problems in the design of training deﬁnition (R4) or analyze the behavior of the trainees (R1). For example, multiple incorrect ﬂags submitted by diverse trainees can indicate unclear or ambiguous instructions; many hints taken in quick succession may suggest a lack of eﬀort caused by improper diﬃculty. The individual walkthrough (Fig. 3, bottom) is based on a step chart with glyphs representing trainees’ actions. It enables the tutors to track outliers’ behavior (R1) and explore the cause of recognized problems in the training session (R2, R4). It provides a detailed insight into a trainee’s advancement and actions or allows comparing two or three trainees selected from the training overview list or the time-score overview. The yaxis represents gained score. The horizontal dashed lines imply the maximal level score. The striped background outlines the estimated level times. A zoom function allows adjusting the view on a selected portion of the chart, which is useful when the events are clustered. On mouse hover, a tooltip shows details for each action. A context view frame below the main chart helps the tutor to get oriented in the zoomed area and shift the time range when needed. Furthermore, the checkboxes in the bottom right corner allow ﬁltering the event types. The main goal was to gain feedback on the TAT’s usefulness in four areas: • Trainees – Is it possible to identify trainees who struggled (e.g., lacking behind, stuck with the task/level)? Can tutors recognize any unusual behavior of trainees (e.g., cheating, prolonged inactivity)? • Training session – Is it possible to recognize when the training is running out of schedule? Can tutors identify scenario design issues? • Visual encoding – Is the visualization easy to understand? What type of information is redundant or missing? • Interaction – How do tutors interact with the visualization? Are the interaction capabilities suﬃcient? We further evaluated the usability and usefulness of the visualizations and gathered remarks on visualization improvements for the following design process iteration. Due to the necessary background knowledge of hands-on cybersecurity training, we conducted a qualitative user study with ﬁve domain experts (P1–P5) and one student (P6). All of them were members of the university cybersecurity team who partake in hands-on training on diﬀerent positions. Table 1 shows their demographic information. In September 2019, we held the formative evaluation sessions in person using 27” iMac with the resolution 2560×1440 and Google Chrome browser version 76. The experimenter took notes and audio recorded the participants’ opinions and thoughts. The user study had two parts, and the participants were asked to think aloud. The sessions lasted about an hour. In the ﬁrst part, the experimenter outlined the procedure. The participant consented and ﬁlled the demography questionnaire. The experimenter presented the TAT and situated the participant in the role of a tutor using the tool. Next, the participant spent 2–3 minutes familiarizing with it using dummy data followed by completing three tasks addressing requirements R1–R4: • T1: Identify an unusual behavior of trainees and name the potential issues. • T2: Find and compare a pair of trainees who: a) have the same score; b) were the best and the worst; c) were the slowest and the fastest. How do they diﬀer? • T3: Identify problems caused by the poor design of the training scenario and propose improvements. DS2. We chose the genuine data since they contain various actions observable during training sessions (e.g., guessing the correct ﬂag, prolonged inactivity, varying trainees’ performance). Their diﬀerent size, number of trainees, and duration show two distinct yet ordinary real-world circumstances. DS1 is from the tutorial on computer forensic skills and consists of six game levels. The goal is to identify and examine malicious software running in the computer system. The trainees learn how to identify a suspicious application, dissect its executable, and process memory. The session lasted 55 minutes, and 16 trainees generated 374 events, making the 23.4 events per trainee on average. DS2 is an attack-oriented training scenario that consists of four game levels with the following puzzles: exploit server vulnerability, gain the root privileges, access a protected data ﬁle, and cover the traces after the attack. Six trainees generated 146 events over 90 minutes, averaging 24.8 events per trainee. Finally, the participant ﬁlled two usability questionnaires and was debriefed. We chose the SUS – System Usability Scale [40] and the SEQ – Single Ease Question [41], two widely used questionnaires for measuring various products’ usability. The former is a widely used method for assessing the usability of the systems. The latter is considered a robust measure to quantify the usability for tasks that are too complex for metrics like task duration time or completion rateand when the number of participants is low, as in our case. 7.3. Results The formative evaluation revealed weaknesses in the early design and helped us understand tutors’ work after the training session. The most acclaimed feature of the training overview visualization is the ability to sort trainees by the time spent at some level and compare them to the estimated level duration (deﬁned in the training scenario). Participants also used the visualization to identify the trainees who signiﬁcantly exceeded the estimated level duration time. For most of the participants, the score overview visualization was a starting point when solving all the tasks. They used it to identify outlying trainees (P2, P4, P6), to assess the diﬃculty of each level based on the time/score distribution of trainees (P1, P2, P4, P6), or to compare it with the maximum score per level (P3, P4). P3 also used the score overview to assess the conceptual design of the training scenarios (the ﬁrst levels should be manageable and short compared to the ﬁnal ones). Participants lacked information about estimated level duration (P1–P4, P6). P6 wanted even more details, such as medians of time and score for each level. Participants often used score overview visualization to highlight trainees in training overview and vice versa. Score overview was also often used in T2 as a selector for trainees to compare. We did not observe any other extensive mutual use of two or all three visualizations. On the other hand, the individual training walkthrough visualization was generally considered ”useful only in a speciﬁc case when the training session is organized as a competition to decide the ﬁnal order of trainees” (P4). The main complaint (mentioned by all) was the absence of a tabular view showing various details of all trainees such as their ﬁnal score, scores per level, number of taken hints, or incorrect ﬂags. Other frequent issues were: the absence of ﬁltering features (P1–P5); a missing overview of the training scenario allowing the users to skim through the texts of tasks, set penalties, and ﬂags (P2–P4, P6); insuﬃcient integration of the visualizations (P1, P2, P4, P5); and the visual encoding (P1, P3, P4, P6) considered by P3 as ”disturbing due to many colors without proper meaning.” The SUS score was 65.4 points (out of 100). It corresponds to the good rating, according to the adjective ratings [42]. Fig. 4 summarizes the SUS questionnaire responses. With the SEQ score of 6.5 (out of 7), the TAT showed to be well-suited for training design analysis (T3: Identify training design issues.). The two tasks focused on identifying and comparing trainees scored 5 (T1: Unusual behavior of trainees) and 6 (T2: Comparison of trainees). While these results conﬁrmed the overall usability and usefulness of the TAT, we had to address the main issues raised by the study participants. We revised the ﬁnal design rationale, visual encoding, and interaction capabilities of the current version of the TAT based on the formative evaluation. The prototype, implemented using Angular and D3.js library, is available at https://tat. surge.sh. The main principles of the three visualizations remain the same. However, we signiﬁcantly redesigned the layout making the training overview the most prominent visualization. We also added more ﬁltering options for selecting individual trainees and revised the use of colors. The formative evaluation also revealed that the coloring of levels is not essential for the users, so we have changed it in the late prototype: the platform on which the training sessions take place generates a unique avatar for each trainee. Therefore, we decided to emphasize the trainees based on the avatar’s color instead. Now, each trainee has a unique color in all three visualizations. These colors are not intended as the exclusive means of trainee identiﬁcation but as complementary visual support (to accompany the ability to highlight or ﬁlter the trainees). To distinguish training levels, we used gray color shades in the late prototype. Finally, we added additional information regarding the training deﬁnition, such as the task descriptions, correct ﬂags, and contextualized trainees’ data with individual levels. Fig. 6 displays the ﬁnal layout, with the collapsed training definition summary and visualization filters sections. The TAT’s upper part (Fig. 5 – A) contains a collapsible panel with the training deﬁnition details, visualization ﬁlters, and avatar-based trainees ﬁlter. The training definition summary serves for the conﬁguration of the tool and synopsis of the training. It provides training scenario parameters (i.e., task assignments, hints, penalties, correct ﬂags). The tabs show data for individual levels (Fig. 6 – A). For each game level, a table summarizing data of individual trainees provides an overview of the gained score, taken hints, incorrect ﬂags, and time spent in the level (R2 and R3). Comparing the results shown in the table with the level content and parameters (e.g., the comparison of incorrect ﬂags with the correct ﬂag or scheduled time allocation with the average or median values) can help the tutors identify problematic parts of the content (R4). The Visualization Filters (Fig. 6 – B) are global ﬁltering options to show or hide glyphs representing hints or ﬂags and switch between trainees’ avatars and names (IDs). The avatars (Fig. 6 – C) are switches for ﬁltering out the trainees from the training overview and time-score overview. We extended the training overview (Fig. 5 – B) with the table summarizing total game duration, achieved score, number of taken hints, and submitted incorrect ﬂags for each trainee. We also added the legend for quicker orientation. The training overview interacts with two complementary views. By clicking on the stacked bar, the individual walkthrough visualization appears, showing score polyline and events of the corresponding trainee. The level bars highlight the corresponding dots in the time-score overview and the polyline in the individual walkthrough on mouseover. Unlike the early prototype version, we added the dashed vertical line to indicate the actual average completion time of the trainees. The striped segments delimit the time estimate for each level. Therefore, the tutors can quickly identify the diﬀerences between the expected and the actual (and averaged) time for each level, as shown in Fig. 7. 8.2.2. Individual walkthrough In the ﬁnal version, the individual walkthrough (Fig. 5 – D) displays upon selecting a trainee in the training overview. The selected trainees are indicated as avatars next to the title. We also reﬂected the main complaints regarding the clutteredness and simpliﬁed the visualization layout. Only the total training duration estimate is shown instead of the estimate for each level. We also added the vertical dashed line to indicate the actual average time, similarly to the time-score overview. The summative evaluation was held in April 2020. We intended to validate the ﬁnal design concerning the user requirements R1–R4, assess the usability and usefulness of the TAT, and identify possible reﬁnements for the ﬁnal integration into the KYPO CRP. 9.1. Participants We asked the same six people who participated in the formative evaluation. We also recruited two more students who passed the CTF design course taught at our university (see Table 1). They represent novice users familiar with CTF games’ basic concepts and only have hands-on experience with their design. Due to the COVID-19 pandemic restrictions, we held it remotely using Google Meet, which we also used to record audio and screen. The participants used their computers or laptops with the 13.3”–27” screens and resolutions ranging from FullHD to UHD. The procedure was almost the same as in the formative evaluation (see Sec. 7.2). The only diﬀerence was a new data set that we used for the tasks. DS3 uses data from a training session held as the introductory lecture of the CTF game design course of Fall 2019. It is an attack-oriented four-level training scenario similar to DS2, in this case, tested on nine trainees who generated 281 events in the session lasting 110 minutes. On average, each participant performed 31.3 events. The participants completed all the tasks without struggle. Despite minor diﬃculties, the immediate feedback was more positive than in the previous evaluation. Since the tasks are complex and depend on the tutor’s knowledge and experience we sought qualitative input rather than measuring user performance. Participants mostly worked with the training overview since it contains most of the necessary information. The time-score overview serves well to identify timing issues and assess level diﬃculty. The training definition summary supports ﬁnding ﬂaws in the puzzle assignments (e.g., misleading texts, wrong instructions for ﬂag format). Further, we did an inductive qualitative analysis [43] of the video recordings, which is summarized below. Visualizations usage. Figure 8 shows the usage of visualizations to solve the tasks by participants. The most preferred was the training overview. All but P5 used the training overview as a starting point when solving all the tasks (P5 preferred the time-score overview). Its most acclaimed feature is the ability to sort trainees by the time spent in individual levels and compare them to the estimated level duration (deﬁned in the training scenario). Participants also used the visualization to identify the trainees who signiﬁcantly exceeded the estimated level duration time. All the sorting options (by time spent in a level, ﬁnal time, score, hints, and incorrect ﬂags) were used at least once by each participant. On the other hand, the zooming function was used only rarely (P1, P6). The participants used the time-score overview to identify outlying trainees (P2, P4, P5), assess each level’s diﬃculty based on their time/score distribution (P1, P2, P4, P5), or compare it with the maximum score per level (P3, P4). The individual walkthrough was still considered the least usable (P1, P2, P5, P6, P7). P1 and P5 did not work with it at all. Others used it only for a direct comparison of two trainees (T2). The TAT allows comparison of trainees beyond time and score. To identify non-standard trainees’ behavior (T1), we observed that all the participants revealed all or almost all occurrences of the most common types, such as taking all hints at once shortly after they entered a new level or guessing the ﬂags in each dataset. The participants found those with the lowest score/largest time, followed by a detailed inspection of the number of taken hints and inserted incorrect ﬂags. The procedure was the same for all. The diﬀerence was only in the starting visualization. While P4, P5, and P7 started with the time-score overview, the rest used the training overview solely. The participants also intensively used the trainee ﬁlter combined with the training overview sorting capabilities to ﬁlter out unwanted trainees quickly, especially for the second task (T2). Despite the individual walkthrough received mixed reactions, most participants (except P1 and P5) used it for a head-to-head comparison. The TAT helps to identify training scenario shortcomings. When dealing with the identiﬁcation of training scenario shortcomings (T3), the participants mainly focused on three areas: correcting the time estimates and maximal score of individual levels, the perceived level diﬃculty, and instructions for a correct ﬂag format. All the participants proposed changing the time estimates or the assigned maximum of points based on the trainees’ overdue in the ﬁrst two levels of D3. Moreover, seniors (P1, P3–P5) also identiﬁed the confusion with the ﬂag formatting instructions in the second level. P3–P5 analyzed the data even more profoundly and revealed the ﬂaw in the game design based on the observation that some trainees used the correct ﬂag for the fourth level in the third one. Except for P1, P2, and P4, the participants used the training definition summary since it clearly shows the diﬀerence between the estimate and real-time. The size of each level allows for a quick comparison of their perceived diﬃculty (the longer it took, the problematic the level was). The glyphs visualizing incorrect ﬂags in the training overview proved to be good indicators for potential issues with the puzzle assignments, including the technical instructions. All the experts (P1–P5) greeted the training definition summary as a convenient way to search for problematic parts of the training deﬁnition. Gaps and drawbacks of the TAT. We received several suggestions for further improvements to the TAT visualizations. P5 suggested adding “the horizontal line also showing the average score per level” in the time-score overview to improve comparing level scores. The two-level ﬁltering (avatars → trainees in the training overview) received mixed feedback. Only three participants (P1, P2, P5) used both to ﬁlter out speciﬁc trainees, while others preferred to keep all of them visible. The evaluation also revealed that with the grayscale for the training overview, highlighting of selected trainees is not very pronounced and will be revised in future development. The main beneﬁt of the individual walkthrough is that the polyline visualizing score development better informs the tutor whether there are similarities in the trainees’ gameplay. Since this is useful only in a speciﬁc use case, we will reconsider its integration in the subsequent design iterations simplifying the user interface. The average SUS score raised to 77.5 (compared to 65.4 for the early prototype), which still equals to good rating. We assume that it is mainly due to the higher complexity of the tool and the remaining issues with the individual walkthrough. The data plot of the SUS questionnaire responses is in Fig. 10. However, the medians 6.0 of SEQ score (Fig. 9) for all the tasks (T1– T3) further supports our statement that the TAT is well-suited for the post-training analysis. In this section, we discuss the ﬁndings and limitations of the studies. The summative evaluation validated our design decisions. scores conﬁrmed that the tools address the elicited requirements. We also revealed three notable ﬁndings regarding the presentation of summaries, sorting and ﬁltering capabilities, and domain speciﬁcity. Summaries. Extending the visualization with pertinent summary data could help tutors to overview the situation and identify anomalies quickly. Especially in analytical tools, even elementary statistics and simple charts are helpful. Although we did not implement such charts in the TAT, some participants asked for them as feature requests. Sorting and ﬁltering. The evaluation revealed that we should work with the sorting and ﬁltering options even more thoughtfully so that tutors can better focus their attention. There must be a real usage scenario for each ﬁlter type. Particular attention should be paid to carefully selecting items for ﬁltering and the batch selecting and ﬁltering shortcuts (e.g., “deselect all”). Domain-speciﬁc insight over universality and scalability. Puzzle-based learning represents a vast area where tutors’ support tools diﬀer vastly among various application domains. Since there are no guidelines or best practices and the user requirements are often contradictory, they have to be considered carefully, and the tools should be tailored to speciﬁc uses. Furthermore, the amount of data from a single session is usually relatively small. Both user studies had two main limitations to the external validity: low number of participants and qualitative focus of the evaluation in the controlled environment instead of the inthe-wild evaluation. To ensure the evaluation’s ecological validity, we needed users with practical experience with organizing hands-on training sessions and knowledge of cybersecurity education. These demands notably restrict our choice of suitable candidates. Our collaborating cybersecurity educators are, no doubt, the primary users of the developed tools. Therefore, they provided relevant feedback, which will serve as a source for further thoughts on both tools’ improvements. We also asked students of the cybersecurity degree program who successfully passed the university course on CTF games design. They represent novice users unfamiliar with analytical visualizations. Due to the qualitative nature of the evaluations, we did not focus on ﬁnding the limits in terms of the total number of trainees and their events since the events with more than 16 participants are literally none due to the space limits of the training facility at our university. We originally planned to perform the case studies to assess the TAT’s ﬁnal design in the actual deployment. Unfortunately, due to the COVID-19 pandemic, the scheduled hands-on training sessions had been canceled, and the only feasible option was to perform the evaluation remotely, using the same procedure as in the summative evaluation. In this work, we restrict ourselves to the case study of handson cybersecurity courses focused on system hacking and cyberattacks. In particular, puzzle-based capture the ﬂag games where the structure and data are well-deﬁned in advance. These restrictions allowed us to provide the tutors with a more indepth insight into this speciﬁc application sub-domain through a pair of visualization tools. Despite these limitations, the provided feedback has been guiding our work and feature requests for the deployment into the KYPO CRP. We introduced the visual analytics tool that, based on the qualitative feedback, improves the tutors’ insight into the training sessions and allows them to assess the quality of the training scenarios and evaluate the training session results. We focused on low-level learning analysis (i.e., analyzing data from a single training session). As we pointed out in Sec. 2, this particular area is often overlooked since the main focus in support tools for tutors and educators is on high-level analysis for MOOC e-learning. We have presented a design study on applying visual analytics to data from hands-on cybersecurity training in the form of CTF games. We introduced two iterations of the Training Analysis Tool, allowing tutors to assess the quality of the training scenarios and gain insight into the trainees’ progress beyond the completion time and ﬁnal score. The summative evaluation validated our design decisions. The verbal feedback from participants and the SEQ and SUS scores conﬁrmed that the tools address the elicited requirements. We gradually learned more about what information tutors would like to display in the visualization and how they interact with the data during the design study. Based on this experience, we believe that a datadriven insight into the training courses could provide surprising insights and knowledge about the design and behavior of trainees. Focusing on puzzle-games principles enabled us to conceptualize the data and visualizations beyond the cybersecurity domain. If we look closely at the information we used, we realize that it is a quadruple: timestamp, the ID of the trainee, type of event, content (arbitrary). Therefore, we believe that our approach can be easily applied in other areas where hands-on training becomes common. We admit that there are further requirements, such as automated processing of user inputs, but even basic logging can provide suﬃcient data. The level of detail depends mainly on the expressiveness of the content component. Consider the university programming course as another application area. The tutors often evaluate students’ assignments using automated compilation and validation tools against predeﬁned unit tests and datasets. The summary of code diﬀs, compiler error logs, and output of the automated tests can be logged. Similar to the cybersecurity domain, these events can be mapped to assessment events (e.g., penalties for unsuccessful unit tests), player actions (e.g., the submission of a piece of code), and progress events (e.g., successful compilation and test of a programming task). Visualizing these events on the timelines (one per student) or further text analysis of the code can be as valuable as our analogy with the cybersecurity CTF games. The support tools for a category of so-called blended classrooms and hands-on courses are still mostly unexplored. Our work addresses only a tiny part of this broad research area. Despite our focus on cybersecurity education, we consider our ﬁndings applicable in other areas of puzzle-based learning and analyzing data from a single training session (i.e., low-level learning analysis). We want to encourage others to explore novel methods for visual analysis of puzzle-based learning courses in diﬀerent areas. The TAT is integrated into the user interface of the KYPO CRP. We also work on additional data integration from sandboxes (e.g., resource usage, executed commands, running processes). Enhancing the current level of event processing with this information will further improve the insight and enable a more detailed analysis of the training and its scenario. Our next goal is to explore the possibilities for visual analysis of multiple training sessions and analyze and assess trainees’ long-term progress. Extending the analysis with automatic highlighting of anomalies or ﬂaws in the training design is another direction of research that needs further study.