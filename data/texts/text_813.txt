Contrastive learning (CL) recently has received considerable attention in the eld of recommendation, since it can greatly alleviate the data sparsity issue and improve recommendation performance in a self-supervised manner. A typical way to apply CL to recommendation is conducting edge/node dropout on the user-item bipartite graph to augment the graph data and then maximizing the correspondence between representations of the same user/item augmentations under a joint optimization setting. Despite the encouraging results brought by CL, however, what underlies the performance gains still remains unclear. In this paper, we rst experimentally demystify that the uniformity of the learned user/item representation distributions on the unit hypersphere is closely related to the recommendation performance. Based on the experimental ndings, we propose a graph augmentation-free CL method to simply adjust the uniformity by adding uniform noises to the original representations for data augmentations, and enhance recommendation from a geometric view. Specically, the constant graph perturbation during training is not required in our method and hence the positive and negative samples for CL can be generated on-the-y. The experimental results on three benchmark datasets demonstrate that the proposed method has distinct advantages over its graph augmentation-based counterparts in terms of both the ability to improve recommendation performance and the running/convergence speed. The code is released at https://github.com/Coder-Yu/QRec. Self-Supervised Learning, Recommender Systems, Contrastive Learning, Graph Augmentation, Graph Learning Recently, a resurgence of contrastive learning (CL) [13,14,18] has been witnessed in deep representation learning. Due to the ability to extract the general features from massive unlabeled data and regularize representations in a self-supervised manner, CL has led to major advances in multiple research elds [5,8,29,36]. As the data annotation is not required in CL, it is a natural antidote to the data sparsity issue in recommender systems [22]. An increasing number of very recent studies [29,33,39,41,44,45] have sought to harness CL for improving recommendation performance Figure 1: Graph contrastive learning with edge dropout for recommendation. and have demonstrated signicant gains. A typical way [29] to apply CL to recommendation is perturbing the original user-item bipartite graph with random edge/node dropout at rst, and then maximizing the consistency between representations of the same user/item learned from dierent augmentations via graph encoders (e.g., graph convolutional network [12,15]). In this situation, the CL task acts as the auxiliary task, and is optimized with the recommendation task under a joint learning setting (see Fig. 1). Despite the encouraging results provided by CL, however, what underlies the performance gains is still a mystery. Intuitively, we expect that contrasting dierent graph augmentations can capture the essential information existing in the original user-item interaction, by randomly removing the redundancy and impurity with the edge/node dropout. Unexpectedly, a few latest works [16,39,46] have reported that the dropout rate only has a trivial impact on the performance, and changing it will not cause distinct performance uctuation. To be more specic, even an extremely sparse graph augmentation (with dropout rate 0.9) in CL can bring decent recommendation performance gains. Such a nding is quite elusive and counter-intuitive because a large dropout rate will result in a huge loss of the raw information and a highly skewed graph structure. It naturally raises a meaningful question that Do we really need graph augmentations when integrating CL with recommendation? To answer this question, we rst conduct experiments with and without the graph augmentation for a performance comparison. The results show that the performance merely slightly drops when the graph augmentation is absent, which is in line with the ndings in the aforementioned works. We then investigate the dierences between the representations learned by non-CL and CL-based recommendation methods. By visualizing the distributions of the representations on the unit hypersphere and associating them with the performance, we nd that what really matters for the recommendation performance is the uniformity of the distributions, rather than the graph augmentation, and the contrastive loss InfoNCE [20] plays a pivot role in regulating the uniformity. Despite not as eective as expected, the graph augmentation is not utterly useless. [1,5] demonstrate that data augmentation can provide more negative samples and data variances for learning representations invariant to the disturbance factors. However, the dropout-based graph augmentation is time-consuming because it requires constant graph adjacency matrix reconstruction during training and the extra memory for the storage of the augmented graphs. A follow-up question then arises. That is - Are there more eective and ecient data augmentation strategies? In this paper, we give a positive response to the question. On top of our nding that the uniformity of the representation distribution is the key point, we innovatively design a graph augmentation-free CL method in which the uniformity is more adjustable. Technically, we follow the graph contrastive learning framework presented in Fig. 1, but we discard the dropout-based graph augmentation and instead directly add the random noises generated by a uniform distribution to the original representations for data augmentation. The added noise vector should be scaled to rotate the original representation by a small angle (see Fig. 3). Then the perturbed representations are propagated through the graph encoders to amplify the deviation. Imposing dierent random noises leads to dierent data augmentations, and we then contrast them to rene the original representations. The principle behind this easy-to-implement way is that, for each item/user representation, the rotation caused by the added noise vector corresponds to an oset. As the noises are uniformly distributed, by tuning the scale of the added noises, we can easily control how far the representation deviates from its original position and adjust the uniformity of the representation distribution in a straightforward way. Additionally, since the graph augmentation is not required in our method, the positive and negative samples for CL can be generated on-the-y so that the running time is signicantly reduced. The major contributions of this paper are summarized as follows: •We experimentally investigate why CL can improve recommendation and reveal that the uniformity of the learned representation distributions, rather than the graph augmentation, is the decisive factor. •We propose a graph augmentation-free CL method to regulate the uniformity by adding simple random noises to the representations, and enhance recommendation from a geometric view. •We conduct extensive experiments on multiple benchmark datasets, and the experimental results show that the proposed method has distinct advantages over its graph augmentation-based counterparts in terms of both the ability to improve recommendation performance and the running/convergence speed. CL is often applied to recommendation with various forms of data augmentations [29,34,41,45]. In this paper, we focus on the most commonly used dropout-based data augmentation on graphs [29, 36], and launch an investigation into the recent CL-based recommendation method, SGL [29], which performs node and edge dropout for data augmentation and adopts InfoNCE [20] for CL. Formally, the join learning scheme in SGL is dened as: which consists of two losses: recommendation lossLand CL lossLcontrolled by the hyperparameter𝜆. The formulation of the CL loss InfoNCE in SGL is dened as: where𝑖, 𝑗are users/items in the node setN,z(z) are𝐿normalized𝑑-dimensional node representations learned from two dierent dropout-based graph augmentations, and𝜏 >0 (e.g., 0.2) is the temperature. The CL loss encourages consistency betweenzandz which are the augmented representations of the same node𝑖and are the positive sample of each other, while minimizing the agreement between𝑧andz, which are the negative sample of each other. To learn the representations from the user-item graph, SGL employs a popular graph encoder LightGCN [12] as its base, whose message passing process is dened as: whereE∈ Ris the randomly initialized node embeddings,|𝑁 |is the number of nodes,𝐿is the number of layers, and ¯A ∈ Ris the normalized undirected adjacency matrix. By replacing¯Awith the adjacency matrices of the corrupted graph augmentations,z(z) can be learned via Eq. (3). Note that,z= andeis the corrupted version ofeinE. For conciseness, here we just abstract the core ingredients of SGL and LightGCN. For more technical details, we refer readers to the original papers [12, 29]. To demystify how CL-based recommendation methods work, we rst investigate the necessity of the graph augmentation in SGL. In this case, we construct a new variant of SGL, termedSGL-WA (WA stands for ‘without augmentation’), in which the CL loss is in the following form: Because we only learn representations from the original user-item graph, then we havez= z= z. The experiments for the performance comparison are conducted on two large benchmark datasets: Yelp2018 and Amazon-Book [12,25]. A 2-layer setting is used and the hyperparameters are tuned according to the original paper of SGL (see more experimental details in Section 4.1). The results are Figure 2: Distributions of item representations learned by dierent methods on the unit hypersphere S plot feature distributions with Gaussian kernel density estimation (KDE) in R point (x,y) ∈ S). The rightmost plot visualizes item feature distributions learned by our proposed model GACL. shown in Table 1. Three variants of SGL proposed in the paper are evaluated (-ND denotes node dropout, -ED is short for edge dropout, and -RW means random walk. CL Only means that only the CL loss in SGL-ED is minimized). Table 1: Performance comparison of dierent SGL variants. As can be observed, all the variants of SGL outperform LightGCN by a large margin, which demonstrates the eectiveness of CL in improving recommendation performance. However, to our surprise, when the graph augmentation is absent, the improvements are still so remarkable that SGL-WA even exhibits superiority over SGL-ND and SGL-RW. Although SGL-ED maintains a small advantage, it requires additional expenses for the reconstruction and storage of the perturbed graph adjacency matrices (see Section 4.2.3), which makes it less valuable. Therefore, we need to rethink the necessity of graph augmentations and reveal the primary cause that may explain the results. Wang and Isola [24] have identied that the contrastive loss optimizes two properties in the visual representation learning: alignment of features from positive pairs, and uniformity of the normalized feature distribution on the unit hypersphere. It is unclear if the CL-based recommendation methods will exhibit similar patterns that can explain the results in Section 2.2. Since top-N recommendation is a one-class problem, we only investigate the uniformity by following the visualization method in [24]. We rst map the learned representations of items (randomly sample 1,000 items for each dataset) to 2-dimensional normalized vectors on the unit hypersphereS(i.e., circle with radius 1) by using t-SNE [23]. Then we plot the feature distributions with the nonparametric Gaussian kernel density estimation [3] inR(shown in Fig. 2). For a clearer presentation, the density estimations on angles for each point onSare also visualized. According to Fig. 2, we can observe notably dierent feature/density distributions. In the leftmost column, LightGCN shows highly clustered features that mainly reside on some narrow arcs. While in the second and the third columns, the distributions become more uniform, and the density estimation curves are less sharp, no matter if the graph augmentations are applied. In the forth column, we plot the features learned only by the contrastive loss in Eq. (2). The distributions are almost completely uniform and form closed circles, and meanwhile the density estimation curves atten out. Two issues may account for the highly clustered feature distributions in the leftmost column. The rst one is the over-smoothing of node embeddings [4] caused by the message passing in LightGCN. The second one could be the popularity bias in the recommendation data. Recall the pairwise BPR loss [21] used in LightGCN: which is with a triplet input(𝑢, 𝑖, 𝑗). By optimizing the BPR loss with the stochastic gradient descent (SGD), we can get: where𝜂is the learning rate,𝜎is the sigmoid function,𝑠 = 𝜎 (ee− ee),eis the user embedding, andeandedenote the positive and negative item embeddings, respectively. Since the recommendation data usually follows a long-tail distribution, when𝑢is a popular user with a large number of interactions, the positive item embedding will be optimized towards𝑢’s direction while the randomly sampled negative is optimized towards the reverse direction. The two issues are coupled with each other (i.e.,eandeaggregate information from each other in the graph convolution) so that exacerbate the clustering problem and cause representation degeneration [7]. This may explain why opposite clusters on the unit hypersphere Sare observed in the leftmost plots. As for the distributions in other columns, the contrastive losses in Eq. (2) and (4) can give a clue. By rewriting Eq. (4), we can derive L=©−1/𝜏 + log(exp(1/𝜏) +exp(zz/𝜏))ª®. (7) Because 1/𝜏is a constant, optimizing the CL loss is actually minimizing the cosine similarity between dierent nodes embeddings eande, which will push connected nodes away from the highdegree hubs in the representation space and lead to a more uniform distribution even under the inuence of the recommendation loss. By associating the results in Table 1 with the distributions in Fig. 2, we can easily draw a conclusion that the uniformity of the distribution has close ties with the recommendation performance. Optimizing the CL loss can be seen as an implicit way to debias. As a result, a more uniform representation distribution can preserve the intrinsic characteristics of nodes and improve the generalization ability. It also should be noted that, by only minimizing the CL loss in Eq. (2), a poor performance will be obtained, which means that a positive correlation between the uniformity and the performance only holds in a limited scope. The excessive pursuit to the uniformity will overlook the closeness of interacted pairs and similar users/items, and impairs recommendation performance. Besides, although the impact of the data augmentation is not as large as expected, [5,11] have evidenced that a good augmentation can provide more informative transformed negatives for the CL loss, helping to learn representations invariant to noise factors. This could be the reason that SGL-ED still maintains a small advantage over SGL-WA on the recommendation performance. Based on the ndings in Section 2, we speculate that by adjusting the uniformity of the learned representation in a certain scope, the balance between the recommendation loss and CL loss can be reached, where the method will show the optimal performance. In this section, we target developing an eective and ecient Graph Augmentation-freeCLmethod (GACL) to freely regulate the uniformity and provide informative data variance. Since manipulating the graph structure for the uniform representation distribution is intractable and time-consuming, we shift our attention to the node representation. Inspired by the adversarial examples [9] which are constructed by adding imperceptibly small perturbation to the input images, we directly add scaled random noises to the representation for an easy-to-implement but eective and ecient data augmentation. Formally, given a node𝑖and its representationein the𝑑dimensional embedding space, we can fulll the data augmentation by following: where the added noise vectorsΔandΔare subject to∥Δ∥= 𝜖 andΔ = |Δ| ⊙ sign(e). The rst constraint controls the scale of the added noises, andΔandΔare numerically equivalent to points on a hypersphere with the radius𝜖. The second constraint requires that e,ΔandΔshould be in the same hyperoctant, so that adding the noises will not cause a large deviation ofe. In Fig. 3, we illustrate Eq. (8) inR. From a geometric perspective, we can understand Eq. (8) more straightforwardly. As shown in Fig. 3, by adding the scaled noise vectors to the original representation, we rotateeby two small angles (𝜃and𝜃). Each rotation corresponds to a deviation ofe, and leads to an augmented representation (eande). Since the rotation is small enough, the augmented representation retains most information of the original representation and meanwhile also keeps some subtlety. Note that, for each node representation, the added random noises are dierent. Following SGL, we adopt LightGCN as the graph encoder to propagate node information and amplify the impact of the deviation. At each layer, dierent scaled random noises are imposed on the current node embeddings. The nal perturbed node representations are learned by: It should mentioned that we skip the input embeddingEin all the three encoders when calculating the nal representations, because Figure 3: An illustration of the proposed random noisebased data augmentation in R. we experimentally nd that skipping it can lead to more stable and better performance in our situation. However, without the CL task, this operation will result in a performance drop. In our method, we also unify the recommendation loss and the CL loss, and then use Adam to optimize the joint loss presented in Eq. (1). The recommendation loss and the CL loss used are as the same as those in Eq. (5) and Eq. (2). Besides the temperature𝜏, another two hyperparameters𝜆and 𝜖can control the eect of CL. But𝜖can provide a ner-grained regulation beyond that provided only by tuning𝜏and𝜆. By freely adjusting the value of𝜖, we can directly control how far the augmented representations deviate from the original. Intuitively, a larger𝜖will lead to a more roughly uniform distribution of the learned representation, because the original representation will oscillate in the sector between the two deviated augmented representations. When they are enough far away from the original, the information lying in their representations is also heavily decided by the noises. As the noises are initially sampled from a uniform distribution𝑈 (0,1), a more uniform distribution of the learned representation is led. In the rightmost column in Fig. 2, we plot the distributions of the nal representations learned by GACL with 𝜖 =0.1 on two datasets. The setting of𝜆is the same as that used in SGL variants. We can clearly observe that the distributions in the rightmost column are evidently more uniform than those learned by SGL variants and LightGCN, especially on the dataset of Yelp2018. Meanwhile, the better performance is also reported in Table 4. In this section, we analyze the time complexity of GACL, and compare it with that of LightGCN and its graph-augmentation based counterpart SGL-ED. Here we only discuss the time complexity in a batch. Let|𝐸|be the edge number in the graph,𝑑be the embedding size,𝐵be the batch size,𝑀be the node number in a batch, and𝜌 denotes the edge keep rate in SGL-ED. We can easily get: •For LightGCN and GACL, no graph augmentations are required, so they just need to normalize the original adjacency matrix which has 2|𝐸|non-zero elements. For SGL-ED, two graph augmentations are used and each has 2𝜌 |𝐸| non-zero elements. •In the graph convolution stage, a three-encoder architecture (see Fig. 1) is employed in both SGL-ED and GACL to learn augmented node representations. So, the time costs of SGL-ED and GACL are almost three times that of LightGCN. •As for the recommendation loss, three methods all use the BPR loss and each batch contains𝐵interactions, so they have the same time cost in this component. •When calculating the CL loss, the computation costs between the positive/negative samples areO(𝐵𝑑)andO(𝐵𝑀𝑑), respectively, because each node only considers itself as the positive, while the other nodes all are negatives. Comparing GACL with SGL-ED, we can clearly see that SGL-ED theoretically spends less time in the stage of graph convolution, and this bonus may possibly oset GACL’s advantage in the stage of the adjacency matrix construction. However, when putting them into practice, we actually observe that GACL is much more time-ecient (See Table 5). That is because, the computation for graph convolution is mostly nished on GPUs, while the graph perturbation and reconstruction are performed on CPUs. Besides, in each epoch, the graph augmentations in SGL-ED have to be reconstructed, while in GACL, the adjacency matrix of the original graph only needs to be generated once at the start of the training. Therefore, most computation of GACL can be nished on GPUs, and the positive and negative samples for CL can be created on-the-y. In a nutshell, GACL is far more ecient than SGL-ED and its other variants, beyond what we can observe from the theoretical analysis. Datasets.Three public benchmark datasets: Douban-Book, Yelp2018 [12], and Amazon-Book [29] are used in our experiments to evaluate GACL. Because we focus on the Top-N recommendation, by following the convention in the previous research [40,41], we discard ratings less than 4 in Douban-Book, which is with a 1-5 rating scale, and reset the rest to 1. The statistics of the datasets is shown 3-Layer in Table 3. We split the datasets into three parts (training set, validation set, and test set) with a ratio of 7:1:2. Two common metrics: Recall@𝐾and NDCG@𝐾are used and we set𝐾=20. For a rigorous and unbiased evaluation, each experiment in this section is conducted 5 times and we then report the average result by ranking all the items in the test set. Baselines.Besides LightGCN and the SGL variants which have been introduced in Section 2, we also compare GACL with the following recent self-supervised or CL-based recommendation methods. • Mult-VAE[17] is a variational autoencoder-based recommendation model. It can be seen as a special self-supervised recommendation model because it has a reconstruction objective. • DNN+SSL[35] is a recent DNN-based recommendation method which adopts the similar architecture and idea in Fig. 1 for contrastive learning. • BUIR[16] has a two-branch architecture which consists of a target network and an online network, and only uses positive examples for self-supervised learning. Hyperparameters.For a fair comparison, we refer to the best hyperparameter settings reported in the original papers of the baselines and then ne-tune all the hyperparameters of the baselines with the grid search. As for the general settings of all the baselines, the Xavier initialization is used on all the embeddings. The embedding size is 64, the parameter for𝐿regularization is 10and the batch size is 2048. We use Adam with the learning rate 0.001 to optimize all the models. In GACL and SGL, we empirically let the temperature𝜏 =0.2, and this value is also reported as the best in the original paper of SGL. As one of the core ideas of this paper is that graph augmentations are not indispensable and inecient in CL-based recommendation, in this part, we conduct a comprehensive comparison between SGL and GACL in terms of the recommendation performance, convergence speed, and running time. 4.2.1 Performance Comparison. We rst further compare SGL with GACL on three dierent datasets under dierent parameter settings. The hyperparameter𝜆, which controls the magnitude of CL in GACL, is set to 0.2 on Douban-Book, 0.5 on Yelp2018, and 2 on Amazon-Book, where the best performances are reached. The magnitude for the added noise𝜖in GACL is 0.1 on all the datasets. We thicken the gures representing the best performance and underline the second best. The improvements are calculated by using LightGCN as the baseline. According to Table 4, we can draw the following observations and conclusions: •All the SGL variants and GACL are eective in improving Light- GCN under dierent settings. The largest improvements are observed on Douban-Book. When the layer number is 1, GACL can remarkably improve LightGCN by 33.5% on Recall, and 40.5% on NDCG. •SGL-ED is the most eective variant of SGL while SGL-ND is the least eective one. When a 2-layer or 3-layer setting is used, SGLWA outperforms SGL-ND in most cases and shows advantages over SGL-RW in a few cases. These results demonstrate that the CL loss is the main driving force of the performance improvement while intuitive graph augmentations may not be as eective as expected, and some of them may even lower the performance. •GACL shows the best performance in all the cases, which proves the eectiveness of the random noised-based data augmentation. Particularly, on the two larger datasets: Yelp2018 and AmazonBook, GACL signicantly outperforms the SGL variants by large margins. 4.2.2 Convergence Speed Comparison. In the original paper of SGL (Section 4.3.2), the authors discover that SGL converges much faster than LightGCN does. In this part, we manage to reproduce their results and further show that GACL has a much faster convergence speed than what SGL shows. A 2-layer setting is used in this part and the other parameter values are not changed. According to Fig. 4 and Fig. 5, we can see that, GACL reaches its best performance on the test set at the 25epoch on DoubanBook, the 11epoch on Yelp2018, and the 10epoch on AmazonBook. By contrast, SGL-ED reaches its best performance at the 38epoch on Douban-Book, the 17epoch on Yelp2018, and the 14epoch on Amazon-Book. GACL only needs 2/3 epochs that SGL variants need to reach its peak. Besides, the curve of SGL-WA almost overlaps that of SGL-ED on Yelp2018 and Amazon-Book, and exhibits the same tendency to convergence. It seems that the dropout-based graph augmentations cannot speed up the model to reach its convergence. Despite that, all the CL-based methods show advantages over LightGCN at the convergence speed. When the other three methods begin to get overtted, LightGCN is still far away from getting converged. In the paper of SGL, the authors guess that the multiple negatives in the CL loss may contribute to the fast convergence. However, with almost innite negative samples created by dropout, SGL-ED is basically on par with SGL-WA in speeding up the training, though the latter only has a certain number of negative samples. As for GACL, we consider that the remarkable convergence speed stems from the noises. They averagely provide a constant increment to the gradient values so that the method can be ahead throughout the convergence speed comparison. In addition to the results in Fig. 4 and 5, we also nd that a larger𝜖leads to faster convergence. But when it is too large (e.g., greater than 1), despite the rapid decrease of BPR loss, GACL requires more time to reach its peak performance. A large𝜖acts like a large learning rate, causing the progressive zigzag optimization that will overshoot the minimum. Table 5: Running time for per epoch (x in the brackets represents times). 4.2.3 Running Time Comparison. In the paper of SGL (section 4.3.2), the authors use the performance and loss curves to prove that SGL can greatly reduce the training time. However, they neglect Figure 4: The performance curves in the rst 50 epo chs. Figure 5: The loss curves in the rst 50 epochs. that SGL may require much longer time in practice for the graph augmentation. In this part, we report the real running time of the compared methods for one epoch. The results in Table 5 are obtained on an Intel(R) Xeon(R) Gold 5122 CPU and a GeForce RTX 2080Ti GPU. As shown in Table 5, we calculate how many times slower the other methods are when compared with LightGCN. Because there is no graph augmentation in SGL-WA, we can see its running speed is very close to that of LightGCN. For SGL-ED, two graph augmentations are required and the computation in this part is mostly nished on CPUs, so that it is even 5.7 times slower than LightGCN on Amazon-Book. The running time increases with the scale of the datasets. By contrast, despite not as fast as SGL-WA, GACL is only 2.4 times slower than LightGCN on Amazon-Book, and the slope of the speed increase is much smaller than that of SGL-ED as well. Considering that GACL only needs half of the epochs that SGL-ED requires to reach the best performance, it outperforms SGL in all aspects. In this part, we investigate the impact of the two important hyperparameters in GACL. Here we adopt the experimental settings used in section 4.2.2. 4.3.1 Impact of𝜆. By xing𝜖at 0.1, we change𝜆to a set of predetermined representative values presented in Fig. 6. As can be observed, with the increase of𝜆, the performance of GACL starts to increase at the beginning, and it gradually reaches its peak when 𝜆is 0.2 on Douban-Book, 0.5 on Yelp2018, and 2 on Amazon-Book. Afterwards, it starts to decline. Besides, in contrast to Fig. 7, more dramatic changes are observed in Fig. 6 though𝜖and𝜆are tuned in the same scope, which demonstrates that𝜖can provide a nergrained regulation beyond that provided only by tuning 𝜆. 4.3.2 Impact of𝜖. We think changing𝜖can regulate the uniformity of the learned representation distribution. A larger𝜖leads to a more Figure 6: Inuence of the magnitude 𝜆 for CL. uniform distribution that can help to debias. However, when it is too large, the recommendation task will be hindered because the high similarity between connected nodes can not be reected by an over-uniform distribution. We x𝜆at the best values on the three datasets as reported in Fig. 6, and then adjust𝜖to see the performance change. As can be seen in Fig. 7, the shapes of the curves are as expected. On all the datasets, when𝜖is near 0.1, GACL achieves the best performance. Three other recent self-supervised learning or CL-based methods also claim themselves as the SOTA methods. However, in our comparison shown in Table 6, they are all uncompetitive, even outperformed by LightGCN in most cases. We think the main reasons are that: (1). LightGCN and GACL have a more powerful graph convolution structure compared with Mult-VAE. (2). DNNs are proved eective when user/item features are available. In our datasets, no features are provided and we mask embeddings learned by DNN to conduct self-supervised learning. Only the weak self-supervision signals are extracted, so that it cannot fulll itself in this situation. (3). In the paper of BUIR, it removes long-tail nodes to achieve a good performance, but we use all the users and items. Besides, its siamese network may also collapse to a trivial solution on some long-tail nodes because it does not use negative examples, which may account for the performance drop. In GACL, we use the random noises sampled from a uniform distribution to obtain data augmentation. However, there are other types of noises including the Gaussian noises and adversarial noises. Here we also test dierent noises and report the results in Table 7 (denotes uniform noises,denotes Gaussian noises generated by the standard Gaussian distribution, anddenotes adversarial Table 6: Performance comparison with other SOTA models. Method noises generated by FGSM [9]). The experimental settings in section 4.2.2 are also used here. According to Table 7, GACLshows comparable performance while GACLis less eective. We think it is because we apply𝐿normalization to the noises generated by the standard Gaussian distribution and then multiply𝜖to meet the rst constraint. The normalized noises can t a much atter Gaussian distribution (can be easily proved) which approximates a uniform distribution. So, the comparable results are observed. As for GACL, the adversarial noises are generated by only targeting maximizing the CL loss while the recommendation loss has a dominant status that impacts the performance more during optimization. Table 7: Performance comparison between dierent GACL variants. Graph Neural Networks (GNNs) [6,10,31] now have become widely acknowledged powerful architectures for modeling recommendation data. This new neural network paradigm ends the regime of MLP-based recommendation models in the academia, and boosts the neural recommender systems to a new level. A large number of recommendation models, which adopt GNNs as their bases, claim that they have achieved state-of-the-art performance [12,26,27,41] in dierent subelds. Particularly, GCN [15], as the most prevalent variant of GNNs, further fuels the development of the graph neural recommendation models like GCMC [2], NGCF [25], LightGCN [12], and LCF [42]. Despite the dierent implementations in details, these GCN-driven models share a common idea that is to acquire the information from the neighbors in the user-item graph layer by layer to rene the target node’s embeddings and fulll graph reasoning [30]. Among these methods, LightGCN is the most popular one due to its simple structure and decent performance. Following [28], it removes the redundant operations such as the transformation matrices and the nonlinear activation functions. Such a design is proved ecient and eective, and inspires a lot of follow-up CL-based recommendation models like SGL [29] and MHCN [39]. As CL works in a self-supervised manner, it is inherently a possible solution to the data sparsity issue [37,38] in recommender systems. Inspired by the achievements of CL in other elds, there has been a wave of new research that integrates CL with recommendation [19,29,33,39,41,45]. Zhou et al. [45] adopted random masking on attributes and items to create sequence augmentations for sequential model pretraining with mutual information maximization. Similar ideas are also used in [35], where a two-tower DNN architecture is developed for recommendation, and the other two towers contrast augmented item features for the self-supervised task. SGL [29] unies the CL task and the recommendation task under a joint learning framework where the data augmentation is based on the stochastic perturbations on graphs including the node/edge dropout and random walk. SEPT [39] and COTREC [32] further proposes to mine multiple positive samples with semi-supervised learning on the perturbed graph for social/session-based recommendation. In addition to the dropout, CL4Rec [34] proposes to reorder and crop item segments for sequential data augmentation. Yu et al. [41], Zhang et al. [43] and Xia et al. [33] leveraged hypergraph to model recommendation data, and proposed to contrast dierent hypergraph structures for representation regularization. In addition to the data sparsity problem, Zhou et al. [44] theoretically proved that CL can also mitigate the exposure bias in recommendation, and developed a method named CLRec to improve deep match in terms of fairness and eciency. In this paper, we revisit the dropout-based graph contrastive learning in recommendation, and investigate how it improves recommendation performance. We reveal that, in CL-based recommendation models, the CL loss is the core and the graph augmentation only plays a secondary role. Optimizing the CL loss leads to a more uniform representation distribution, which helps to debias in the scenario of recommendation. We then develop a graph augmentationfree CL method to regulate the uniformity of the representation distribution in a more straightforward way. By adding directed random noises to the representation for dierent data augmentations and contrast, the proposed method can signicantly enhance recommendation. The extensive experiments demonstrate that the proposed method outperforms its graph augmentation-based counterparts and meanwhile the running time is dramatically reduced.