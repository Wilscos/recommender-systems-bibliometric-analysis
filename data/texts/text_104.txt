<title>RECOMMENDING RESEARCHERS IN MACHINE LEARNING </title> <title>BASED ON AUTHOR-TOPIC MODEL </title> <title>1  Introduction </title> Topic models are a family of statistical models for discovering the latent topics in a                 group  of  documents  [1].  In  topic  models,  each  topic  is  modeled  as  a  probability  distribution  over  words  in  the vocabulary of  the corpus  and  each document in  the  corpus is modeled as a mixture of topics giving a multinomial distribution over the  topics [2]. The probabilistic Latent Semantic Indexing (pLSI), proposed by Hofmann  [3], works toward the probabilistic modeling of text; it however does not provide a  probabilistic model at the level of documents. In order to overcome this problem, the  latent  Dirichlet  allocation  (LDA)  model  is  discussed  by  Blei  et  al.  [4],  which  is   similar to the pLSI except the topic distribution in LDA is assumed to have a Dirichlet  prior. The assumption usually brings about more reasonable mixtures of topics in a  document.  The  LDA  is a  generative  probabilistic model for  collections  of discrete  data such as text corpora [5]. It is based upon the idea that the probability distribution  over words in a document can be expressed as a mixture of topics, i.e., each document  may  be  viewed  as  a  mixture  of  various  topics.  LDA  model  can  be  viewed  as  a    generative process. A document can be generated in following three steps as firstly to                  sample a mixture proportion from a Dirichlet distribution. Secondly, to sample a topic  index according to the mixture proportion for each word in the document. Finally, to  sample a word token from a multinomial distribution over the words specific to the  sampled topic.  The document-topic and topic-word distributions learned by LDA describe the best  topics for the documents and the most descriptive words for each topic. An extension  of LDA is the author-topic model (ATM) [6, 7], which is based on the author-word  model [8]. In ATM, a document is represented as a product of the mixture of topics of  its authors, where each word is generated by the activation of one of the topics of an  author of that document. ATM model has ability to give researchers a means to gain  intuition  about  authorship  and  content  in  terms  of  topics  at  once.  The  authors  associate with many kinds of metadata in documents, e.g. tags on posts on the web.  This model can be used for author (or tag) prediction, data exploration, as features in  machine  learning  pipelines,  or  to  simply  leverage  the  topic  model  with  existing  metadata.  It  provides  a  relatively  simple  probabilistic  model  for  exploring  the   relationships between the authors, documents, topics, and words, wherein each author  is represented by a multinomial distribution over topics and each topic is represented  by a multinomial distribution over words. The words in a document co-authored by  multiple authors are assumed to be the result of a mixture of topic mixture of each  author. The topic-word and author-topic distributions are learned from  text corpus.  Compared  to  the  LDA,  the ATM  provides  the  increase  in  salient  topics and  more  reasonable researchers’ interest patterns. It has indeed been proved to be an essential  way to uncover the research interests of a researcher.   In  this  study,  we  uncover  the  researchers  in  machine  learning  research  using   the author-topic model and also find the similar researchers with their similarity score.  This  approach  may  be  helpful  to  new  researchers  to  find  the  most  prominent        researchers  w.r.t.  their  research  area  for  further  research.  The  author  of  research         papers are considered as researchers in our study. The rest of the paper is organized as  follows. Section 2 provides an introduction of methodology adopted in our study and  section 3 presents the result analysis. Finally, section 4 concludes the paper with some  future direction.  <title>2  An Introductory Discussion On Methods Adopted </title> In  this  section,  we  briefly  discuss  the  Author-Topic  model  and  process  of                      finding similar researchers as these methods are used in our study.   The  ATM  is  used  to  uncover  the  research  interest  of  an  author.  The  model  is                     described by a set of linked probability distributions as follows:  where,  Eq.  (3).  says  that  the  author  of  a  word     is  drawn  uniformly  with                         probability one over the number of authors in document . Eq. (4). says that we draw   from  , assuming  . The intuition behind each of these parameters are  discussed below:  •    is a probability vector such that  is the probability that author  writes  about topic .  topic  is equal to  •    is a latent variable to indicate which author is responsible for word  in  document .  •     is  also  a  latent  variable  to  indicate  which  topic  generates  word    in                   document .  Figure 1 shows the graphical model of ATM. We can interpret the edges in graph as  dependency between two variables, e.g.,   depends on  , and the absence of an  edge  represents  conditional  independence,  e.g.,  when  conditioned  on   ,     is   independent of  , i.e.    does not depend on  The author-topic model may be viewed as a process that generates words in a document based on the authors of that document.  •  For each author   , construct Eq. (1).  •  For each topic   , construct (2).  •  For each document   -  Given document  authors,  -  For each word in document   ▪  Assign an author to current word by constructing Eq. (3).  ▪  Conditioned on  , assign a topic by constructing Eq. (4).  ▪  Conditioned on  , assign a topic by constructing Eq. (5).  The  Variational  Bayes  (VB)  method  is  used  for  approximate  inference  in  the                        author-topic model to maximize the lower bound on the log likelihood. Though the  lower bound and the conditional likelihood tell how well the algorithm converges, yet  they  don't tell  much  about the  quality  of the  topics [9]. Mimno  [10] discusses the        domain experts annotated quality of topics from a trained LDA model. They devise a  measure to be strongly correlated with the human annotator's judgment of the topic  quality, which is the topic coherence.   Here, we set up a system that takes the name of researchers as input and produces the  researchers  that  are  most  similar,  which  is  used  as  a  component  in  information                 retrieval (i.e. a search engine of finding similar researchers). The common similarity  measure between two researchers is computed using the cosine distance. We however  use  the  Hellinger  distance  as  it  is  a  natural  way  of  measuring  the  distance  (i.e.   dissimilarity)  between  two  probability  distributions,  which  is  defined  in  discrete                   version as follows:  where  ,   are topic distributions for two different researchers,  is number of topics in author-topic model.  We define the similarity between authors   and   as follows:  <title>3  Analysis of Our Result </title> Here, we first look at the dataset used for our study and preprocess it before applying  the  Author-topic  model  and  then  plot  the  researchers  to  explore  the  author-topic                           representation. Finally, we present the researchers with highest topic distribution with  their similar researchers in machine learning research.  We start collecting the data by preparing a list of appropriate well-known journals that  publish the high-quality research in machine learning. Figure 2 shows the distribution  of number of research papers included in our study in different colors as shown in  legends.   We  have  included  well-established  journals  like  Springer  machine  learning                (Sp-ML),  ScienceDirect  neural  networks  (ScD-NN),  ScienceDirect  pattern      recognition  (ScD-PR),  Journal  of  machine  learning  research  (JMLR),  IEEE   transactions on neural networks (IEEE-NN) and IEEE transactions on pattern analysis  and machine intelligence (IEEE-PAMI). The dataset is broken into 4 intervals such as  1997~2001, 2002~2006, 2007~2011. 2012~2016. The titles, authors, and abstracts of  the  research  papers  have  been  taken  from  the  electronic  library  of  the  abovementioned journal articles. We consider only journal articles in our study.   The preprocessing of the research papers is done by tokenizing the text including title  and  abstract,  replace  all  whitespace  by  single  spaces,  remove  all  punctuation  and  numbers, filtering terms (words), by applying a custom stop word list along with the  original stop words of the SMART system [11], which is a common preprocessing  step  in  text  mining.  Then  apply  stemming  [12]  to  words,  to  perform  their  lemmatization,  add  multi-word  named  entities,  add  frequent  bigrams  and  remove  frequent and rare words.  Construct a mapping from author names to documents as  required  by  ATM.  Finally,  the  vectorized  representation  of  the  text  by  computing          bag-of-words with authors mapping is given to ATM.   We  train  the  author-topic  model  on  the  preprocessed  data  prepared  in  the                  above sections. For our experimental purpose, we use the Gensim as a pure Python  library for implementing the ATM, which is an NLP software framework created on  the idea of document streaming [13]. This framework has two objectives, first one                          represents  the  indexing  of  digital  document  and  similarity  search,  and  second  one                        represents  the  memory-efficient,  fast  and  scalable  algorithms  for  Singular  Value   Decomposition for unsupervised learning and semantic analysis of plain text in digital  collections.  It  requires  the  open  source  NumPy  for  n-dim  array  manipulation  and  SciPy for numerical integration and optimization. The advantages of Gensim are fast  processing  of  large  datasets  and  memory  independence  because  the                                    term-by-document matrix does not have to be stored in memory. In our experiments  of ATM, the number of topics is fixed as 5 for each dataset, the symmetric Dirichlet  priors  and  are set as 0.5 and 0.1, respectively, and VB is run for 2000 iterations.  We have run the model for same setting with different random states for five times  and chosen the best model for further analysis. Table 1 shows the statistical details of  our best ATM.  Here we explore the author-topic representation in an intuitive manner. We take all  the  author-topic  distributions  and  embed  them  in  a  2-D  space  by  reducing  the          dimensionality of this data using the t-SNE. The t-SNE is a method that attempts to  reduce the dimensionality of a dataset, while maintaining the distances between the  points.  If  two  researchers  are  close  together  in  plot  below,  then  their  topic                distributions  would  be  similar.  Figures  3(a)-3(d)  represent  plots  of  researchers  for  dataset in different time-period in which the circles refer to individual researchers and  their sizes represent the number of documents attributed to the corresponding author.  Large clusters of researchers tend to reflect some overlap in interest. As evident from  these  figures,  the  number  of  researchers  has  increased  in  machine  learning  in        successive years, indicating the popularity of this area.   The  researchers  for  each  topic  with  highest  topic  distribution,  along  with  their                   similar researchers of same interest are discussed in this section. Table 2 represents  the  list of  the  researchers  with their similar  researchers  of each topic  for  the  time          period 1997~2001. Each researcher represents his topic distribution and top 10 words  in each topic. The top researcher is selected based on the highest topic distribution.  Then, the similar researchers are evaluated on the similarity score based on Hellinger  distance as discussed in section 2.2.  The  topic  label  of  five  topics  in  Table  2  are  image  processing,  video  processing,  neural network, fuzzy model, and supervised/unsupervised learning. It also contains  the top researchers selected based on its highest topic proportion among researchers  and also the top five similar researchers, selected for each top researcher.  video  processing  at  time  period  1997~2001,  who  has  become  the  most  similar   researcher in the neural network during the time period 2002~2006.   Table 4 represents the list of the researchers along with their similar researchers of  each topic for the time period 2007~2011. The topic labels of five topics in Table 4  are  optimization/approximation  for  neural  network,  linear/non-linear  classification,  face recognition, deep neural network and image/video processing. During this time  period,  new  researchers  have  made  their  presence.  During  this  time,  the  machine  learning research has evolved and more optimization of the neural network as deep  learning was presented in the research community.   Table 5 represents the list of the researchers along with their similar researchers of  each topic for the time periods 2012~2016. The topic labels of five topics in Table 5  are  computer  vision,  support  vector  machine,  optimizing  the  neural  network,  face  recognition  and  unsupervised  learning.  The  researcher  Jinde  Cao  was  the  lead  researcher  in neural  network and  amazingly we observe  on the researcher that  Jun  Wang as the most prominent researcher in neural networks during the time periods  1997~2001, 2002~2006 and 2012~2016. Similarly, the researcher Isabelle Bloch was  one of the researcher in face recognition and has become the researcher in computer  vision during the time period 2012~2016.  In this section,  we have  presented the  results  of our study  and the top researchers  along with similar researchers during different time periods. This study has facilitated  new researchers to find the prominent researchers in their field of interest.  <title>4  Conclusions and Future Work </title> T1  imag, detect, object,  Ashutosh Saxena  0.9939  Isabelle Bloch (0.999845),   T4  imag, face, local, feature,  David Zhang  0.9974  J. Yang (0.999626),  T2  comput, vector, effici,  Yuan-Hai Shao  0.9940  Da-Han Wang (0.999561),  T3  neural, network,                 Jinde Cao  0.9988  Jun Wang (0.999828),  T5  learn, classif, cluster,  R. Polikar  0.9981  G. Rosen (0.999951),  segment, recognit,                Kaiming He (0.999821),  recognit,  graph, robust,  Mingbo Zhao (0.999444),   support, approach,  Zhuang Wang (0.997633),  function, control,  Tingwen Huang (0.999745),  feature, set, classifi,     Licheng Jiao (0.999759),   feature, system, visual,  Jun Zhang (0.999694),   estim, spars, shape  Y. Xu (0.998836),   Topic Top 10 words in Topic  machin, train, kernel,  Irfan Ahmad (0.997278),  system, estim, optim,  S. Jagannathan (0.999697),  select, label, class,         Zhi-Hua Zhou (0.999277),  approach, human  Guodong Guo (0.999331),   In So Kweon (0.998639),  Id  Researcher  Topic  Researcher Names  time, optim  Yali (0.995624),  learn, condit, bound  H. Zhang (0.999656),  approach  Steven C. H. Hoi (0.999210),   Top Researchers  Top Five Similar Researchers  Name   (Similarity Score)  H. Tang (0.999294)   Jialei Wang (0.995135)  W. X. Zheng (0.999542)  Y. Xiang (0.998565)   Jun Zhu (0.999178),  researchers  for  top  researcher  on  each  topic.  This  study  can  be  helpful  the  new                    researchers in machine learning area to get insight of top researchers related to their  area  of  interest.  In  future,  we  intend  to  extend  this  study  to  find  the  research                 communities  depending on  the research interest and  analyze the  research trends in  communities in different time periods.  <title>REFERENCES </title>