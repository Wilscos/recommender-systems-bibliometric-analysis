<title>Selecting Optimal Trace Clustering Pipelines with AutoML</title> <title>1 Introduction</title> <title>arXiv:2109.00635v1  [cs.LG]  1 Sep 2021</title> event logs which brace the history of the process. The executions generating the same sequence of activities are observed as the same trace by Process Mining (PM) algorithms that can group multiple executions in a single representation. Often, the variability of traces is however remarkable, and traces by themselves do not oﬀer a helpful representation of the process. This variability causes problems for existing PM techniques. For instance, business processes with high trace variability generate spaghetti-like models, i.e., complex models with an enormous number of relations, often unreadable for the ﬁnal user [2]. Trace clustering techniques have been adopted to solve this issue by identifying sub-logs grouped by trace similarity. This way, by detecting groups with homogeneous behavior, process discovery techniques can be executed in these sub-logs, producing higher quality models, which are instead accessible for stakeholders [14]. Trace clustering has also been studied in the context of explainability for PM [19], and, more recently, adapted to incorporate expert knowledge [18]. However, selecting the appropriate clustering technique is not simple. Many transformation methods were presented, treating traces as vectors generated from bags of activities [22], edit distance [5] or dependency spaces [12], discriminant rules [15,25] or log footprints [19]. The set of clustering algorithms applied is also ample, e.g., k -means [15], hierarchical clustering [5], spectral clustering [12], constrained clustering [18], among others. Given this large set of options to setup a clustering pipeline, a non-expert user can likely feel overwhelmed. Considering the challenge of designing pipelines for identifying the correct encoding method, clustering algorithm, and hyperparameters to be used for a speciﬁc log, we propose an AutoML framework based on Meta-learning (MtL). Our framework recommends the trace clustering pipeline that best ﬁts a speciﬁc event log. MtL is a learning process applied to meta-data representing other learning processes and has been used successfully to emulate expert’s recommendations, maximize performance, and improve quality metrics [16]. In this work, the meta-data consist of a large set of event log features that are provided in input to the MtL workﬂow that outputs trace clustering pipelines described by an encoding technique, a clustering algorithm, and hyperparameters. In our scenario, MtL learning serves as an AutoML approach as it suppresses the need for expert interaction to work properly. The relationship between event log features and quality of PM techniques has been already pointed out in the literature [4,3]. In this work, we introduce a general framework for studying this relationship for the trace clustering task using MtL. Moreover, we instantiate this framework to provide an example of its functionality. In particular, in our experiments we submit the method to a set of 1091 event logs described by 93 log features, four encoding techniques (one-hot, position proﬁles, bi-gram, and tri-gram), and three clustering algorithms (k-means, dbscan, and agglomerative). Results show that our approach achieves 0.77 and 0.61 F1 scores for recommending encoding and clustering techniques, respectively. We also provide a comparison with two baseline performances, highlighting the improvement supported by the MtL strategy. Although, the same framework could be applied to other techniques to further investigate the domain. The remainder of this paper is organized as follows. Section 2 gives a historical overview of trace clustering solutions, focusing on the employed transformation and clustering methods. Section 3 deﬁnes the problem and its conﬁguration steps, while Section 4 presents our proposed framework to solve the trace clustering recommendation issue. Section 5 presents the material used for experiments, the techniques, and quality metrics adopted. Section 6 shows the results and raises a discussion around them. Section 7 concludes the paper. <title>2 Related Work</title> Trace clustering research is deeply connected to the variant analysis problem, that is, detecting groups of similar behavior within a single business process [19]. As stated by Koninck et al. [18], clustering traces is partitioning an event log into groups of comparable traces such that each trace is assigned to a unique group, named cluster. Since its initial adoption, trace clustering has been proposed as an instrument to reduce variability. Discovering process models from clusters, for example, generally improves quality [14]. An early work in the area, presented by Greco et al. [15], uses a set of n-grams to encode a trace activity sequence, thus, transforming traces to feature vectors and input clustering techniques. Song et al. [25] went further by deﬁning multiple encoding procedures, named proﬁles, to represent traces as vectors. Furthermore, the authors call attention to the modularity between the proﬁling and clustering steps. Bose and van der Aalst [5] represent traces as strings and apply edit distance to measure trace similarity. Delias et al. [12] proposed a measure to calculate trace distance based on dependency. Following a similar line, Appice and Malerba [1] developed a co-training strategy to cluster traces based on multiple perspectives. The clusters are created using similarity measures based on multiple dimensions, namely activity, resource, sequence, and time diﬀerence. However, approaches based on instancelevel similarity may be applicable only to particular domains depending on how the similarity is extracted. Thaler et al. [26] highlight that bags of activities may lose key information regarding the execution order. Delias et al. [12], show that no single optimal similarity metric is applicable for all domains and applications. Zandkarimi et al. [30] stated that trace clustering is a context-speciﬁc task. A better clariﬁcation of the problem is achieved by Koninck et al. [19], which characterize the complexity of clustering with the assessment of the best event log splitting operations. Considering the plethora of available proﬁling techniques and clustering algorithms, we envision two main building blocks regulating the success of clustering techniques. The ﬁrst regards the encoding method, converting the trace sequences into feature vectors or computing similarity metrics. The latter comprises the clustering techniques as given the algorithm’s availability, one may not manage to choose a method. The approaches currently available in the literature are strictly attached to a speciﬁc combination of encoding and clustering algorithms; hence, they do not oﬀer a means to study the relationship between the diﬀerent steps that can compose a pipeline. <title>3 Problem Statement</title> As seen in Section 2, past research has gathered heterogeneous approaches to the trace clustering problem. An expert may be able to assess business process characteristics and relate them to clustering approaches. However, given the plethora of conﬁguration steps and parametrization, designing the appropriate trace clustering pipeline is a complex issue even for experts. We identiﬁed in the literature three conﬁguration steps that highly aﬀect the clustering results: (i) trace encoding, (ii) clustering algorithm, and (iii) hyperparameters regulating the clustering algorithm. The choice of each step is critical since slight changes deeply aﬀect the clustering results, hindering the accessibility of solutions for non-expert users. Regarding trace encoding, Barbon et al. [4] stated that a well-performing encoding method improves a wide range of posterior analyses without the need of tuning them. In PM applications, encoding aims at transforming traces into mathematical representations, most frequently vectors, which map process instances into a feature space. The authors also showed that there is no best encoding method for every scenario in the anomaly detection task, that is, diﬀerent event logs are encoded better, considering several quality criteria, by diﬀerent encoding techniques. A similar conclusion is achieved by Thaler et al. [26] when analyzing clustering algorithms applied to PM. The authors stated that some techniques are suitable for particular scenarios, reinforcing the argument that process characteristics may guide the decision of the appropriate clustering technique. Besides, diﬀerent from supervised approaches, unsupervised learning performance is severely aﬀected by small changes in hyperparameters, depending heavily on user-domain knowledge [17]. This implies the solutions proposed today are far from optimal as they are attached to a unique set of encoding and clustering algorithms. <title>4 AutoML as a Solution for Trace Clustering</title> Trace clustering solutions must be able to adapt according to domain characteristics. We then propose a framework grounded in AutoML capable to deliver suitable recommendations according to diﬀerent business process behaviors. The main goal of our approach is recommending a tuple hencoding, clustering, hyperparametersi that maximizes quality metrics for the trace clustering problem. Fig. 1 shows the overview of building blocks controlling the framework. First, an event log repository is created to represent diﬀerent business scenarios. The Meta-feature Extraction step mines features for each event log in the repository, creating meta-features according to MtL terminology. The description quality of the meta-features is an important constraint aﬀecting the performance of the complete pipeline. Moreover, the Meta-target Deﬁnition deﬁnes a set of encoding and clustering (coupled with its hyperparameter) techniques that are assessed by quality metrics and ranked according to a ranking function. Then, the Metadatabase combines the meta-features and meta-targets deﬁned in previous steps, creating a data set populated by meta-instances. Using the meta-database, the Meta-learning step induces a Meta-model that is, then, used to recommend a tuple hencoding, clustering, hyperparametersi for a given event log considering its meta-features. It is worth mentioning that multi-output machine learning modeling for the meta-model can bring important achievements in terms of performance considering the interrelations between each step of the pipeline. In Fig. 1, green arrows indicate the steps that are used for the creation and training of the framework, while blue arrows represent a production environment where one assesses the meta-model for recommending. Given the adaptable setup of our framework, one can implement it using a diﬀerent set meta-features and meta-targets. The automatic aspect of this approach provides the user with recommendations based on event log behavior considering the possible options among the conﬁgurable steps. Moreover, other aspects are adaptable, such as the adopted quality metrics and the ranking function. Nonetheless, we note that the robustness of the approach depends on the AutoML structure, which must be maintained when the framework is instantiated in real scenarios. Fig. 1: Overview of AutoML proposal for Trace Clustering. <title>5 Experimental Setup</title> MtL beneﬁts from using a large set of instances in the meta-database. Hence, we are aiming at a heterogeneous set of business process logs, representing diﬀerent scenarios and behaviors. For that, we rely on the set of logs proposed by Barbon et al. [3]. These event logs were grouped to represent a plethora of business behaviors, mapping the relationship between process characteristics and quality metrics. This set contains both real and synthetic event logs. Regarding reallife data, there are six logs from past Business Process Intelligence Challenges (BPIC) , the environmental permit , helpdesk and sepsis logs. For synthetic data, the authors adopted 192 logs from the Process Discovery Contest (PDC) 2020 , an annual event organized to evaluate the eﬃciency of process discovery algorithms. The PDC logs are complex given the nature of employed behaviors, such as dependent tasks, loops, invisible and duplicate tasks, and noise. The next group of synthetic data contains 750 logs proposed in the context of online PM [8]. These logs are built to depict process drifts, i.e., behavior change during the business process execution. For that, a model was created and perturbed by 16 change patterns, representing diﬀerent changes from the original model. Moreover, the logs contain four drift types, ﬁve noise percentages, and three trace lengths. The ﬁnal group of synthetic event logs was proposed for the evaluation of trace encoding techniques [4]. This set contains 140 logs generated from ﬁve process models, six anomaly types, and four frequency percentages. The performance of the meta-model is directly dependent on the quality of the meta-features. Thus, the group of meta-features extracted from event logs must correctly capture the process behavior and describe it from complementary perspectives. As proposing log descriptors is out of the scope of this work, we adopted the featurization introduced in [3]. The authors presented a group of features that capture several layers of business processes, i.e., activity, trace, and log. Regarding activity-level features, the group is subdivided into: all activities, start activities, and end activities. 12 features are extracted for each group, they are the number of activities, minimum, maximum, mean, median, standard deviation, variance, the 25th and 75th percentile of data, interquartile range, skewness, and kurtosis coeﬃcients. To capture behavior at the trace-level, the authors propose features for trace lengths and trace variants. The former group contains 29 attributes: minimum, maximum, mean, median, mode, standard deviation, variance, the 25th and 75th percentile of data, interquartile range, geometric mean and standard variation, harmonic mean, coeﬃcient of variation, entropy, and a histogram of 10 bins along with its skewness and kurtosis coeﬃcients. Trace variants are captured by 11 descriptors: mean number of traces per variant, standard variation, skewness coeﬃcient, kurtosis coeﬃcient, the ratio of the most common variant to the number of traces, and ratios of the top 1%, 5%, 10%, 20%, 50% and 75% variants to the total number of traces. Log-level behavior is captured by: number of traces, unique traces, and their ratio, and number of events. Finally, to describe log complexity, entropy-based measures have been adopted recently in PM literature [2]. The entropy metrics proposed in [2] aim at the discretization between logs that are better mined by declarative or imperative algorithms. Hence, such metrics capture log structuredness and variability. The 14 entropy features we adopt are: trace, preﬁx, k -block diﬀerence and ratio (applied with k values of 1, 3 and 5), global block, k -nearest neighbor (applied with k values of 3, 5, and 7), Lempel-Ziv, and Kozachenko-Leonenko. Considering all groups, 93 meta-features were used to extract log characteristics. Many PM techniques rely on encoding to transform event log-speciﬁc representations to other formats [28,20,23,9]. The transformation usually applies at the trace-level, that is, converting the sequence of activities respective to a unique trace into a feature vector. In [4], the authors compared 10 diﬀerent encoding techniques through the lens of quality metrics measuring data dispersity, representativeness, and compactedness. These encoding methods were inspired by three diﬀerent families: PM native, word and graph embeddings. A classiﬁcation task for anomaly detection was also employed to measure encoding quality. As pointed out by the authors, there is no encoding that excels in all tasks and perspectives concomitantly. For instance, graph embeddings outperform the others in the classiﬁcation task and representation quality. However, these encoding methods are costly and usually sparse, meaning that there are better encoding techniques considering space and time complexity. The trace clustering literature has already experimented with several types of encoding methods. In [15] and [25], the authors adopt the one-hot encoding technique to transform traces before the clustering step. In [5], the authors employ edit distance to compute the trace distance preceding the clustering. Koninck et al. [19] used log footprints, i.e., control-ﬂow relations depicting activity sequences. In [10], the authors apply activity proﬁles, bi-gram and tri-gram as methods for trace encoding. Nonetheless, Leoni et al. [11] pointed that no trace similarity measure is general enough to be applicable in all scenarios. In this work, we adopt four encoding techniques that were frequently applied in the context of trace clustering. The ﬁrst one is one-hot encoding. This technique encodes activities as categorical dimensions, creating a feature vector of binary values for each trace, based on the occurrence of activities in a trace. Next, we adopt n-grams, a common technique used in text mining applications. This encoding maps groups of activities of size n into a feature vector, accounting for their occurrence or not. More speciﬁcally, we apply bi-gram and tri-gram. Finally, we applied position proﬁles [7], an approach that relates activity frequency and position. A log proﬁle is created by computing the activity appearances in each trace position and its respective frequency. It follows that a trace is encoded considering the frequency of its activities in their positions according to the log proﬁle. We selected three clustering techniques commonly applied in data mining and trace clustering literature. These techniques are grounded in diﬀerent heuristics, that is, each algorithm approaches the clustering problem from a unique perspective. With this, we aim at evaluating if a particular clustering structure outperforms the others. First, we adopt the Density-based Spatial Clustering of Applications with Noise (dbscan) algorithm [13]. The dbscan method guides its clustering based on the density of the feature space, hence, instances in high-density regions form a cluster while instances sitting at low-density regions are regarded as outliers. The main hyperparameter aﬀecting the clustering results is eps, which regulates the maximum distance between two points for them to be considered of the same neighborhood. We explore diﬀerent conﬁgurations of the eps hyperparameter to evaluate its impact and to recommend the best conﬁguration in the metamodel step. For that, we apply the following eps values: 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50. Moreover, we adopt k -means [21], a clustering technique that randomly selects centroids, which are the initial cluster points, and works by iteratively optimizing the centroid positions. The optimization stops either when centroid positions are stable or when the maximum number of repetitions is achieved. The k -means technique requires the expected number of clusters (k ) from a given data set as a hyperparameter. We set k to these values: 2, 3, 4, 5, 6, 7, 8, 9, 10. Finally, the last technique is agglomerative clustering [29], a type of hierarchical clustering with a bottom-up approach. The algorithm starts by considering each point as a cluster. Further, it merges the clusters as the hierarchy moves up, creating a tree-like structure depicting the cluster levels and merges. Cluster pairs are merged given a linkage distance, two clusters that minimally impact the linkage distance are merged recursively. As with k -means, agglomerative clustering requires the number of clusters to be found, we then adopted the same range of values for the k parameter. To complete the creation of a meta-database, meta-targets must be deﬁned for each meta-instance. This way, a ranking strategy is required to compare both encoding and clustering techniques. Hence, the technique sitting at the top of the ranking strategy is the one recommended for a meta-instance, i.e., it is deﬁned as the meta-target. As pointed out in the literature [4,11], there is no unique solution for a problem that outperforms the others from all perspectives. Considering this hypothesis, we propose three complementary metrics to evaluate trace clustering solutions, this way, capturing diﬀerent degrees of performance. Moreover, a user applying a trace clustering solution may expect to evaluate the results from several perspectives. Here, we support such a user by assessing clustering quality from a set of criteria. Silhouette coeﬃcient, the ﬁrst metric we propose to measure performance, is based on the traditional clustering literature, commonly applied in data mining domains [24]. The Silhouette score is computed at the cluster level to capture its tightness and separation, judging instances that ﬁt their cluster or are in between diﬀerent clusters. The scores of a group of clusters can be combined to assess the relative quality of the clustering technique. Equation 1 demonstrates how the Silhouette coeﬃcient (s) is obtained for a single sample considering the mean intra-cluster distance (a) and the mean nearest-cluster distance (b). The average of the Silhouette score for all samples is the ﬁnal coeﬃcient for one clustering space, that is, the average of Equation 1 for all samples in the feature space. The Silhouette coeﬃcient domain is [−1, 1], where −1 is the worst value, 0 indicates overlapping clusters and 1 is the best value. To complement this evaluation with a PM-inspired metric, we propose to measure the quality of clusters concerning trace variants. This way, by computing the trace variant frequency in each cluster, we can evaluate if the solution provides a clear separation of variants in the feature space. For that, we compute the unique traces in a cluster, and by a weighted mean, the Variant score is reached. Consider C the cluster of index i, C the group of all clusters, #variants the number of unique traces found in cluster C and #traces the total number of traces in the event log, Equation 2 depicts the Variant score calculation, 0 is the optimal value. As resource consumption is an important aspect in organizations, we also consider the computational time (t) of clustering as a metric to assess its quality. The lower the t metric for a particular solution, the better it is ranked in comparison to others. Given this set of metrics, i.e. s for silhouette coeﬃcient, v for variant score, and t for computational time, a meta-target hencoding, clustering, hyperparametersi has to successfully balance between all metrics to be considered a good set. This way, our approach rewards techniques that excel in the three metrics, such as ignoring one or more may lead to lack of tightness, improper variant identiﬁcation, and high resource consumption. Hence, we propose a ranking strategy (R) that combines all dimensions. Table 1 presents an example of the ranking strategy we propose. For each pair of encoding techniques and clustering algorithms, we apply it for a given event log (L) and measure the quality metrics (s, v, t). Following, a rank is built for each metric (R , R , R ), i.e, comparing the pairs of encodings and clustering in each dimension. Finally, a rank (R) is computed by the average of the metrics ranks. For example, considering the pairs hE , C i, hE , C i and hE , C i, their respective ﬁnal ranks are 2, 1.67 and 2.33. The solution chosen as the meta-target is the one that minimizes the R function, in this example the pair hE , C i. Table 1: Example of ranking encoding and clustering pairs. The ﬁnal rank function is the average rank of each quality dimension. Before introducing the meta-learner used to create the meta-model, we ﬁrst provide some particular details about the type of problem faced in our AutoML framework. As presented in Section 5.4, we need indeed to suggest both the best trace clustering algorithm and the best trace encoding technique. This implies that given a meta-instance the system recommends the tuple that achieves the maximum performance for the combined metrics. Most research in supervised learning proposes algorithms for single-label problems, where instances are associated with a single label λ from a set of disjoint labels L. However, in the proposed setup, we are facing a multi-output problem, where a set of labels Y ⊆ L is associated with a single instance [27]. Following the taxonomy proposed by Tsoumakas et al. [27], we adopt a problem transformation approach, which converts the data into a format that can be used in conjunction with traditional techniques. More speciﬁcally, we employed the Binary Relevance (BR) transformation approach. BR works by transforming the original data set into q data sets D , where j = [1, ..., q] contains all instances of the original data that are labeled according to the existence or not of λ . Thus, BR learns q binary classiﬁers, one for each label L. Given a new instance, BR provides the union of the labels λ predicted by the q classiﬁers. Regarding the meta-learner, we applied the Random Forest (RF) algorithm [6] due to its robustness, being less prone to overﬁtting. RF creates a collection of decision trees with a bagging technique, i.e., randomly selecting features for each tree. This way, our meta-model combines the RF with the BR approach. Moreover, we applied a simple hyperparameter tuning technique to improve performance in the recommendation task. For that, we divided the meta-database into three sets: train, validation, and test, respectively containing 80%, 10%, and 10% of the total number of meta-instances. The grid search strategy was used for tuning. This method exhaustively evaluates all combinations of chosen hyperparameters and uses cross-validation splitting to capture an average performance. The results reported in Section 6 were extracted when applying the tuned meta-model to the test set. The hyperparameters tuned were: (i) the number of trees composing the forest, (ii) the criterion measuring split quality, (iii) the minimum required number of samples for a node split, (iv) the minimum number of samples required to be a leaf node, and (v) the number of considered features for a split. <title>6 Results and Discussion</title> In this section, we present and discuss the main experimental results regarding the proposed strategy to recommend a trace clustering pipeline based on AutoML. We started by exploring the meta-database composition by observing the encoding techniques and clustering algorithms chosen by their performance and balancing. Next, an overall analysis, including the comparison of the proposed strategy with the baselines (random and majority), is introduced, while a detailed assessment of meta-features is presented in the last part. The results, considering all algorithms for setting the meta-database, including the metrics used for ranking the meta-targets, are presented in Fig. 2. The heatmap plots show the ranking of the metrics s, v, and t for encoding (Fig. 2a) and clustering (Fig. 2b) used to sort and identify promising algorithms as metatargets. Each ranking varies from 1 to 81, in which 1 is the best-ranked algorithm for a given metric. Fig. 2: Ranking of encoding (a) and clustering (b) to identify the meta-target. Color variation represents the variation of ranking position. Observing the encoding techniques (Fig. 2a), it is possible to identify a large discrepancy between them when evaluated by Silhouette, revealing the superiority of one-hot and position proﬁle algorithms. Variant score and Time do not present a prominent variation such as Silhouette, leading to closer ranking positions. Based on these results, it is possible to support the hypothesis of the “no free lunch theorem” due to the ranking balance since there is no best technique for all quality criteria concurrently. However, when observing the clustering algorithms (Fig. 2b), it is possible to note a balance regarding Silhouette, whereas Variant score and Time reveal discrepancies. The ﬁrst one, Variant score, exposes the importance of hyperparameter deﬁnition since agglomerative and k-means ranged throughout the rankings, when changing their hyperparameter k. Moreover, the Time metric delivered an important perspective, in which each clustering algorithm is recognizable regardless of its hyperparameters. In particular, agglomerative and dbscan were superior to k -means. This superiority led to no usage of k-means as a clustering meta-target. The meta-database was built using the combination of the top ranked algorithms for each meta-instance (event logs). This combination leads to an imbalanced multi-output dataset, which was handled to support the induction of the meta-model. This imbalanced scenario can be seen in Fig. 3, where combinations such as one-hot enconding with agglomerative clustering using 10 as k value (onehot agglomerative k10) represented 469 meta-instances. The second most frequent combination was position proﬁle with agglomerative clustering using 10 as k value (position profile agglomerative k10), reaching 171 meta-targets. The third was one-hot using dbscan adopting a eps equals 0.001 (onehot dbscan eps0.001) in 125 meta-instances. Fig. 3 represents in blue the one-hot combinations, in pink the position proﬁle, bi-gram is brown and trigram gray. The domination of one-hot, followed by position proﬁle and bi-gram is evident. Tri-gram was the best one, combined with dbscan, only with four meta-instances. Fig. 3: The combinations of encoding techniques and clustering algorithms are links, which represents a meta-instance that best ﬁt linked algorithms colored by encoding. When evaluating from an encoding perspective (Fig. 4), we observe a balance between dbscan with a wide range of eps and agglomerative using k equals 10. Diﬀerent values of k for agglomerative did not meet many meta-instances. Conversely, dbscan demonstrate the necessity of hyperparameter adjustments since diﬀerent values of eps could match particular meta-instances. Fig. 4: The combinations of encoding and clustering algorithms are links, which represents a meta-instance that best ﬁt linked algorithms colored by clustering. The imbalance issue was addressed by removing the minority classes combinations, that is, pairs of encoding techniques and clustering algorithms that appear as a meta-target for less than ﬁve meta-instances. The ﬁnal meta-database was composed of 1036 samples, with ﬁfteen diﬀerent combinations of one-hot, position proﬁle, and bi-gram with agglomerative (k in {8, 9, 10}) and dbscan (eps in {0.001, 0.005, 0.05, 0.01, 0.1, 0.5, 1}). Using RF as our meta-model built over the meta-database, we analyzed the performance for both encoding and clustering algorithm recommendations. It is worth mentioning that our problem was modeled as a multi-output problem, addressing encoding and clustering at once, taking advantage of possible intercorrelations between both steps. Our proposal obtained an F1 of 0.77 (±0.01) when recommending the encoding technique and an F1 of 0.61 (±0.01) for clustering algorithm recommendation. To bring insights on the performance achieved, we compared the results with the majority classes (one-hot as encoding technique and agglomerative k10 as clustering algorithm) and with a random selection, seen in Fig. 5. The majority baseline for encoding obtained an F1 of 0.70 (±0.00). The random baseline for encoding achieved 0.41 (±0.04) of F1. Considering clustering, the majority obtained 0.48 (±0.00) of F1 and random selection reached 0.14 (±0.04), respectively. Regarding the mean predictive performance in terms of F1, for the whole trace clustering pipeline, our proposed AutoML approach obtained 0.69 (±0.07). The results were superior to the majority and random baselines, which achieved 0.59 (±0.11) and 0.28 (±0.15), respectively. Note that the majority results are boosted by the imbalanced scenario, for balanced meta-databases, the tendency is to underperform. Fig. 5: Performance of the AutoML framework to recommend the encoding technique and clustering algorithm in terms of accuracy and F1. We interpreted the outputs of our meta-model to predict encoding (Fig. 6) and clustering (Fig. 7) by taking the average absolute value of the Shapley Additive Explanation (SHAP) values. The higher relative importance for predicting encoding algorithms were obtained by the number of events (n events), the maximum number of activities (activities max), and the entropy of trace length (trace len entropy). Similarly, the top three meta-features in terms of importance were n events, activities max and trace len entropy when predicting clustering. It is important to mention that n events represented more than half of the importance among all meta-features for both encoding and clustering. The n events meta-feature indirectly indicates log complexity as the higher the number of events, the more heterogeneous behavior might appear, even more when considering that many of the logs come from complex models and include anomalies. Thus, n events becomes an important discriminator for encoding and clustering performances. These results highlight relevant directions for future research in feature extraction in PM. <title>7 Conclusion</title> In this paper, we proposed an AutoML framework to recommend the best pipeline for trace clustering based on a speciﬁc event log. For that, we extract meta-features to describe event logs and matched them with the best clustering pipeline by assessing three complementary metrics (Silhouette, Variant score, and Time). The framework recommends a tuple hencoding, clustering, hyperparametersi, making trace clustering solutions accessible for non-expert users. Fig. 6: The relative importance for each feature, obtained by taking the average absolute value of the SHAP values when recommending encoding algorithms. Fig. 7: The relative importance for each feature, obtained by taking the average absolute value of the SHAP values when recommending clustering algorithms. Results have shown that the framework outperforms baseline approaches. We have also provided a discussion about meta-feature inﬂuence in the decision process using SHAP values. In future research, we aim to extend the experimental evaluation to gather further insights into the relationship between trace clustering quality and event log behavior. <title>References</title>