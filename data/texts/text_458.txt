Online recommendation requires handling rapidly changing user preferences. Deep reinforcement learning (DRL) is an eective means of capturing users’ dynamic interest during interactions with recommender systems. Generally, it is challenging to train a DRL agent, due to large state space (e.g., user-item rating matrix and user proles), action space (e.g., candidate items), and sparse rewards. Existing studies leverage experience replay (ER) to let an agent learn from past experience. However, they adapt poorly to the complex environment of online recommender systems and are inecient in determining an optimal strategy from past experience. To address these issues, we design a novel state-aware experience replay model, which selectively selects the most relevant, salient experiences, and recommends the agent with the optimal policy for online recommendation. In particular, the model uses locality-sensitive hashing to map high dimensional data into low-dimensional representations and a prioritized reward-driven strategy to replay more valuable experience at a higher chance. Experiments on three online simulation platforms demonstrate our model’s feasibility and superiority to several existing experience replay methods. Recommender System, Deep Reinforcement Learning, Experience Replay Online recommendation aims to learn users’ preferences and recommend items dynamically to help users nd desired items in highly dynamic environments [35]. Deep reinforcement learning (DRL) naturally ts online recommendation as it learns policies through interactions with the environment via maximizing a cumulative reward. Besides, DRL has been widely applied to sequential decision-making (e.g. in Atari [22] and AlphaGo [31]) and achieved remarkable progress. Therefore, it is increasing applied for enhancing online recommender systems [2, 4, 37]. DRL-based recommender systems cover three categories of methods: deep Q-learning (DQN), policy gradient, and hybrid methods. DQN aims to nd the best step via maximizing a Q-value over all possible actions. As the representatives, Zheng et al. [40]introduced DRL into recommender systems for news recommendation; Chen et al. [4]introduced a robust reward function to Q-learning, which stabilized the reward in online recommendation. Despite the capability of fast-indexing in selecting a discrete action, Qlearning-based methods conduct the “maximize" operation over the action space (i.e., all available items) and suer from the stuck agent problem [8]—the “maximize" operation becomes unfeasible when the action space has high dimensionality (e.g., 100,000 items form a 10k-dimensional action space) [3]. Policy-gradient-based methods use the average reward as guideline to mitigate the stuck agent problem [3]. However, they are prone to converge to suboptimality [25]. While both DQN and policy gradient are more suitable for small action and state spaces [19,33] in a recommendation context, hybrid methods [3,8,13,38] has the capability to map large high-dimensional discrete state spaces into low-dimensional continuous spaces via combines the advantages of Q-learning and policy gradient. A typical hybrid method is the actor-critic network [18], which adopts policy gradient on an actor network and Q-learning on a critic network to achieve Nash equilibrium on both networks. Actor-critic networks have been widely applied to DRL-based recommender systems [5, 20]. Existing DRL-based recommendation methods except policygradient-based ones rely heavily on experience replay to learn from previous experience, avoid re-traversal of the state-action space, and stabilize the training on large, sparse state and action spaces [34]. They generally require long training time, thus suering from the training ineciency problem. Further more, in contrast to the larger, diverse pool of continuous actions required in recommendation tasks, existing experience replay methods are mostly designed for games with a small pool of discrete actions. Therefore, a straightforward application of those methods may result in strong biases during the policy learning process [12], thus impeding the generalization of optimal recommendation results. For example, Schaul et al. [28]assume that not every experience is worth replaying and propose a prioritized experience replay (PER) method to replay only the experience with the largest temporal dierence error. Sun et al. [32]propose attentive experience replay (AER), which introduces similarity measurement into PER to boost the eciency of nding similar states’ experience, but attention mechanisms cause ineciency on large sized state and action spaces [17]. We present a novel experience replay structure, Locality-Sensitive Experience Replay (LSER), to address the above challenges. Diering from existing approaches, which apply random or uniform sampling, LSER samples experiences based on expected states. Inspired by collaborative ltering (which measures the similarity between users and items to make recommendations) and AER [32], LSER only replays experience from similar states to improve the sampling eciency. Specically, we introduce a ranking mechanism to prioritize replays and promote the higher reward experiences. We further use𝜖-greedy method to avoid replaying high-rears states excessively. Considering the high-dimensionality of vectorized representations of states, We convert similarity measurement for highdimensional data into a hash key matching problem and employ locality-sensitive hashing to transform states into low-dimensional representations. Then, we assign similar vectors the same hash codes (based on the property of locality-sensitive hashing). Such a transformation reduces all the states into low dimension hash keys. In summary, we make the following contributions in this paper: •We propose a novel experience replay method (LSER) for reinforcement-learning-based online recommendation. It employs a similarity measurement to improve training eciency. •LSER replays experience based on the similarity level of the given state and the stored states; the agent thus has a higher chance to learn valuable information than it does with uniform sampling. •The experiments on three platforms, VirtualTB, RecSim and RecoGym, demonstrate the ecacy and superiority of LSER to several state-of-the-art experience replay methods. In this section, we will briey introduce the proposed LSER method with theoretical analysis. The overall structure of using LSER in DRL RS can be found in Figure 1. Online Recommendation aims to nd a solution that best reects real-time interactions between users and the recommender system and apply the solution to the recommendation policy. The system needs to analyze users’ behaviors and update the recommend policy dynamically. In particular, reinforcement learning-based recommendation learns from interactions through a Markov Decision Process (MDP). Given a recommendation problem consisting of a set of users U = {𝑢, 𝑢, · · ·𝑢}, a set of itemsI = {𝑖, 𝑖, · · · 𝑖}and user’s demographic informationD = {𝑑, 𝑑, · · · , 𝑑}, MDP can be represented as a tuple(S, A, P, R,𝛾), whereSdenotes the state space (i.e., the combination of the subsets ofIand its corresponding user information)Adenotes the action space, which represents agent’s selection during recommendation based on the state spaceS,P denotes the set of transition probabilities for state transfer based on the action received,Ris a set of rewards received from users, which are used to evaluate the action taken by the recommender system (each reward is a binary value to indicate whether user has clicked the recommended item or not), and𝛾is a discount factor 𝛾 ∈ [0, 1] for the trade-o between future and current rewards. Given a user𝑢and an initial state𝑠observed by the agent (or the recommender system), which includes a subset of item setIand user’s prole information𝑑, a typical recommendation iteration for the user goes as follows: rst, the agent takes an action𝑎 based on the recommend policy𝜋under the observed state𝑠 and receives the corresponding reward𝑟—the reward𝑟is the numerical representation for user’s behavior such as click through or not; then, the agent generates a new policy𝜋based on the received reward𝑟and determines the new state𝑠based on the probability distribution𝑝 (𝑠|𝑠, 𝑎) ∈ P. The cumulative reward (denoted by𝑟) after𝑘iterations from the initial state is as follows: DRL-based recommender systems uses a replay buer to store and replay old experience for training. Given the large state and action space in a recommender system, not every experiences are worth to replay [6]—replaying experience that does not contain useful information will increase the training time signicantly and introduce extra uncertainty to convergence. Hence, it is reasonable to prioritize replay important experience for DRL recommender systems. The ideal criterion for measuring the importance of a transition in RL is the amount of knowledge learnable from the transition in its current state [11,28]. State-of-the-art methods like AER are unsuitable for recommendation tasks that contain large, higher dimensional state and action spaces as their sampling strategies may not work properly. Thus, we propose a new experience replay method named Locality-sensitive experience replay (LSER) for online recommendation, which uses hashing for dimension reduction when sampling and storing the experiences. We formulate the storage and sampling issue in LSER as a similarity measure problem, where LSER stores similar states into the same buckets and samples similar experiences based on state similarities. A popular way of searching similar high-dimensional vectors in Euclidean space is Locality-Sensitive Hashing (LSH), which follows the idea of Approximate Nearest Neighbor (ANN) while allocating similar items into the same buckets to measure the similarity. However, standard LSH conducts bit-sampling on the Hamming space; it requires time-consuming transformation between the Euclidean space to the Hamming space, liable to lose information. Aiming at measuring the similarity between high-dimensional vectors without losing signicant information, we propose using𝑝-stable distribution [23] to conduct dimensionality reduction while preserving the original distance. This converts high-dimensional vectors (states) into low-dimensional representations easier to be handled by the similarity measure. To address possible hash collision (i.e., dissimilar features may be assigned into the same bucket and recognized as similar), we introduce the formal denition of the collision probability for LSH. Then, we theoretically analyze the collision probability for𝑝-stable distribution to prove that our method has a reasonable boundary for collision probability. Figure 1: The proposed LSER with DDPG. The environment provides the current state 𝑠 DDPG model; the action 𝑎can be obtained by 𝑎= 𝜋 (𝑠). 𝑟will be provided by the user (e.g. click or not). LSER takes 𝑠 the input and encodes it on the projective space. Given the enco ded states, LSER will return the most similar experience for DDPG to update he parameters. After that, this transition ℎ(𝑠) : (𝑠 Definition 1 (Collision probability for LSH in𝑝-stable distribution). Given an LSH functionℎ∈ Hand the probability density function (PDF) of the absolute value of the𝑝-stable (𝑝 ∈ [1, 2]) distribution𝑓(𝑡)in𝐿space, the collision probability for vectorsu and v is represented by: where 𝑐 = ∥u − v∥and 𝑤 is a user-dened xed distance measure. Here, we use a 2-state distribution, i.e., normal distribution for dimension reduction. We randomly initialize𝑛hyperplanes based on normal distribution on the projective spacePto get the hash representation for a given state𝑠, where𝑛is the dimension of the state. The hashing representationℎ(𝑠)for the given state𝑠is calculated as follows:( The collision probability of the above method can be represented as: 𝑃 = Pr [ℎ(u) = ℎ(v)] = 1 −𝐴𝑛𝑔(u, v)𝜋 where 𝐴𝑛𝑔(u, v) = arccos|u ∩ v|(3) Eq.(2) formulates the information loss during the projection, where we use term𝑒to represent the quantication between the real value𝑝 · 𝑣and hashed results induced fromℎ(v). Since the relative positions in original space are preserved during the hash transformation with an extra measurement𝑒, the upper bound and lower bound of collision probability boundary in projective space , 𝑎, 𝑠, 𝑟) will be stored. is guarantee to be intact. That means the more dissimilar states will not receive a higher probability to be allocated into the same hash result. Lemma 1. Given an arbitrary hash functionℎ∈ H, the collision probability for a given vector u and v is bounded at b oth ends. Proof.SincePr [ℎ(u) = ℎ(v)]monotonically decreases in𝑐for any hash function from the LSH familyH, the collision probability is bounded from above by𝑃𝑟 [ℎ(u) = ℎ(v)]for𝑐 −𝑒 and from below by 𝑃𝑟 [ℎ(u) = ℎ(v)] for 𝑐 + 𝑒. Then, we have the upper bound: ≤𝑃 +𝑒𝑤𝑞𝑓(𝑞) 𝑑𝑞 ≤ 𝑃 +𝑒𝑐 − 𝑒 and the lower bound: We compute the upper bound based on Hölder’s inequality in 𝐿space: Considering the 𝐿space, we have: We use the similar method in 𝐿to compute the lower bound: and in 𝐿: The collision probability𝑃𝑟 [ℎ(𝑢) = ℎ(𝑣)]is bounded from both ends as follows: 𝑃 − min𝑐 + 𝑒,2(𝑐 + 𝑒), 𝑃 + min𝑐 − 𝑒,2(𝑐 − 𝑒) Note that, when calculating the lower and upper bounds,𝑞representsand, respectively. The algorithm of LSER is shown in Algorithm 1. In the following, we demonstrate from two perspectives that LSER can nd the similar states eciency. First, we show the efcacy of LSER with theoretical guarantee, i.e., similar states can be sampled given the current state. We formulate ‘the sampling of similar states’ as a neighbor-nding problem in the projective space and provide a theoretical proof of the soundness of LSER. Given a set of statesS, and a query𝑞, LSER can quickly nd a state𝑠 ∈ Swithin distance𝑟or determine thatShas no states within distance𝑟. Based on existing work [15], the LSH family is(𝑟, 𝑟, 𝑝, 𝑝)-sensitive, i.e., we can nd a distributionHsuch that𝑝≥ 𝑃𝑟[ℎ(u) = ℎ(v)]whenuandvare similar and 𝑝≤ 𝑃𝑟[ℎ(u) = ℎ(v)] when u and v are dissimilar. Theorem 2. LetHbe(𝑟, 𝑟, 𝑝, 𝑝)-sensitive. Suppose𝑝> 1/𝑛 and𝑝> 1/𝑛, where𝑛is the size of data points. There exists a solution for the neighbor nding problem in LSER within𝑂 (𝑛𝑝log 𝑛) query time, and 𝑂 (𝑛𝑝) space. Proof.Assume𝑟, 𝑟, 𝑝, 𝑝are known,𝜌 =, and 𝑘 =where𝑘is the number of hash functions, and LSH initializes 𝐿 tables. Based on the denition in [15], we have: 𝑘𝐿 = 𝑘𝑝≤ 𝑘 (𝑒+ 1) ≤ 𝑘 (𝑛/𝑝+ 1) = 𝑂 (𝑛/𝑝log 𝑛) The space complexity is calculated as𝑂 (𝐿𝑛𝑑)where𝑑is the dimension of state𝑠. It can be written as𝑂 (𝑛/𝑝𝑑)(by applying 𝐿 = 𝑛/𝑝) and further simplied into 𝑂 (𝑛/𝑝). Then, we prove LSER can nd similar neighbors. The𝐿table can be classied into two categories: similar and dissimilar. Given a state𝑠, the similar category gives similar states while the dissimilar category provides dissimilar states. We split the two categories such that𝐿 = ⌊𝑛⌋ +⌈𝑚⌉and its corresponding⌊𝑘⌋,⌈𝑘⌉. Given any state 𝑠 ∈ Sin the distance𝑟, LSER must be able to nd the most similar states in a high probability—the query and the data need to share the same hash-bucket in one of the tables. The probability of their not sharing the same hash-bucket is (1 − 𝑝)(1 − 𝑝)≤ (1 − 𝑝)(1 − 𝑝)(5) where𝛼 =⌈𝑘⌉−𝑘. We have applied the denitions𝑝= 𝑝= 𝑛 for step 6 to step 7 and𝑛𝑝+𝑚𝑝= 𝑛for step (7) to (8). Finally, we get the probability of LSER’s getting the similar states as follows: Existing experience replay methods in DRL research assume that the recent experience is more informative than older experience. Therefore, they simply replace the oldest experience with the newest experience to update the experience buer in DRL-based recomender systems without further optimization. As such, some valuable experience might be discarded, i.e., catastrophic forgetting. In contrast, we design a state-aware reward-driven experience storage strategy, which removes the experience with the lowest reward—instead of following the First-In-First-Out (FIFO) strategy—when the replay buer is full. Formally speaking, a transition𝜏: (𝑠, 𝑎, 𝑠, 𝑟) will be stored in the replay buer based on the valueℎ(𝜏.𝑠). If the replay buer is full, the transition with the same value of ℎ(𝜏.𝑠)but lower reward will be replaced. In practice, an indicator𝑚is stored in the transition as well to indicate when the recommendation should terminate. Figure 2: Given a high dimensional space, three random hype-planes are initialized based on normal distribution. Each hype-plane splits the space into two hash areas 0 and 1. The space is split into six hash areas. We can nd that, states are encoded into a binary string e.g.,{111, 101, 011, 001, 000} Sampling strategy is another crucial component of LSER, which determines which experience should be selected for the agent to optimize in LSER. We propose a state-aware reward-driven sampling strategy that only replays the experience with the top-N highest rewards in the same hashing area; this way, the agent can quickly nd the correct direction for optimization. We call our sampling strategy ‘state-aware’ because we use a hash key to encode the state and replay the experience based on the hash key. Compared with uniform sampling, our strategy has a higher chance to replay the correct experience. Here, we illustrate how to address three related challenges faced by our sampling strategy: exploitation-vsexploration dilemma, bias annealing and non-existence dilemma. Exploitation vs. exploration dilemma. Exploitation and exploration dilemma is a well-known dilemma when training an agent for RL, including LSER. TWhile our reward-driven strategy forces the agent to exploit existing high-rewarding experiences, the agent may converge to a sub-optimal policy instead of the globally optimal one. We use a similar method to𝜖-greedy to achieve a trade-o between exploitation and exploration. LSER rst draws a random probability 𝑝 ∈ [0, 1]then uses reward-driven sampling if the probability less than a threshold,𝜖and random sampling otherwise. The threshold allows LSER to replay low priority experience to fulll the exploration requirement. Bias annealing. Prioritizing partial experiences among the replay buer may introduce inductive bias [28,32]—the training process is highly non-stationary (due to changing policies); even a small bias introduced by the sampling strategy may change the solution that the policy converges to. A common solution is to let the priority anneal periodically so that the agent can visit those less-replayed experiences. By using the threshold, our 𝜖-greedy method has the similar eect as annealing on allowing low-priority experiences to be replayed. Non-existence dilemma. When split the projective space into areas to initialize hyperplanes, some areas may not have any data points (esp. when the number of hyperplanes is large), causing the ‘non-existence dilemma’. Consequently, when a new transition comes, the algorithm will stop if no experience can be found on ℎ. We use the similarity measure to overcome this problem. Specically, we nd the two hash areas that are most similar to each other (based on currentℎ) and conduct sampling on those two states. We use Jaccard similarity to measure the similarity between hash codes𝐴, 𝐵. As such, LSER can always replay the relevant experience. We use Deep Deterministic Policy Gradient (DDPG) [19] as the training backbone. We choose an actor-critic network as the agent and train two parts of the actor-critic network simultaneously. The critic network aims to minimize the following loss function: where𝜃and𝜃are the critic and actor parameters,𝑁is the size of the mini-batch from the replay buer,𝜓and𝜙are the target critic and target actor network, respectively. We apply the OrnsteinUhlenbeck process in the action space to introduce perturbation; this encourages the agent to explore. The target network will be updated based on the corresponding hyper-parameter 𝜏. We conduct experiments on three widely used public simulation platforms: VirtualTB [30], RecSim [14] and RecoGym [27], which mimic online recommendations in real-world applications. VirtualTBis a real-time simulation platform for recommendation, where the agent recommend items based on users’ dynamic interests. VirtualTB uses a pre-trained generative adversarial imitation learning (GAIL) to generate dierent users who have both static interest and dynamic interest. It’s worth to mention that, the GAIL is pre-trained by using the real-world from Taobao, which is one of the largest online retail platforms in China. Moreover, the interactions between users and items are generated by GAIL as well. Benet from that, VirualTB can provide a large number of users and the corresponding interactions to simulate the real-world scenario. RecSimis a congurable platform for authoring simulation environments that naturally supports sequential interaction with users in recommender systems. RecSim diers from VirtualTB in containing dierent, simpler tasks but fewer users and items. There are two dierent tasks from RecSim, namely interest evolution and long-term satisfaction. The former (interest evolution) encourages the agent to explore and fulll the user’s interest without further exploitation; the latter (long-term satisfaction) depicts an environment where a user interacts with content characterized by the level of ‘clickbaitiness.’ Generally, clickbaity items lead to more engagement yet lower long-term satisfaction, while non-clickbaity items Algorithm 1: LSH memory by using dictionary input : Transition for storage 𝜏 : (𝑠, 𝑎,𝑚, 𝑠, 𝑟), = encode(𝑠); have the opposite eect. The challenge lies in balancing the two to achieve a long-term optimal trade-o under the partially observable dynamics of the system, where satisfaction is a latent variable that can only be inferred from the increase/decrease in engagement. RecoGymis a small Open AI gym-based platform, where users have no long-term goals. Dierent from RecSim and VirtualTB, RecoGym is designed for computational advertising. Similar with RecSim, RecoGym uses the click or not to represent the reward signal. Moreover, similar with RecSim, users in those two environments do not contain any dynamic interests. Considering RecoGym and RecSim have limited data points and do not consider users’ dynamic interests, we select VirtualTB as the main platform for evaluations. Our model is implemented in Pytorch [26] and all experiments are conducted on a server with two Intel Xeon CPU E5-2697 v2 CPUs with 6 NVIDIA TITAN X Pascal GPUs, 2 NVIDIA TITAN RTX and 768 GB memory. We use two two-hidden-layer neural networks with 128 hidden unit as the actor network and the critic network, respectively.𝜏,𝛾, and𝑐are set to 0.001, 0.99 and 1𝑒, respectively, during experiments. The evaluation metrics are environment-specic. For VirtualTB and RecoGym, click-through rate is used as the main evaluation metric. For RecSim, we use the built-in metric, which is a quality score, as the main evaluation metric. We compare our method with the following baselines. •Prioritized Experience Replay (PER) [28]: an experience replay method for discrete control, which uses TD-error to rank experience and a re-weighting method to conduct the bias annealing. •Dynamic Experience Replay (DER) [21]: an experience replay method designed for imitation learning, where stores both human demonstrations and previous experience. Those experiences are selected randomly without any priority. •Attentive Experience Replay (AER) [32]: an experience replay method that uses attention to calculate the similarly for boosting sample eciency with PER. •Selective Experience Replay (SER) [16]: an experience replay method for lifelong machine learning, which employs LSTM as the experience buer and selectively stores experience. •Hindsight Experience Replay (HER) [1]: an experience replay method that replays two experience (one successful, one unsuccessful) each time. For AER, PER, SER and HER, We use the same training strategy as LSER. For DER, we use its original structure to run experiments without human demonstrations. The size of the replay buer is set to1, 000, 000for VirtualTB and10, 000for RecSim and RecoGym. The number of episodes for our experiments is set to90, 000for VirtualTB and1, 000for RecSim and RecoGym. Note that only PER, AER and SER contains a prioritize operation to rank or store the experience. Results for the three platforms (Fig 3) demonstrate our method (LSER) outperformed the baselines: LSER yields signicant improvements on VirtualTB, which is a large and sparse environment; while AER, DER, PER and SER nd a correct policy within around50, 000 episodes, ours takes around30, 000episodes; HER does not perform well because it introduces too much failed experience and has a slow learning process; DER introduces the human demonstration Figure 3: Result comparison with four baseline methods on VirtualTB, RecSim and RecoGym. The experiments are repeated ve times, and mean values are reported. 95% condence intervals are shown. (a) is the result for VirtualTB; (b) is the result for long-term satisfaction in RecSim; (c) is the result for interest evolution in RecSim; (d) is the result for RecoGym; (e) is the result for 𝜖 study; (f) is the ablation study to show the eectiveness for each component into the vanilla ER which is hard to acquire for recommendation task. Applying PER to DDPG slightly outperforms applying DER to DDPG, which is consistent with observations by previous work [24, 32]. As PER was originally designed for Deep Q-learning, it uses the high TD-error to indicate the high informative experience for the value-network. When applying PER into DDPG, which is an actor-critic based algorithm, the sampled experience is also used to update the policy network. Those experiences with high TD-error normally diverge far away from the current policy and harm the updates of the policy-network. In contrast, LSER selects experience according to the similary with the current state. This preference for on-distribution states tends to discard experiences that contain old states and stabilize the training process of the policy network. AER does not perform as well as PER in VirtualTB because it heavily relies on the attention mechanism to calculate the similarity score between states. LSER’s𝜖-greedy method can enforce agent to do more exploration when user’s interest shift. All methods gained similar results on RecSim and RecoGym because all methods can iterate all possible combinations of states and actions. Fig. 3b, 3c and 3d show that LSER is slightly better and more stable than the baselines on RecSim and RecoGym. Since the two platforms are quite small, similarity matching and𝜖-greedy do not signicantly improve performance. We report the running time of the selected experience replay methods in Table 1 to evaluate the eciency of LSER. LSER outperforms all While performing poorly on RecSim and RecoGym, it is faster than most of the baselines. In comparison, LSER introduces extra running time in small environments (e.g, RecSim and RecoGym) than large environments. For VirutalTB, AER takes much longer time than all other methods, due to attention calculation [17]. We further investigate the eect of LSER’s store and sampling strategy by replacing our store strategy with the normal strategy and our sampling strategy with random sampling. The results of our ablation study are shown in Fig. 3f, where LSER-P denotes LSER with the replaced store strategy and LSER-S denotes the LSER with the replaced sampling strategy. We found the sample strategy played the most critical role in achieving good performance, as LSER-S underperformed LSER signicantly. The store strategy also contributed to the better performance. LSER-P was less stable Table 1: Comparison of running time for DER, PER, SER, AER and LSER coupling with DDPG in three dierent environments when running the experiments in 90, 000 episodes (indicated by a wider error bar). but outperformed LSER at∼ 30, 000 episodes, due to occurrence of sub-optimal policies. In our method, the number of hyperplanes is critical to determine the length of the result hash-bits of a given state. Longer hashbits can provide more accurate similarity measurement result but low eciency, while shorter hash-bits can increase eciency but decrease the accuracy. It’s a trade-o which needs a middle-point to balance between eciency and accuracy. We want to answer the following question:“Does increase the hyperplanes always boost the recommendation performance?” and nd out the optimal number. We report the experimental results in VirtualTB, where we evaluate the eect by varying number hyperplanes in LSER (shown in Fig 4). The results on the other two platforms show the similar pattern. The performance gradually increases with more hyperplanes, Figure 4: Performance Comparison of dierent number of hyperplanes but it levels o or even drops when number of hyperplanes reaches 20. Fig 3a shows LSER suers instability after reaching the rst peak at episode∼ 50, 000. Dierent from the other methods, LSER can quickly reach the optimal policy but suers uctuation. That indicates𝜖-greedy tends to lead the agent towards learning from low-priority experience after the optimal policy is reached. We alleviate the issue by adjusting the value of𝜖. Here, we tried𝜖 = {0, 0.9, 0.99, 1}to determine the best choice of the𝜖on VirtualTB. The results are shown in Fig. 3e, where𝜖 = 1corresponds to greedy sampling while𝜖 = 0refers to randomly sampling. Besides, we provide an intervention strategy to stabilize the training process—the agent will stop exploration once the test reward is higher than a reward threshold𝑇. This strategy allows the agent to nd an nearoptimal policy at an early stage. We examined the performance under 𝑇=0.95, which delivers a better training process. Zhao et al. [38]rst introduced DRL to recommender systems Zhao et al. [38]. They use DQN to embed user and item information for news recommendaion, where Vanilla ER is used to help the agent learn from past experience. And until present, most methods use only vanilla ER, which uniformly samples experiences from the replay buer. Among them, Zhao et al. [39]apply DQN to online recommendation and RNN to generate state embeddings; Chen et al. [4]point out that DQN receives unstable rewards in dynamic environments such as online recommendation and may harm the agent; Chen et al. [3]found that traditional methods like DQN become intractable when the state becomes higher-dimensional; DPG addresses the intractability by mapping high-dimensional discrete state into low-dimensional continuous state [5, 36]. Intuitively, some instances are more important than others; so a better experience replay strategy is to sampling experiences according to how much current agent can learn from each of them. While such a measure is not directly accessible, proxies propose to retain experiences in the replay buer or to sample experiences from the buer. Replay strategies reply on optimization objectives. In simple continuous control tasks,experience replay should contain experiences that are not close to the current policy to prevent tting to local minima, and the best replay distribution is in between an on-policy distribution and uniform distribution [7]. However, they De Bruin et al. [7]also note that such a heuristic is unsuitable for complex tasks where policies are updated for many iterations. In DRL problems, when the rewards are sparse, the agent can learn from failed experiences by replacing the original goals with states in reproduced articial successful trajectories [1] For complex control tasks, PER [28] measures the importance of experiences using the TD-error and designs a customized importance sampling strategy to avoid the eect of bias. Based on that, Ref-ER [24] actively enforces the similarity between policy and the experience in the replay buer, considering on-policy transitions are more useful for training the current policy. AER [32] is an experience replay method that combines the advantages from PER and Ref-ER. It uses attention score to indicate state similarity and replays those experiences awarded high similarity with high priority. All aforementioned work focuses on optimizing the sampling strategy, aiming to select the salient and relevant agent’s experiences in replay buer eectively. Selective experience replay (SER) [16], in contrast, aims to optimize the storing process to ensure only valuable experience will be stored. The main idea is to use an Long-short term memory (LSTM) network to store only useful experience. In this paper, we propose state-aware reward-driven experience replay (LSER) to address the sub-optimality and training instability issues with reinforcement learning for online recommender systems. Instead of focusing on improving the sample eciency for discrete tasks, LSER considers online recommendation as a continuous task; it then uses locality-sensitive hashing to determine state similarity and reward for ecient experience replay. Our evaluation of LSER against several state-of-the-art experience-replay methods on three benchmarks (VirtualTB, RecSim and RecoGym) demonstrate LSER’s feasibility and superior performance. In the future, we will explore new solutions for improving stability, such as better optimizers to help the agent get rid of saddle points, new algorithms to stabilize the training for DDPG, and trust region policy optimization to increase training stability [29]. Moreover, more advance reinforcement learning algorithms could be used to replace the DDPG such as soft actor-critic (SAC) [10] or Twin Delayed Deep Deterministic (TD3) [9].