The key to personalized news recommendation is to match the user’s interests with the candidate news precisely and eciently. Most existing approaches embed user interests into a representation vector then recommend by comparing it with the candidate news vector. In such a workow, ne-grained matching signals may be lost. Recent studies try to cover that by modeling ne-grained interactions between the candidate news and each browsed news article of the user. Despite the eectiveness improvement, these models suer from much higher computation costs online. Consequently, it remains a tough issue to take advantage of eective interactions in an ecient way. To address this problem, we proposed an endto-endSelectiveFine-grainedInteraction framework (SFI) with a learning-to-select mechanism. Instead of feeding all historical news into interaction, SFI can quickly select informative historical news w.r.t. the candidate and exclude others from following computations. We empower the selection to be both sparse and automatic, which guarantees eciency and eectiveness respectively. Extensive experiments on the publicly available dataset MIND validates the superiority of SFI over the state-of-the-art methods: with only ve historical news selected, it can signicantly improve the AUC by 2.17% over the state-of-the-art interaction-based models; at the same time, it is four times faster. Nowadays, people are overwhelmed with information, exhausted to seek things they’re interested in. Online news platforms e.g. MSN Newsgreatly alleviate this information overload problem by recommending news articles according to user’s specic interests [16,19,31,41]. The key technology of these news platforms is personalized news recommendation [13]. Due to the particular large-scale and time-sensitive property of news, the news recommenders must be both eective and ecient so that it can be deployed in real production systems. Figure 1: Example of a user’s behavior log in MSN. Three historical news articles of the user are shown andcis the news she actually clicked afterward. The text marke d in the same color is the ne-grained (term-term) matching signals. The darker the color, the more they match. A lot of existing news recommendation approaches [1,1,16,21, 29–33,39] follow a representation-based matching strategy. They learn a representation vector for the candidate news and encode the user’s history news into a vector to form the user representation in the same semantic space. The matching score between these vectors is calculated as the click probability. However, the user vector is an aggregation of multiple historical articles, so it hardly keeps the ne-grained information and may contain noise in the articles. For example in Figure 1, the candidate news matches user’s ne-grained interests Biden and Hillary (ne-grained interaction happens), which motivates the user’s current click. Unfortunately, the aggregated user vector mixes all terms inh, handhand blurs these ne-grained interests, thus degrades the capacity of user modeling. Even worse, noises such as wind and wildres are also included and they are unrelated to the current click. Though some recent “multi-channel” methods [2,20] attempt to cover richer information by maintaining multiple representation vectors, they are still limited to modeling ne-grained interaction in an explicit and reliable manner. In order to capture ne-grained matching signals between the candidate news and the user, Wang et al. [26] proposed an interactionbased model. It computes similarity matrices between the candidate news and every historical news piece of the user at word level to derive the click probability. Despite the eectiveness improvement, the model is especially slow. It has to recompute term-level interaction matrices with every historical article when scoring each candidate, which is far more expensive than dot product used in representation-based methods. Intuitively,we shouldn’t involve all the historical articles in interaction. For example,hshould be excluded within this click because it is irrelevant to the current candidatecand there would be no interactions between them. Tailoring the user history to several recent browsed news items seems to be a straightforward solution to save eciency. However, blindly interacting with only the latest browsed news limits the recommendation eectiveness, mainly because: 1) When the length of the kept browsing history is short, there isn’t sucient news for the model to learn the user’s interests well. Back to the Figure 1, if we cut o earlier history news hand h, the interaction quality would greatly decrease since the most informative one, h, is lost. 2) When the capacity becomes larger, irrelevant historical news, such as h, is involved. As mentioned above, such news is harmful to the recommendation accuracy. It hardly contributes to motivate the click and would act like noise, reducing the matching score of the candidate that the user’s truly interested in. To tackle the above problem, in this paper, we proposeSFI, a SelectiveFine-grainedInteraction framework. The key idea of it is toselect a small number of historical news articles with higher informativeness, then perform ne-grained interaction over them only. The biggest challenge of SFI is to select the informative history news sparsely, precisely, and eciently. Most previous works about feature selection employ gating operator [7,18,40]. But it cannot be directly used in SFI because gating doesn’t eliminate zero entries so the total computations are not lessened. Two-stage training [8] is not desirable either, because no ground-truth label (indicates which historical news interacts with the candidate) is available to train the selector. Last but not least, we’d better manipulate news-level representations to guide selection for the sake of eciency. In this work, we design thelearning-to-selectmechanism to fulll all our goals. Specically, SFI learns a selection vector for each news article, and computes the cosine similarity between candidate news and every history news in the selection space, taking the result as the informativeness of each historical article. Next, we design two successive selection networks. The hard-selection network enforces sparsity. It selects𝐾most informative news and excludes others from following interactions. Within the output of the hardselection, the following soft-selection network masks news whose informativeness is below a given threshold and attaches dierent weights to the unmasked ones. This renement allows the gradient ow through to optimize the selection vectors, so that the model can learn to highlight valuable features for selection hence achieve higher eectiveness. Extensive experiments on the publicly available dataset MIND show that SFI outperforms all baselines in terms of both eectiveness and eciency: with only ve historical news articles selected, it signicantly improves the recommendation eectiveness by 2.17% over the state-of-the-art interaction-based models with four times faster speed (almost reaches the fastest speed of representationbased methods and outperforms it by 2.71% in AUC). We also comprehensively compare SFI with its naive recent𝐾counterpart and investigate the eciency eectiveness trade-o brought by SFI. The main contributions of this paper can be summarized into three aspects: (1) We propose SFI, a selective ne-grained interaction framework, to take full advantage of the ne-grained interaction in a highly ecient way. (2) We design the learning-to-select mechanism to sparsely and automatically select informative historical news w.r.t. the candidate. (3) We conduct extensive ablation studies to verify the advantage of selection; and further investigate the eciency-eectiveness trade-o that SFI achieves. In this section, we rst review the traditional recommendation methods, then the neural news recommendation methods. A lot of traditional recommendations methods [4,12,15] are based on collaborative ltering (CF). CF-based methods cluster users by “co-visitation” relationships to recommend news to similar users [4]. Another line of CF studies apply Matrix Factorization [12] and Factorization Machine [23] to model the interaction between users and items. However, these methods face the problem of cold-start and sparsity, which is severe in news domain. They also require dicult and labor-consuming feature engineering. As the counterpart of CF, content-based recommendation methods become the main focus of news recommendation [14,19] because of the rich text information in news articles. In recent years, deep learning techniques are widely used in news recommendation systems and achieve better results than traditional methods. They can be categorized as follows: 2.2.1 Feature-based Methods. Following traditional recommendation approaches, feature-based methods feed the model with news content together with manually designed features, then employ neural networks to model the complex interactions among all the features [3,6,17]. For example, Cheng et al. [3] propose to combine shallow and deep neural networks to extract valuable information from a variety of manual features. Guo et al. [6] add deep layers over the factorization machine to model high order interactions. 2.2.2 Representation-based Methods. More methods proposed to learn representations of news and users from raw texts and browsing histories respectively [1,10,16,21,29–33,39]. Numerous well designed models are proposed: multi-layer perceptron over trigrams [10], denoising auto-encoder [21], convolution neural networks [1,16,29–32,39], and various attention-based methods [29, 30,33]. Multi-channel structure is also explored [20]. Besides, some approaches [9,34,36] focused on using graph neural networks to represent news and users with their neighbors. Several methods [27, 28] proposed to incorporate knowledge to construct knowledgeaware representations of news and users. In spite of the improvements these methods have made, all of them embed the news and the user into one or several one-fold vectors in the semantic space, where the ne-grained information is limited. And the representations can only meet each other in the prediction phase, which may impair ne-grained matching signals between the user and the candidate news. 2.2.3 Interaction-based Methods. To address the above problem, interaction-based models, which match user’s interests with the candidate news at more delicate levels, are proposed. Wang et al. [26] designed the state-of-the-art interaction-based method for news recommendation. They constructed segment-to-segment similarity matrices between the candidate news and every historical news article of the user from 3 dierent granularities. Then they use 3D-CNN to highlight salient matching signals to make recommendations. Although FIM achieves better results feature- and representation-based methods, its main drawback is the especially slow inference speed. In this work,we explore the interaction-based methods and aim to eciently select fewer historical articles with higher value to perform interactions. Several works in other elds have proposed to select important features for interaction [7,18,40], but they all employ the gating operator, which fails to discard the unselected items hence cannot be directly used to achieve our goal. The most related work [8] splits selection to another training stage, forbidding the model learning to select. Our proposedlearningto-select mechanism eectively addresses these issues. First we formulate the news recommendation problem. Given a user𝑢, we have a set of historical news articles browsed by her at the platform, denoted asH = {h,h, · · · ,h}. For a candidate news c, our goal is to infer the probability that the user clicks this news article based on her browsing historyH, denoted as𝑝 (c|H ). The architecture of SFI is presented in Figure 2. Specically, it contains four major modules. Thenews encoder modulelearns word- and news-level representations, the ne-grained ones are intended for interaction and the coarse-grained ones are further transformed for selection. The followinghistory selector module manipulates news-level representation to eciently and precisely select informative news from the user’s browsing history. The negrained representations of selected news are fed into thenews interactor moduleto compute interactions. The coarse-grained matching signals are also modeled in this module. Finally theclick predictor moduleincorporates all matching signals to predict the click probability𝑝 (c|H ). Next, we introduce each component in our model, especially the history selector. Since users’ click decisions on news platforms are usually based on the title of news articles [30], the news encoder learns the news representation from title only. Denote the word sequence of a news title as𝑆 = {w,w, · · · ,w}, where𝑁is the length of𝑆. First of all, we transform𝑆into a sequence of vectors𝐸 = {e, e, · · · , e} by word embedding matrixW∈ R.𝑉is the vocabulary size and 𝐷 is the dimension of embeddings. Usually, the local contexts of a word across dierent spans play a big role in representing the word [26,29,30]. Therefore, we employ a hierarchical dilated convolution [38] to extract context features from dierent semantic granularities. In the𝑙-th convolution layer, the representation of the 𝑖-th word is calculated as: ! whereÉmeans the concatenation operation for vectors.Fis the convolution kernel of size 2𝑤 +1,𝛿denotes dilation rate,bdenotes bias and𝑓denotes the number of lters. A detailed description of dilated convolution can be found in [26]. By hierarchically stacking dilated convolutions with expanding dilation rate, local contexts of dierent distances are fused into the word representations. Afterward, the output of each convolution layer is appended to the nal representation of𝑖-th word:r= {r}, where{·}denotes the vertical alignment of a matrix, and𝐿denotes the total number of the stacked convolution layers. The representation of each semantic level may contain information of dierent importance for matching. For example, in the news title “Restaurants to Satisfy Late Night Cravings in Louisville and Beyond”, phrase-level local contexts “Late Night Craving” for the word “Night” matter more than that of sentence-level, e.g. “Restaurants ... Night ... And”. Therefore, we use an attentive pooling technique [1,29,39] to highlight the important local contexts of a single word. Specically, a trainable vectorsq∈ Ris introduced as the query of attention. The representationrof the word wthat fuses information across every semantic level is computed as: Similarly, dierent words may contribute dierently in expressing the news. For example, “Louisville” is more informative than “Beyond” because it reveals the location. We use another query vector q∈ Rto highlight the informative words in the news title and obtain the overall representation of the entire news: So far, the ne-grained representation of each wordr= {r} and the coarse-grained representation of the entire newsrare generated by news encoder. We further explore other kinds of stateof-the-art encoders, and study their performance in Section 5.3. The history selector is the core component of our model. It selects the informative historical news sparsely, automatically, and eciently with a learning-to-select mechanism. Then the selected news pieces are fed into next module for ne-grained interactions. Denote the news-level representation of𝑖-th clicked news in the user’s history ash, and that of candidate news asc. Recall that the news-level representation is attentively aggregated from the word vectors in the title, which are optimized for the nal matching and thus aren’t selection-oriented. Therefore, directly usinghandcfor selection may lead to sub-optimal results. In learning-to-select, we project all the news-level representations into the same selection space using a fully-connected network to mitigate such conicts: wherercould be eitherhorc. Considering selection eciency, we dene the candidate-aware informativeness of every historical article as the cosine similarity between selection vectors: In selection, no supervision signals are available fors, so it’s critical to optimize the parameters end-to-end to allow the model learn valuable features for selection. Besides, sparsity must be enforced, otherwise the model would still be slow. Keeping both constraints in mind, we design two complementary selection networks. 3.2.1 Hard-Selection Network. This sub-module enforces sparsity: it keeps the top𝐾most informative history news and discards others. Formally, whereargTOPK(s)gets the index of the top𝐾value in the vector s. The corresponding history news sliced byxis selected and its ne-grained representations will get involved in the ne-grained interaction later. To do that, we rst transformxto one-hot encoding matrixX = one_hot(x) ∈ R, where the𝑖-th row inXis the one-hot vector ofx, then use matrix multiplication to prune the browsing history to a smaller size: where𝐻 = {{h}}∈ Ris the ne-grained representation tensor of user’s browsed news,ˆ𝐻 ∈ Ris that of the selected news. By regulating hyper parameter𝐾, we can elastically control the model’s eciency. However, this sub-module has three defects that may limit the eectiveness: 1) BecauseXis sparse, the gradient cannot be passed to optimizeW; 2) Because the informativeness distributions of dierent users vary greatly, some noisy news articles are not ltered out among top𝐾; 3) All of the selected news articles are weighted equally even though some of them are more informative. To tackle the above problems, a soft renement is proposed. 3.2.2 So-Selection Network. This sub-module makes the gradient ow through selection. It is essentially a gating operator with a threshold [40], which further rules out noise (namely authentically uninformative news) and improves eectiveness. Given the output of hard-selection, the soft-selection network masks the news whose informativeness is below the threshold and attaches dierent importance to the unmasked ones: ⊙is Hadamard Product, and𝛾denotes the threshold.𝜎 (·)is elementwise.Expand(˜s)repeats the elements in˜s, expanding it intoR. ˜𝐻is the renedˆ𝐻, where all representations of the news whose informativeness is lower than 𝛾 are masked as 0. The number of the authentically informative news articles is oating per candidate, so a dynamic quantity of news items is kept. This entitles more exibility to the selection operation. Meanwhile, with˜𝑠attached toˆ𝐻, the news interactor can attend to more informative ones, and the parameters inProj(·)can be optimized by the selecting step since the element-wise multiplication is dierentiable. This helps SFI to learn features that are important for selection and will enhance the eectiveness remarkably. In back propagation, gradient from the loss function is applied to the news interactor, then to the selected ne-grained representation tensor˜𝐻. For simplication,˜𝐻is reshaped into a vector ˜R ∈ Rwhere𝑑 = 𝐿 × 𝑓, together with its gradient∇= reshape(∇) ∈ R. The same operation is taken forˆ𝐻, formingˆR ∈ R. Then the gradient for Wis: whereDiag(ˆR)is the matrix with the elements ofˆRas the diagonal. Z = {h}∈ Ris the news-level representation matrix of the historical news, andZ= {Proj(h)}, c= Proj(c)are the corresponding selection vectors. 𝑔(𝑠) is the derivative for 𝜎 (𝑠): In this way, the gradient safely ows through the selection stage and reachess, to increase the score of the useful news pieces and vice versa. It is further spread to optimizeWto achieve the above adjustment, however, only from the selected entries. The selected historical news articles are fed into this module to perform ne-grained interactions with the current candidate. We denote the representation of the words in𝑣-th selected news asd= {t}wheret= {t}∈ Ris the stacked representation of 𝑗-th word. Similarly, the representation of each word in the current candidate news isc= {p}. Resembling FIM [26], we construct pair-to-pair similarity matrixMof𝑙-th semantic granularity, where each entry is the scaled dot product between the ne-grained representations of 𝑣-th selected news and the candidate news: Next, the similarity matrices of each granularity across all the selected history news are fused into a 3D cube𝑂 ∈ R, where a series of 3D CNN and 3D max pooling is applied to highlight the signicant matching signals. Outputs of the nal pooling layer are attened as the vector containing ne-grained interactive information across the user and candidate news, denoted as𝜙. Other state-of-the-art interactors are studied in Section5.3. In SFI, ne-grained matching information𝜙only engage selected news articles. However, it is important not to leave out the unselected ones. Although conducting ne-grained interactions on them is unnecessary, we still value the coarse-grained matching signals of them, which come from the matching between news-level representations: 𝜓= {𝜓,𝜓, · · · , 𝜓= h 𝜓gives an overall matching degree of the user and the candidate news and is complementary to𝜙. It facilitates the model to learn more precise correspondences between the matching signals and the click probability. Another critical point is that by involving𝜓 #impressions15,777,377#clicks24,155,470 to score the candidate, the gradient can be delivered by all of the historical news articles rather than only the selected ones. The click predictor module incorporates the output from news interactor then predicts the probability of a user clicking on a candidate news article. The news articles with higher click probability are ranked higher in the nal user interface. Given vectors containing course- and ne-grained matching information,𝜓and𝜙respectively, we propose to incorporate both by: Following [10,30], we use negative sampling to simulate the unbalanced distribution of clicked news in an impression. For each ground-truth candidate, we randomly sample𝑚news that is not clicked by her in the same impression as negative samples: Thus, it is converted to a𝑚 +1 classication problem, and the negative log likelihood loss is going to be minimized when training: wherecis the ground-truth news piece which the user clicked, and S denotes all training samples. Finally, we jointly train the news encoder, histor y selector, newsinteractor and click predictor through the nal click signal. In such a way, the model can better learn dependencies among modules. Our experiments are conducted on MIND [35], a large-scale dataset collected from the users’ click logs of the Microsoft News platform from Oct. 12 to Nov. 22, 2019. The statistics of MIND are shown in Table 1. We use the same training-testing partition as [35]. In our experiments, the dimension𝐷of word embeddings is set to 300. We use the pre-trained Glove embeddings [22], to initialize the embedding matrixW. The maximum length of news titles is set at 20. the maximum number of clicked news for learning user representations was set to 50. In the news encoder, we stack 3 convolution layers with dilation rates[1,2,3]. The kernel size and the number of lters is set to 3 and 150 respectively. We employ a 2layer composition for news-interactor module, the output channels and the window size is set at 32− [3,3,3]and 16− [3,3,3]. Each convolution component is followed by a max pooling layer with size[3,3,3]and stride[3,3,3]. We apply the dropout strategy [25] to the word embedding layer to mitigate overtting. The dropout rate is set at 0.2. Adam [11] is used as the optimization algorithm. The batch size is set to 100 when training and 400 when predicting, and the encoding process is executed oine when predicting. Since there are 40 kernels in total on our machine, we set 40 parallel threads to load data in order to minimize the latency caused by processing data. We independently repeat each experiment for 5 times and report the average performance. We conduct all experiments on a machine with Xeon(R) Silver 4114 CPUs and a TITAN V GPU. Following existing studies, we use the average AUC, MRR, nDCG@5, and nDCG@10 scores over all impressions to evaluate the eectiveness of the models. All results come from the ocial test entry. Moreover, given the same batch size, we use the prediction speed i.e. iterations per second to evaluate the eciency. In one iteration, the batch size of candidate news articles is scored. We compare SFI with the following baseline methods: (1) General Recommendation Methods:LibFM[24], a state-ofthe-art feature-based matrix factorization approach for recommendation;DSSM[10], a deep structured semantic model that uses multiple dense layers upon tri-grams. All of the users’ clicked news are concatenated as the query, and the candidate news is regarded as documents;Wide&Deep[3], a widely used recommendation method that uses the combination of a wide channel and a deep channel for memorization and generalization;DeepFM[6], a popular neural recommendation method which combines factorization machine with deep neural networks; (2) Representation-based Methods:DFM[17], which uses dense layers for dierent channels and attentively fuse outputs;GRU[21], which learn news representations with an auto-encoder and utilizes GRU to learn user representations;Hi-Fi Ark[20], a multi-channel representation approach for recommendation;NPA[30], which highlights informative words and news with personalized attention; NRMS[33], which learns delicate representations of news and users by multi-head self-attention;LSTUR[1], which models longand short-term user interests with GRU; (3) Interaction-based Methods:FIM[26], the state-of-the-art interaction-based approach for neural news recommendation, which encodes news by hierarchical dilated CNN and performs interaction between each of the user browsed news articles and the candidate. Recent(𝐾), the naive counterpart of SFI, which keeps the recent𝐾 historical news for interaction only (Recent(50) equals FIM). The overall recommendation eectiveness of all models is shown in Table 2. Based on the results, we have the following observations: (1)Our proposed model SFI consistently outperforms other baselines in terms of all metrics. On the one hand, SFI captures ne-grained interactions to model user interests, gaining 2.71% up to 3.23% AUC improvements over all of the state-of-the-art Table 2: The performance of dierent metho ds for news recommendation. The number of news items involving in interactions is bolded in ( ) for interaction-based methods. The result with superscript * is referencing the one in [35] where MIND is presented. † indicates a signicant improvement over all baselines with paired t-test (𝑝 < 0.01). Interaction based0.347537.8643.51 representation-based methods. On the other hand, SFI(𝐾)outperform Recent(𝐾) baseline by 6.4% and 2.17% when𝐾 =5 and 50 respectively. This result substantiates the power of the learning-toselect mechanism. (2) The variant SFI(50) that keeps the entire browsing history outperforms SFI(5) that selects only 5 historical news. This is as expected because after removing noise, SFI(50) covers richer information to model the user. Interestingly, the improvement is tiny compared with the margin between Recent(50) i.e. FIM and Recent(5). We will study this phenomenon in detail in Section 5.4.1. (3) The interaction-based methods for news recommendation outperform all representation-based methods, which validates the benet of capturing ne-grained matching signals. However, simply pruning the user’s history to a smaller size to save speed is not feasible because it hurts the eectiveness seriously. Without Bert [5], expanded SFI (with extra news abstract) ranks among the top 15 on the ocial testing leaderboard. Since the motivation of SFI is mainly concerned with eciency, we further compare the inference speed between SFI and several baselines. Results in Table 3 substantiates the superiority of SFI: with the selection capacity of5, it can infer almostfourtimes faster than the state-of-the-art interaction-based method, while signicantly improving the recommendation eectiveness. SFI(5) also achieves comparable speed with the state-of-the-art representation-based method NRMS, and outperforms it by2.71% in AUC. The eciency of SFI(𝐾) and Recent(𝐾) signicantly drops from𝐾 =5 to𝐾 =25, which will be further studied in Section 5.4.1. Table 3: The inference speed comparison of dierent methods for news recommendation. The improvement over FIM is given in the bracket. Table 4: The eectiveness of SFI with dierent news encoders and news interactors. Since SFI is essentially a exible framework, we conduct extensive ablation studies to gain comprehensive insights into every module. In each subsection, we pose our claim rst before explanations. 5.3.1HDCNN and 3DCNN are the most eective encoder and interactor among a variety of state-of-the-art architectures.In the interaction-based workow, the history selector can be easily inserted between any kind of news encoder and interactor. This exibility motivates us to study how state-of-the-art encoders and interactors would perform. We compare among 1D-CNN with Personalized Attention [30] (denoted as PCNN), Hierarchical Dilated CNN [26] (denoted as HDCNN), Multi-head Self Attention [33] (denoted as MHA), LSTM for the news encoder and 2D-CNN, 3DCNN [26], KNRM [37], Multi-Head Self Attention [33] (denoted as MHAI) for the news interactor. The AUC scores are reported in Table 4. We ndHiearchical Dilated CNNcombined with3D-CNN is the best setting. 5.3.2Every sub-module in history selector is critical to improving eectiveness.The learning-to-select mechanism comprises three parts: a selection projection, a hard selection, and a soft renement. The hard selection is the cornerstone of our work so we no longer verify its impact. For the other two components, we compare SFI with the variant that applies only hard-selection, and that applies hard-selection followed by soft-selection without learning extra selection vectors. The result is reported in Figure 3. As we observe, the soft-selection network improves the eectiveness. This is because it lters the authentically uninformative history news, and makes the gradient ow through to optimize the representation vectors used for selection. However, without selection projection, these news-level representations are optimized for two incompatible goals: selecting and matching, which may decrease the recommendation accuracy. Experiments validates our claim: Figure 3: The eectiveness of history selector and coarsegrained information. the model benets a lot from selection projection. Thanks to it, SFI can encode selective features into selection vectors, leaving the news-level representations to focus on the nal matching. 5.3.3The coarse matching signals of the unselected articles are also important.In Figure 3, SFI outperforms its variant that totally abandons the coarse-grained matching signals. This observation veries that the coarse-grained matching signals are complementary. Note that doing so won’t reduce eciency because batched matrix multiplication is fast on GPU. 5.3.4SFI benefits from end-to-end training.When deployed in production, the news encoding process could be done oine to speed up inference, known as a pipeline convention. It’s natural to migrate it to the training phase, where we rst pre-train SFI without the history selector to acquire coarse-and-ne representations of news. Then we replace the news encoder with a lookup table constructed from these representations and ne-tune them with history selector applied. End-to-end training is the counterpart, in which we jointly train the news encoder, history lter, news-interactor and click predictor by the nal classication loss simultaneously. In Figure 3, the rst four bars in every group are the performance of SFI trained in pipeline, and the last bar is that of trained end-to-end under the same setting. As expected, end-to-end training leads to better eectiveness. It’s because optimizing the parameters rather than directly updating the vectors can make the news encoder learn more precise representations for both selection and interaction. Also, the modules can better learn the correspondence among them in end-to-end training. 5.4.1 Influence of the Interaction Capacity. The predened interaction capacity𝐾is the most important hyper parameter since the eciency-eectiveness trade-o is up to it. We study its inuence by drawing the inference speed curve against the AUC score of the model with dierent𝐾in Figure 4. Motivated by several observations in Section 4, we also include Recent(𝐾), which only interacts with the latest𝐾historical news to save eciency. We add Figure 4: The eciency and eectiveness of SFI with different numbers of selected news. The number next to the marker indicates the selected news count. Figure 5: The informativeness score of history news at each position. Smaller x-axis represents more recent history. two dashed lines to mark the best eectiveness and eciency that baseline models ever achieve. The optimal result would be situated at the upper right corner. According to the gure, we nd:First, from𝐾 =5 to𝐾 =50, SFI(𝐾) is far more eective than its naive counterpart Recent(𝐾), this again validates the eectiveness of history selector. However, due to the time consumption of selection, SFI(𝐾) is a bit slower. Overall, SFI(5) is the optimal setting because it greatly outperforms NRMS and all Recent(𝐾) including FIM, while providing a much higher efciency over FIM.Second, when𝐾is growing, the eectiveness of both Recent(𝐾) and SFI(𝐾) is improving, which is because a bigger capacity keeps richer information to learn the user’s interests. As a side eect, the model becomes slower.Third, as𝐾increases, the eectiveness of SFI(𝐾) grows slower than Recent(𝐾) and is about saturated at𝐾 =40. Intuitively, with the learning-to-select mechanism, SFI(𝐾) can consistently select the most eective articles for interaction, so increasing the capacity only brings a little more Figure 6: The eectiveness of SFI with dierent value of 𝛾. valuable information. In contrast, Recent(𝐾) cannot access the informative historical news in earlier history unless the capacity is big enough. These observations motivate us to study what informativeness scores of the historical articles at dierent positions are learnt by the model itself. We report the informativeness score at each history position (averaged from𝐾 =5 to𝐾 =50) in Figure 5. The blue line is the mean value of informativeness, and the shade indicates standard deviation. The horizontal black line marks the threshold of the soft-selection. According to the gure, the increasing mean value of informativeness tells us that more recent reading history helps more in expectation. At the same time, the signicantly high variance conrms that historical news at each position has the potential to interact with the candidate. Therefore, with a small 𝐾, Recent(𝐾) cannot access earlier historical news that tends to be useful in interaction. Rather, SFI is quite able to inspect them and involve them in ne-grained interactions intelligently. Moreover, the informativeness of the history news whose position is farther than 40 hardly reaches the threshold. So they are considered authentically uninformative and masked even though they are among the top𝐾 ≥40. This explains the saturation of SFI’s eectiveness and justies our intuition. 5.4.2 Influence of the Informativeness Threshold. Another crucial factor of SFI is the informativeness threshold in the soft-selection network. The eectiveness of SFI with dierent threshold settings is shown in Figure 6. In summary, the threshold shouldn’t be too large or too small. When𝛾 <0.1, almost all history news articles are considered informative, so the selection fails. When𝑔𝑎𝑚𝑚𝑎 >0.3, the history selector rules out too many history news articles, including the valuable ones. The gradient cannot be passed adequately, either. Hence the model’s eectiveness declines. When it reaches 1, all ne-grained representations are masked as 0, completely disabling the news interactor. Recall that the coarse-grained matching signals persist, leading to better results than random recommendation. Overall, 𝛾 = 0.2 is the optimal conguration. Capturing ne-grained interactions brings more accuracy and higher online costs for news recommenders. In this work, we proposed a selective ne-grained interaction framework to select a small number of valuable historical articles for interaction, drawing a good balance between eciency and eectiveness. With the help of thelearning-to-selectmechanism, the selection can be performed eciently, sparsely, and automatically. Experimental results show SFI can signicantly improve the recommendation eectiveness by 2.17% over the state-of-the-art models with four times faster speed. We experimented a lot to provide comprehensive insights of SFI and studied the eciency-eectiveness trade-o it achieves. In the future, we will dig deeper into representing users with terms to further improve the eciency while keeping the eectiveness.