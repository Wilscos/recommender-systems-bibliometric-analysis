Abstract—A major challenge in collaborative ﬁltering methods is how to produce recommendations for cold items (items with no ratings), or integrate cold item into an existing catalog. Over the years, a variety of hybrid recommendation models have been proposed to address this problem by utilizing items’ metadata and content along with their ratings or usage patterns. In this work, we wish to revisit the cold start problem in order to draw attention to an overlooked challenge: the ability to integrate and balance between (regular) warm items and completely cold items. In this case, two different challenges arise: (1) preserving high quality performance on warm items, while (2) learning to promote cold items to relevant users. First, we show that these two objectives are in fact conﬂicting, and the balance between them depends on the business needs and the application at hand. Next, we propose a novel hybrid recommendation algorithm that bridges these two conﬂicting objectives and enables a harmonized balance between preserving high accuracy for warm items while effectively promoting completely cold items. We demonstrate the effectiveness of the proposed algorithm on movies, apps, and articles recommendations, and provide an empirical analysis of the cold-warm trade-off. Index Terms—Recommender Systems, Collaborative Filtering, Cold-Start, Representation Learning, Cold-Warm Harmonization, Cold Items, Hybrid Recommenders Recommender systems models are generally categorized into three main categories: (1) Collaborative Filtering (CF) models [1]–[7] that are based on ratings or implicit usage information, (2) Content Based (CB) models that utilize items’ content and metadata [8], and (3) hybrid models that combine both information sources [9], [10]. Historically, CF models have performed better than CB models in many well-known competitions, e.g., the well known Netﬂix Prize [11] and the Yahoo! Music Challenge [12]. Hence, in the presence of ratings or implicit usage data, CF models are usually considered more accurate than CB models [13]. However, CF models are known to suffer from the cold start problem when it comes to recommending items with little or no ratings or implicit usage data. , Ori Katz, Avi Caciularu, In the past, utilizing certain types of items’ content such as textual descriptions and images required employing challenging processing techniques. Hence, hybrid models were very limited in the types of content data they could employ. However, recent advancements in deep learning methods gave rise to a new family of deep hybrid recommender systems that can utilize almost any type of digitally available content data [14]. By incorporating items’ content together with ratings or implicit usage data, hybrid models employ a dual objective: mitigating the cold-start problem while simultaneously utilizing the items’ content to improve warm item representations. As such, modern hybrid models appear to be the ultimate approach. Yet, this dual objective pose a challenge: the model strives to adhere to the CF objective and preserve accurate recommendations for warm items, while simultaneously promoting cold items to the right users. When warm items are considered, hybrid models utilize the content data as additional “side information” that aids and improves the CF representation. However, when completely cold items are considered (items with no usage at all), the content data needs to “take over” and replace the missing usage representation. Alas, existing hybrid models are not optimized to deal with completely cold items effectively. In fact, as we show later, without proper treatment of completely cold items during the training phase, a hybrid model would still attempt to utilize the content information as a mere “correction” over the missing CF representation that in the case of completely cold items, simply does not exist. In other words, in the absence of the usage data, the content representation is not optimized to completely replace (overtake) the missing CF representation and the ﬁnal result is sub-optimal. In this paper, we consider a common scenario in which a recommender system is expected to perform well both on warm items as well as on completely cold items that should be promoted to relevant users. The trade-off between these two objectives has a major signiﬁcance in real-world commercial systems where new items are regularly introduced to an existing system in order to be exposed to the right audience. In fact, the promotion and introduction of new (cold) items is a critical stage that often has signiﬁcant and lingering effects on the product life cycle and overall consequential sales [15], [16]. However, as we show next, when optimizing a machine learning based recommender system, these two objectives are contradicting and this trade-off was mostly overlooked in the literature. Ideally, a hybrid recommender would perform well on all items, and the proper balance between cold and warm item modeling would be determined based on the application at hand and the business needs e.g., the proportion between the warm and cold items in the catalog, or the desired amount of exposure or promotion for cold items. To this end, we present the Cold-Warm Harmonization (CWH) model - a novel deep hybrid model with an emphasis on harmonizing cold and warm item recommendations. Unlike previous hybrid models, that utilize items content in order to improve warm item recommendations with very limited contribution to cold items, CWH directly addresses the items’ “cold start” problem while supporting a tunable balance between warm and cold item modeling. In common with previous works, CWH learns hybrid item representations that utilize both usage (CF) data and content data in a joint manner. However, for cold items, CWH utilizes a novel dual content based representation: The ﬁrst representation is similar to the one used for warm items, while the second CB representation is dedicated to cope with cold items only, and compensates for the missing CF representation. Additionally, a stochastic tunable gate (modeled as an observed Bernoulli variable) determines the state of each item as either warm or cold to enable artiﬁcially injecting cold items, hence simulating cold start scenarios, during training. This unique architecture alleviates the aforementioned conﬂicting roles of the CB representation and allows for optimal utilization of items’ content. By introducing both cold and warm items during the training phase, the model is forced to adapt to both cases - effectively coping with completely cold items while still maintaining good performance on the warm items. In other words, CWH simulates cold-start scenarios during training time, hence compelling the model to cope with both objectives simultaneously, and in a controlled manner. The remainder of this manuscript is organized as follows: In Section II, we cover related work and focus speciﬁcally on earlier attempts to balance cold and warm item modeling. In Section III, we describe the CWH model in detail. Then, in Section IV we provide evaluations and experimental results that demonstrate the effectiveness of CWH both as a hybrid recommender system as well as an effective solution for cold items when no ratings or implicit usage is available. Finally, we provide ﬁnal words and conclusions in Section V. The cold start problem is an active research ﬁeld in the recommender systems community [13], [17]–[22] and hybrid models have been studied extensively in the literature [9], [10], [14], [23]–[29]. However, the problem of balancing cold and warm item modeling in hybrid recommenders (as explained in Section I) has been mostly overlooked. While we cannot cover the plethora of related work on the cold-start problem or hybrid models in general, in what follows, we relate our work to the latest state-of-the-art hybrid models as well as a few earlier works that touch upon the problem of cold-vs-warm trade-off. Collaborative Topic Regression (CTR) [9] and Collaborative Deep Learning (CDL) [30] are two highly popular hybrid models that combine a CF model based on Matrix Factorization (MF) [2] together with Latent Dirichlet Allocation (LDA) [31] or stacked denoising autoencoders [32], respectively. The purpose of the CB objective is to support cold items when usage data is unavailable. Both CTR and CDL tackle the warm-vs-cold trade-off by employing a hyperparameter to control the importance of the MF objective with respect to the CB objective. In [30], Wang et al. compared both CTR and CDL. As seen in Figure 4 in [9], tuning this parameter has a negligible impact on cold items for both models. Moreover, CTR’s use of the LDA model for utilizing textual information is rather anachronistic compared to modern day approaches that are usually based on word embeddings and transformers [33]–[35]. Hence, we conclude that while both models do tackle the balancing problem, these models are mostly focused on warm items, and do not fully exploit the CB information for completely cold items. Collaborative Variational Autoencoder (CVAE) [36] is a hybrid model that was recently shown to outperform CDL, CTR as well as several other state-of-the-art baselines. CVAE employs a generative model for the CB data which is combined with learning a CF task based on implicit relationships between users and items. In contrast to the model in this paper, CVAE does not provide any mechanism to simulate a completely cold start scenario during training. Other deep hybrid recommender systems include the multimedia model for movie recommendations [37], HybridSVD [38], explainable hybrid systems [39] and contentcollaborative disentanglement representation learning [40]. However, these works do not propose any way to deal with the cold-warm trade-off in hybrid models. Recently, CB2CF [22] - a deep content-to-collaborative ﬁltering model was introduced for completely cold item recommendations. CB2CF works in two stages: First, CF representations are learned based on usage data. Then, the CB2CF model is applied to learn a mapping from warm items’ content into their latent CF representations. This mapping is later employed in order to embed cold items into an existing CF model. In contrast to hybrid models, CB2CF is focused solely on completely cold items (items with zero ratings or implicit usage data) while disregarding warm items. As such, CB2CF was shown to outperform existing hybrid models on the problem of modeling completely cold items. However, during inference time, the warm items rely on their pre-existing CF representations which are unaware of the additional representations injected by CB2CF to completely cold items. This type of discrepancy poses a challenge to realworld systems that need to address both warm and cold items simultaneously. In this work, we present CWH, an end-to-end hybrid model that addresses a problem not discussed by the aforementioned works - balancing two conﬂicting objectives: learning warm and cold item representations in a single uniﬁed recommender system. By employing an effective stochastic (yet controlled) gate, we inject fake cold items during the training phase, to force the model to adapt to both cold and warm items, simultaneously. Moreover, CWH employs a dual CB representation for cold items that compensate for the absence of the CF representation and alleviates the aforementioned conﬂicting roles of the content data. Consequently, CWH improves upon the state-of-the-art in several facets: (1) A novel hybrid recommender that is capable of effectively handling both warm and cold items, simultaneously. (2) A uniﬁed training procedure that improves accuracy and (3) A novel framework to balance between warm and cold item learning. In this section, we formulate the problem setup and the CWH model. Let I = {i}and J = {j}be sets that index Nusers and Nitems, respectively. In addition, we assume that each item j is associated with Ntypes of content (information sources), X= {x}, where x∈ C represent item j’s kth information source. For example, C can be images (a visual signal), and Ccan be the textual descriptions (unstructured text). The overall available content for the entire set of items is denoted by X = {X}. We denote the set of user-item interactions (i.e., the CF relations) by I= {(i, j)|user i consumed item j}. In addition, we deﬁne Y = {y|(i, j) ∈ I × J }, where yis a two-point observed random variable s.t. y= 1 if (i, j) ∈ I, and y= −1 otherwise. Namely, yindicates whether the user i consumed the item j or not. Let f: C→ Rbe a content analyzer function (parameterized by θ) that maps x ∈ Cto a d-dimensional vector f(x). For example, fmay be a deep neural network that analyzes the item’s textual description (e.g., BERT [29], [41]) or visual content (e.g., ResNet [42]), and encodes it as a d-dimensional vector. The unobserved parameters θare learned during the model’s training phase (in practice, we use a pretrained network as a backbone model, and may extend it with subsequent layers as necessary). For simplicity, we denote f, f(x), which stands for the application of the content analyzer fto the content information of type k that is associated with the item j. In addition, we collectively denote θ= {θ, ..., θ}.P Let φ: R→ R, where d=d, be a multiview content analyzer that receives the concatenated multiview representation f= [f, ..., f] and outputs the following d-dimensional vector: Therefore, φencodes all types of content that are associated with item j. In our implementation, we set φto be a fully connected neural network with a single ReLU activated hidden layer. Let U = {u}and V = {v}be the unobserved user and item CF representations (U, V ⊂ R). In order to score the afﬁnity between user i and item j, we deﬁne a neural scoring function s: R× R× R→ R (parameterized by θ) that receives u, φand vas input and outputs an afﬁnity score (scalar). In this work, sis parameterized by θ= {W, W, W, r, r, r} as follows: where with W∈ R, W, W∈ R, r, r∈ Rand r∈ R. Hence, sis a neural network with two ReLU activated hidden layers: The ﬁrst hidden layer produces an item vector qthat combines the CF and CB information of the item j. The second hidden layer combines the CF user vector uwith qto a single representation hthat is ﬁnally transformed to a score via a linear classiﬁer. For simplicity, we denote s , s. Finally, the likelihood of a user i to like (or dislike) an item j is given by: where σ(z) ,is the logistic function. The CWH model is not a standard hybrid model. It includes a novel mechanism that simulates completely cold items and forces the model to fully utilize the CB information in cases where the CF information is not available (as in completely cold items). Next, we explain the challenge of cold-warm harmonization and describe in detail our solution. So far, the model’s objective, as deﬁned in Equation 4, is inline with many previous hybrid models. However, it poses a challenge when dealing with a completely cold item a. In this case, the item a is associated with the content representation φonly, while the CF representation vis missing. This is a common scenario in real world practice, and one that we particularly want to address in scope of this work. Since the model never actually encounters completely cold examples during training (by deﬁnition), it cannot adapt to this case. In other words, since completely cold items do not appear in the training data, the model is never actually required to use the content representation φalone. Instead, the model treats the content representation φas a mere ‘correction’ over the CF representation v, and when vis missing, the item’s representation is incomplete. In Section IV, we investigate this case, and show that without proper treatment, the results on cold items are sub-optimal. In order to alleviate the aforementioned problem, we propose the novel Cold-Warm Harmonization (CWH) mechanism that simulates cold-start scenarios during the training phase. To this end, we introduce the novel CWH likelihood: The CWH likelihood in Equation 5 introduces two new terms, band vas follows: - is a stochastic gate, based on an observed Bernoulli variable, that determines the likelihood state of the item as either warm or cold. At each iteration, bis resampled from a Bernoulli distribution. In the warm state (b= 0), the likelihood falls back to Equation 4. However, in the cold state (b= 1), the likelihood simulates a cold start scenario, where the term v∈ Rreplaces the missing CF representation v. In Section III-C, we elaborate on the distribution of band propose a concrete scheme for tuning its parameter (success probability) based on the popularity distribution of warm items. φ, φ(f), and φ: R→ Ris a neural network with an identical architecture as φ, but parameterized by a different set of (learned) parameters θ. m ∈ Ris a global learned embedding vector (independent of j) that can be seen as a global positional bias. The role of bis to expose the model to fake completely cold items during training. In this case (b= 1), vis used instead of the CF representation v, ensuring the model learns a CF compensation based on the items’ content. It is important to clarify that φand φplay different roles: φis trained to produce φ- a CB vector which enhances the learned CF representation with complementary CB information (via the concatenation qin Equation 3). On the other hand, φtogether with m are trained to replace the missing CF representation in the cold start scenario. Then, in the inference phase, when a completely cold item a is introduced to the system, the role of φis to replace and compensate for the missing CF representation vbased on its content data X. The combination of the gate b, together with the network φ, forms a novel architecture and a key contribution of CWH that alleviates the aforementioned conﬂicting roles of the CB data as well as the discrepancy between training and inference in hybrid recommenders. A schematic illustration of the CWH model is depicted in Figure 1. In many collaborative ﬁltering datasets, items exhibit a power law distribution in which few popular items account for most of the user-items interactions in the dataset. As a consequence, the model’s exposure to speciﬁc types of content is imbalanced as well. For example, consider the MovieLens dataset [43] from Section IV. One type of content metadata is the set of actors participating in each movie. Naturally, popular actors are mostly associated with popular movies. As a result, the model’s exposure to actors is imbalanced: less popular actors are rarely introduced to the model and the model’s ability to learn their CF compensation via vis limited. To mitigate this problem, we propose to suppress the probability of simulated fake cold-start scenarios for popular items, but enhance it in the case of rare items. By taking this approach, we equipoise the model’s exposure to types of content as follows: Denote the normalized popularity score of item j by 0 ≤ c≤ 1, where c= 1 is associated with the most popular item, and c= 0 is associated with the least popular item, e.g., by employing min-max normalization. Then, we set the parameter of the Bernoulli variable bto be p(γ, c) = γ. Figure 2 depicts p(γ, c) for different popularity scores. We see that popular items with popularity score of c>produce a convex behaviour of p(γ, c) with respect to the control knob γ, while rare items with popularity score c<produce a concave behaviour of p(γ, c) with respect to γ. As a consequence, the probability to use v for an item j with popularity score of c>(c<) would be less (greater) than γ. We believe that a careful selection of pcan be beneﬁcial for the model’s ability to learn CF compensation via v. However, further investigation of the Bernoulli parameter p is left outside the scope of this work and reserved for future work. We denote B = {b|(i, j) ∈ I × J } and Θ = {U, V, m, θ, θ, θ, θ}. Then, by assuming normal priors over the unobserved model variables, we can write the negative log joint distribution as follows: = −logσ(ys(u, (1 − b)v+ bv, φ) where τ is the precision hyperparamter that controls the strength of the normal prior (similar to Lregularization). In practice, the negative examples ((i, j) /∈ I) that appear in the likelihood term in Equation 6 are sampled in a stochastic manner, according to the procedure in [3]. We propose a Maximum A-Posteriori (MAP) estimation, which is equivalent to the minimization of L w.r.t. the unobserved variables, i.e., Θ= arg minL, where the optimization is carried out using stochastic gradient descent. At inference, we compute the odds of user i to like an item j following Equation 5 by setting y= 1, with b= 0 or b= 1, if j is warm (j ∈ J ) or cold (j /∈ J ), respectively. We present experimental evaluation, consisting of two major parts: In the ﬁrst part (P1), we evaluate the capabilities of CWH in handling completely cold-start scenarios (new items that were just introduced to the system and hence don’t have any usage data). We consider a case in which several completely cold items are integrated into an existing warm catalog of items. We focus on the inherent trade-off that arises when integrating warm and cold items together and demonstrate the ability of CWH to gently balance between the two objectives: preserving the performance on the warm catalog and promoting the items from the new (completely cold) catalog. The second part of the evaluation (P2) demonstrates the performance of CWH as a hybrid recommender system utilizing both usage data (implicit ratings) as well as the content based data that exist for the items. Our evaluations are based on a classical user-item prediction task, i.e. the ability to recommend the correct item to the right user. Our datasets consist of users and their lists of items (items purchased or consumed by each user). In what follows, we describe the datasets that are used in this research. 1) Datasets: Three datasets from different domains are considered: dataset [43]. It consists of 22M ratings for N= 34K movies by N= 247K users. Each user-movie interaction is rated by a 5-star scale, and we considered 3.5 stars and above as a positive signal. Importantly, we enriched the movies with content metadata we collected from IMDB. The metadata for each movie consists of textual, categorical and numerical data ﬁelds as follows: the plot, the list of actors, the director, a list of genres, languages, the year of release and additional descriptive tags. We also extracted the poster image for each movie as a visual content data. lection of users’ reading lists crawled from the CiteULike website taken from [30]. It consists of 205K user-item interactions of N= 17K articles that were read by N= 5.5K users (we used the CiteULike-a dense fold as in [30]). The metadata for each article consists of the article’s title, the abstract, and a list of descriptive tags. the Microsoft Windows Store. The dataset consists of 5M anonymous user sessions, where each session consists of a list of applications (apps) that were co-clicked during the same browsing session. It contains 20M user-item interactions and N= 33K unique apps. The apps metadata consists of the app’s title, its description, the release date, a list of descriptive tags, and the app’s icon image (as visual content). 2) Train / Test Partitioning: We now turn to describe the train / test split employed on each of the above datasets. We considered users with at least 8 items. For each user, we randomly drew two items to form the test set and another two items to form the validation set. Then, we choose 20% of the items and removed all their interactions from the training set in order to simulate cold items. Half of these items (10% of the 20%), were used in the validation set and the second half were used for the test set. Cold items that were selected for the validation set, were removed from the test set and vice versa. The warm and cold items in the test set were used for the ﬁrst part (P1) of the evaluations, while in the second part (P2) of the evaluation we used the warm items without the (completely) cold items that do not appear in the training set. Hence, the second part (P2) of the evaluation is comparable to standard evaluation of recommender system. The validation set was used to tune the model’s hyperparameters. We run each experiment 10 times with different realizations of the train / validation / test partition and report the mean results. Special care is required in the selection process of the simulated cold items. In a recent review and evaluation of modern recommendation algorithms, Dacrema et al. showed that the popularity distribution of test items has a signiﬁcant impact on a model’s performance evaluation (see Section 3.6 in [45]). This ﬁnding stems from the fact that the items’ content distribution cannot be considered statistically independent of their popularity. In other words, popular items exhibit different content distribution than the content distribution of rare items. Hence, without proper selection of cold items, the evaluations would be inaccurate, inconsistent and irreproducible in realworld, especially in datasets with a small number of items or a high popularity skew [45]. When new items are introduced to the system, these items are cold by deﬁnition. However, we should not assume that these items will remain unpopular in the long run. If we concentrate our evaluations on the unpopular items (similar to many previous works), our results will not reﬂect the actual business scenario at hand: introducing new items that may, with time, become popular. In other words, a real-world recommender needs to handle different cold (new) items, some of which are expected to become popular in the future while others will remain unpopular also in the long run. Therefore, the popularity of items in the test set should follow that of regular items in the training set. This is in contrast to other works that did not take this consideration into account [45]. In order to guarantee similar popularity distributions between the train, test and validation sets, we devised the following procedure: (1) We sort the items according to their popularity. Then, (2) we select each tenth item for the test set and its successive item for the validation set. The rest of the items consist the train set (the ratios can be adjusted as needed). Different folds are obtained by considering different offsets. The resulting train, test and validation sets consist of cold items that exhibit the same popularity distribution as new items in a real-world scenario. 3) Evaluation Measures: Using the test sets described above, the following measures are reported: Hit Rate at K outputs 1 if the target item j is ranked among the top K recommendations w.r.t. to the user i, otherwise 0. We report the average HR@K across all test pairs. computes the reciprocal rank of the target item j among the top K recommendations w.r.t. to the user i. If the rank of the item j is not the top K recommendations, the result is 0. Finally, MRR@K is obtained by averaging the reciprocal rank over the entire test set of user-item pairs. We refer the reader to [46] for a broader discussion and the motivation behind these evaluation measures. 4) Baselines and Hyperparameters Conﬁguration: All hyperparameters were set based on the separate validation set (described in the beginning of Section IV-A). All the models were trained with an early stopping procedure, based on the monitored measures (utilizing the same validation set). We evaluated the following models: model from Section III. We used three types of content analyzers, each analyzes a different type of content data: – For categorical data ﬁelds and tags we used a linear – For visual content, we used the backbone of a pref, f, fto form a 300 dimensional vector as the input to the multi-view content analyzer function φand to the CF compensation function φ. The output dimensions of φand φwere set d = 100, which matches the CF dimension as well. CWH was optimized by using a default Adam [48] optimizer with a batch size of 32. The regularization hyperparameter was set to τ = 1e − 5 (based on the validation set). Convergence was observed after 50-60 epochs, depending on the dataset. uses its CF component only. The architecture of the CF component is based on the Neural Collaborative Filtering (NCF) model from [49]. This baseline is a pure CF model, hence unable to utilize content and cannot support cold item recommendations. Therefore, results for CF-only are reported in the second part (P2) of the evaluation only. does not learn the CF item representations. The CF items’ representations: V, vand m are effectively set to zero. It uses the same multiview content embedding function φand the same objective function as CWH. deep regression model that learns a mapping from items CB representations to their corresponding CF representations. This mapping is later used in order to estimate the CF representations for cold items based on their content. [45] that was recently shown to outperform multiple stateof-the-art hybrid models (e.g., CVAE [36]). The original version of ItemKNN-CFCBF from [45] does not support completely cold items since it assumes the existence of a CF representation. Hence, in order to extend the applicability of ItemKNN-CFCBF to cold items, we replace set their CF representations by a weighted average of the CF representations of their nearest warm items, where the afﬁnity between the cold item to the warm items (and hence the weighting) is determined by the cosine similarity between the CB representations. For fairness, all the evaluated models (except for CF-only) utilized the same multiview CB data as well as the same pretrained models as backbones (BERT, ResNet). B. Warm vs. Cold Balancing (P1) We begin by evaluating the ability of CWH to conduct an adaptable integration of multiple cold items into an existing model (of warm items) and produce recommendation lists that include both cold and warm items. This ability is determined by the γ hyperparameter that serves as a knob to control the exposure rate of the vtoken at the training phase and has a signiﬁcant effect on the results of CWH (note that we set c= 0.5 for all j ∈ J ). Therefore, when γ = 0, the model is never exposed to cold items and acts as an ordinary hybrid model. Similar to previous hybrid models, in this case, CWH is focused on warm item recommendations. While it manages to produce cold item representations based on its content analyzers, these representations are sub-optimal. On the other hand, when γ = 1, the model is exposed to cold items only. As a consequence, it cannot learn CF item representations and collapses to a pure CB model. In this case, CWH is agnostic to the fact that an item is warm or cold since it considers its content only. As we show next, an insufﬁcient exposure of the model to cold items hinders the model’s ability to generalize for such items. In contrast, over exposure to cold items hinders the model’s performance for the warm items. This trade-off is clearly noticeable in Fig. 3, where we present the HR@20 of warm and cold items for different exposure levels (0 ≤ γ ≤ 1). The results in Fig. 3 align with the aforementioned theory. Speciﬁcally, one can notice that when γ = 1, the warm and cold lines coincide (up to an empirical variance). These results indicate that CWH provides the ability to balance between warm and cold items according to varying business needs. Next, we turn to compare CWH against the other baselines. Unlike CWH, these baselines do not feature a control-knob to smoothly balance between warm and cold items. For each method we report the evaluation measurements for warm and cold items separately. In addition, we consider the case of a uniﬁed test set consisting of 90% warm items and 10% cold items. This, we believe, corresponds to a realistic scenario in which the warm catalog is extended using a new cold catalog which istimes its size. The results for Movies, CiteULike, and Apps are presented in Table I. For CWH, we report the results for γ, i.e., the γ value that maximizes the MRR@20 on the uniﬁed catalog on each of the validation sets. We see that CWH outperforms the baselines across all datasets and test sets (warm, cold, and uniﬁed). C. Warm Items Analysis (P2) In this section, we focus on evaluating CWH as a hybrid recommender on warm items only i.e., items that were present in the training set. Some of the warm items are highly popular, while others are extremely rare with an insufﬁcient number of observations a.k.a “long tail” items. As a consequence, CF models suffer from performance degradation for items within the “long tail” [50]–[53]. A hybrid model is expected to compensate for the lack of data in rare items by utilizing the available content data. In order to evaluate this capability, we perform a careful analysis according to the amount of available data (popularity). We denote the test set of user-item pairs as T = {(i, j)} and denote the set of the r most popular items as A. Then, we deﬁne T= {(i, j) : j /∈ A}, i.e. Tas a subset of T in which excludes the top r most popular items. We compute the MRR@20 for each T, where r ∈ {0, 40, 100, 200, 500}. The results for the Movies, CiteULike, and Apps datasets are depicted in Fig. 4. For the sake of clarity, we omit ItemKNN-CFCBF and CB2CF as they perform the worst. Yet, we do include CB-Only for ablation purposes and the CF-only baseline for examining the behaviour of a CF system across the different popularity regimes. This analysis reveals different “layers” that are usually concealed due to the predominance of popular items. Figure 4 shows that CWH outperforms the other baselines in the majority of popularity values. Hence, we conclude that CWH attains excellent results as a regular hybrid recommender (in addition to its ability to integrate completely cold items to an existing catalog). We discuss a common scenario in which a recommender system is expected to perform accurately both on warm items as well as on completely cold items. To this end, we present the CWH model - a deep hybrid model that employs a novel stochastic gating mechanism speciﬁcally designed to mitigate the aforementioned difﬁculty. Through a tunable knob, CWH allows to ﬁnd the desired balance according to business needs. The effectiveness of CWH is demonstrated on three datasets, where it is shown to outperform other alternatives.