A key distinguishing feature of conversational recommender systems over traditional recommender systems is their ability to elicit user preferences using natural language. Currently, the predominant approach to preference elicitation is to ask questions directly about items or item attributes. These strategies do not perform well in cases where the user does not have sucient knowledge of the target domain to answer such questions. Conversely, in a shopping setting, talking about the planned use of items does not present any diculties, even for those that are new to a domain. In this paper, we propose a novel approach to preference elicitation by asking implicit questions based on item usage. Our approach consists of two main steps. First, we identify the sentences from a large review corpus that contain information about item usage. Then, we generate implicit preference elicitation questions from those sentences using a neural text-to-text model. The main contributions of this work also include a multi-stage data annotation protocol using crowdsourcing for collecting high-quality labeled training data for the neural model. We show that our approach is eective in selecting review sentences and transforming them to elicitation questions, even with limited training data. Additionally, we provide an analysis of patterns where the model does not perform optimally. • Information systems → Recommender systems; Users and interactive retrieval. Conversational recommender systems, preference elicitation, question generation ACM Reference Format: Ivica Kostric, Krisztian Balog, and Filip Radlinski. 2021. Soliciting User Preferences in Conversational Recommender Systems via Usage-related Questions. In Fifteenth ACM Conference on Recommender Systems (RecSys ’21), September 27-October 1, 2021, Amsterdam, Netherlands. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3460231.3478861 Traditionally, recommender systems predict users’ preference towards an item by performing oine analysis of past interaction data (e.g., click history, past visits, item ratings) [7]. These systems often do not take into account that users might have made mistakes in the past (e.g., regarding purchases) [28] or that their preferences change over time [9]. Additionally, for some users, there is little historical data which makes modeling their preferences dicult [11]. A conversational recommender system (CRS), on the other hand, is a multi-turn, interactive recommender system that can elicit user preferences in real-time using natural language [10]. Given its interactive nature, it is capable of modeling dynamic user preferences and take actions based on users current needs [7]. One of the main tasks of a conversational recommender system is to elicit preferences from users. This is traditionally done by asking questions either about items directly or item attributes [4–7,12,25, 26,30,32]. Asking directly about specic items is inecient due to a vast number of items in the collection; therefore, the majority of the research is focused on the estimation and utilization of users preferences towards attributes [7]. Common to these approaches is that the user is explicitly asked about the desired values for a specic product attribute, much in the spirit of slot-lling dialogue systems [8]. For example, in the context of looking for a bicycle recommendation, we might have wheel dimensions or the number of gears as attributes in our item collection. In this case, a system might want to ask a question like How thick should the tires be? or How many gears should the bike have? However, ordinary users often do not possess this kind of attribute understanding, which might require extensive domain-specic knowledge. Instead, they only know where or how they intend to use the item. For example, a user might only be interested in using this bike for commuting but does not know what attributes might be good for that purpose. The novel research objective of this work is to generate implicit questions for eliciting user preferences, related to the intended use of items. This stands in contrast with explicit questions that ask about specic item attributes. Our approach hinges on the idea that usage-related experiences are captured in item reviews. By identifying review sentences that discuss particular item features or aspects (e.g., fat tires) that matter in the context of various activities or usage scenarios (e.g., for conquering tough terrain), those sentences can then be turned into preference elicitation questions. In our envisaged scenario, a large collection of implicit preference elicitation questions is generated oine, and then utilized later in real-time interactions by a CRS; see Fig. 1 for an illustration. In this paper, our focus is on the oine question generation part. Specically, we start with candidate sentence selection, which can eectively be implemented based on part-of-speech tagging and simple linguistic patterns. Given a candidate sentence as input, question generation produces an implicit question or the label Figure 1: Conceptual system overview. Our focus in this paper is on the implicit question generator component. N/A (not applicable). This is done by ne-tuning a pre-trained, sequence-to-sequence model for text generation [24]. The main challenge associated with this task is the collection of high-quality training data. We develop a multi-stage data annotation protocol via crowdsourcing to generate a sentence-to-question dataset. The process consists of generating questions, validating them, as well as expanding the variation of questions. Evaluating our proposed approach against held-back test data shows its eectiveness and its capability of generating questions that are suitable for preference elicitation, can simply be answered, and are grammatically correct. In summary, our main contributions in this paper are as follows: (1) Introduce the novel task of eliciting preferences in CRSs via implicit (usage-oriented) questions. (2) Devise an approach for generating usage-related questions based on a corpus of item reviews, consisting of two main steps: candidate sentence selection (based on linguistic patterns) and question generation (using a neural sequence-to-sequence model). (3) Develop a multistage data annotation protocol using crowdsourcing for collecting high-quality ground truth data. (4) Perform an experimental evaluation of the proposed approach, followed by an analysis of results. The resources developed in this paper (crowdsourced dataset and question generation model) are made publicly available at https://github.com/iai-group/recsys2021-crs-questions. In this paper, we focus on question-based user preference elicitation and natural language generation, both of which are identied as major challenges in [7]. That is, we provide novel answers to questions what to ask and how to ask. Commonly, preference elicitation questions target either items or their attributes. Typical of early studies on CRS, item-based elicitation approaches ask for users’ opinions on an item itself, using a combination of methods from traditional recommender systems, such as collaborative ltering, with user interaction in real time [27,35]. The selection of items may be approached as an optimization problem using a static preference questionnaire method [25] or multi-armed bandit algorithms that capture the exploration-exploitation tradeo [6,27]. Asking about items directly has been found inecient, as large item sets would require several conversational turns and in turn increase the likelihood of users getting bored [7]. Alternatively, attribute-based elicitation aims to predict the next attribute to ask about. It is often cast as a sequence-to-sequence prediction problem, lending itself naturally to sequential neural networks. However, obtaining large conversational datasets to train conversational recommender systems is challenging [10], therefore, non-conversational data is often leveraged. Christakopoulou et al. [5]propose a question & recommendation (Q&R) method, utilize data from a non-conversational recommendation system, and develop surrogate tasks to answer questions: What to ask? and How to respond? A similar approach of training a sequential neural network on non-conversational data is taken by Zhang et al. [32], who convert Amazon reviews into articial conversations. The assumption is that the earlier aspect-value pairs appear in the review, the more important they are to the user and should be prioritized as questions. Additionally, they develop a heuristic trigger to decide whether the model should ask about another attribute or recommend an item. Another way to elicit preferences is in the form of critiques, i.e., feedback on attribute values of recommended items [4]. For example if the recommendation is for a phone, a critique might be not so big or something cheaper. Such methods often employ heuristics as elicitation tactics [18,19]. In recent work, Balog et al. [1]study the problem of robustly interpreting unconstrained natural language feedback on attributes. While there is research on end-to-end frameworks to enable CRSs to both understand user intentions as well as generate uent and meaningful natural language responses [13], the predominant approach is still to use templates or construct the utterances using predened language patterns [7]. Looking at the broader eld of dialogue systems, there are two additional strands of research that could be applied to CRSs as well: retrieval-based and generation-based methods. Instead of relying on a handful of templates, retrieval-based methods utilize a large collection of possible responses. The basic approach to retrieving the appropriate response is based on some notion of similarity between the user query and candidate responses, with the simplest being inner product [31]. Generation-base d methods in dialogue systems are typically based on sequence-to-sequence modeling. These models are usually trained on a hand-labeled corpus of task-oriented dialogue [3]. Our proposed approach shares elements of both of these methods: it generates questions using a sequence-to-sequence model and stores them in a collection that can be queried using retrieval-based methods. We present an approach for generating a collection of implicit elicitation questions from a review corpus. Item review datasets tend to be very large, with both the number of items and reviews in the thousands or even millions, making labeling the entire dataset extremely expensive [14]. To overcome this, we extract candidate sentences from a corpus that have a high probability of mentioning item-related activity or usage (Section 3.1). We wish to train a model that can take a candidate sentence as input and generate a preference elicitation question out of that or the label N/A if it is not possible (Section 3.2). We opt for a pre-trained, transformer-based, state-of-the-art, sequence-to-sequence model (T5). We identify sentences that describe some item feature or aspect (§3.1.1) and mention some activity or usage (§3.1.2). 3.1.1 Aspect-Value Pair Extraction. An aspect in this context is a term that characterizes a particular feature of an item [17] (e.g., wheel, seat or gear are aspects of a bicycle). Value words are terms that describe an aspect (e.g., a wheel might be large or small, a seat can be hard or comfortable). Here, we extract all sentences that mention some aspect-value pair for a given category of items, using phrase-level sentiment analysis proposed by Zhang et al. [33,34]. The motivation for this step stems from the assumption that an activity or usage can be mapped to a particular aspect of an item. 3.1.2 Activity Identification. In this step, the goal is to classify sentences that mention some item-related activity or usage. Inspired by Benetka et al. [2], our approach revolves around using part-ofspeech (POS) analysis and rules of the English language. We lter for the preposition for followed by a verb in progressive tense heuristically, by looking for -ing endings (e.g., for commuting, for hiking). Note that there might be other formulations that describe activity or usage. Our goal is not to extract all possible sentences containing mentions of activity or usage; a high recall approach would likely come at the cost of a larger fraction of false positives. Instead, we focus on achieving high precision. The main motivation for this step is generating natural-sounding questions that are easy for users to understand and answer, without needing any additional context. Consider the sentence The fat tires are perfect for conquering tough terrain. An example of converting it to a yes or no usage-related question might be Would you like a bike that is great for conquering tough terrain? Note that not all candidate sentences that pass our selection heuristic are viable for conversion to a question, e.g., Thank you so much for coming up with such a great product. This sentence is too vague and does not mention any action or usage for the item, and thus should be labeled as not applicable (N/A). Learning to generate questions is done by ne-tuning a large, pre-trained, sequence-to-sequence language model. There are two main benets of using transfer learning from a pre-trained model. First, it increases the learning speed; as both syntax and semantics of the English language are already learned, there are fewer things the model needs to learn. Second, it reduces the amount of labeled data needed to train models to high performance. Specically, we use T5 [24] as it is a state-of-the-art approach that can be used for both N/A-classication and generation in one go, where N/Aclassication is posed as a text-to-text problem. Obtaining highquality labeled data for ne-tuning the model is a challenge on its own; we develop a multi-step data collection protocol using crowdsourcing, which we discuss in Section 4.2. In our experiments (in Section 5), we evaluate our question generation models using dierent number of parameters for pre-training and varying amount of training data for ne-tuning. This section describes the process of creating our dataset, which consists of a set of review sentences and either (i) a corresponding set of ve preference elicitation questions or (ii) the label not applicable (N/A) for each. The starting point for getting the candidate sentences are the Amazon review and metadata datasets [22],where item reviews from Amazon are extracted along with product metadata information such as title, description, price, and categories. There are, in total, 233.1M reviews about 15.5M products. Due to the sheer dataset size, we focus our research on three main categories: Home and Kitchen, Patio, Lawn and Garden, and Sports and Outdo ors . From these, we further sub-select 12 diverse subcategories (simply referred to as categories henceforth): Backpacking Packs, Tents, Bikes, Jackets, Vacuums, Blenders, Espresso Machines, Grills, Walk-Behind Lawn Mowers, Birdhouses, Feeders, and Snow Shovels. Sentence splitting and aspect-value pair extraction is performed using the Sentires toolkit [33,34].This step discards many nonviable sentences. The remaining ones are POS-tagged using the Stanford NLP toolkit [20]. Finally, we lter for sentences that match our activity detection heuristic (“for + [verb in progressive tense]”). Our sentence selection process is designed to favor precision over recall, and was validated by manual inspection of the results. Upon completion of the crowdsourcing tasks (described in Section 4.2), we nd that over 75% of the selected sentences can be turned into questions. This shows that our simple method can indeed identify candidate sentences with high precision. Our nal candidate sentence set contains 100 sentences for each of the categories, except Birdhouses, where only 15 candidate sentences are found due to the size of that category, that is, a total of 1,115 sentences over 12 categories. Crowdsourcing was done on the Amazon Mechanical Turk (AMT) platform in three steps. The task was available to workers with 95% approval rate and with at least 1,000 approved human intelligence tasks (HITs). 4.2.1 Step 1: estion Collection. Crowd workers are given a review sentence (describing some aspect or use for a product) and a product category as input, and tasked with rewriting it into a question or marking it as not applicable. They are specically instructed to formulate a question that a salesperson or a recommender agent might ask a customer, such that it is a standalone question that can simply be answered with yes/no. For every input sentence, we collected responses from three dierent workers. Sentences found non-applicable by at least two workers are set as N/A. The task was re-run if a single worker responded with N/A. This process resulted in approx. 2,600 sentence-question pairs. 4.2.2 Step 2: Validation and Filtering. Next, we validate all responses (i.e., generated questions) for applicable sentences collected in Step 1 using crowdsourcing. We employ three dierent workers in Step 2, who are requested to answer four multiple-choice questions: (1) Is the question grammatically correct? [Yes/No] (2) Can the question be answered by yes or no? [Yes/No] (3) Does the question mention any trait or use for a product? [Yes/No] (4) Who is most likely to ask this question in a sales setting? [Buyer/Salesperson/Neither]. Generated questions that are found invalid by all three workers on a single aspect or at least two workers on at least two aspects are automatically rejected. Those that are marked invalid on multiple aspects but do not fall into the former category are manually checked by an expert annotator (one of the authors). All other questions are approved. Steps 1 and 2 were run multiple times until all questions were approved. 4.2.3 Step 3: Expanding estion Variety. Our main motivation for expanding the question variety is to add new ways of asking implicit questions. To this end, we task a new set of workers to paraphrase the questions we obtained and validated in Steps 1 and 2. Each worker receives all three versions of the questions from Step 1 as input and is asked to produce a new (paraphrased) question the expresses the same meaning. Note that this set of workers do not get to see the original sentences, only the questions generated from them by other workers. For each set of three questions, two additional paraphrases were collected. Considering that generating paraphrases proved to be a much simpler task than generating questions from review sentences, no additional quality assurance steps were necessary. Out of the 1,115 candidate sentences, 277 were labeled as nonapplicable (not containing relevant usage-related information), which is below 25%. This shows that our high-precision approach to selecting candidate sentences is eective. We note that our sentence selection method works better for some categories than for others. The fraction of viable sentences ranges between 52% (Espresso machine category) to 84% (Backpacking pack category). For the remaining 838 sentences, a total of ve questions are generated, three based on the candidate sentence and two via paraphrasing. Table 1 shows two example sentences from our dataset. The total cost of generating the dataset was $1,200. With our experiments, we aim to answer the following research question: How eective is our method for generating implicit questions for preference elicitation? Specically, given a candidate sentence as input, our approach should either generate a question or label it as non-applicable (N/A), if a usage-related question cannot be generated. We train small, base, and large T5 models, which vary in the number of layers, self-attention heads, and the dimension of the nal feedforward layer. The dierence is shown in the number of parameters in Table 2. We use 80% of the data for training, while the rest is test data. In our training, we employ teacher forcing [29], regularization by early stopping [21], and adaptive gradient method AdamW [16] with linear learning rate decay. For each sentence, we have either N/A or a set of reference questions. We evaluate question generation both as a classication task, in terms of Accuracy (detecting N/A), and as a machine translation task, where the set of human-generated questions serve as reference translations. Specically, we report on BLEU-4, which uses modied n-gram precision up to 4-grams [23], and ROUGE-L, a recall-based metric based on the longest common subsequence [15]. Table 2 shows the results in terms of non-applicability classication (Accuracy) and question generation (BLEU and ROUGE). On both tasks, larger pre-trained models tend to perform better, which is Table 2: Question generation performance using dierent pre-trained models that are ne-tuned on all available training data (i.e., ve questions or N/A per sentence). Best scores for each measure are in boldface. Model #Parameters Accuracy BLEU-4 ROUGE-L expected. The dierence, however, is more pronounced for nonapplicability detection than for question generation. We further investigate how the amount of training data aects model performance, by considering dierent ways of data reduction. We use the best performing model for this experiment, i.e., T5 large. In sentence-based data reduction, shown in Fig. 3 (Left), only a subset of the available sentences is used for training (using all available questions corresponding to those sentences). We observe a drop in Accuracy when we reduce the amount of training data to 25% or lower, while question generation performance is less severely aected. In question-based data reduction, shown in Fig. 3 (Right), we split the dataset based on the number of questions available for each sentence. We consider using a single question (Q1), the three initially generated questions (Q3), and the three initial questions plus the two paraphrases (Q5). We nd that reducing the number of questions has surprisingly little eect. This suggests that it is more benecial to collect a small number of questions for a larger set of sentences than vice versa. A closer look at specic sentence-question pairs reveals two patterns that leave room for future improvement. We nd that some of the generated questions are too generic. These are correct in terms of grammar and structure, but unsuitable for eliciting meaningful user preferences, e.g., Do you need a grill that is good for grilling certain things? Instead of returning N/A (which is indeed the corresponding response in our dataset), the model generated a question that is so vague and generic that it is hard to think of a scenario where it would not be answered armatively. The second pattern concerns complex questions that ask about more than one usage or activity, e.g., Are you looking for a backpacking pack that is a good size for traveling on an airplane or going on a camping trip for a few days or packing for a few days trip? This question is too complex Figure 3: Model performance (T5-large) with sentence-based (Left) or question-based (Right) training data reduction. to elicit any meaningful information without the user having to elaborate which options they agree with and which they do not. Such questions should instead be split into several simpler ones where it is both easier to interpret the question and to answer it. Note that crowd workers were not instructed to simplify complex questions, therefore it is not surprising that is what the model has learned. In this paper, we have studied the question of how a conversational recommender system can solicit user’s needs through natural language by using indirect questions about how the product wanted will be used. This contrasts with most prior work that considers how to directly ask about desired product attributes. Our method starts with a corpus of reviews, then identies statements that characterize how products are used, and how this ties to product attributes. These statements are then transformed into preference elicitation questions. We show that our approach eectively selects such statements (with high precision), and transforms them into eective questions. We emphasize that this work focuses on this rst stage of recommendation, understanding the user’s needs, and doing this in an engaging way. The most important future direction is determining how answers to these questions should best be applied to the recommendation task once the user’s need is understood. Here, we believe that sentence embedding techniques are likely to be eective. Second, as this work builds on top of large language models, language safety is a key consideration warranting further study before our approach could be used in practice. Nevertheless, during experimentation we did not observe concerning language nor hallucinations. We also note that the oine question generation process lends itself well to even manual control over the language model output.