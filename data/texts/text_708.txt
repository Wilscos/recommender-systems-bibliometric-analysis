user preference over various items in many online services such as news portals [2] and social networks [3]. As the clue to user choices, implicit feedback (e.g., click and purchase) are typically the default choice to train a recommender model due to their large volume. However, prior work [2], [4], [5] points out the gap between implicit feedback and the actual user satisfaction due to the existence of noisy interactions (a.k.a. false-positive interactions) where the users dislike the interacted items. For instance, in E-commerce, a large portion of purchases end up with negative reviews or being returned. This is because implicit interactions are easily affected by the ﬁrst impression of users and factors [6] like caption bias [7] and position bias [8]. Moreover, existing studies [5] have validated the detrimental effect of such noisy interactions on user experience. However, little work on recommendation has considered the noisy nature of implicit feedback. In this work, we argue that false-positive interactions would prevent a recommender model from learning the actual user preference, leading to low-quality recommendations. Table 1 provides empirical evidence on the negative effects of false-positive interactions when we train a competitive recommender model, Neural Matrix Factorization (NeuMF) [3], on two real-world datasets. We construct a “clean” testing set by removing the false-positive interactions for recommender evaluation. As shown in Table 1, training NeuMF with false-positive interactions (i.e., normal training) results in sharp performance drop as compared to the NeuMF trained without false-positive interactions (i.e., clean training). Therefore, it is essential to denoise implicit feedback for recommender learning. Existing studies [9]–[11] on denoising implicit feedback mainly fall into two categories: 1) negative experience identiﬁcation [10]; and 2) the incorporation of various feedback [11], [12]. As illustrated in Figure 1(b), the former identiﬁes the false-positive interactions with additional user behaviors (e.g., dwell time and gaze pattern) and auxiliary item features (e.g., length of the item description) [2] before recommender training. Besides, Figure 1(c) shows that the latter leverages extra feedback (e.g., favorite and skip) to prune the effects of false-positive interactions [12], for example, multi-task learning [13]. A key limitation with these methods is that they heavily rely on extra feedback or auxiliary item features, which might be hard to collect. Furthermore, extra feedback (e.g., rating or favorite) is often of a smaller scale, which will suffer from the sparsity issue. For instance, many users do not give any extra feedback after watching a movie or purchasing a product [4]. This work aims to automatically reduce the negative effect of false-positive interactions, where we mainly count on the implicit interactions for denoising. Prior research on robust learning [14], [15] and curriculum learning [16] demonstrates that noisy samples are relatively harder to ﬁt into models, exhibiting distinct patterns in the model training. We conduct preliminary experiments across different recommender models and datasets, obtaining a similar observation: the loss values of false-positive interactions are relatively larger in the early stages of training (cf. Figure 2). Consequently, false-positive interactions will mislead the recommender training in the early stages. Worse still, the recommender model ultimately ﬁts the false-positive interactions due to its high representation capacity, which could be overﬁtting and hurt the generalization. In this light, a potential solution of denoising is to prune the interactions with large loss values in the early training stage. Towards this end, we propose Adaptive Denoising Training (ADT), which dynamically prunes the large-loss interactions along the training process (illustrated in Figure 1(d)). To avoid losing generality, we only modify the training loss, which can be applied to any differentiable models. In detail, we devise two paradigms to formulate the training loss: 1) Truncated Loss, which discards the large-loss interactions dynamically with a threshold function, and 2) Reweighted Loss, which adaptively assigns hard interactions (i.e., large-loss ones) with smaller weights to reduce their inﬂuence on the recommender optimization. To keep the fexibility of ADT in the scenarios with easily available extra feedback, we propose two training strategies to utilize extra feedback (Figure 1(e)): 1) ﬁnetuning, which reﬁnes the recommender model with extra feedback after ADT; and 2) warm-up training, which trains the recommender model with extra feedback before ADT. To tackle the sparsity issue of extra feedback, we design an inference strategy, called colliding inference, which leverages the colliding effect [17] of extra feedback on the predictions of sparse users for better recommendation. From a causal view, extra feedback and the predictions of sparse users are d-separated due to the sparsity issue. By conditioning on a collider between them, we make them dconnected and leverage the colliding effect to augment the predictions of sparse users [18]. We instantiate ADT on the widely used binary cross-entropy (CE) loss. On three benchmarks, we test the Truncated Loss and Reweighted Loss over three representative recommender models: Generalized Matrix Factorization (GMF) [3], NeuMF [3], and Collaborative Denoising Auto-Encoder (CDAE) [19]. The results show signiﬁcant performance improvements of ADT over normal training. Besides, we conduct extensive experiments to validate the effectiveness of the strategies that incorporate extra feedback into ADT. They further improve the performance of the vanilla ADT, and outperform some competitive baselines that use extra feedback for training. Codes and data are publicly available at: https://github. com/WenjieWWJ/DenoisingRec. Our main contributions are summarized as: and propose Adaptive Denoising Training to prune the large-loss interactions dynamically, which introduces two paradigms to formulate the training loss: Truncated Loss and Reweighted Loss. training, and colliding inference) to incorporate extra feedback into ADT, which provides the potential to further improve the denoising ability of ADT. effectiveness of the proposed ADT and three strategies in improving the recommendation quality. The inﬂuence of noisy training samples has been explored in many conventional machine learning tasks such as image classiﬁcation [14], [15]. However, little work has considered denoising implicit feedback in recommendation, which is inherently different from conventional tasks due to the relations across training samples, e.g., interactions on the same item. We investigate the effects of false-positive interactions on recommender training by comparing the performance under two settings: 1) normal training, which utilizes all observed user-item interactions, and 2) clean training, which discards false-positive interactions. An interaction is identiﬁed as false-positive or true-positive one according to the explicit feedback. For instance, a purchase is false-positive if the following rating score ([1, 5]) < 3. Although such explicit feedback is sparse, the scale is sufﬁcient to conduct a pilot experiment and construct a clean testing set. The recommender models are evaluated on a holdout clean testing set with only truepositive interactions kept, i.e., the evaluation focuses on recommending more satisfying items to users. More details can be seen in Section 5. Results. From Table 1, we can observe that, as compared to clean training, the performance of normal training drops signiﬁcantly (e.g., 21.81% w.r.t. Recall@20 on Adressa). It validates the negative effects of false-positive interactions on recommending satisfying items to users. Worse still, if the recommender models under normal training are deployed, they will have higher risk to produce more false-positive interactions, which further hurt the user experience [2]. In this section, we detail the proposed ADT and three strategies to incorporate extra feedback into ADT. Prior to that, the task formulation of denoising recommender training and some observations that inspire the strategy design are introduced. 3.1 Task Formulation Generally, the target of recommender training is to learn a scoring function ˆy= f(u, i|Θ) to assess the preference of user u over item i with parameters Θ. Ideally, Θ is learned from reliable feedback between users (U) and items (I) by minimizing a recommendation loss L (e.g., the binary CE loss). Formally, Θ= arg minL(D) where D= {(u, i, y)|u ∈ U, i ∈ I}, and y∈ {0, 1} represents whether the user u really prefers the item i. The recommender model with Θwould be reliable to generate high-quality recommendations. In practice, due to the lack of large-scale reliable feedback, recommender training is typically formalized as:¯Θ = arg minL(¯D), where¯D = {(u, i, ¯y)|u ∈ U, i ∈ I} is a set of implicit interactions. ¯ydenotes whether the user u has interacted with the item i, such as click and purchase. However, because noisy interactions in¯D would mislead the learning of Θ, the typical recommender training might lead to a poor model (i.e.,¯Θ) that lacks generalization ability on the clean testing set. As such, we formulate a denoising recommender training task as: aiming to learn a reliable recommender model with parameters Θby denoising implicit feedback¯D. Formally, by assuming the existence of inconsistency between y and ¯y, we deﬁne noisy interactions (a.k.a. false-positive interactions) as {(u, i)|y= 0 ∧ ¯y= 1} [1]. 3.2 Observations False-positive interactions are harder to ﬁt in the early stages. According to the theory of robust learning [14], [15] and curriculum learning [16], easy samples are more likely to be the clean ones and ﬁtting the hard samples may hurt the generalization. To verify the existence of this phenomenon in recommendation, we compare the loss of true- and false-positive interactions along the process of normal training. Figure 2 shows the results of NeuMF on the Adressa dataset. Similar trends are also found over other recommender models and datasets (see more details in Section 5.2). From Figure 2, we observe that: interactions converges to a stable state with close values, which implies that NeuMF ﬁts both of them well. It reﬂects that deep recommender models can “memorize” all the training interactions, including the noisy ones. As such, if the data is noisy, the memorization will lead to poor generalization performance. decrease differently in the early stages of training. From Figure 2(b), we can see that the loss of false-positive interactions is clearly larger than that of the true-positive ones, which indicates that false-positive interactions are harder to memorize than the true-positive ones in the early stages. The reason might be that false-positive ones represent the items that users dislike, and they are more similar to the items the user didn’t interact with. The ﬁndings also support the prior theory in robust learning and curriculum learning [15], [16], [20]. Overall, the results are also consistent with the memorization effect [21]: deep recommender models will learn the easy and clean patterns in the early stage, and eventually memorize all training interactions [15] because of the strong representation ability. According to the observations, we propose ADT strategies for recommender models, which estimate P (y= 0|¯y= 1, u, i) according to the training loss. In particular, ADT either discards or reweighs the interactions with large loss values to reduce their inﬂuences on the recommender training. Towards this end, we devise two paradigms to formulate loss functions for denoising training without using extra feedback: interactions to 0 with a dynamic threshold function. with smaller weights during training. Note that the two paradigms can be applied to various recommendation loss functions, e.g., CE loss, square loss, and BPR loss [22]. In the work, we take CE loss as an example to elaborate them. Functionally speaking, the Truncated Cross-Entropy (shorted as T-CE) loss discards positive interactions according to the values of CE loss. Formally, we deﬁne it as: L(u, i) =(u, i) > τ ∧ ¯y= 1L(u, i), otherwise,(2) where τ is a pre-deﬁned threshold. The T-CE loss removes any positive interactions with CE loss (i.e., L) larger than τ from the training. While the T-CE loss is easy to interpret and implement, the ﬁxed threshold may not work properly. This is because the loss value is decreasing with the increase of training iterations. Inspired by the dynamic adjustment ideas [23], we replace the ﬁxed threshold with a dynamic threshold function τ(T ) w.r.t. the training iteration T , which changes the threshold value along the training process (Figure 3). Besides, since the loss values vary across different datasets, it would be more ﬂexible to devise τ(T ) as a function of the drop rate (T ). There is a bijection between the drop rate and the threshold, i.e., we can calculate the threshold based on the drop rate (T ) at each iteration. Based on prior observations, a proper drop rate function should satisfy: 1) (·) should have an upper bound to limit the proportion of discarded interactions so as to prevent data missing; 2) (0) = 0, i.e., it should allow all the interactions to be fed into the models in the beginning; and 3) (·) should increase smoothly from zero to its upper bound, so that the model can learn and distinguish the trueand false-positive interactions gradually. To this end, we formulate the drop rate function as: where is an upper bound and α is a hyper-parameter to adjust the pace to reach the maximum drop rate. Here we increase the drop rate in a linear fashion rather than a more complex function such as a polynomial function or a logarithm function. Despite the expressiveness of these functions, they have more hyper-parameters, increasing cost of tuning a recommender model. The algorithm is formally explained in Algorithm 1. 3.3.2 Reweighted Cross-Entropy Loss Generally, the Reweighted Cross-Entropy (shorted as R-CE) loss down-weights the hard positive interactions. Formally, where ω(u, i) is a weight function that adjusts the contribution of an interaction to the training objective. To properly down-weight the hard interactions, the weight function ω(u, i) is expected to have the following properties: 1) it dynamically adjusts the weights of interactions during training; 2) the function makes the inﬂuence of a hard interaction weaker than an easy interaction; and 3) the extent of weight reduction can be easily adjusted to ﬁt different models and datasets. Inspired by the success of Focal Loss [24], we estimate ω(u, i) according to the prediction score ˆy. The prediction score is within [0, 1] whereas the value of CE loss is in [0, +∞]. And thus the prediction score is more accountable for further computation. Note that the smaller prediction score on the positive interaction leads to larger CE loss. Formally, we deﬁne the weight function as: where β ∈ [0, +∞] is a hyper-parameter to control the range of weights. From Figure 4(a), we can see that R-CE loss can signiﬁcantly reduce the loss of hard interactions (e.g., ˆy 0.5) as compared to the original CE loss. Furthermore, the proposed weight function satisﬁes the aforementioned requirements: changes along the training process. “outlier” in Figure 4(b)) will be assigned with very small weights because ˆyis close to 0. Therefore, the inﬂuence of such large-loss interactions is largely reduced. Besides, as shown in Figure 4(b), harder interactions always have smaller weights because the function ˆymonotonically increases w.r.t. ˆywhen ˆy∈ [0, 1] and β ∈ [0, +∞]. As such, it can avoid that false-positive interactions dominate the optimization during training [25]. between the weights of hard and easy interactions. By observing the examples in Figure 4(b), we can ﬁnd that: 1) the gap between the weights of easy and hard interactions becomes larger as β increases from 0 to 1 (e.g., d< d) when ˆy> 0.5; and 2) the R-CE loss will degrade to a standard CE loss if β is 0. Lastly, in order to prevent negative interactions with large loss values from dominating the optimization, we revise the weight function as: Indeed, it may provide a idea to alleviate the impact of falsenegative interactions, which is left for future exploration. 3.3.3 In-depth Analysis Due to depending totally on recommender models to identify false-positive interactions, the reliability of ADT needs to be discussed. Actually, existing work [14], [15] has pointed out the connection between the large loss and noisy interactions, and explained the underlying causality: the “memorization” effect of deep models [21]. We discuss the memorization effect of recommender models by experiments in Section 3.2 and 5.2. Another concern is that discarding hard interactions would limit the model’s learning ability since some hard interactions may be more informative than the easy ones. Indeed, as discussed in the prior studies on curriculum learning [16], hard interactions in the noisy data probably confuse the classiﬁer rather than help it to establish the right decision surface. It’s actually a trade-off between denoising and learning. In ADT, the (·) of T-CE loss and β of R-CE loss are used to control the balance. 3.4 ADT with Extra Feedback Although extra feedback (e.g., ratings) is usually sparse, it is reliable to reﬂect the actual user satisfaction, i.e., indicating the true-positive interactions. We thus further utilize extra feedback for ADT when available. 3.4.1 Training with Extra Feedback By considering the order of training with implicit feedback and extra feedback, we introduce two training strategies: ﬁnetuning and warm-up training. Finetuning. As shown in Figure 5(a), an intuitive idea is to retrain the recommender model by ﬁtting the available true-positive interactions. Speciﬁcally, we ﬁrst train the recommender model with ADT over the implicit feedback, and ﬁnetune the model over the extra feedback. The ﬁnetuning will reﬁne the recommendation lists of users based only on the extra feedback. Formally, given the extra feedback D, the model parameter Θobtained from ADT will be further optimized by: where the negative interactions in Dare sampled with the help of implicit feedback¯D. Only the interactions which are not in¯D are sampled into Dto partly avoid using the missing true-positive interactions as negative ones. Warm-up Training. From Section 3.2, we know that the recommender model learns to distinguish the truepositive and false-positive interactions in the early stage of training. If extra feedback Dis available, we can leverage it to “warm up” the recommender model before ADT (illustrated in Figure 5(b)). Warm-up training will prevent the recommender model from being misled by false-positive interactions in the beginning of training. The recommender model will thus be more reliable to identify false-positive interactions by the large loss values during ADT. Formally, warm-up training can be seen as optimizing the model initialization for ADT by Equation (7). The key to the success of incorporating extra feedback lies in resolving its sparsity issue. For the users with sparse extra feedback, the inﬂuence of extra feedback Don their ﬁnal prediction scores ˆy is weakened due to the small number of extra feedback as opposed to implicit feedback. Therefore, our target is to enhance the effect of reliable Don ˆy. Implementation. We achieve the target during model inference by supplementing the model prediction for sparse user u with similar users. Formally, where ˆy= [ˆy, ..., ˆy]denotes the model prediction of user u over all items (i.e., item set I), Nis the neighbor set of user u, ˆyrepresents the prediction scores of neighbor j, and wdenotes the weights to adjust the contribution of each neighbor. Speciﬁcally, we calculate the user similarity via inner product in the user representations space learned from extra feedback, and then select |N| similar neighbors for each user. The weighted sum of prediction scores of the neighbors summarizes the colliding effect of extra feedback on the ﬁnal prediction score ˆy(see the following section for the explanation). Besides, a hyper-parameter λ is employed to balance the original prediction scores ˆyand the colliding effect. ˆyis the ﬁnal ranking score for making recommendation. Intuitively, the neighbors are obtained from a user representation space learned from true-positive interactions, where the interactions reﬂect the actual user preference of sparse users. Due to the sparsity of extra feedback, some interests of sparse users are not reﬂected by the representation, making the representation unsuitable for making prediction. Nevertheless, such representation reﬂects user similarity regarding their true interests. As such, we lookup neighbors according to the representation and augment the recommendations of the sparse user with neighbors’ recommendations. Rationality. The idea of Equation 8 comes from the colliding effect in causality theory [17]. In this work, we utilize warmup training as an example to illustrate the underlying theory of colliding inference because of its superior performance to enhance ADT (see the empirical evidence in Section 5.3). As shown in Figure 6, we introduce a causal graph [17], [26] to inspect the causal relations in ADT with warmup training for sparse users, where nodes and edges represent the variables and causal relations, respectively [6]. Speciﬁcally, it presents extra feedback (D), implicit feedback (¯D), user representations after warm-up training (E), user representations after ADT (ˆE), prediction scores over items after warm-up training (Y), and prediction scores over items after ADT (ˆY ). In particular, learned from the user feedback. ¯D → E: the user representations after warm-up training are affected by implicit feedback because it is used to sample negative interactions during training. → YorˆE →ˆY: the prediction scores are based on the user representations. According to the d-separation principle [17], Dis independent from¯D,ˆE, andˆY , which blocks the effect of extra feedback Don the ﬁnal prediction scoreˆY of sparse users. Considering that Eis a collider between Dand ¯D, conditioning on E* will make DandˆY conditionally d-connected [17], [18]. In this way, we can estimate the colliding effect [26] of DonˆY , which is formulated as:, =P (ˆY |¯D)(P (¯D|E, D= d) − P (¯D|E, D= ∅)), where D= ∅ denotes the reference status without any extra feedback [6], [18] and dindicates the true-positive interactions of user u. P (ˆY |E,¯D) = P (ˆY |¯D) because E can only affectˆY via¯D. Intuitively, the causal effect in Equation (9) quantiﬁes the inﬂuence of extra feedback of user u on the prediction scores by comparing two statuses. We utilize the technique of Distilling Colliding Effect [18] to calculate Equation (9). In particular, we will employ a weighting method to estimate the causal effect. Formally, P (¯D|E, D= d) − P (¯D|E, D= ∅) indicates the the probability of selecting users’ implicit feedback¯D given Eand D, which can be denoted as the weight W (¯D, E, D) [18], corresponding to the win Equation (8). Besides, P (ˆY |¯D) represents the prediction scores after ADT given the implicit feedback¯D of one user. Because of the large sample space of¯D, the sum over¯D is approximated by only selecting the implicit feedback of neighbors N with the highest weights W (·). In this way, Equation (9) is estimated by the weighted sum of P (ˆY |¯D) over theP neighbors, i.e.,wˆy. Correspondingly, we search the neighbors of each sparse user according to similarity in the space of E, and then reﬁne the user’s prediction scoresˆY by combining the prediction scores of the neighbors. Denoising implicit feedback is highly related to the negative experience identiﬁcation, incorporating various feedback, and the robustness of recommendation. Besides, we introduce causal recommendation that is related to colliding inference. Negative Experience Identiﬁcation. To mitigate the gap between implicit feedback and the actual user preference, many studies have focused on identifying negative experiences in implicit signals [9], [27], [28]. Prior work usually collects the various users’ feedback (e.g., dwell time [10], gaze patterns [29], and skip [9]) and the item characteristics [2], [30] to predict the user’s satisfaction. However, they need additional feedback and extensive manual label work, e.g., users have to tell if they are satisﬁed for each interaction. Besides, the quantiﬁcation of item quality is non-trivial [2], which largely relies on the manually feature design and the labeling of domain experts [2]. The unaffordable labor cost hinders the practical usage of these methods, especially in the scenarios with constantly changing items. Incorporating Various Feedback. To alleviate the impact of false-positive interactions, previous approaches [11], [31] also consider incorporating more feedback (e.g., dwell time [12], skip [5], and adding to favorites) into training directly. For instance, Wen et al. [5] proposed to train the recommender using three kinds of items: “click-complete”, “click-skip”, and “non-click” ones. The last two kinds of items are both treated as negative samples but with different weights. However, extra feedback might be unavailable in complex scenarios. For example, we cannot acquire dwell time and skip patterns after users buy products or watch movies in a cinema. Most users even don’t give any informative feedback after clicks. In an orthogonal direction, this work explores denoising implicit feedback without additional data during training, i.e., ADT. Furthermore, if some extra feedback (e.g., ratings) is available, we also introduce three strategies to utilize it to enhance ADT. Robustness of Recommender Systems. Gunawardana et al. [32] deﬁned the robustness of recommender systems as “the stability of the recommendation in the presence of fake information”. Prior work [33], [34] has tried to evaluate the robustness of recommender systems under various external attacks, such as shilling attacks [33], fuzzing attacks [34],1) Extra feedback is unavailable during training (Section and fake feedback from fraudsters [35]. To build more robust recommender systems, some auto-encoder based models [19], [36], [37] introduce the denoising techniques. These approaches (e.g., CDAE [19]) ﬁrst corrupt the interactions of user by random noises, and then try to reconstruct2) Sparse extra feedback is available during training (Section the original one with auto-encoders. Besides, adversarial learning [38] might serve as a reasonable solution to defend the diverse attacks or detect fake users [39], [40]. And robust learning [15] will also be a promising choice to empower recommender models with the ability of denoising user feedback [41]. However, as these methods focus on external attacks or random noises, they ignore the natural falsepositive interactions in data. This work highlights the negative impact of natural noisy interactions, and improve the robustness against them. Causal Recommendation. Recently, causal inference [17], [26] has been widely used to liberate deep learning from blindly ﬁtting the correlation [42]–[45]. Theoretically, it leverages causal graphs to describe the causal relationships between different variables and conducts three-level reasoning over the causal graph: observation, intervention, and counterfactual [17]. As to the information retrieval domain, early research [46]–[49] starts from using causal inference to remove various bias in user feedback [50], such as position bias [51] and popularity bias [52]. Recently, more techniques in causal inference have been applied to solve the issues in recommendation [53]–[55], for example, causal intervention [56] and counterfactual inference [6]. However, existing work on causal recommendation has never studied the colliding effect or the causality-based inference strategy. Dataset. To evaluate the effectiveness of ADT, we conducted experiments on three benchmarks: Adressa, Amazon-book, and Yelp. Extra feedback (e.g., ratings and dwell time) is used to identify the false-positive interactions. Adressavisen[57]. It includes user clicks over news and the dwell time for each click, where the clicks with dwell time < 10s are treated as false-positive ones [10], [12]. datasets[58]. It records users’ purchases over books with rating scores. A rating score below 3 is regarded as a false-positive interaction. : It covers the businesses in the catering industry (e.g., restaurants and bars), which are reviewed by users. Similar to Amazon-book, the rating scores below 3 are false-positive interactions. We split the dataset into training, validation, and testing sets, and explored two experimental settings: 5.1 and 5.2). To evaluate the performance of denoising implicit feedback, we kept all interactions, including the false-positive ones, in training and validation, and tested the models only on true-positive interactions. 5.3). We assume that partial true-positive interactions have already been known, which will be used to verify the performance of the proposed three strategies: ﬁnetuning, warm-up training, and colliding inference. Evaluation Protocols. For each user in the testing set, we predicted the preference score over all the items except the positive ones used during training. Following existing studies [3], [59], we reported the performance w.r.t. two widely used metrics: Recall@K and NDCG@K, where higher scores indicate better performance. For both metrics, we set K as 50 and 100 for Amazon-book and Yelp, while 3 and 20 for Adressa due to its much smaller item space. Testing recommender models. To demonstrate the effectiveness of our proposed ADT strategy on denoising implicit feedback, we compared the performance of recommender models trained with T-CE or R-CE and normal training with standard CE. We selected two representative userbased neural models, GMF and NeuMF [3], and one item-based model, CDAE [19], which is a representative recommender model which can defend random noises in implicit feedback. More details can be found in [1]. Parameter Settings. For the three testing recommender models, we followed their default settings, and veriﬁed the effectiveness of our methods under the same conditions. For GMF and NeuMF, the factor numbers of users and items are both 32. As to CDAE, the hidden size of MLP is set as 200. In addition, Adam [23] is applied to optimize all the parameters with the learning rate initialized as 0.001 and he batch size set as 1,024. As to the ADT strategies, they have three hyper-parameters in total: α and in the T-CE loss, and β in the R-CE loss. In detail, is searched in {0.05, 0.1, 0.2} and β is tuned in {0.05, 0.1, ..., 0.25, 0.5, 1.0}. As for α, we controlled its range by adjusting the iteration number to the maximum drop rate , and is adjusted in {1k, 5k, 10k, 20k, 30k}. In colliding inference, the number of neighbors Nis tuned in {1, 3, 5, 10, 20, 50, 100}, wis set as 1/|N|, and λ is searched in {0, 0.1, 0.2, ..., 1}. We used the validation set to tune the hyper-parameters and reported the performance on the testing set. Overall Performance. Table 3 summarizes the performance of the three testing models trained with standard CE, T-CE, or R-CE. From Table 3, we could observe: improve the performance, e.g., NeuMF+T-CE outperforms NeuMF by 15.62% on average over the three datasets. The signiﬁcant performance gain indicates the better generalization ability of neural recommender models trained by the T-CE loss and R-CE loss. It validates the effectiveness of ADT, i.e., discarding or down-weighting hard interactions during training. CE loss performs better in most cases. We postulate that the recommender model trained with the R-CE loss still suffers from the false-positive interactions, even though they have smaller weights and contribute little to the overall training loss. In addition, the superior performance of the T-CE Loss might be attributed to the additional hyper-parameters in the dynamic threshold function which can be tuned more granularly. Further improvements might be achieved by a ﬁne-grained userspeciﬁc or item-speciﬁc tuning of these hyper-parameters, which can be done automatically [60]. increase on NeuMF, which validates the effectiveness of ADT to prevent vulnerable models from the disturbance of noisy data. On the contrary, the improvement over CDAE is relatively small, showing that the design of defending random noise can also improve the robustness against false-positive interactions to some extent. Nevertheless, applying T-CE or R-CE still leads to performance gain over CDAE, which further validates the rationality of denoising implicit feedback. In the following, GMF is taken as an example to conduct thorough investigation to save computation cost. Performance Comparison w.r.t. Interaction Sparsity. Since ADT prunes many interactions during training, we explored whether ADT hurts the preference learning of inactive users because their interacted items are sparse. Following the former studies [59], we split testing users into four groups according to the interaction number of each user where each group has the same number of interactions. Figure 7 shows the group-wise performance comparison where we could observe that the proposed ADT strategies achieve stable performance gain over normal training in all cases. It validates that ADT is also effective for the inactive users. Memorization of False-positive Interactions. Recall that false-positive interactions are memorized by recommender models eventually under normal training. We then investigated whether they are also ﬁtted well when ADT is applied. From Figure 8(a), we could ﬁnd the CE loss values of false-positive interactions eventually become similar to other interactions, indicating that GMF ﬁts false-positive interactions well at last. On the contrary, as shown in Figure 8(b), by applying T-CE, the loss values of falsepositive interactions increase while the overall training loss stably decreases step by step. This indicates that the model parameters are not optimized over these false-positive interactions, validating the capability of T-CE to identify and discard such interactions. As to R-CE (Figure 8(c)), the loss of false-positive interactions also shows a decreasing trend, showing that the recommender model still ﬁts such interactions. However, their loss values are still larger than the real training loss, indicating assigning false-positive interactions with smaller weights is effective. It prevents the model from ﬁtting them quickly. Therefore, we could conclude that both paradigms reduce the effect of falsepositive interactions on recommender training, which can explain their improvement over normal training. Study of Truncated Loss. Since the Truncated Loss achieves promising performance, we studied how accurately it identiﬁes and discards false-positive interactions. We ﬁrst deﬁned recall to represent what percentage of false-positive interactions in the training data are discarded, and precision as the ratio of discarded false-positive interactions to all discarded interactions. Figure 9 visualizes the changes of the recall and precision along the training process. We take random discarding as a reference, where the recall of random discarding equals to the drop rate (T ) during training and the precision is the proportion of false-positive interactions within the training batch at each iteration. From Figure 9, we observed that: interactions after the drop rate keeps stable, greatly reducing the impact of noisy interactions. This might explain the superior performance of T-CE in Table 3. e.g., only 10% precision in Figure 9, which implies that it inevitably discards many true-positive interactions. However, given the overall superior performance of the Truncated Loss, it seems worthwhile to prune noise interactions even at the cost of losing many true-positive interactions. Nevertheless, how to further improve the precision is a promising research direction in the future. Hyper-parameter Sensitivity. ADT introduces three hyperparameters into the two paradigms. Speciﬁcally, and are used to control the drop rate in T-CE loss, and β adjusts the weight function in R-CE loss. To facilitate the future tuning of the two paradigms, we studied how the hyper-parameters affect the performance. From Figure 10, we could have the following ﬁndings: performs well when ∈ [0.1, 0.3]. If exceeds 0.4, the performance drops signiﬁcantly because a large proportion of interactions are discarded. Therefore, the upper bound should be restricted. especially on Amazon-book, and the performance still increases when > 30k. This shows that a limitation of the T-CE loss is the big search space of hyperparameters. Therefore, it is promising to introduce some automatic hyper-parameter tuning methods [60], which could further improve denoising performance of the Truncated Loss. across different datasets, and the best results happen when β ranges from 0.15 to 0.3. These observations provide insights on how to tune β if it’s applied to other recommender models and datasets. Performance Comparison. To avoid the detrimental effect of false-positive interactions, a popular idea is to incorporate the extra user feedback (e.g., ratings and favorite) for training although they are usually sparse. Existing work either adopts the additional feedback by multi-task learning [13], or leverages it to identify the truepositive interactions [2], [5]. In this work, we introduced two classical models for comparison: Neural Multi-Task Recommendation (NMTR) [13] and Negative feedback Reweighting (NR) [5]. In particular, NMTR considers both click and extra feedback with multi-task learning. NR uses extra feedback (i.e., dwell time or rating) to identify true-positive interactions and re-weights the false-positive and non-interacted ones. We applied NMTR, NR, the proposed ﬁnetuning, and warm-up training to the testing recommender models and reported the results of GMF in Table 4. Other results are omitted due to the similar trend. From Table 4, we could have the following observations: GMF, which validates the effectiveness of using extra feedback. However, the results of NMTR and NR are inferior to those of the T-CE loss and R-CE loss, which might be attributed to the sparsity of extra feedback. Indeed, the clicks with satisfaction (i.e., high ratings) is much fewer than the total number of true-positive interactions. Hence, NR will lose extensive positive training interactions while NMTR will inevitably use many false-positive interactions for training. training outperforms the vanilla GMF but performs worse than NMTR or NR, showing the advantages of multi-task training [13] and the re-weighting technique [5]. the performance of ADT over both R-CE and T-CE, further justifying the effectiveness of using extra feedback on denoising methods. Besides, warm-up training is more effective than ﬁnetuning. It validates the idea that warmup training will expose the recommender model to truepositive interactions in the early training stage, and thus improve the model’s identiﬁcation ability during ADT. Effectiveness of Colliding Inference. Since colliding inference is designed for the users with sparse extra feedback, we leveraged the interaction numbers of extra feedback and implicit feedback to measure the sparsity and divided users into groups. Speciﬁcally, we deﬁned user ratio as, and then selected the users with the ratios smaller than the threshold to conduct colliding inference. We chose warm-up training as an example to test colliding inference due to its superior performance, and the results of T-CE and R-CE equipped with warm-up training and colliding inference are summarized in Table 5. From Table 5, we could observe that: 1) colliding inference signiﬁcantly improves the performance of T-CE and R-CE with warm-up training over the users with low ratios. It validates that distilling colliding effect does utilize extra feedback to extract some useful knowledge from dense users, leading to the better rankings for sparse users; and 2) the performance margin is becoming smaller over the user groups from ratio < 0.1 to ratio < 0.3, implying that colliding inference is more effective for sparse users. For the users with high ratios, a larger proportion of extra feedback will prevent the casual effect of EonˆE (in Figure 6) from being blocked during ADT. Hyper-parameter Analysis. To study the effect of λ in Equation (8), we show the performance comparison over GMF+T-CE w.r.t. different λ values in Figure 11. The results of R-CE are omitted due to the similar trend. From the ﬁgure, we could see that: 1) utilizing the prediction scores of neighbors is effective because the best performance happens when λ < 1. It further validates the effectiveness of colliding inference. And 2) the peaks of the curves occur at λ > 0.5 in most cases, and move close to 1 as the ratio increases. This ﬁrst shows that the personalization is essential although the neighbors are selected from the reliable user representation space. Besides, distilling the knowledge from neighbors is more important for the users with sparse extra feedback (i.e., low ratio) because the causal effect of extra feedback on their prediction scores will be easily reduced during ADT with noisy implicit feedback. In this work, we explored denoising implicit feedback for recommender learning. We found the negative effects of noisy implicit feedback, and proposed adaptive denoising training to reduce their impact. In particular, this work contributes two paradigms to formulate the loss functions: Truncated Loss and Reweighted Loss. Both paradigms are general and can be applied to different recommendation loss functions, neural recommender models, and optimizers. In this work, we applied the two paradigms on the widely used binary cross-entropy loss and conducted extensive experiments over three recommender models on three datasets, showing that the paradigms could effectively reduce the disturbance of noisy implicit feedback. In addition, we also developed three strategies: ﬁnetuning, warm-up training, and colliding inference, which can incorporate extra feedback to further improve the denoising performance of the two paradigms. This work formulates the task of denoising implicit feedback for recommendation, which points to some new research directions. Speciﬁcally, adaptive denoising training is not speciﬁc to the recommendation task, and it can be widely used to denoise implicit interactions in other domains, such as Web search and question answering. Besides, it is interesting to explore how the proposed two paradigms perform on other loss functions. Lastly, we propose a novel causality-based inference strategy to improve the ranking performance over sparse users, which inspires the exploration of causal inference into the inference stage of more tasks.