Sequential recommendation can capture user chronological preferences from their historical behaviors, yet the learning of short sequences is still an open challenge. Recently, data augmentation with pseudo-prior items generated by transformers has drawn considerable attention in improving recommendation in short sequences and addressing the cold-start problem. These methods typically generate pseudo-prior items sequentially in reverse chronological order (i.e., from the future to the past) to obtain longer sequences for subsequent learning. However, the performance can still degrade for very short sequences than for longer ones. In fact, reverse sequential augmentation does not explicitly take into account the forward direction, and so the underlying temporal correlations may not be fully preserved in terms of conditional probabilities. In this paper, we propose a Bidirectional Chronological Augmentation of Transformer (BiCAT) that uses a forward learning constraint in the reverse generative process to capture contextual information more effectively. The forward constraint serves as a bridge between reverse data augmentation and forward recommendation. It can also be used as pretraining to facilitate subsequent learning. Extensive experiments on two public datasets with detailed comparisons to multiple baseline models demonstrate the effectiveness of our method, especially for very short sequences (3 or fewer items). Source code is available at https://github.com/juyongjiang/BiCAT. Recommender systems are a key engine underlying B2C commercial platforms such as the YouTube video and the Amazon shopping recommendation. Sequential recommendation, in particular, represents recommender systems that learn from historical user behaviors to capture the sequential preference and to infer the next possible item (Liu et al. 2016; Yu et al. 2016). A fundamental feature of sequential recommendation is the sequential order of user actions, which characterizes the inﬂuence of past behaviors on the current and future one (Donkers, Loepp, and Ziegler 2017; Ma, Kang, and Liu 2019). Based on this, transformer models such as SASRec (Kang and McAuley 2018) and BERT4Rec Equal Contribution. Corresponding Author. Copyright © 2022, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. Figure 1: The Recall@5 performance w.r.t sequence length distribution (bar) on Amazon Beauty when using (1) no sequence augmentation (SASRec, red dots); (2) reverse sequence augmentation (ASReP, green squares) and (3) our forward-constrained reverse augmentation (BiCAT, blue stars, being particularly advantageous on short sequences). (Sun et al. 2019) have shown extraordinary performance in many sequential recommendation tasks. Despite the success of state-of-the-art transformers in the sequential recommendation, researchers have reported that they are not effective on short sequences (Liu et al. 2021b). With limited information about the user sequential patterns, short sequences are much harder to predict, which is also known as the cold-start issue (Li et al. 2019a; Wang, Ding, and Caverlee 2021). Yet in the meantime, short sequences can be dominant in real-world datasets. As shown in the exempliﬁed histogram of the Amazon Beauty dataset in Figure 1, 75% sequences have less than 7 items and only 5% sequences have more than 20 items. To address this problem, data augmentation methods (Li et al. 2017; Yuan et al. 2020; Wang et al. 2021) are typically adopted to increase the length of the training sequences. Early augmentations methods (Xie et al. 2020; Zhou et al. 2020) such as crop, mask and reorder may break the sequential property. Therefore, it could exaggerate the coldstart issues since short sequences are vulnerable to the slight change of a few items (Liu et al. 2021a). Recently, ASReP (Liu et al. 2021b) was proposed to learn the reverse sequential correlation (i.e., future-to-past) from data to generate pseudo-prior items at the beginning of the original sequences. Given the length-augmented sequences, it then ﬁne-tunes the transformer from the original direc- Figure 2: The optimization gap between the correlations of reversely augmented sequence and original sequence. tion (i.e., past-to-future) to predict the next items. We illustrate the Recall@5 for the next-item prediction on Amazon Beauty dataset with and without ASReP in Figure 1. As noted, recommendation performance on short sequences is worse than on long sequences. Although ASReP can improve the learning of short sequences, the performance on short sequences is still worse than on long sequences. ASReP has an advantage over other methods in that it does not change the original sequences and only ﬁlls in the prior ”missing” items. However, the augmentation process is a backward process (future to past), so the temporal correlations underlying the generated sequence may not be fully consistent with the original sequence in terms of the forward direction (past to future). As shown in Figure 2, although the reverse learning can maximize the reverse sequential probability, the generated item B may not ﬁt in the original sequential correlation to maximize the forward probability to discover the future ground-truth item A. To further illustrate the reasoning, we provide theoretical explanations in the methodology section with a counterexample in Figure 3. Since the reverse augmentation of ASReP may not ﬁt in the correlation of the forward recommendation step, to preserve the sequential correlation in the augmentation process, we propose a Bidirectional Chronological Augmentation of Transformer (BiCAT) that uses a forward learning constraint in the loss to capture the contextual information when generating items reversely. Namely, when using the newly generated items to predict their subsequent items, such an extra representation learning condition in the augmentation phase is more consistent with the forward recommendation phase, which we believe could lead to better generalization performance. In the meantime, BiCAT can also serve as a better pretraining for the recommendation task. We verify the effectiveness of our BiCAT by reproducible experimental results on two public datasets. Comparisons to multiple baseline models show that BiCAT can improve the learning of short sequences by a large margin and accelerate the training. The main contributions of this paper are: • We propose Bidirectional Chronological Augmentation of Transformer (BiCAT) that adds a novel forward learning constraint in the loss to preserve the original correlation when generating items reversely, which bridges the gap between augmentation and recommendation. • We prove theoretically that the sequential correlations of two different chronological directions may not necessarily be equivalent, which explains why reverse data augmentation alone may produce sub-optimal results. • We give extensive experiments to show that BiCAT can improve the transformer on public datasets signiﬁcantly. For short sequences with lengths smaller than 3 in particular, BiCAT can improve the transformer by 40.12% on average and improve the ASReP augmented model by 6.71% on average. A ﬁne-tuning comparison shows that BiCAT pre-training converges faster than ASReP. The core of sequential recommendation is the modeling of user-item interaction pattern and the prediction of future items based on each user’s historical behaviors. Early works (Koren, Bell, and Volinsky 2009; Rendle, Freudenthaler, and Schmidt-Thieme 2010; He and McAuley 2016) mainly reply on Markov chains to model the pair-wise item transition. Later, with the rise of deep learning, Markov models are extensively replaced by neural networks. Recurrent neural networks (RNNs) (Hidasi et al. 2015; Yu et al. 2016; Liu et al. 2016) that can store the sequential memory are widely used in sequential recommendations. RNNs with invariant structures and/or gating mechanisms (Quadrana et al. 2017; Ma, Kang, and Liu 2019; Song et al. 2019) are also proposed for different recommendation tasks. Other improvements for RNNs are also studied, such as using contextual features (Smirnova and Vasile 2017) and alternative sampling tricks (Hidasi and Karatzoglou 2018). Convolutional neural networks (CNNs) (Tang and Wang 2018; Yuan et al. 2019) are also proposed to focus on learning the local transitions. These models with limited reception ﬁelds still faces the long-term dependency problem. More recently, attentionbased models (Kang and McAuley 2018; Kang et al. 2019a; Sun et al. 2019; Ma et al. 2020b; Luo, Liu, and Liu 2021) are adopted in sequential recommendation tasks, which achieve outstanding performance. For example, SASRec applies the transformer layer from NLP tasks to adapt to sequential recommendation. BERT4Rec further considers a bidirectional transformer layer. Additionally, other techniques such as relation awareness (Ji et al. 2020), graph learning (Ma et al. 2020a) and reinforcement learning (Xin et al. 2020) are also used to adapt to various sequential recommendation tasks. Data augmentation is a useful component in machine learning venues to increase the volume of data and hence the model performance. In the literature of recommender systems, many kinds of data augmentation methods are proposed. For example, (Tan, Xu, and Liu 2016) uses data augmentation of sequence preprocessing and embedding dropout to enhance training and reduce overﬁtting. (Yuan et al. 2020) proposes to integrate future interaction as a data augmentation to improve model training. Generative augmentations (Wang et al. 2019; Li et al. 2019b) are proposed to add extra user-item interaction data by generative adversarial networks or Seq2Seq models. A rising technique that can be classiﬁed as data augmentation is self-supervised learning for sequential recommendation (Zhou et al. 2020; Yao et al. 2020; Xie et al. 2020; Liu et al. 2021a), which uses operations such as crop, reorder, mask, substitute, and insert to supplement the original sequences. These methods are designed for improving model robustness, but may not address the cold-start problems since they may inevitably break the sequential property of the original sequences and the learning of short sequences is vulnerable to the change of a few items (Liu et al. 2021a). Recently, a reverse data augmentation method ASReP is proposed for the cold-start learning of short sequences, which adds pseudo-prior items in the beginning of the original sequences by learning to predict the past items reversely. Traditionally, cold-start problems are mainly addressed by using side information (Saveski and Mantrach 2014), knowledge transfer (Kang et al. 2019b), and meta-learning (Dong et al. 2020; Zheng et al. 2021). Chronological correlation In this section, we prove by Bayes’ theorem that the forward sequential correlation is not necessarily consistent with the reverse sequential correlation, so the reverse augmentation may lead to sub-optimal augmentation. Assume that A and B are two items interacting with a user chronologically. Since the user interacts with A ﬁrst, we denote the forward correlation as P (B|A) and write P (A|B) = where P (B) 6= 0. Thus, the reverse correlation P (A|B) equals the forward correlation P (B|A) only when P (A) = P (B). It indicates that the two items always appear concurrently, which is not always true in practical item sequences. We illustrate this with an example in Figure 3. We deﬁne three events (i.e., A, B, C), and two probabilities (i.e., forward probability Pand reverse probability P). We consider four sequences as the training data in Figure 3 (a) to indicate the various inference probabilities of the model. For instance, to calculate the forward correlation P(A|C), we count the cases with A on the right side and C on the left side (i.e., ”CA” in the forward order) and divide it by the number of times with A on the right (i.e., ”XA” where X can be any events). In this case, P(A|C) = P(C, A)/P(C) = 3/4 with three ”CA” and one ”CD” as shown in Figure 3 (b)(i.). Now, if we use C to predict the next item A in the forward recommendation task, what would be the best choice for augmenting C with a prior (left) item? By reverse inference, P(B|C) = 3/4 is the largest among other possibilities, so the reverse augmentation model should choose B as shown in Figure 3 (b)(ii.). However, does B improve the model performance? By forward inference on top of an augmented ”BC”, we ﬁnd that P(A|CB) = 2/3 < 3/4, which is, in fact, worse than the original performance. This counterexample shows that reverse augmentation does not always improve the learning of the original sequential correlation. In light of this challenge, we can also consider a different constraint that chooses the left element by maximizing the forward correlation instead of the reverse correlation. As Figure 3: A counterexample that the reverse augmentation could break the original forward sequential correlation and result in the decrease of model predictive performance. shown in Figure 3 (b)(iii.), we ﬁnd that D can achieve that and make the forward recommendation more accurate with P(A|CD) = 1/1 > 3/4. This example shows that the forward-constrained augmentation can effectively preserve of the original forward sequential correlation. We formally introduce our method in the following sections. Problem Statement Nnotations. U = {u, u, ..., u} denotes a set of users, V = {v, v, ..., v} denotes a set of items, and S= [v, v, ..., v] denotes the interaction sequence in chronological order of user u ∈ U. Each vrefers to an item in V that u has interacted with at time step i. Sequential recommendation aims to predict the item that user u will interact with at the (n + 1)th step. The probability of each item v ∈ V becoming the (n + 1)th item is denoted as p(v= v|S), given the training sequence S. Model Architecture Here, we introduce an effective data augmentation model of pseudo-prior items for short sequences, termed Bidirectional Chronological Augmentation of Transformer (BiCAT). It is built upon the prior work (Liu et al. 2021b) and (Kang and McAuley 2018). The proposed BiCAT consist of 1) an embedding layer; 2) multi-stacked unidirectional transformer layers; and 3) a bidirectional chronological pre-trained transformer for short sequences augmentation. With these modules, the left-to-right transformer is ﬁne-tuned using the augmented sequences for the next item prediction. Our method employs a forward learning constraint in data augmentation to capture the contextual information when generating items reversely. The model architecture is shown in Figure 4. Embedding Layer In standard Transformer (Vaswani et al. 2017), the embedding layer consists of item embedding and positional embedding. The item embedding layer encodes each item ID v∈ Rinto a latent representation e∈ R. The item embedding transforms the scalar item IDs into dense vectors, aiming to reduce computation and improve representation learning. To make a fair comparison, we follow (Liu et al. 2021b) to hold a position embedding table P ∈ R, where n is the maximum sequence length and d is the same embedding dimensions of the item embedding. Note that for a sequence S, we truncate it to be the last n items if its length is longer than n or utilize zero-padding if its length is shorter than n (Kang and McAuley 2018). Overall, the embedding layer transforms the sequence of S= [v, v, ..., v] ∈ Rinto E(S) = [e+ p, e+ p, ..., e+ p] ∈ R. Transformer Layer As shown in Figure 4 (a), the basic Transformer layer Trm is composed of two sub-layers, a Multi-Head Self-Attention sub-layer and a Position-wise Feed-Forward Network. Multi-Head Self-Attention Speciﬁcally, the Multi-Head Self-Attention sub-layer adopts a multi-head attention layer and then a normalization operation on the embedded sequence. Each attention layer adopts the scaled dot-product attention layer used in (Vaswani et al. 2017; Sun et al. 2019; Wu et al. 2020; Kang and McAuley 2018; Liu et al. 2021b), which potentially learn the different importance pattern of items in sequences from h sub-spaces. Here, E = E(S) ∈ Ris the embedded interaction sequence. Note that we omit the layer subscript l for convenience. h denotes the number of attention layers. W∈ denote four different and learnable parameters that process the query Q, key K, value V, and the output of the multi-√ head attention layer. The effect of temperatured is to avoid overly large values of the inner product and extremely small gradients (Hinton, Vinyals, and Dean 2015; Vaswani et al. 2017; Kang and McAuley 2018; Sun et al. 2019). The tensor dimension of the ﬁnal output of the multi-head attention sub-layer is still maintained as R. Position-wise Feed-Forward Network To enhance the representation learning capability, we use a Position-wise Feed-Forward Network to project the output of the multihead attention sub-layer to the sequence embedding at each position i. Speciﬁcally, the FFN contains two afﬁne transformations layers with a ReLU activation function, as used in all the prior works (Vaswani et al. 2017; Kang and McAuley 2018; Liu et al. 2021b). PFFN(H) = Concat[FFN(h), ..., FFN(h)] FFN(x) = ReLU(xW+ b)W+ b where W∈ R, W∈ R, b∈ R, and b∈ Rare learnable parameters and shared across all positions. We omit the layer subscript l for the sake of simplicity. Bidirectional Chronological Pre-training We use the transformer in SASRec (Kang and McAuley 2018) as our backbone, as shown in the left-hand side of Figure 4. First, we reverse the original input sequence of S= [v, v, ..., v] ∈ RintoS= [v, v, ..., v] ∈ R. Then, we inject theSinto the Transformer to generate the next items as the pseudo-prior items for short sequences. This reversely generation (inference) can be formulated by: where v∈ V denotes the pseudo-prior items. In the training phase, we mask the left items inSby zero-padding, use them as labels, and train the Transformer to predict them. The reverse learning of the Transformer can capture the reverse item correlations (Sun et al. 2019; Liu et al. 2021b) and learn a reversely chronological user’s preferences. However, as shown in Figures 2 and 3, the reverse augmentation may be inconsistent with the original correlation. Herein, we propose the forward learning constraint, which bridges the gap between reverse augmentation and forward recommendation. Speciﬁcally, the forward constraint leverages the generated pseudo-prior item vto predict vitem in the forward direction in parallel with Eq. (6). Formally, we feed the augmented sequenceS= [v, v, ..., v] ∈ Rinto the identical Transformer to concurrently maximize the future item prediction where v∈ V denotes the future items of the original sequence at position n. Similarly, we also mask the right items at position n inSby zeros and train the Transformer to predict them. Since we unearth Bi-Chronological user’s preferences from their historical behaviors simultaneously, we named our augmentation model BiCAT in short. Prediction Layer After L layers adaptively and hierarchically aggregate information across all positions, we get the ﬁnal output H∈ Rfor all items of the input sequence. Then we apply an matrix factorization (MF) (Koren, Bell, and Volinsky 2009) layer to predict the relevance of item v: where ris the relevance of item vbeing the next item given the ﬁrst j items (i.e., v, v, ..., v), and N∈ Rdenotes an item embedding table. Note that we use shared item embedding table in the input and output layer. Network Training With the bidirectional chronological losses, the model has a stronger learning criterion to achieve a better performance. In this work, following SASRec (Kang and McAuley 2018), we adopt the binary cross entropy loss combined by losses of both left and right directions: Here, λ is a hyper-parameter to balance the forward constraint and the reverse generation. It is set as 1 by default. Our model takesS= [ v, v, ..., v] ∈ Rand S= [v, v, ..., v] ∈ Ras inputs. We denote the corresponding sequencesO= [o, o, ..., o] ∈ R andO= [o, o, ..., o] ∈ Ras the outputs. Algorithm 1: Recursive Generation. Output: [v, . . . , v, v]. Short Sequence Augmentation After the training of the bidirectional chronological pre-trained Transformer, following (Liu et al. 2021b), we use it to recursively generate pseudo-prior items, as illustrated in Figure 4 (b). To be speciﬁc, we use K to denote the number of augmented items and M to denote the length threshold of whether a sequence is a ”short sequence”. Formally, we indicate the K augmented pseudo-prior items as [v, . . . , v, v] and arrange them at the beginning of the original short sequence S= [v, v, ..., v] ∈ Rfor the next ﬁne-tuning stage, as shown in Figure 4 (c). The recursive generation of pre-trained BiCAT is presented in Algorithm 1. Finally, we ﬁne-tune the pre-trained transformer with the augmented sequences to predict the next item vin the left-to-right direction. Note that the forward learning constraint is only used in the pseudo-prior item generation stage, not in the ﬁne-tuning stage. The ﬁne-tuning process of our BiCAT is identical to that of ASReP (Liu et al. 2021b), except that our pre-trained transformer learns forward sequential pattern that is in line with the recommendation task. In this section, we present our experimental setup and evaluate the BiCAT on two public benchmarks. We answer the following Research Questions (RQs): • RQ1: Does BiCAT outperform the state-of-the-art sequential recommendation models? • RQ2: How does the BiCAT perform in particular on short sequences? • RQ3: What is the inﬂuence of key hyperparameters in the BiCAT architecture? • RQ4: Does BiCAT provide better pre-train model for the next left-to-right ﬁne-tuning stage? Datasets In this paper, our main contribution is the more effective data augmentation. To compare fairly with prior works to show the effectiveness of our method, we use the same datasets as used in (Liu et al. 2021b). Amazonis a series of datasets introduced in (Kang and McAuley 2018) which consist of large corpora of product reviews crawled from Amazon.com by McAuley et al. The data is split as separate datasets according to the top-level product categories on Amazon. In our experiments, we use the Amazon Beauty (5-core) and Amazon Cell Phones and Accessories (5-core) (McAuley et al. 2015). We followed the common data preprocessing procedure from (Kang and McAuley 2018; Sun et al. 2019; Liu et al. 2021b). We treat each rating or review as a presence of a user-item interaction, group the interactions by user ids, and sort them by timestamps to form a sequence for each user. The detailed statistics of the datasets can be found in Table 1. Note that more than 75% sequence length are less than 9 and 7 for beauty and phones, respectively. Experimental Settings To evaluate all models, we adopted the widely used leaveone-out evaluation evaluation task (He et al. 2017; Kang and McAuley 2018; Sun et al. 2019; Liu et al. 2021b) to evaluate the sequential recommendation models. For each user, we use the last item of each use for testing, the second last item for validation, and the remaining items for training. Moreover, we follow the common practice in (Kang and McAuley 2018; Li, Wang, and McAuley 2020; Sun et al. 2019; Cho, Park, and Yoo 2020; Liu et al. 2021b) to pair each groundtruth item in the test set with 100 randomly sampled negative items that are not interacted by the user. We employ three evaluation metrics: Recall (HR), and Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR). The reported results are averaged for ten random seeds. Codes to reproduce the results are uploaded. Baselines & Implementation Details For the two transformer-based models SASRec (Kang and McAuley 2018) and BERT4Rec (Sun et al. 2019), and the two static models BPR-MF (Rendle et al. 2012) and LightGCN (He et al. 2020), we cite the experimental results from ASReP (Liu et al. 2021b). To implement ASReP and its re-training (RT) variants, we use code provided by the authors. To ensure fairness, our method’s implementation is based on their codes and only adds our Table 2: Comparison of recommendation performance with baseline models in Beauty and Phones datasets based on Recall@5, NDCG@5, and MRR. Bold scores are the best in each row. Underlined scores are the second best. core component on it. Additionally, we also create the retraining (RT) variants of the BiCAT, which means training a new Transformer from scratch using the augmented sequences instead of ﬁne-tuning the pre-trained Transformer. For all unique hyper-parameters and initialization strategies, we use the same setting of each model and report the results under each model’s own optimal hyper-parameter setting. For reproducible checking, the hyperparameters of BiCAT are listed in the following. For beauty dataset, the best hyper-parameters are: max length n = 100, embedding size d = 128, the layer number L = 2, head number h = 4, dropout rate 0.7, `weight decay of L2 = 0.0, short sequence length threshold M = 18, and K = 15. For Phones dataset, n = 100, d = 32, L = 2, h = 2, dropout rate 0.5, L2 = 0.0, M = 18, and K = 17. Overall Performance Comparison (RQ1) Table 2 shows the overall recommendation performance of our model and baselines on two datasets. We make two observations of overall comparison and ablation study: • Overall Comparison. (1) The transformer-based models outperform the static models on all metrics. (2) However, they are worse than augmented models ASReP and BiCAT as they suffer from the cold-start problem for short sequences. Compared with SASRec, BiCAT has signiﬁcant relative improvements on all three metrics by 24.77% on average. (3) Moreover, we observe that BiCAT outperforms ASReP on all three metrics by 3.45% on average. Note that the comparison is for sequences of all lengths. For long sequences with length over threshold M, we do not augment them with pseudo-prior items. • Ablation Comparison with re-train variants. (1) The two variants RT-ASReP and RT-BiCAT are worse than ASReP and BiCAT, which veriﬁes that pre-training is useful for sequential recommendations. (2) RT-BiCAT and BiCAT outperform ASReP in all datasets on all evaluation metrics. The improvement is based on the learning of bichronological item correlation to generate higher-quality pseudo-prior items for forward recommendation task. Table 3: Comparison of recommendation performance on short sequences (L ≤ 3) with baseline models in comprehensive evaluation metrics over Beauty and Phones dataset. Bold scores are the best in each row. Underlined scores are the second best. Table 4: Hyperparameter Analysis (Recall@5) over Beauty and Phones datasets. Bold score indicates the highest one. Performance on Short Sequences (RQ2) In this section, we conduct a study to discuss the effectiveness of BiCAT on short sequences and report the results in Table 3. To sum up, BiCAT outperforms other baselines on all six metrics. For short sequence with lengths smaller than 3 on both datasets, our BiCAT improves SASRec by an average increase of 40.12% on recall rates and ASReP by an average increase of 6.71% on recall rates. These results demonstrate that data augmentation of pseudo-prior items can be a promising method to alleviate the deﬁciency of short sequences learning. More importantly, the results verify that our method can generate higher-quality pseudo-prior items to improve recommendation performance. Hyperparameter Study (RQ3) To analyze the effect of the two core hyper-parameters K and M, which indicate the number of pseudo-prior items and the threshold for short sequences respectively, we report the performance results in Table 4. Note that, for each result, we only change the current one hyper-parameter but keep others the same as the optimal settings. We ﬁnd that the performance generally improves on both datasets when K and M increase. It veriﬁes the efﬁcacy of augmenting pseudo-prior items for short sequences. Moreover, for Phones dataset, it shows that our BiCAT M = 14 is better than M = 18, the optimal setting in ASReP. For a direct comparison that lines with baselines, we still report the M = 18 results of BiCAT on Phones instead of the better M = 14 in Table 2. Figure 5: Fine-tuning on Beauty and Phones dataset respectively. The blue curves denote ASReP method, and orange curves denote our BiCAT method. In this plot, BiCAT have a great advantage to obtain better pre-train model and accelerate the convergence of network. Training Efﬁciency w.r.t Fine-tuning (RQ4) In this section, to further verify our motivation that BiCAT can bridge the gap from augmentation to recommendation, we visualize the fune-tuning training loss. As shown in Figure 5, our pre-trained BiCAT brings the signiﬁcant optimal initialized network parameters to facilitate the training of recommendation. We follow the same fune-tuning design in (Liu et al. 2021b) to make a fair comparison. As a common data augmentation method, our BiCAT can be applied in other models, not limited to sequential recommendations. In this paper, we propose a state-of-the-art model for the learning of short sequences in sequential recommendation. Data augmentation of pseudo-prior items as a useful method can enhance the learning of short sequences and address the cold-start problem. We prove theoretically that the reverse augmentation ASReP may not be consistent with the forward sequential correlation. To generate higher-quality items, we propose BiCAT that uses a forward learning constraint in the loss to capture the contextual information when generating items reversely. Since the forward constraint is in line with the sequential recommendation task, it can bridge the gap between reverse augmentation and forward recommendation. Extensive experiments show the superiority of BiCAT by signiﬁcant improvement over the state-of-the-art ASReP and SASRec. Moreover, our BiCAT can conduct a better pre-trained Transformer to help main task quickly converge at the ﬁne-tuning stage. For future studies, the generalization of BiCAT may also apply to other tasks such as video and language generation. How to generate dynamic items with respect to timestamps also pends future works.