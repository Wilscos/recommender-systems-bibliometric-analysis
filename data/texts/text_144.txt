Much of contemporary Artiﬁcial Intelligence is based on supervised machine learning, a methodology that leverages large datasets that are manually “annotated”, that is, associated with categorical labels in a case-wise manner (see Table 1). The common and undisputed pipeline for the preparation of these datasets includes the deﬁnition of the classiﬁcation schema (i.e., the eligible labels), data collection and annotation Since annotation, interpretation and rating are terms that can be used indiscriminately to indicate both the process (i.e. the labelling) and its eﬀect or result (i.e., the labels), here we prefer to speak of phenomenon and objects (to be classiﬁed); which is associated with a judgement and some labels, as result of an interpretation act by some raters, although all the synonyms mentioned above are equally good as long as they do not generate notational ambiguity. In the same vein, we speak of rating and rater, where other authors could use the terms label and annotator, respectively. To help the reader with this terminological fuss, we produced Table 1. Abstract. Most Artiﬁcial Intelligence applications are based on supervised machine learning (ML), which ultimately grounds on manually annotated data. The annotation process is often performed in terms of a majority vote and this has been proved to be often problematic, as highlighted by recent studies on the evaluation of ML models. In this article we describe and advocate for a diﬀerent paradigm, which we call data perspectivism, which moves away from traditional gold standard datasets, towards the adoption of methods that integrate the opinions and perspectives of the human subjects involved in the knowledge representation step of ML processes. Drawing on previous works which inspired our proposal we describe the potential of our proposal for not only the more subjective tasks (e.g. those related to human language) but also to tasks commonly understood as objective (e.g. medical decision making), and present the main advantages of adopting a perspectivist stance in ML, as well as possible disadvantages, and various ways in which such a stance can be implemented in practice. Finally, we share a set of recommendations and outline a research agenda to advance the perspectivist stance in ML. Keywords: Machine Learning · Reliability · Artiﬁcial Intelligence · Multirater Labelling · Observer Variability , and, critically, label aggregation. This latter step is often performed in terms of a majority vote. This has been proved to be problematic in many work settings, especially when the object to classify is so complex that most of the raters can get it wrong and the real experts are a minority [3,7,10,13,14], or the object is ambiguous as it is often the case in Natural Language Processing (NLP) tasks [4]. Recently, these issues have attracted the interest of the AI and ML community, thanks to initiatives like the Data Nutrition Project as well as the recent proposal of Data-Centric AI by Andrew Ng greater attention and transparency related to the data production and annotation processes. In this article, we argue that the issues with aggregation are not necessarily only a data quality problem (but rather also regard process quality and responsibility) and, furthermore, are not necessarily restricted to highly subjective phenomena, but are also relevant to ﬁelds where the phenomena to be classify are traditionally considered “objective”, like in clinical decision support. . https://datanutrition.org/ https://www.youtube.com/watch?v=06-AZXmwHjo Table 1. Glossary of the main concepts discussed in this article. We propose and argue for a paradigm shift, which could move away from monolithic, majority-aggregated gold standard datasets, towards the adoption of methods that more comprehensively and inclusively integrate the opinions and perspectives of the human subjects involved in the knowledge representation step of modeling processes. Our proposal comes with important and still-to-investigate implications: ﬁrst, supervised models equipped with full, non-aggregated annotations have been reported to exhibit a better prediction capability [2,15,44], in virtue of a better representation of the phenomena of interest; secondly, new techniques for AI explainability can be devised that describe the classiﬁcations of the model in terms of multiple and alternative (if not complementary) perspectives [7,37]; ﬁnally, we should consider the ethical implications of the above mentioned shift and its impact on cognitive computing, whereas the new generation of models can give voice to, and express, a diversity of perspectives, rather than being a mere reﬂection of the majority [37,40]. As anticipated above, in this paper we propose what we denote as a perspectivist approach in producing the ground truth (i.e., the ground truthing task) to be used in supervised classiﬁcation tasks by systems developed with Machine Learning (ML) methods and techniques. This general stance can be articulated in two main versions, which could be connoted as either a weak or strong approach (see Figure 1). A weak perspectivist approach is adopted when researchers involved in ground truthing are not content to collect a single label for each object to be classiﬁed, that is to produce a gold standard label set; but rather aim to collect as many raters and annotations as possible, i.e., to build what in [15] has been called diamond standard (see Figure 1). We emphasize the distinction between raters and annotations because, as simply as it can be, raters could express more than one label for a given object to classify [19,32] (also as a way to express their indecision in case of strictly alternative labels), or they could expressly be asked to rank available labels in terms of pertinence or to associate each class with a conﬁdence/probability level [17]. One could rightly wonder why ML researchers would want to collect such redundant information about the phenomena about which they design and develop decision support systems that are usually aimed at improving human decision making by proposing the one best label for each object to classify [3]. For now, we are not going to dispute that conceiving the output of such systems as single labels is the best way to improve human decision making single label, ML researchers could want to collect multiple labels for single objects to classify when they deem relying on single judgments either too limiting (like in multi-label tasks, where labelling categories are not disjoint and they overlap to As a matter of fact, alternative ways, like in conformal prediction, could improve it more [5], especially if we consider improvement also beyond the mere dimension of error rate Fig. 1. A BPMN (Business Process Model and Notation) diagram of the ground truthing process in a perspectivist setting. Many tasks are common with more traditional ML pipelines, but the core distinction with these latter ones lies in the exclusive gateway (X) in the centre of the diagram. Parallel gateways (+) indicate opportunities for parallel activities, which we made explicit to emphasize the importance of the comprehensive reporting of the ground truthing process. some extent) or too error-prone [48]. This could happen for two main reasons: 1) distrust in the raters involved, as it can be the case in crowdsourcing initiatives or questionnaire-driven studies where researchers cannot oversee the annotation task or ensure its accuracy [21,48]; 2) the recognized variability (what later we will call their inter-subjective nature) of the phenomenon of interest [3], that is the recognition that diﬀerent raters could classify the same object diﬀerently, and not necessarily because they are wrong. Indeed, ratings can diﬀer not only because the raters are fallible, but also for a number of other factors, among which we recall: the intrinsic ambiguity and complexity of the phenomenon, including the so-called cumulative mess, that is the condition when the same object can legitimately be classiﬁed as many things at the same time [8]; the (in)stability of the phenomenon over time [21]; the complexity of the task, also in terms of number of distinct states or conﬁgurations of the phenomenon to detect [41] and of the concentration that is requested to the raters, and their necessary proﬁciency to detect and understand the phenomenon [27]; the raters’ susceptibility to somehow systematic cognitive biases, both at individual and team level, like overconﬁdence, conﬁrmation and availability bias, anchoring and halo eﬀects [20], as well as to more contingent and context-driven external factors (what Kahneman and colleagues have denoted as chance variability of judgments, or “decision noise” [29]). Thus, collecting multiple labels from a sample of individuals can help ML researcher get a sample of perceptions, opinions and judgments that could be maximally representative of the population of interest; also, and more practically speaking, doing so would help them draw from such a sample some qualiﬁed majority [13] for the sake of higher accuracy Weak perspectivism would then advocate the production of gold standards by considering and taking into account multiple perspectives. This does not necessarily require to make each perspective symbolically explicit, in terms of multiple labels: a meeting where multiple experts are invited to share their opinion about an object would be perspectivist as long as all people involved could express their opinions and views in a discussion, which could then be summarized into single positions. Likewise, a weak perspectivist approach would require to collect multiple labels from a corresponding number of observers or raters but then would combine these labels and select one single label for each object to be annotated, mostly by some kind of majority voting (e.g., weighted voting). This process has been called perspective reduction in [15]. On the other hand, we would speak of strong perspectivism whenever the researchers’ aim is to collect multiple labels, or multiple data about each class, about a speciﬁc object, and keep them all in the subsequent phases of training or benchmarking of the classiﬁcation models. Doing so certainly impacts model training and evaluation, but can be realized in several ways, of varying complexity [15,44,47]. The easiest way, that is the most backward-compatible way that does not require ad-hoc implementations, is to replicate each object in the training set to reﬂect the number of times this object has been associated with a certain label by the raters [52]; nonetheless, also other methods have been proposed in the literature, that we will better describe in Section 4. From a conceptual point of view, the process of ground truthing can be understood as a process of collection of a set of measures, according to the theory of scales of measurement [18]. In this light, any mapping from a phenomenon or object of interest (also a portion of the reality) to a sign (mostly a symbol of conventional meaning) can be assimilated to a measurement; the related measure (symbol) that can be used to annotate its data representation is but the result of the process above that can be called in multiple, yet equivalent, ways: rating, judgement, perception, opinion, annotation. As we said above, a perspectivist approach can be advocated especially for the annotation of those phenomena that we would consider as “highly subjective”, from a common sense stance, that is aﬀected by high observer variability, as measured with some inter-rater agreement metrics, like kappa [22], alpha [31] or rho [13]. According to a common deﬁnition, a judgment is considered subjective when it is mainly “based on, or inﬂuenced by, personal feelings, tastes, or opinions.”; we usually contrast this concept with that of objective, a term that With qualiﬁed majority we intend either a statistically signiﬁcant majority (with respect to some hypothesis testing procedure), or an overwhelming majority, regardless of how this may be deﬁned, and thus irrespective of what perspectives are considered and how they are distributed within the sample characterises judgements that, ideally, are not inﬂuenced by personal feelings or idiosyncrasies and which, on a practical level, the vast majority of people would see and label in the same way (barring clear errors or oversights). In metrology, the science of measurement, the above terms are associated with slightly diﬀerent meanings: A recent proposal [35] asserts that any measurement that aims to be dependable (as ground truth must be, by deﬁnition) should exhibit both the characteristics of objectivity, that is being related to an object, even to a “non-physical property such as chess playing ability”; and that of being interpersonal, that is independent of the subject or “identically interpretable by diﬀerent measurers” [35]. In the perspectivist discourse, we recognize that the opposition obj/subj-ective is somehow too much aﬀected by common sense and, worse yet, value judgments (in that subjective usually means “bad” or, to our practical aims, unreliable; and objective means good, and reliable). Therefore, we prefer to consider these two concepts as sort of ideal extremes of a full range of ways in which phenomena can present themselves to the interpretation and judgement of the members of a human community, what we call the intersubjectivity spectrum (see Figure 2). Fig. 2. The intersubjectivity spectrum and its relationship with inter-rater agreement. In this range, the extremes are essentially abstract: no phenomenon, and hence opinion on it, can be said totally subjective, as human beings are all immersed in a culture and social ensemble (trivially, they may share a language and semantic conventions), even if we recognized the ineﬀable and undetermined nature of the phenomenon at hand; likewise, full objectivity is a mirage because any judgement (or measurement) is situated and aﬀected by contingent and contextual conditions, even if executed by a sensor or machine. Appraising where a certain phenomenon of interest can be located in this conceptual continuous range, that is assessing its level of intersubjectivity, can be preparatory to understand what kind of perspectivist approach, whether weak or strong, to adopt in one’s ML pipeline; and whether keeping multiple labels allows to preserve individual expression and intuition or, conversely, can undermine the reliability (and hence utility) of the ground truth. To this respect, highly intersubjective phenomena would be phenomena that are close to the objective end of the intersubjective spectrum and for which disagreement among the raters involved is limited or absent. On the other hand, low-intersubjectivity objects would be those for which the complexity and ambiguity of the phenomenon at hand, but also the experience, commitment and competence that are required of the involved raters (like in medical diagnosis) would make one expect low agreement scores [41]. This can happen also in unsuspected domains, like medical diagnosis: just as a few examples, agreement in the grading of ovarian cancer slides has been found associated with kappa scores lower than 30% (κ = .25 [6]), like in ischemia detection from EEGs (κ = .29 [36]) and in phase lag detection in ECGs (κ = .23 [23]). Such a low agreement is anything but rare in classiﬁcation tasks (even in medicine); being aware of this can call for the application of some advanced methods, where the single contributions collected are weighted so as to identify the most reliable ones in the subsequent phases of ML development. In what follows, we report on the main research contributions that either inspired or adopt some form of perspectivism in the ML pipeline (see Figure 1). Annotation is a crossing task that regard diﬀerent types of data, including texts (and any part of them), images (including segmented portions), and more or less structured data, like case records. The scientiﬁc communities interested in ML, like NLP, Computer Vision (CV) and medical informatics, have traditionally relied on gold standard datasets to design, develop and evaluate supervised models: these datasets have usually been obtained by the annotation of a single rater or by means of the majority aggregation of few raters, and their reliability, and representativeness of the real task under consideration, have scarcely been questioned. In the recent years, however, due to the huge increase in raw data availability, the increasing reliance on crowdsourcing and similar annotation protocols has highlighted the issue of observer variability in Machine Learning tasks [3,14], an issue which was already well known in certain settings such as the computational linguistics [4] and medical ones [28,34]: Beyer et al. [10] and Yun et al. [51] showed the majority-aggregated labels in the original ImageNet dataset to be not representative of the images in the dataset due to observer variability and the un-reliability of the annotation process; Svensson et al [45] noted the inﬂuence of observer variability on the performance of Machine Learning in a task of cancer detection and proposed ways to measure model performance in settings aﬀected by variability; similarly, the impact of observer variability in NLP has been explored by Akhtar et al. [1,2] in the task of hate speech detection. While many works, and speciﬁcally those focusing on the crowdsourced learning setting, have adopted a weak perspectivist stance for the development of ML methods able to account for this observer variability [16,25,42,46]; the need for ML methods explicitly taking into account a strong perspectivist approach, however, has only recently started to become a focus of research: Akhtar et al. [2] showed that a strong perspectivist approach to model training may also lead to performance increase; Sudre et al. [44] and Guan et al. [24] proposed a multi-task approach to deal with observer variability in medical imaging, showing how jointly learning the majority consensus and the individual raters’ labels improves classiﬁcation accuracy; Sharmanska et al [43] showed how accounting for disagreements among raters may help improving the performance in CV tasks with crowdsourced labels; Peterson et al. [38] showed that accounting for raters’ disagreement and uncertainty may lead to generalizability and performance improvements in CV tasks; Plank et al. [39] proposed a loss based on inter-rater agreement metrics to address variability in part-of-speech tagging; Uma et al. [47] proposed the use of soft losses as a perspectivist approach for the training of ML models; Campagner et al [15] proposed a soft loss ensemble learning method, inspired by possibility theory and three-way decisions, for the training of ML models in perspectivist settings. In a similar direction, recent works [7,13,40] explicitly explored the impact of strong perspectivism on the development and evaluation of supervised models, also from a more conceptual perspective. In particular, in [7,13] experiments are presented in support of the thesis that disagreement in annotation may come from the subjectivity (i.e., low inter-subjectivity) of a task to varying extent [41], and therefore it should not be cast away as noise in the data, but rather it should be systematically accounted for at evaluation time. Furthermore, in [7,40] the authors also show an additional advantage of strong perspectivism in supervised learning, namely its potential impact on the interpretability and fairness of the models. In an experiment on real data (an annotated corpus of hate speech), the authors of [7] show how individual labels can be used to cluster the raters by aﬃnity, leading to the emergence of patterns that helps identifying socio-demographic aspects of the raters themselves, which are in principle opaque, especially in a crowdsourcing scenario; while in [40], the authors describe how a similar approach could be used to detect biases in the data and labels provided by raters. The related works, and the main ML techniques for perspectivist ground truths, are summarized in Table As anticipated above, a perspectivist approach to ground truthing requires to preserve the classiﬁcation multiplicity instead of getting rid of it by majority voting (if original labels have been produced) or consensus surveys (if original labels do not exist). Obviously, as also highlighted in Section 4 when discussing the related works setting the background for our proposal, this comes with some advantages and also some shortcomings, which we discuss in what follows. The main beneﬁts of the perspectivist approach are: 1. To provide a theoretical backbone that recognizes and accepts the categorical irreducibility of some phenomena. This is especially relevant to those phenomena which exhibit a natural ambiguity, such as many tasks in NLP [4], or seemingly inconsistent clinical manifestations [14]; Table 2. A summary of methods proposed in the literature to train ML models based on perspectivist ground truths. 2. To extract valuable knowledge from what it is usually discarded as noise (cf. 3. To avoid to ratify and legitimize the opinion of raters belonging to a majority 4. To be able to build models that learn typical human error patterns (if it is 5. To be able to develop models that can leverage label-uncertain [47] and fuzzy 6. To allow for three-way, fuzzy, probabilistic methods that represent the sub- Since there is no rose without thorns, here we enumerate the main shortcomings that are associated with a perspectivist approach to supervised ML. 1. Need to involve multiple raters: this may represent an important bottleneck in 2. Incompatibility with standard ML approaches, which are usually not designed label noise [29]), i.e., disagreement. Such extra information is valuable for a decision support to be more useful in border-line and complex cases; group, re-iterating their truth in seemingly objective advice. Instead, the perspectivist approach aims at giving voice to the few who hold a minority view [37], or to those who are intimidated in collective debates; plausible to deﬁne “errors” on the basis of minority stances) and use this information as a form of decision support; data [15] to improve performance, generalizability and robustness; jectivity and uncertainty in the considered phenomena [14,30], and provide decision makers with useful advice, that is methods that can improve trust, enhance user experience, and possibly mitigate the risk of automation bias and deskilling. terms of costs or time, and may thus result impractical or expensive in some domain (e.g., medicine) or when dealing with large datasets; to take into account multiple perspectives or annotation, and need to design 3. More complex validation/evaluation, due to the absence of a uniquely deﬁned This article aims to disseminate a renovated interest for an alternative approach to ground truthing with respect to the “reductionist” one where multiple ratings collected about a single object are reduced into single labels. As we saw in the previous section, Section 5, this approach entails both advantages and challenges, posed by the will to cope with information richness and manage complexity (also in terms of redundancy, uncertainty and inconsistency) instead of getting rid of it, in light of the research that we presented in Section 4. In this Section, in lieu of a conclusion, we proceed with two sections that shed light on the future: we present a set of agile recommendations for those willing to adopt a perspectivist approach to their ground truthing tasks; and then we propose a number of possible proposals calling for further research and contributions. In what follows, we share some recommendations to embrace a perspectivist stance in ground truthing. While the impact of some of these practices must still be soundly evaluated, we also mention some of the main studies providing preliminary evidence supporting the recommendations, when available. – Design annotation schemes that allow raters to associate objects with multiple – Involve enough raters. This can mean diﬀerent things depending on the appli- – Involve heterogeneous raters, both in regard to origin and culture and to exper- – Be aware of the limits of majority voting, especially for complex objects to ad-hoc ML methods. While certain classes of learning algorithms (e.g. multilabel ones) may be able to handle multiple labels, it is not clear if these methods can be proﬁcuously applied in the perspectivist ground truthing setting; ground truth. While in some cases majority labels can be used as a benchmark ground truth [45], these may not be appropriate in strongly subjective or ambiguous settings. labels, or also with a ’none of these’ label, to account for multiple perspectives directly acknowledged by the single raters. Moreover, allow the raters to express a judgment of inadequacy of the available label set (see [3]); cation domain: a number that allows for statistically signiﬁcant majorities to emerge (e.g., at least 12 raters for dichotomous tasks) or a number of raters that allows to create a majority of superhuman accuracy, according to some estimate of the average accuracy of the raters (see [13,29]); tise and skills: diﬀerent opinions are not always a source of noise, as asserted in [29], but rather of richness [3,49]. classify, like in the medical domain. Collecting more information about each judgement, like the rater’s conﬁdence, should become common practice to help researchers detect these cases, and enable alternative reductions [15] with respect to simple majority, like weighted majority or any strong version of data perspectivism (see Table 2). – Evaluate ML models also with respect to robustness, or their capability to ad- – Report about the rater enrollment process and about the quality of their rat- – Collect additional information from the raters involved, so as to take actual ac- These recommendations are complementary to those proposed by the Perspectivist Data Manifesto research agenda in the AI community. In particular, the signatories of the above manifesto also recommend to create and distribute non-aggregated datasets, in order to foster the discussion on the principles of data perspectivism among the research community and to facilitate experimental research in this direction. Finally, in what follows we delineate some possible open problems and research directions that we deem relevant to advance the perspectivist stance in predictive computing: 1. First, and in connection with the manifesto mentioned in the previous section, https://pdai.info equately perform also on external datasets (that is data coming from settings other than where the training data were produced). If performance relevantly degrades on external datasets, this could suggest to adopt a perspectivist approach as a way to mitigate the risk that the models “overﬁt” the ratings of a non-representative sample of users. ings in detail. In [12] the authors recommend to describe the process of ground truthing in terms of: a) Number of raters involved to produce the labels; b) The raters’ profession and expertise (e.g., years from specialization or graduation); c) The incentive provided, if any; d) Particular instructions given to raters for quality control (e.g., which data were discarded and why); e) how long the labelling process took and, in the case of critical domains such as medicine and law, where and under what conditions it took place (e.g., controlled conditions, real-world interruptions); f) Any chance-adjusted measure of inter-rater agreement (e.g., Rho [13], Alpha [31], Kappa [22]); g) The Labelling technique (e.g., majority voting, Delphi method [33], consensus iteration). count of their perspective and way of seeing the objects at hand. For instance, in [13] we recommended to collect the conﬁdence expressed by the rater about each of their annotations in terms of an ordinal score, as well as other dimensions (e.g., complexity, diﬃculty, rarity, relevance) along a similar scale. This information can be used to detect the cases for which the raters’ conﬁdence was the lowest, thus suggesting that they are the most diﬃcult ones to decide about; or those cases that, for their relevance, it is important that the ML model gets right (i.e., with a suﬃcient average accuracy) not to mislead its users. we recommend the creation and dissemination of benchmark datasets that could be used to evaluate perspectivist ML models, possibly with respect to diﬀerent data types (like images, texts, structured data) and for diﬀerent classiﬁcation tasks (e.g., detection, risk stratiﬁcation, forecasting). Such datasets 2. Disagreement (and possibly errors) in the multi-rater labels are part and par- 3. The usual assumption in ML development is that a model that learnt the 4. The perspectivist stance in ground truthing amounts to associating multiple 5. In the literature, diﬀerent algorithmic approaches able to account for a per- 6. While some ways to evaluate such ML models have been proposed in the liter- 7. Similarly to the above point, we need further research on the impact of observer 8. The main concept underlying data perspectivism in ground truthing is that are necessary for the evaluation of novel algorithmic proposals, and they could be used also as benchmarks for setting up challenges, which have recently proven to be important drivers for the development of novel techniques and approaches; cel of the perspectivist stance to ground truthing. It is of interest to develop techniques and approaches that are able to exploit the multiple labels to understand and model how the raters err, or on which types of objects they disagree more, so as to develop learning models with the ability to predict the chance that the raters would err on a new object; majority label from an ensemble of raters could actually provide a better prediction than the majority prediction of an ensemble of models that learnt the single raters’ labels. Further research could address this conjecture and investigate if good results (that is useful and reliable predictions) could be achieved also by preserving multiplicity at ground truth level and reducing variability/uncertainty at prediction level through some computational technique. labels to each object. However, especially in low-intersubjectivity tasks, a certain amount of divergent labels is expected in, and may be intrinsic to, the task, and a part of these could be due to errors, inattention or negligence. Thus, it would be interesting to develop ML models that are eﬀectively able to disentangle subjectivity from error, and also to characterize (from a learning theoretic perspective) when this disentanglement is possible at all; spectivist ground truth have been proposed (see Table 2): these include data augmentation strategies based on the replication of objects associated with multiple labels, ensemble learning methods [15], or soft loss approaches [47]. It would be interesting to assess, on real-world problems and applications, the performance of these (and other) approaches, so as to understand their properties and the appropriateness of diﬀerent algorithms for speciﬁc tasks; ature (e.g., by evaluating the performance of ML models on objects associated with “overwhelming majorities” [15,45], or by adopting soft loss metrics [47]), these proposals should be uniﬁed to develop novel metrics that can better take into account the (chance-adjusted) agreement rate between the raters, or the presence of chance eﬀects and label noise; variability on the predictive performance of ML models. Simulating this kind of variability would allow to test for the robustness of these models and get an estimate of the extent the model is “overﬁtting” on either opinions that are not representative of the user population or on partial aspects of the phenomenon of interest. collecting multiple opinions and perceptions about a single phenomenon or object to classify – and preserving this multiplicity in ML pipelines – is valuable, and can convey value to decision makers and users at prediction time through 9. As previously described in [7,40] (see also Section 4), the perspectivist stance 10. Finally, we believe it would be interesting to consider settings in which raters To conclude, the perspectivist approach is essentially aimed at caring about the representativeness and reliability of the ground truth of ML systems. More speciﬁcally, and programmatically, data perspectivism fosters wariness toward aggregated gold standards: these reference datasets express single-truth assumptions that can fall short of capturing the necessary complexity of the phenomena for which we want to have support from computational means [8]. After all, ML systems are complicated machines that essentially reiterate past judgments and legitimate them [26] by putting the perceptions of very few (relatively speaking) raters to the attention of countless users and decision makers. There is no guarantee that new objects will be comparable to (or equatable with) those with which such models had been trained, not to speak of the contingent context. We believe that adapting the single-truth assumption of ML to the perspectivist paradigm is not only more fair towards minority opinions, but it also has the potential to yield apt decision support [7,15]. However, a dataset characterized by multiple perspectives, that is by some relevant extent of disagreement between the raters, could be seen as rich as it is noisy. While standard measures of inter-rater agreement (or similar measures, like entropy) can capture this “noise” aspect within a diamond standard, these measures are not fully capable to represent the informational content and value of such multi-faceted information [15]. Therefore, this approach needs the deﬁnition of a theoretical framework (also in terms of the deﬁnition of suitable metrics) by which to evaluate the compromise between richness (in that the more diﬀerent perspectives, the better) and reliability (in that multiplicity does not indicate confusion but complementarity): though we recognize this as a faint approximation of this concept, such an approach could in principle be based on some apt combination of an inter-rater agreement and an information-theoretic (e.g. conditional entropy, or some variants of entropy for imprecise probabilities [50]) measure. In doing so, learning-theoretic characterizations of perspectivist ML can be developed, which could enable to understand when a given dataset can be reliable “ground” for sound decision support; or, conversely, when its quality needs to be improved with some intervention of decision hygiene [29], like, e.g., aggregating judgements with methods that leverage professional expertise; could also have potential applications and impacts in terms of model interpretability and algorithmic fairness, e.g. by enabling the modeling and detection of biases and discrimination induced by (some groups of) raters. Further research should thus be devoted toward the investigation of possible applications of perspectivist ML in eXplainable and fair AI; are able to express more information than a single label [3] – for example, by providing a ranking of the possible labels or expressing their conﬁdence in the labels that they propose [13]. Given the similarity of these settings to the problems typically investigated in the ﬁeld of computational social choice [11] further research should investigate the approaches proposed in that context, and how they could be applied to design perspectivist ML methods that could deal with more general, structured labeling representations. a more accurate and explainable quantitative evaluation of the trained models, as shown by the recent works that are surfacing exploring this research direction [47,7]. Such a stance creates the necessary room for asking questions such as whose opinion is the model relying on in its prediction?, and what opinions do we want to project into the interpretation of the unexpected new? ; and for reﬂecting on the implications of these matters on the ethical nature and impact of the decisions made with the support of predictive models.