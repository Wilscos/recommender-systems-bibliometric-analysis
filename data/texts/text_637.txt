Abstract— Recent advances in neural networks have solved common graph problems such as link prediction, node classiﬁcation, node clustering, node recommendation by developing embeddings of entities and relations into vector spaces. Graph embeddings encode the structural information present in a graph. The encoded embeddings then can be used to predict the missing links in a graph. However, obtaining the optimal embeddings for a graph can be a computationally challenging task specially in an embedded system. Two techniques which we focus on in this work are 1) node embeddings from random walk based methods and 2) knowledge graph embeddings. Random walk based embeddings are computationally inexpensive to obtain but are sub-optimal whereas knowledge graph embeddings perform better but are computationally expensive. In this work, we investigate a transformation model which converts node embeddings obtained from random walk based methods to embeddings obtained from knowledge graph methods directly without an increase in the computational cost. Extensive experimentation shows that the proposed transformation model can be used for solving link prediction in real-time. With the advancement in internet technology, online social networks have become part of people’s everyday life. Their analysis can be used for targeted advertising, crime detection, detection of epidemics, behavioural analysis etc. Consequently, a lot of research has been devoted to computational analysis of these networks as they represent interactions between a group of people or community and it is of great interest to understand these underlying interactions. Generally, these networks are modeled as graphs where a node represents people or entity and an edge represent interactions, relationships or communication between two of them. For example, in a social network such as Facebook and Twitter, people are represented by nodes and the existence of an edge between two nodes would represent their friendship. Other examples would include a network of products purchased together on an E-commerce website like Amazon, a network of scientists publishing in a conference where an edge would represent their collaboration or a network of employees in a company working on a common project. Inherent nature of social networks is that they are dynamic, i.e., over time new edges are added as a network grows. Therefore, understanding the likelihood of future association between two nodes is a fundamental problem and is commonly known as link prediction [1]. Concretely, link prediction is to predict whether there will be a connection between two nodes in the future based on the existing structure of the graph and the existing attribute information of the nodes. For example, in social networks, link prediction can suggest new friends; in E-commerce, link prediction can recommend products to be purchased together [2]; in bioinformatics, it can ﬁnd interaction between proteins [3]; in co-authorship networks, it can suggest new collaborations and in the security domain, link prediction can assist in identifying hidden groups of terrorists or criminals [4]. Over the years, a large number of link prediction methods have been proposed [5]. These methods are classiﬁed based on different aspects such as the network evolution rules that they model, the type and amount of information they used or their computational complexity. Similarity-based methods such as Common Neighbors [1], Jaccard’s Coefﬁcient, Adamic-Adar Index [6], Preferential Attachment [7], Katz Index [8] use different graph similarity metrics to predict links in a graph. Embedding learning methods [9], [3], [10], [11] take a matrix representation of the network and factorize them to learn a low-dimensional latent representation/embedding for each node. Recently proposed network embeddings such as DeepWalk [11] and node2vec [10] are in this category since they implicitly factorize some matrices [12]. Similar to these node embedding methods, recent years have also witnessed a rapid growth in knowledge graph embedding methods. A knowledge graph (KG) is a graph with entities of different types of nodes and various relations among them as edges. Link prediction in such a graph is known as knowledge graph completion. It is similar to link prediction in social network analysis, but more challenging because of the presence of multiple types of nodes and edges. For knowledge graph completion, we not only determine whether there is a link between two entities or not, but also predict the speciﬁc type of the link. For this reason, the traditional approaches of link prediction are not capable of knowledge graph completion. Therefore, to tackle this issue, a new research direction known as knowledge graph embedding has been proposed [13], [14], [15], [16], [17], [18], [19]. The main idea is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. Neither of these two approaches, however, can generate “optimal” embeddings “quickly” for real-time link prediction on new graphs. Random walk based node embedding methods are computationally efﬁcient but give poor results whereas KG-based methods produce optimal results but are computationally expensive. Thus, in this work, we mainly focus on embedding learning methods (i.e., Walk based node embedding methods and knowledge graph completion methods) which are capable of ﬁnding optimal embeddings quickly enough to meet real-time constraints for practical applications. To bridge the gap between computational time and performance of embeddings on link prediction, we propose the following contributions in this work: tational cost of both Random walk based node embedding and KG-based embedding methods and empirically determine that Random walk based node embedding methods are faster but give sub-optimal results on link prediction whereas KG based embedding methods are computationally expensive but perform better on link prediction. embeddings from Random walk based node embedding methods and output near optimal embeddings without an increase in computational cost. extensive experimentation on various social network datasets of different graph sizes and different combinations of node embeddings and KG embedding methods. Let G= hV, E, Ai be an unweighted, undirected homogeneous graph where V is the set of vertices, E is the set of observed links, i.e., E ⊂ V × V and A is the adjacency matrix respectively. The graph G represents the topological structure of the social network in which an edge e = hu, vi ∈ E represents an interaction that took place between u and v. Let U denote the universal set containing all (|V | × |V | − 1)/2 possible edges. Then, the set of nonexistent links is U − E. Our assumption is that there are some missing links (edges that will appear in future) in the set U − E. Then the link prediction task is given the current network G, ﬁnd out these missing edges. Similarly, let G= hV, E, Ai be a Knowledge Graph (KG). A KG is a directed graph whose nodes are entities and edges are subject-property-object triple facts. Each edge of the form (head entity, relation, tail entity) (denoted as hh, r, ti) indicates a relationship r from entity h to entity t. For example, hBob, isF riendOf, Sami and hBob, livesIn, NewY orki. Note that the entities and relations in a KG are usually of different types. Link prediction in KGs aims to predict the missing h or t for a relation fact triple hh, r, ti, used in [20], [21], [14]. In this task, for each position of missing entity, the system is asked to rank a set of candidate entities from the knowledge graph, instead of only giving one best result [20], [14]. We then formulate the problem of link prediction on graph G such that G ≡ G≡ G, i.e., KG with only one type of entity and relation. Link prediction is then to predict the missing h or t for a relation fact triple hh, r, ti where both h and t are of same kind. For example hBob, isF riendOf, ?i or hSam, isF riendOf, ?i. Graph embedding aims to represent a graph in a low dimensional space which preserves as much graph property information as possible. The differences between different graph embedding algorithms lie in how they deﬁne the graph property to be preserved. Different algorithms have different insights of the node (/edge/substructure/whole-graph) similarities and how to preserve them in the embedded space. Formally, given a graph G = hV, E, Ai, a node embedding is a mapping f: v→ y∈ R∀i ∈ [n] where d is the dimension of the embeddings, n the number of vertices and the function f preserves some proximity measure deﬁned on graph G. If there are multiple types of links/relations in the graph then similar to node embeddings, relation embeddings can be obtained as f : r→ y∈ R∀j ∈ [k] where k the number of types of relations. 1) Node Embeddings using Random Walk: Random walks have been used to approximate many properties in the graph including node centrality [22] and similarity [23]. Their key innovation is optimizing the node embeddings so that nodes have similar embeddings if they tend to co-occur on short random walks over the graph. Thus, instead of using a deterministic measure of graph proximity [24], these random walk methods employ a ﬂexible, stochastic measure of graph proximity, which has led to superior performance in a number of settings [25]. Two well known examples of random walk based methods are node2vec [10] and DeepWalk [11]. 2) KG Embeddings: KG embedding methods usually consists of three steps. The ﬁrst step speciﬁes the form in which entities and relations are represented in a continuous vector space. Entities are usually represented as vectors, i.e. deterministic points in the vector space [13], [14], [15]. In the second step, a scoring function f(h, t) is deﬁned on each fact hh, r, ti to measure its plausibility. Facts observed in the KG tend to have higher scores than those that have not been observed. Finally, to learn those entity and relation representations (i.e., embeddings), the third step solves an optimization problem that maximizes the total plausibility of observed facts as detailed in [26]. KG embedding methods which we use for experiments in this paper are TransE [14], TransH [15], TransD [16], RESCAL [27] and SimplE [28]. Transformation model is suggested to expedite ﬁne-tuning process with KG-embedding methods. Let Gbe a graph Fig. 1: Transformation Model. Input Graph: Green edges are missing links and red edges represents present links. First, a random walk method outputs node embeddings (source) for a graph. These embeddings are then used to initialize KG embedding method, which outputs ﬁnetuned embeddings. A transformation model is then trained between source and ﬁnetuned embeddings. with n vertices and m edges. Given the node embeddings of the graph G, we would want to transform them to optimal node embeddings. The input graph Gis fed into one of the random walk based graph embeddings methods (node2vec [10] or DeepWalk [11]), which gives us the node embeddings. Let f be a random walk based graph embedding method and Edenotes the output node embeddings: where Gis the igraph in the dataset of graphs D = {G, G, ...} and E∈ Rwith the embedding dimension d. In a KG-based embedding algorithm (such as TransE), the input is a graph and the initial embeddings are randomly initialized. The algorithm uses a scoring function and optimizes the initial embeddings to output the trained embeddings for the given graph. Since we are working with homogeneous graph with only one type of relation, we don’t need to learn the embeddings for the relation, hence they are kept constant and only node embeddings are learnt. Let Ebe the initial node embeddings, Ebe the trained embeddings and g the KG method with parameters α. where E∈ Rand E∈ R. Instead of using randomly initialized embeddings E to obtain target embeddings E, we can initialize with Ein Eq. (1) as where E∈ Rare ﬁne tuned output embeddings. This idea of better initialization has also been explored previously in [29], [30] where it has been shown to result in embeddings of higher quality. Using the node embeddings Efrom Eq. (1) and ﬁnetuned KG embeddings Efrom Eq. (3), we train a transformation model which can learn to transform the node embeddings from a node-based method to KG embeddings. We adopt self-attention [31] on graph adjacency matrix as explained in Algorithm 1: E= Self Attention(G, E where E∈ Rare the transformed embeddings and θ are the parameters of the self-attention model. The error between the ﬁne-tuned and transformed embeddings is calculated using squared euclidean distance as: The loss on batch X of graphs is measured as: where X = {(E, E)} and b is the batch size. Since KG embeddings are trained from facts/triplets which are obtained from the adjacency matrix of the graph, a self-attention model reinforced with information of the adjacency matrix when applied to node-embeddings is able to learn the transformation function as observed in our experiments (Figure 3). The proposed algorithm is summarized in Algorithm 2. Input: Dataset of Graphs D= {G, G, ..., G} Yang, et. al [32] introduced social network datasets with ground-truth communities. Each dataset D is a network having a total of N nodes, E edges and a set of communities (Table I). The communities in each dataset are of different sizes. They range from a small size (1-20) to bigger sizes (380400). There are more communities with small sizes and their frequency decreases as their size increases. This trend is depicted in Figure 2. Fig. 2: Histogram showing community size vs its frequency. DBLP, YouTube and Amazon datasets have smaller size communities and LiveJournal and Orkut have larger size communities. TABLE II: Selected datasets and graph size for experiments. YouTube, Orkutand LiveJournalare friendship networks where each community is a user-deﬁned group. Nodes in the community represent users, and edges represent their friendship. DBLPis a co-authorship network where two authors are connected if they publish at least one paper together. A community is represented by a publication venue, e.g., journal or conference. Authors who published to a certain journal or conference form a community. Amazonco-purchasing network is based on Customers Who Bought This Item Also Bought feature of the Amazon website. If a product i is frequently co-purchased with product j, the graph contains an undirected edge from i to j. Each connected component in a product category deﬁned by Amazon acts as a community where nodes represent products in the same category and edges indicate that we were purchased together. We consider each community in a dataset as an individual graph Gwith vertices representing the entity in the community and edges representing the relationship. For training the transformation model, we select communities of Fig. 3: Performance evaluation of different embeddings on link prediction using MRR (y-axis). Source (green) refers to embeddings from node2vec (left) and DeepWalk (right). Target (brown) refers to KG embeddings from TransE, TransH, TransD, SimplE, RESCAL, or DistMult. For each source and target pair, we evaluate ﬁnetuned (orange) embeddings (obtained by initializing target method with source embeddings) and transformed (red) embeddings (obtained by applying transformation model on source embeddings). Results are presented on different datasets of varying graph sizes. Fig. 4: ANOVA test of MRR scores from two embedding methods (Method 1 and Method 2). The difference of MRR scores between the two methods is signiﬁcant when their p-values are <0.05 (light green) and not signiﬁcant otherwise (light red). The values in each cell are the difference between the means of MRR scores from two methods (Method 2 − Method 1). The text in bold represents when Method 2 did better than Method 1. Source method refers to node2vec (left) and DeepWalk (right). Target method refers to TransE, TransH, TransD, SimplE, RESCAL, or DistMult in each row. Fig. 5: CPU Time (left y-axis) vs Graph Size (x-axis) and Mean MRR (right y-axis) vs Graph Size comparison of ﬁnetuned (TransE ﬁnetuned from node2vec) and transformed embeddings (from node2vec). As the graph size increases the time to obtain embeddings from KG methods (TransE) also increases signiﬁcantly. However, there is no signiﬁcant increase in time for the transformation (from node2vec) once we have the transformation model. The Mean MRR scores of both ﬁnetuned and transformed embeddings also drop with the increase in graph size, however, they perform equally good (for graphs <76). Note that ﬁnetuning time and transformation time both include time to obtain node2vec embeddings as well. particular size range which acts as dataset D of graphs (Table II). We randomly disable 20% of the links (edges) in each graph to act as missing links for link prediction. In all the experiments, the embedding dimension is set to 32, which works best in our pilot test. We used OpenNEfor generating node2vec and DeepWalk embeddings and OpenKE [33] for generating KG embeddings. The dataset D of graphs is split into train, validation and test split of 64%, 16%, and 20% respectively. For evaluation, we use MRR and Precision@K. The algorithm predicts a list of ranked candidates for the incoming query. To remove pre-existing triples in the knowledge graph, ﬁltering operation cleans them up from the list. MRR computes the mean of the reciprocal rank of the correct candidate in the list, and Precision@K evaluates the rate of correct candidates appearing in the top K candidates predicted. Due to space constraints, we only present the results for MRR. Results of Precision@K can be found at our GitHub. From the results depicted in Figure 3, we observe that the target KG embeddings (TransE, TransH, etc.) almost always outperforms random-walk based source embeddings (node2vec and DeepWalk) except in case of SimplE and DistMult where both the methods perform poorly. This can also be observed in Figure 4. Finetuned KG embeddings achieved better or equivalent performance as compared to target KG embeddings. This can be conﬁrmed by ANOVA test in Figure 4 where there is no signiﬁcant difference between the MRRs obtained from ﬁnetuned and target KG embeddings in most cases. Specifically, translational based methods such as TransE, TransH, and TransD have equivalent performance for ﬁnetuned and target embeddings whereas SimplE, RESCAL, and DistMult have better ﬁnetuned embeddings than target embeddings as the graph size grows. Transformed embeddings consistently outperform source embeddings and have similar performance to ﬁnetuned embeddings at least for graphs of sizes up to 65. The performance drop starts from graph size 71-75 in the transformation to TransD from DeepWalk whereas 81-85 in the transformation to TransE from node2vec. For RESCAL, the transformation works for larger sized graphs in node2vec and till 121-125 in DeepWalk. As the graph size increases (top to bottom), the overall MRR scores decrease for all the embeddings as expected. In Figure 5, we compare computation time and MRR performance of transformed embeddings and ﬁnetuned embeddings where source method is node2vec and target method is TransE. It can be seen that the transformed embeddings give similar performance as ﬁnetuned embeddings (without any signiﬁcant increase in computational cost) up to graphs of size 71-75. Thereafter the transformed embeddings perform poorly, we attribute this to poor ﬁnetuned embeddings on which the transformation model was trained. In this work, we have demonstrated that random-walk based node embedding (source) methods are computationally efﬁcient but give sub-optmial results on link prediction in social networks whereas KG based embedding (target & ﬁnetuned) methods perform better but are computationally expensive. For our requirement of generating optimal embeddings quickly for real-time link prediction we proposed a self-attention based transformation model to convert walkbased embeddings to optimal KG embeddings. The proposed model works well for smaller graphs but as the complexity of the graph increases, the transformation performance decreases. For future work, our goal is to explore better transformation models for bigger graphs.