East China Normal University,Shanghai Jiao Tong University, Sequential Recommendation aims to recommend items that a target user will interact with in the near future based on the historically interacted items. While modeling temporal dynamics is crucial for sequential recommendation, most of the existing studies concentrate solely on the user side while overlooking the sequential patterns existing in the counterpart, i.e., the item side. Although a few studies investigate the dynamics involved in the dual sides, the complex user-item interactions are not fully exploited from a global perspective to derive dynamic user and item representations. In this paper, we devise a novel Dynamic Representation Learning model for Sequential Recommendation (DRL-SRe). To better model the user-item interactions for characterizing the dynamics from both sides, the proposed model builds a global user-item interaction graph for each time slice and exploits time-sliced graph neural networks to learn user and item representations. Moreover, to enable the model to capture ne-grained temporal information, we propose an auxiliary temporal prediction task over consecutive time slices based on temporal point process. Comprehensive experiments on three public real-world datasets demonstrate DRL-SRe outperforms the state-of-the-art sequential recommendation models with a large margin. • Information systems → Recommender systems; Personalization; Temporal data. sequential recommendation, user behavior analysis, graph neural networks, temporal point process ACM Reference Format: Zeyuan Chen, Wei Zhang, Junchi Yan, Gang Wang, Jianyong Wang. 2021. Learning Dual Dynamic Representations on Time-Sliced User-Item Interaction Graphs for Sequential Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482443 Sequential recommendation [26] aims to recommend items that a target user prefers to interact with in the near future based on the interacted items in the past. It has become a paradigmatic task in recommender systems in recent years. This owes to the fast-developing online services (e.g., e-commerce platforms and streaming media) wherein user sequential behaviors are ubiquitous, and mature information technologies that make the collection of the sequential behaviors become easier. Compared to conventional recommendation settings [29] that form user-item interaction records into a time-independent matrix and model each user-item pair separately, sequential recommendation is more accordant with real situations. It could exibly adjust recommendation results with the emergence of users’ latest behaviors. In the literature of sequential recommendation, there are roughly two branches of studies: session-based recommendation [13] and user identity-aware sequential recommendation [27]. For the former branch, it tends to consider users’ short behavior sequences in recent history. Usually, user identities are not available for sessionbased recommendation. This is because in short sessions, users may not log onto accounts. As a result, the majority of studies in this regard focus on building models with item sequences [15]. For user identity-aware sequential recommendation, it is accustomed to taking long-term behavior sequences. And user identity (ID) is commonly assumed to be known for model construction. This paper concentrates on the latter branch for leveraging users’ long-term behaviors, which is more promising to pursue better recommendation performance. Unless otherwise specied, we use sequential recommendation to represent user identity-aware sequential recommendation throughout this paper. Sequential recommendation has been extensively studied in recent years [9,14,20,23,27]. Most of the studies share the same spirit that models the change of behavior sequences to characterize the dynamic user interests and learn the corresponding user representations. As a supplement, user IDs are usually mapped to the same embedding space to enhance user representations. Although the temporal dynamics in the user side are largely investigated, item representations are often assumed to be static by existing studies. As such, the sequential patterns involved in the item side are often overlooked. For example, down jackets are bought by more and more users with the coming of winter. Grasping such popularity trends of items is hopeful for deriving better item representations, which would bring a positive eect on sequential recommendation. To model the temporal dynamics in both the user and item sides, only a very few studies have been conducted [24,39,41]. They share a similar workow that two deep sequential models (e.g., recurrent neural networks) are built for the user and item side, Figure 1: Sketch for the time-sliced user-item interaction graph construction. Each dashed box corresponds to a time slice. It is worth noting that the numbers of users/items are not exactly the same for dierent time slices since not all users and items occur in each time slice. respectively. The user behavior sequence (consisting of interacted items) and the item interaction sequence (consisting of interacted users) are commonly taken as model input to obtain their dynamic representations corresponding to dierent time steps/slices. Although performance improvements could be achieved as compared to the sequential recommendation models only considering user dynamics, the workow still encounters one major limitation: the abundant interactions between users and items happened in the past are often neglected and not explicitly modeled in a global perspective. This will inevitably cause the user and item representations in user behavior sequences and item interaction sequences to be sub-optimal, which further aect the target user and item representations. In this paper, we devise a novel Dynamic Representation Learning model for Sequential Recommendation (DRL-SRe) to tackle the above limitation. It relies on the segmentation of the whole timeline into a specied number of equal-length time slices. For each time slice, a global user-item interaction graph is built to take all the users and items occurring in this slice as nodes. Therefore it enables us to distill knowledge from all the user-item interactions to benet the user and item representation learning. The sketch for the time-sliced user-item interaction graph construction is shown in Figure 1. Given a sequence of the constructed user-item interaction graphs, DRL-SRe develops time-sliced graph neural networks over dierent time slices to propagate node representations so as to get user and item representations w.r.t. each time slice. To correlate the timesliced representations belonging to the same user or item, DRL-SRe utilizes two recurrent neural networks (RNNs) for the user side and the item side, respectively. As such, the graph neural networks of dierent time slices are integrated into a unied model architecture. To alleviate the issue of temporal information loss caused by the timeline segmentation, we further propose an auxiliary temporal prediction task over consecutive time slices based on temporal point process [7]. This empowers DRL-SRe with the ability to perceive the ne-grained temporal information and compensates for the primary sequential recommendation task. In the end, DRL-SRe generates the possibility of the interaction between the target user and item in the near future. We summarize the main contributions of this paper as follows: •To our best knowledge, we are the rst study to propose timesliced graph neural networks for sequential recommendation, which is able to model the abundant and high-order user-item interactions from a whole perspective to obtain better dynamic user and item representations. •In order to enable DRL-SRe to capture ne-grained temporal information, we integrate temporal prediction over consecutive time slices into the training of the proposed model and show that the auxiliary task can obviously promote the sequential recommendation task. •Extensive evaluations on three public real-world datasets have demonstrated the proposed model signicantly outperforms the state-of-the-art sequential recommendation models and validated the contributions of its crucial components. This section concretely discusses the sequential recommendation studies and the relevant techniques for recommendation, i.e., graph neural networks and temporal point process. Compared to session-based recommendation [15], sequential recommendation studied in this paper considers user IDs and their behavior sequences in a longer time period. The early work [45] proposes to learn users’ whole feature sequences by recurrent neural networks, wherein user IDs are simply taken as input. On the contrary, Quadrana et al. [27] segmented the whole user behavior sequences into multiple short sessions and adopted a hierarchical recurrent neural network to integrate these sessions. To discriminate the short-term user dynamics and long-term user preference, the studies [9,19] rst learn long- and short-term representations and then fuse them for prediction. Beyond the above schemes for handling sequences, some recent studies investigate advanced mechanisms to obtain behavior sequence representations [14], such as self-attention mechanisms [33], or apply sequential recommendation to specic scenarios where additional information (e.g., spatial information) is utilized [20]. However, these studies do not consider the temporal dynamics in the item side. Among the studies for sequential recommendation, [24,39,41] are most relevant to this study. The pioneering model RRN [39] utilizes two RNNs to learn the temporal dynamics of the user side and the item side, respectively. The output representations of the last time step for the two RNNs are coupled to denote user preference and item features for computing the interaction probability. DEEMS [41] shares a similar spirit of applying two RNNs to get user and item representations, and provides a new perspective of loss function construction. It not only adopts a user-centered sequential recommendation loss, but also introduces item-centered information dissemination loss to form an enriched loss function. Since the above two methods loosely couple the two sequences and model them independently, SCoRe [24] is proposed to leverage the past user-item interactions to facilitate the representation learning. Nevertheless, the considered user-item interactions by SCoRe are limited to the direct neighbor nodes of a target user-item pair, thus lacking an eective manner to globally utilize all user-item interactions in a time slice. It is worth noting that there are several studies [2,16,18] that learn user and item representations in a continuous time fashion. Nevertheless, they are conned to the setting where the next useritem interaction time is required to be known in advance to derive real-time user and item representations, which is entirely dierent from this study. Moreover, each user-item interaction in the history could only be modeled once under this situation, thus lacking an eective mechanism to capture high-order relations between users and items through repeated propagation over user-item interactions. Graph Neural Networks.Graph Neural Networks (GNNs) [46] have been undergoing rapid development currently and observing many applications in recommender systems. Because user-item interactions are one kind of edges, some studies [11,37] convert the commonly used user-item rating matrix into a user-item interaction graph and develop GNNs to act upon it. Despite user-item interactions, some other studies design GNNs for modeling user social relations [8,31] and item relations in knowledge graphs [34]. Besides, the scalability issue of GNNs in recommender systems is also treated. For example, node sampling is used in representation propagation in graphs [43]. In light of session-based recommendation where only user dynamics are considered, SR-GNN [42] is the rst GNN-based model that builds an item-item interaction graph for each session. Inspired by this, three recent studies [25,35,38] have put forward to build a global item-item interaction graph over all sessions instead of each session. Moreover, LESSR [3] proposes to preserve the order of interactions when using GNNs for each session. However, very few studies have investigated GNNs for the studied sequential recommendation problem where both user and item dynamics should be modeled based on user-item interactions in time slices. Although GLS-GRL [36] models user-item interactions in the sequential scenario, the GNN-based model design serves for learning user group representations and the recommended items are towards user groups but not individuals. In this paper, we develop time-sliced graph neural networks for modeling user-item interactions in multiple graphs to realize sequential recommendation. Note that although a few studies [22,28,40] research on dynamic graph neural networks over graph snapshots/time slices, none of them have been applied to the scenario of sequential recommendation. Temporal Point Process.Temporal point process [7] is an elegant mathematical tool for modeling asynchronous time series. Its highlight is to capture the continuous temporal gaps compared to time discretization methods. The last several years have witnessed its application to user behavior trajectories [4,6,21,32]. Most of these studies adopt sequential models to achieve interaction prediction and time prediction simultaneously. By contrast, this paper diers from them by conducting temporal prediction over the past consecutive time slices, hoping to mitigate the temporal information loss incurred by the time slices. In the domain of recommender systems, a matrix𝑹 = {𝑟}is usually dened to record user-item interactions. We assume𝑢 ∈ U and𝑖 ∈ I, and𝑀and𝑁represent the size of user setUand item setI, respectively. The values in the matrix are determined based on whether user feedback is explicit (e.g., rating) or implicit (e.g., click). Here we focus on implicit feedback since it is more common and easily obtained than explicit feedback. As such, we dene𝑟=1 if user𝑢has interacted with item𝑖, and𝑟=0 otherwise. To adapt to sequential recommendation, each user-item interaction is associated with a timestamp𝑡to indicate when the interaction happens. Accordingly, we have a triplet(𝑢, 𝑖, 𝑡)to denote one interaction and deneT = {(𝑢, 𝑖, 𝑡)}to cover all the observed interactions. To facilitate the learning of dual dynamic representations based on graph neural networks, we segment the whole timeline into𝑇 time slices with the equal-length time intervalΔ𝑇. Therefore we haveT = {T, T, ..., T}whereTcontains all the observed triplets that occur in the𝑠-th time slice. It is worth noting that some users and items might not have any interaction in a time slice. Under this situation, the corresponding user and item representations would keep unchanged in that time slice. Now we proceed to formulate the sequential recommendation problem as follows: Problem 1 (Seqential Recommendation Problem). For a target user𝑢, a candidate item𝑖, and all the observed user-item interactionsT, the aim of the sequential recommendation problem is to learn a function𝑓that predicts their potential interaction probability in the near future, which is dened as Overview:Figure 2 depicts the overall architecture of the proposed model DRL-SRe, which takes all the observed user-item interactions as input and outputs the predicted interaction probability for a target user-item pair. Within the model, time-sliced graph neural networks are proposed to learn two sequences of representations for the user and item sides. Each representation in the two sequences corresponds to one time slice, facilitating to characterize the temporal dynamics. Then the user and item representations of the current time slice, together with the static representations gotten from user and item IDs, are fed into the prediction part. For the optimization part, the primary sequential recommendation lossL and the auxiliary temporal prediction lossL, consisting ofLfor item𝑖andLfor user𝑢, are leveraged to jointly optimize the model end-to-end. In what follows, we organize the model specication in a logical fashion. Following the convention of modern deep recommendation methods [44], we map a user ID or an item ID to a dense vectorized representation. This time-independent representation is thus static and used to reveal user long-term interests and item long-term characteristics. Formally, taking user𝑢and item𝑖as examples, we dene the following mapping functions: where𝑬∈ Rand𝑬∈ Rare trainable embedding matrices for users and items, respectively.𝑑denotes the embedding dimension.𝒐is the one-hot encoding for user𝑢and it is analogous to 𝒐of item 𝑖. Before delving into the computational formulas of this module, we rst clarify how to build user-item bipartite graphsG = {G, G, ..., G}for each time slice. Taking graphGfor example, we build it based on all the user-item interaction records inT. Assume there are𝑀users and𝑁items occurring in the𝑠-th time slice, then we have a node feature matrix𝑿∈ Rand an adjacency matrix𝑨∈ {0,1}for the graph. An entry in the adjacency matrix takes a value of 1 if the corresponding user-item interaction occurs, and 0 otherwise. We do not take ne-grained edge weights since the ratios of repeated user-item interactions in a time slice are very small or even 0 for the used datasets. However, we should emphasize that our graph could be easily generalized to take ne-grained edge weights if necessary. The module of time-sliced graph neural networks is responsible for learning node representations based on the constructed graphs. By convention, it performs representation propagation along with the edges of graph G, which is dened as follows: whereˆ𝑨= 𝑫𝑨𝑫denotes the normalized adjacency matrix without self loops and𝑙is the index of the propagation layer. We let𝑿= 𝑿, consisting of input user and item representations (e.g., 𝒆and 𝒆). After propagating for𝐿layers, we obtain multiple layer-wise representation matrices, i.e.,˜𝑿= [𝑿;𝑿;. . .;𝑿]. Since the representations of dierent layers capture dierent semantics, it is urgent to design a reasonable method to combine these representations. We implement ve types of operations and validate user/item-specic GRUs [5] can lead to good performance in general (see experimental results in Section 5.2.3). To be specic, we use˜𝑿to denote the user-specic representations in˜𝑿and it is analogous to˜𝑿. Consequently, we obtain the updated user representations¯𝑿and item representations¯𝑿by: whereGRU(·)|means returning the hidden representation from the recurrent step of𝐿 +1.ΘandΘare the learnable parameters for the respective GRU model. Finally, to model the evolution of the temporal dynamics in the user and item sides, we further correlate the user and item representations across time slices by another two GRUs. This is achieved by collecting time-sliced user representations and item representa- Upon this, we perform recurrent computations to incorporate sequential information into the contextualized representations by: where Θand Θare the parameters of the above GRU models. Based on the proposed time-sliced graph neural networks, we have two sequences of representations for user𝑢and item𝑖, i.e., the potential interaction probability for the considered user-item pair, we combine their dynamic representations and static embeddings. Specically, we take the output of time-sliced graph neural networks, i.e., the last time-sliced user representations𝒉for user 𝑢and item representations𝒉for item𝑖, as the dynamic representations. The representations mapped from IDs are regarded as static representations. After concatenating them, multi-layer perceptrons (MLPs) are adopted to calculate the probability, which is given by: where we use ReLU as the middle-layered activation function.𝜎is the sigmoid function to let the value has a range(0,1).⊕means the concatenation operation.Θrepresents all the parameters involved in MLPs. For training the model DRL-SRe, we formulate a sequential recommendation loss function w.r.t. the user-item pair(𝑢, 𝑖)based on the cross-entropy loss, which is given by: As discussed previously, although the timeline segmentation enables to model high-order and complex user-item relations through the time-sliced graph neural networks, the detailed temporal information is inevitably lost, which might cause the representations𝑯 and𝑯to be sub-optimal. To mitigate this issue, we introduce an auxiliary temporal prediction task based on temporal point process to compensate for the primary task. Temporal point process takes the conditional intensity function 𝜆(𝑡)as the most important component in capturing the continuoustime temporal dynamics. By convention, we usein a function to indicate it is history-dependent. Within a short time interval [𝑡, 𝑡 + 𝑑𝑡),𝜆(𝑡)represents the occurrence rate of an event given the history𝐻and satises:𝜆(𝑡)𝑑𝑡 = 𝑃 {𝑐 ∈ [𝑡, 𝑡 + 𝑑𝑡]|𝐻}. Based on this, the density function is given by: where𝑡is the timestamp of the last event or a starting timestamp. Under the situation of time-sliced user-item interaction graphs, we take the last interaction of a user or an item in a time slice as an event. The corresponding time of the interaction is regarded as the event time. Particularly, we use[𝑡, 𝑡, · · · , 𝑡]and[𝑡, 𝑡, · · · , 𝑡] to denote the detailed temporal information for user𝑢and item𝑖 in dierent time slices, respectively. Following the study [6] that provides a well-designed conditional intensity function to derive an analytical form of the density function, we dene the conditional intensity functions for user𝑢and item 𝑖 w.r.t. the 𝑠-th time slice as follows: whereΘ= {𝒘, 𝜔, 𝑏}andΘ= {𝒘, 𝜔, 𝑏}are the trainable parameters. Through this manner, both the dynamic representations and temporal information are associated with intensities to characterize the continuous-time gaps over consecutive time slices from the dual sides. The analytical form of the density functions could now be easily obtained. For user 𝑢, it is dened as follows: 𝑓(𝑡) = 𝜆(𝑡)𝑒𝑥𝑝 (−𝜆(𝜖) 𝑑𝜖) = exp{𝒘𝒉+ 𝜔(𝑡 − 𝑡) + 𝑏+exp(𝒘𝒉+ 𝑏)(10) −exp(𝒘𝒉+ 𝜔(𝑡 − 𝑡) + 𝑏)} . Analogously, its density function could be derived for item𝑖. Finally, the loss function of the auxiliary temporal prediction task is formulated as the negative joint log-likelihood of generating temporal sequences: Beneting from this loss function, learning𝒉and𝒉is guided by the ne-grained continuous-time information. In the end, we unify the primary sequential recommendation task and the auxiliary temporal prediction task for jointly training DRLSRe. The hybrid objective function is dened as follows: where𝛽is a hyper-parameter to control the relative eect of the auxiliary task. The lossLis easy to be extended to a mini-batch setting where multiple user-item interactions are included. L2 regularization and dropout strategies are commonly employed to alleviate the overtting issue. We use the Adam optimizer to learn the model parameters Θ = {𝑬, 𝑬, Θ, Θ, Θ}. Since the time-sliced graphs are only constructed for users and items having interactions in corresponding time slices, the number of the edges from all time-sliced graphs are of the same order of magnitude as the edge number of a global user-item interaction graph. Also, the complexity of sparse matrix multiplication depends on the number of edges in the Laplacian matrix. As such, the computation does not cost much. This section rst claries the experimental setups and then provides comprehensive experimental results, striving to answer the pivotal questions below: Q1.What are the results of the comparison between DRL-SRe and the state-of-the-art sequential recommendation models? Q2.Does the performance of DRL-SRe suer from a notable drop when removing any crucial component from the model? Q3.How do alternative designs and hyper-parameter settings of DRL-SRe aect the nal performance? # Interactions 699,254 4,402,067 10,971,024 5.1.1 Datasets. To evaluate the performance of all the models while ensuring reliability, we choose three datasets that are publicly available and with dierent origins, which are introduced as follows: Babyis a category-specic subset extracted from the large public dataset [10] of the e-commerce platform Amazon, ranging from May 1996 to July 2014. This kind of dataset source is used in the relevant dual sequence model [41]. Yelpis a review-based dataset that was released by Yelp in 2019. It contains user ratings on businesses such as restaurants, bars, and spas, etc. We regard the ratings with scores larger than 3 as positive feedback and select their corresponding user-item interactions to build the dataset. Netix[1] is a commonly-used dataset from the Netix contest which contains 100M ratings collected between November 1999 and December 2005. This kind of dataset source is also used in sequential recommendation with dual sides [39]. To ensure the statistical signicance of the datasets, we remove the users and items with less than 5 interactions. The basic statistics of the three datasets are summarized in Table 1. As shown in the line of “Time slices” in the table, we segment the whole timeline into𝑇time slices with an equal-length time interval — shown in the line of “Time interval” — for each dataset. We use the user-item interactions coming from the rst time slice to the(𝑇 −2)-th time slice as the training data. The interactions in the(𝑇 −1)-th time slice are collected to constitute the validation data. And the interactions in the last time slice are leveraged as the testing data. This data setting is shared by all the adopted sequential recommendation models introduced later, so as to make fair comparisons. It is worth noting that deep sequential recommendation models are usually formulated as predicting the next interacted item. But they could be easily extended to train and predict the next several items [30] that will be interacted with a target user in the next time slice. 5.1.2 Evaluation Protocols. We choose three widely used metrics in recommender systems: (1) HR@k (Hit Ratio@k) is the proportion of recommendation lists that have at least one positive item within top-k positions. (2) NDCG@k (Normalized Discounted Cumulative Gain@k) is a position-aware ranking metric that assigns larger weights to the top positions. As the positive items rank higher, the metric value becomes larger. (3) MRR (Mean Reciprocal Rank) measures the relative position of the top-ranked positive item and takes a value of 1 if the positive item ranks at the rst position. As such, HR@k and MRR mainly focus on the rst positive item, while NDCG@k considers a wider range of positive items. Since it is too time-consuming to iterate over all user-item pairs to generate the complete sorting list and compute the above metrics, we follow the negative sampling strategy which is commonly observed in recommendation studies considering implicit feedback [12,14,24]. Specically, for each ground-truth item, we randomly sample 100 negative items for metric computation. Without loss of generality, we show the results w.r.t. HR@10, NDCG@10, and MRR. 5.1.3 Baselines. We choose some representative sequential recommendation models that only consider the temporal dynamics in the user side as follows: o GRU4Rec[13]. This is a pioneering model that successfully applies recurrent neural networks to the sequential recommendation problem. It takes the interacted items for a target user as input to generate sequential recommendations. o Caser[30]. To see how convolutional neural networks (CNNs) behave on the used datasets, we choose Caser that combines CNNs and a latent factor model to learn users’ sequential and general representations. o SASRec[14]. SASRec is a well-performed model that heavily relies on self-attention mechanisms to identify important items from a user’s behavior history. These important items aect user representations and nally determine the next-item prediction. o TiSASRec[17]. It is recently proposed by the same research group of SASRec and could be regarded as its enhancement. In particular, continuous time interval information between interacted items is encoded to facilitate the self-attention computation. o ARNPP[21]. ARNPP is a joint item and time prediction model that is empowered by temporal point process and attentive recurrent neural networks. Note that social relations utilized in the study [21] are not considered in this paper. o LESSR[3]. This model inherits the benet of graph neural networks for session-based recommendation and additionally proposes an edge-order preserving aggregation strategy to capture the sequential order of interacted items. We also take the following three well-known dual sequential recommendation models for comparisons: o RRN[39]. As the rst model to address the dynamics in dual sides under the situation of sequential recommendation, it couples two RNNs to model the user temporal patterns and item temporal patterns, respectively. o DEEMS[41]. DEEMS also relies on the architecture of the coupled RNNs and improves the objective function with a novel perspective of building dual prediction tasks. We use DEEMSRNN for its slightly better performance. o SCoRe[24]. This is the state-of-the-art dual sequence recommendation model that considers cross-neighbor relation modeling to better model relations between a target user and a candidate item. An interactive attention mechanism is adopted to gain an overall representation for the user or the item. 5.1.4 Model Implementations. We implement our model by Tensorow and deploy it on a Linux server with GPUs of Nvidia GeForce GTX 2080 Ti (11G memory). The model is learned in a mini-batch fashion with a size of 100. For the adopted Adam optimizer, we set the learning rate to 5e-4 and keep the other hyper-parameters by default. We add L2 regularization to the loss function (Equation 6) by setting the regularization weight to 1e-4. The embedding size of all the relevant models is xed to 16 for ensuring fairness. Table 2: Main results w.r.t. HR@10, NDCG@10, and MRR for sequential recommendation on three datasets. The best and second-best performed methods in each metric are highlighted in “bold” and underline, respectively. Improv. denotes the relative improvement over the second-best results. The number of layers used in MLP is set to 3. The source code of this paper is available athttps://github.com/weizhangltt/dualrecommend. 5.2.1 Performance Comparison (Q1). Table 2 presents the overall performance of our model and all the adopted baselines, from which we have the following key observations: •Compared with other models, GRU4Rec achieves poor results on the three datasets. This conforms to the expectation since only using the representation from the last time step of an RNN is insucient. The reason is that standard RNNs are not good at modeling long-range dependencies and have the forgetting issue. As such, user preference information hidden in behavior sequences could not be eectively distilled. •SASRec outperforms both the RNN-based model GRU4Rec and the CNN-based model Caser. Such improvement might be attributed to the self-attention mechanism, which can assign larger weights to the important interacted items that will aect future user-item interactions. •Compared to other sequential recommendation models that do not consider the dynamics of the item side, TiSASRec achieves better performance in most cases. It makes sense as TiSASRec is an enhancement of SASRec by additionally considering the time interval information between the interacted items in behavior sequences. Thanks to this, attention computation is further promoted. •By comparing ARNPP with GRU4Rec, we could see the benets brought by temporal point process. By further investigating the performance of LESSR, we nd it is even comparable with some dual sequence recommendation models in some cases. This demonstrates the power of graph neural networks in modeling user-item interactions. •The three dual sequential recommendation models exhibit the relatively good performance from a whole perspective, demonstrating the positive eect of modeling the dual dynamics. Among them, RRN is not as good as the other two models. This might be caused by the limited ability of RNNs in modeling long-range dependency in a sequence. DEEMS obtains better results than RRN, showing the positive contribution of introducing the dual prediction loss. Thanks to the incorporation of local user-item interactions constrained to one sequence, SCoRe achieves the second-best results in many cases. •Our model DRL-SRe yields consistently better performance than all the baselines. In particular, DRL-SRe improves the secondbest performed models w.r.t. NDCG@10 by 3.39%, 10.22%, and 9.62% on Baby, Yelp, and Netix, respectively. This makes sense because: (1) DRL-SRe can distill knowledge from all the user-item interactions that happened in the past to benet the dynamic user and item representation learning. (2) The introduction of the auxiliary temporal prediction task over consecutive time slices can signicantly improve the sequential recommendation task, which is validated in Table 3. Table 4: Results of model variants for DRL-SRe. Improvements over variants are statistically signicant with p < 0.01. 5.2.2 Ablation Study (Q2). We further conduct an ablation study to validate the contributions of key components in DRL-SRe. Specifically, (1) “w/o graph” represents removing graph neural networks and using simple mean-pooling to aggregate the neighbor information for users and items. (2) “w/o RNN (time slices)” means not using Equation 4 and performing a mean-pooling operation for the user/item representations obtained by time-sliced graph neural networks. (3) “w/o concat ID embedding” simplies Equation 5 by not concatenating the static user and item embeddings. (4) “w/o auxiliary task” is equivalent to setting𝛽 =0 in Equation 12, meaning not utilizing the auxiliary task. And “- w/ time embedding” leverages temporal embeddings for discretized time slots as model input instead of using the temporal prediction task. Throughout the result analysis of the ablation study shown in Table 3, we observe that: •“w/o graph” suers severe performance degradation. It validates the crucial role of time-sliced user-item interaction graphs for characterizing the dynamics of users and items. This is intuitive since learning from full user-item interactions in a time slice could lead to accurate time-sliced user and item representations. •“w/o RNN (time)” also sees its signicant performance drop. This phenomenon reveals that explicitly correlating time-sliced user representations and item representations is indispensable. The reason might be that RNNs could introduce sequential information that each time-sliced representation does not have. •The concatenation of static embeddings has a positive impact by seeing the results of “w/o concat ID embedding”. • “w/o auxiliary task” is obviously inferior to the full model DRL- SRe, verifying that using temporal point process to capture negrained temporal information is advantageous for deriving effective dynamic representations. Moreover, simply feeding time embeddings into model input is not as good as leveraging the temporal prediction task for sequential recommendation. 5.2.3 Analysis of Model Alternatives (Q3). To deeply investigate why the proposed model works, we design several alternatives of the model: (1) “w/ global graph” replaces the multiple timesliced graphs with a global user-item interaction graph. (2) “w/ last graph” replaces the multiple time-sliced graphs with a last user-item interaction graph. (3) “w/ user slices” only uses the sequences w.r.t. interacted users to do nal prediction, i.e.,𝒉⊕𝒆⊕𝒆in Equation 5. (4) “w/ item slices” only adopts the sequences w.r.t. interacted items to perform prediction, i.e., 𝒉⊕ 𝒆⊕ 𝒆in Equation 5. Besides, we investigate some dierent operations to fuse layerwise representations from time-sliced graph neural networks, as shown in Equation 3. “w/ concatenation” means concatenating the layer-wise representations. “w/ last-layer output” denotes only using the representation of the last layer. “w/ mean-pooling” uses mean-pooling to summarize the layer-wise representations. And “w/ single GRU” does not dierentiate the user side and the item side by only employing a single GRU. From the results shown in Table 4, we nd that: •In the rst part of the table, the performance degradation of “w/ global graph” is signicant. It is intuitive since only using a global user-item interaction graph is unable to capture the temporal dynamics of users and items, although the ability to encode enriched user-item interactions by GNN is maintained. Similarly, “w/ last graph” is degraded heavily, which reects the great necessity of modeling long behavior sequences for the studied task. Moreover, by evaluating the results of “w/ user slices” and “w/ item slices”, we could conclude that learning from dual dynamics is protable. •In the second part of the table, the four alternatives perform worse than using user/item-specic GRUs in DRL-SRe. In particular, the comparison between “w/ single GRU” and the full model validates that dierentiating the dual sides of users and items consistently boosts the recommendation performance. 5.2.4 More Discussions (Q3). This part provides some more discussions about how some hyper-parameters aect DRL-SRe, including hyper-parameter 𝛽 and the propagation layer number of GNNs. Figure 3: Result variation with dierent 𝛽 on Yelp (left) and Netix (right). Eect of hyper-parameter 𝛽.To analyze the inuence of hyperand illustrate the recommendation performance changing curve of DRL-SRe on the Yelp and Netix datasets in gure 3. We can observe that better results are achieved when setting𝛽in a suitable middle value range and too large values might even hurt the performance. Figure 4: Result variation when increasing the number of propagation layers (shown in x-axis). Eect of layer numbers in GNN.To investigate whether our model can benet from the multi-layer propagation, we vary the number of GNN layers in the range of{1,2,3,4,5}. Figure 4 depicts the performance trend by HR@10. When increasing the layer number from 1 to 2, DRL-SRe achieves consistently better performance on the three datasets, indicating that introducing neighbor information through representation propagation is protable. However, the continual increase of the layer number even harms the performance. This might be caused by the over-smoothing and overtting issues. This paper studies the sequential recommendation problem by modeling the dual temporal dynamics for both the user and item sides. The novel model DRL-SRe is proposed, which innovatively develops the time-sliced graph neural networks to learn dynamic user and item representations, and introduces the auxiliary temporal prediction task to compensate for the primary sequential recommendation task based on temporal point process. The comprehensive experiments conducted on three public and large datasets validate the superiority of DRL-SRe and the eectiveness of its main components. This work was supported in part by National Natural Science Foundation of China under Grant (No. 62072182) and the Fundamental Research Funds for the Central Universities.