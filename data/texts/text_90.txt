The explanation of a recommendation list plays an increasingly important role in the interaction of a user with a recommender system: the pervasiveness of economic interest and the inscrutability of most Articial Intelligence systems make users ask for some form of accountability in the behavior of the systems they interact with. Given the explanation that a system can provide to a user we identify at least two characteristics that the explanation part should enforce [1, 2, 3]: •Adherence to reality: the explanation should mention only features that really pertain to the recommended item. For instance, if the system recommends the movie “Titanic”, it should not explain this recommendation by saying “because it is a War Movie” since it is by no means an adherent description of that movie; •Constancy in the behavior: when the explanation is generated based on some sample, and such a sample is drawn with a probability distribution, the entire process should not exhibit a random behavior to the user. For instance, if the explanation for recommending the movie “The Matrix” to the same user is rst “because it is a Dystopian Science Fiction”, and then “because it is an Acrobatic Duels Movie”, for the same user, this behavior would be perceived as nondeterministic, and thus reducing its trustworthiness. Among several ways of generating explanations, we study here the application of LIME [4] to the recommendation process. LIME is an algorithm that can explain the predictions of any classier or regressor in a faithful way, by approximating it locally with an interpretable model. LIME belongs to the category of post-hoc algorithms and it sees the prediction system as a black box by ignoring its underlying operations and algorithms. Since we can consider the recommendation task as a particular Machine Learning task, the LIME approach can also be applied to recommendation. LIME-RS [5] is an adaptation of the general algorithm to the recommendation task and can be considered in all respects as a black-box explainer. This means that it generates an explanation by drawing a huge number of (random) calls to the system, collecting the answers, building a model of behavior of the system, and then constructing the explanation for the particular recommended item. While the fact of adopting a black-box approach lets LIME-RS to be applicable for every recommender system, the way of building a model by drawing a huge random sample of system behaviors makes it lose both adherence and constancy, as our experiments show later on this paper. This suggests that the direct application of LIME-RS to recommender systems is not advisable, and that further research is needed to assess the usefulness of LIME-RS in explaining recommendations. The paper is organized as follows: Section 2 reviews the state of the art on explanation in recommendation; Section 3 details LIME to make the paper self-contained. Section 4 shows the results of experiments with two mainstream recommendation models: Attribute Item-kNN and Vector Space Model. We discuss the outcomes of the experiments in Section 5, and conclude with Section 6. In recent years, the theme of Explanation in Articial Intelligence has come to the foreground, capturing the attention not only of the Machine Learning and related communities – that deal more specically with the algorithmic part – but also of elds closer to Social Sciences, such as Sociology or Cognitivism, which look with great interest to this area of research [1]. The growing interest in this area is also dictated by new regulations of both Europe [6] and US [7] with respect to sensitive issues in the eld of personal data processing, and legal responsibility. This trend has also touched the research eld of recommender systems [8,9,10,11]. However, topics such as explanation are by no means new to this eld. In fact, we can date back to 2014 the introduction of the term “explainable recommendation” [12], although the need to provide an explanation that accompanies the recommendation is a need that emerged as early as 1999 by Schafer et al.[13], when people began trying to explain a recommendation with other similar items familiar to the user who received that recommendation. Catalyzation of interest around the topic of explanation of recommendations coincides also with the awareness achieved in considering metrics beyond accuracy as fundamental in evaluating a recommendation system [14,15]. Indeed, all of the well-known metrics of novelty, diversity, and serendipity are intended to improve the user experience, and in this respect, a key role is played by explanation [3,16]. “Why are you recommending that?"—this is the question that usually accompanies the user when a suggestion is provided. Tintarev and Mastho[2]detailed in a scrupulous way the aspects involved in the process of explanation when we talk about recommendation. They identied 7 aspects: user’s trust, satisfaction, persuasiveness, eciency, eectiveness, scrutability, and transparency. This is the starting point to dene Explainable Recommendation as a task that aims to provide suggestions to the users and make them aware of the recommendation process, explaining also why that specic object has been suggested. Gedikli et al.[3]evaluated dierent types of explanations and drew a set of guidelines to decide what the best explanation that should equip a recommendation system is. This is due to the fact that popular recommendation systems are based on Matrix Factorization (MF) [17]; for this type of model, trying to provide an explanation opens the way to new challenges [1, 18, 19, 20]. There are two dierent approaches to address this type of issue. •On the one hand, the model-intrinsic explanation strategy aims to create a user-friendly recommendation model or encapsulates an explaining mechanism. However, as Lipton[21]points out, this strategy will weigh in on the trade-o between the transparency and accuracy of the model. Indeed, if the goal becomes to justify recommendations, the purpose of the system is no longer to provide only personalized recommendations, resulting in a distortion of the recommendation process. •On the other hand, we have a model-agnostic [22] approach, also known as post-hoc [23], which does not require to intervene on the internal mechanisms of the recommendation model and therefore does not aect its performance in terms of accuracy. Most recommendation algorithms take an MF-approach, and thus the entire recommendation process is based on the interaction of latent factors that bring out the level of liking for an item with respect to a user. Many post-hoc explanation methods have been proposed for precisely these types of recommendation models. It seems evident that the most dicult challenge for this type of approach lies in making these latent factors explicit and understandable for the user [9]. Peake and Wang[23] generate an explanation by exploiting the association rules between features; Tao et al.[24]in their work, nd benet from regression trees to drive learning, and then explain the latent space; instead, Gao et al.[25]try a deep model based on attention mechanisms to make relevant features emerge. Along the same lines are Pan et al.[11], who present a feature mapping approach that maps the uninterpretable general features onto the interpretable aspect features. Among other approaches to consider, [12] proposes an explicit factor model that builds a mapping between the interpretable features and the latent space. On the same line we also nd the work by Fusco et al. [26]. In their work, they provide an approach to identify, in a neural model, which features contribute most to the recommendation. However, these post-hoc explanation approaches turn out to be built for very specic models. Purely model-agnostic approaches include the recent work of Tsang et al.[27], who present GLIDER, an approach to estimate interactions between features rather than on the signicance of features as in the original LIME [4] algorithm. This type of solution is constructed regardless of the recommendation model. Our paper focuses on the operation of LIME, a modelagnostic method for a surrogate-based local explanation. When a user-item pair is provided, this model returns as an outcome of the explanation a set of feature weights, for any recommender system. However, the recommendation task is very specic, so there is a version called LIME-RS [5] that applies the explanation model technique to the recommendation domain. In this way, any recommender is seen as a black box, so LIME-RS plays the role of a model-agnostic explainer whose result is a set of interpretable features and their relative importance. The goal of LIME-RS is to exploit the predictive power of the recommendation (black box) model to generate an explanation about the suggestion of a particular item for a user. In this respect, it exploits a neighborhood drawn according to a generic distribution compared to the candidate item for the explanation. It seems obvious that the choice of the neighborhood plays a crucial role within the process of explanation generation by LIME-RS. We can compare this sample extraction action to a perturbation of the user-item pair we are using to generate the explanation. In the case of LIME-RS this perturbation must generate consistent samples with respect to the source dataset. We see that this choice represents a critical issue for all the post-hoc models which base their expressiveness on the locality of the instance to explain. This trend is conrmed in several papers addressing this issue of surrogate-based explanation systems such as LIME and SHAP [28]. In two recent papers, Alvarez-Melis and Jaakkola[29]have shown how the explanations generated with LIME are not very robust: their contribution aims to bring out how small variations or perturbations in the input data cause signicant variations in the explanation of that specic input [30]. In their paper, a new strategy is introduced to strengthen these methods by exploiting local Lipschitz continuity. By deeply investigating this drawback, they introduced self-explaining models in stages, progressively generalizing linear classiers to complex yet architecturally explicit models. Saito et al.[31]also explored this issue by turning their gaze to dierent types of sampling to make the result of an explanation generated through LIME more robust. In particular, in their work, they introduce the possibility of generating realistic samples produced with a Generative Adversarial Network. Finally, Slack et al.[32]adopt a similar solution in order to control the perturbation generating neighborhood data points by attempting to mitigate the generation of unreliable explanations while maintaining a stable black-box model of prediction. From a formal point of view, we can dene a LIMEgenerated explanation for a generic instance𝑥 ∈ 𝒳 produced by a model 𝑓 as: whereℒrepresents the delity of the surrogate model to the original𝑓, and𝑒represents a particular instance of the class𝐸of all possible explainable models. Among all the possible models, the one most frequently chosen is based on a linear prediction. In this case, an explanation refers to the weights of the most important interpretable features, which, when combined, minimize the divergence from the black-box model. The function 𝜋measures the distance between the instance to be explained𝑥 ∈ 𝒳, and the samples𝑥∈ 𝒳extracted from the training set to train the model𝑒. Finally,Ω(𝑒) represents the complexity of the explanation model. Two pieces of evidence make the application of LIME possible: (i) the existence of a feature space𝒵on which to train the surrogate model of𝑓, (ii) and the presence of a surjective function that maps the space mentioned above (𝒵) to the original space of instances (𝒳). Going into more detail, we consider the delity functionℒas the mean square deviation between the prediction for a generic instance𝑥∈ 𝒳of the black-box model and that generated for the counterpart𝑧∈ 𝒵by the surrogate model. Starting from these considerations we can express ℒ with the following formula: In the formula above𝜋plays a fundamental role as it expresses the distance between the instance to be explained and the sampled instance used to build the surrogate model. From a generic perspective, we can express this function as a kernel function like𝜋= 𝑒𝑥𝑝(−𝐷(𝑥, 𝑥)/𝜎), where𝐷is any measure of distance. The full impact of this distance is captured when the delity function also considers the transformation of the surrogate sample in the original space. As mentioned earlier, we consider a surjective function𝑝that maps the original space into the feature space𝑝 : 𝒳 → 𝒵. We can also consider the function that allows us to move in the opposite direction𝑝: 𝒳 → 𝒵. At this point, Equation (2) becomes: From this last equation, we can grasp the criticality of the surjective mapping function. Indeed, the neighborhood in𝒵-space cannot be guaranteed with the transformation in𝒳-space. Thus, some samples selected to train the surrogate model could not satisfy the neighborhood criterion for which they were chosen. We must therefore stress on the centrality of the sampling function: how do we extract the neighborhood of our instance to be explained? If we look at the application of LIME to the recommendation domain, we can compare this sampling action to a local perturbation around our instance𝑥; however, this perturbation aims to generate𝑛samples𝑥, which might contain inconsistencies: as an example, suppose we want to explain James’s feeling about the movie The Matrix. The original triple of the instance to be explained associates to the user-item pair the genre of the movie (representing the explainable space) and in this case it is of the type ⟨𝐽𝑎𝑚𝑒𝑠, 𝑇 ℎ𝑒𝑀𝑎𝑡𝑟𝑖𝑥, 𝑆𝑐𝑖-𝐹 𝑖⟩. A perturbation around this instance could generate inconsistencies of the type ⟨𝐽𝑎𝑚𝑒𝑠, 𝑇 ℎ𝑒𝑀𝑎𝑡𝑟𝑖𝑥, 𝑊 𝑒𝑠𝑡𝑒𝑟𝑛⟩. For this reason, in LIME-RS the perturbation considers only real and not synthetic data. This choice is dictated by the avoidance of the out-of-sample (OOS) process phenomenon. Closely related to this problem predicted by OOS is that the interpretation examples selected in LIME-RS represent the ability to capture the locality through disturbance mechanisms eectively. One of the disadvantages of LIME-like methods is that they sometimes fail to estimate an appropriate local replacement model but instead generate a model that focuses on explaining the examples and is also aected by more general trends in the data. This issue is central to our work, and it involves two aspects: (i) the rst one concerns the sampling function of the samples precisely. In the LIME-RS implementation, this function is driven by the popularity distribution of the items within the dataset. (ii) The second critical issue concerns the model’s ability to wittily discriminate the user’s taste from the neighborhood extracted to build the surrogate model. A model that squashes too much on bias or is inaccurate cannot bring out the peculiarities of user taste that are critical in building the explainable model which are, in turn, useful in generating the explanation for the instance of interest. These observations dictate the two research questions that motivated our work: RQ1Can we consider the surrogate-based model on which LIME-RS is built to generate always the same explanations, or does the extraction of a different neighborhood severely impact the system’s constancy? RQ2Are LIME-RS explanations adherent to item content, despite the fact that the sampling function is uncritical and based only on popularity? This section is devoted to illustrating how the experimental campaign was conducted. The datasets used for this phase of experimentation are Movielens 1M [33], Movielens Small [33], and Yahoo! Movies. Their characteristics are shown in Table 1. Table 1 Characteristics of the datasets involved in the experiments. As for the choice of the models to be used in this work is concerned, we selected two well-known recommendation models that are able to exploit the information content of the items to produce a recommendation: Attribute Item kNN (Att-Item-kNN) and Vector Space Model (VSM). The two chosen models represent the simplest solution to address the recommendation problem by exploiting the content associated with the items in the catalog. Att-Item-kNN exploits the characteristics of neighborhood-based models but expresses the representation of the items in terms of their content and, based on this representation, it computes a similarity between users. Starting from this similarity and exploiting the collaborative contribution in terms of interactions between users and items, Att-Item-kNN tries to estimate the level of liking of the items in the catalog. VSM represents both users and items in a new space to link users and items to the considered information content. Once obtained this new representation, with an appropriate function of similarity, VSM estimates which are the most appealing items for a specic user. The implementation of both models are available in the ELLIOT [34] evaluation framework. This benchmarking framework was used to select the best conguration for the two recommendation models by exploiting the corresponding conguration le. Our experiments start by selecting the best congurations based on nDCG [35,36] for the two models on the considered datasets. Then, we generate the top-10 list of recommendations for each user, and we take into account the rst item𝑖on these lists for each user𝑢. Finally, each recommendation pair(𝑢, 𝑖)is explained with LIME-RS. The explanation consists of a weighted vector(𝑔, 𝑤)where𝑔is the genre of the movies in the dataset – i.e., the features – and𝑤is the weight associated to𝑔by LIME-RS within the explanation. Then, this vector is sorted by descending weights. In this way, the genres of the movies which play a key role within the recommendation, as explained by LIME-RS, are highlighted at the rst positions of the vector. These operations are then repeated𝑛 = 10times and changing the seed each time, as10is likely to be a good choice to detect a general pattern in the behavior of LIME-RS. At this point, for each pair(𝑢, 𝑖), we have a group of10explanations ordered by descending values of𝑤, which we exploit to answer our two research questions. RQ1.Empirically, since in a real scenario of recommendation a too verbose explanation is not useful, we consider only the rst ve features in the sorted vector representing the explanation of each recommendation. In order to verify the constancy of the behavior of LIME-RS, given a (𝑢, 𝑖)pair, we exploit the𝑛previously generated explanations for this pair. Then for𝑘 = 1, 2, . . . , 5, we dene 𝐺as the multiset of genres that appear in𝑘-th position – for instance, if “Sci-Fi” occurs in the rst position of 7 explanations, then “Sci-Fi” occurs 7 times in the multiset 𝐺, and similarly for other genres and multisets. Then, we compute the frequency of genres in each position as follows: given a position𝑘, a genre𝑔, and the number 𝑛of generated explanations for a given pair(𝑢, 𝑖), the frequency 𝑓of 𝑔 in 𝑘-th position is computed as: where||·||denotes the cardinality of a multiset. Then, all this information is collected for each user in ve lists — one for each of the𝑘positions — of pairs⟨𝑔, 𝑓⟩sorted by frequency. One can observe that the computed frequency is an estimation of the probability that a given genre is put in that position within the explanation generated by LIME-RS sorted by values. Hence, the pair ⟨𝑔, max (𝑓)⟩ describes the genre with the highest frequency in the𝑘-th position of the explanation for a pair (𝑢, 𝑖). Finally, it makes sense to compute the mean𝜇 of the highest probability values in each position𝑘of the explanations for each pair(𝑢, 𝑖). Formally, by setting a position 𝑘, the mean 𝜇is computed as: where𝑈is the set of users for whom it was possible to generate a recommendation for. Observing the value of 𝜇, we can state to what extent LIME-RS is constant in providing the explanations until the𝑘-th feature: the higher the value of𝜇, the higher the constancy of LIMERS concerning the 𝑘-th feature. By looking at Table 2, we can see that for Att-ItemkNN the LIME-RS explanation model is reliable as long as it considers at most three features in the weighted vector presented as an explanation of the recommendation. Extending the explanation to four features, we have a constancy that falls below 65%, while arriving at an explanation with ve features is more likely to run into explanations that exhibit an unacceptably random Table 2 Constancy of LIME-RS. A value equal to 0 means that the genre(s) provided by LIME-RS in the first𝑘position(s) is always dierent (worst case: completely inconstant behavior); A value equal to 1 means that the genre(s) provided by LIME-RS in the first𝑘position(s) is always the same (total constancy). behavior. On the other hand, we can see that for VSM the values are much more stable. In this case, we have a constancy that, regardless of the length of the weighted vector of the explanation, stabilizes on average around 80%. An aspect emerges that will be discussed in detail later: LIME-RS is conditioned by the ability of the black-box model to discriminate the user’s tastes locally. RQ2.With the aim of providing an answer about the adherence to reality of LIME-RS, we make a comparison between the genres claimed to explain a recommended item and its actual genres. Indeed, the explanations about an item should t the list of genres the item is characterized by. This means that, in an ideal case, all highly weighted features within the explanation should match the genres of the item. From the results in Table 2, we notice that using Att-Item-kNN the constancy of LIMERS reaches a low value after the third feature. Hence, it is a futile eort to go deeper in the study of the explanation. To this aim, we intersected each explanation limited to the set𝐸of its rst𝑘genres with the set of genres𝐹characterizing the rst recommended item, for𝑘 = 1, 2, 3. Upon completion of this operation for all the𝑛explanations generated for each(𝑢, 𝑖)pair, we computed the number of times we obtained an empty intersection of these sets, normalized by the total number of explanations𝑛 × |𝑈 |, in order to understand to what extent an explanation is (not) adherent to the item. Formally, for a given value of𝑘, the value𝑎𝑑ℎ𝑒𝑟𝑒𝑛𝑐𝑒 is computed as: 𝑎𝑑ℎ𝑒𝑟𝑒𝑛𝑐𝑒=𝑛 × |𝑈|(6) where𝑈is the set of users of the dataset for whom it was possible to generate a recommendation,𝑛is the number of generated explanations for each pair(𝑢, 𝑖), and byΣ[· · · ]we mean that we sum 1 if the condition inside[· · · ]is true, and 0 otherwise. One can note that 𝑎𝑑ℎ𝑒𝑟𝑒𝑛𝑐𝑒∈ [0, 1], where a value of 1 indicates the worst case in which for none of the𝑛explanations under consideration at least one genre of the item is in the rst𝑘features of the explanation. In contrast, the lower the value of𝑎𝑑ℎ𝑒𝑟𝑒𝑛𝑐𝑒, the higher the adherence of LIME-RS. Table 3 Adherence of LIME-RS. For value equals to 1 no genre provided by LIME-RS in the first𝑘real genres of the movie (worst case); For value equals to 0 at least one genre provided by LIME-RS in the first𝑘genres is always among the real genres of the movie. Observing the results from Table 3, Att-Item-KNN performs well in terms of adherence since, in approximately 75% of cases, even considering only the main feature of the explanation, it falls into the set of the item genres, as for Movielens dataset family. This performance is a 10% lower for Yahoo! Movies. In contrast with this result, VSM shows poor performances on both dataset of the Movielens family, by failing half the time about Movielens 1M as regards adherence. A surprising result is achieved for Yahoo! Movies dataset because, enlarging the study to the rst three features among the explanation, the error is almost completely absent. The reasons we found to explain this dierence in the performances concern the characteristics and the quality of the dataset, as we highlight later on. This work investigates how well a post-hoc approach based on local surrogates – such as the LIME-RS algorithm – explains a recommendation. Instead of studying the impact of explanations on users (that is a well-studied topic in the literature and is beyond our scope), we focus on objective evidences that could emerge. In this respect, we have designed specic experiments, which introduced two dierent metrics, to evaluate adherence and constancy for this kind of algorithms. For instance, Table 2 shows a dierent behavior for Att-Item-kNN and VSM. On the one hand, Att-Item-kNN seems to guarantee a good constancy in explanations up to the third feature. This suggests that an explanation that exploits the rst three features of the list produced by LIME-RS could be barely considered as reliable (i.e., reaching a constancy of 0.69on Movielens 1M). On the other hand, VSM exhibits a much more "stable" behavior, demonstrating in all cases (except for the rst feature with Movielens 1M) better performance than Att-Item-kNN in terms of constancy, with peaks up to 97%. A straightforward consequence of these observations could be analyzed in terms of condence or probability. If the constancy steadily decreases, it means that the probability that LIME-RS suggests the same explanatory feature decreases. In practical terms, we could say that LIME-RS is less condent about its explanation. In fact, this is the behavior of Att-Item-kNN. Conversely, VSM shows high values of constancy, resulting in a more "deterministic" behavior. With VSM, LIME-RS is more condent of its explanations. This could increase user’s trustworthiness, since LIME-RS behavior is more reliable. However, these results could also be interpreted together with the ones from Table 3. They show how often at least one feature – out of𝑘features provided by LIME-RS– adheres to the features that describe the item being explained. In other words, they measure the probability that LIME-RS succeeds in reconstructing at least one feature of a specic item. Combining the results of Table 2 and those of Table 3, Att-Item-kNN, as already mentioned, shows good performance regarding adherence and identies 3 times out of 4 the rst fundamental feature of the explanation among those present in the set of features originally associated with the item. As expected, if the number𝑘of LIME-RS-reconstructed features increases, the number of times such a set has a nonempty intersection (with the features belonging to the item) – i.e., adherence – increases. It could be noted that Att-Item-kNN on Yahoo! Movies shows the worst behavior in terms of adherence. VSM shows a dierent behavior. Despite the excellent performance regarding constancy, it could be observed that on both Movielens datasets, the performance in terms of adherence is poor, and worse for Movielens 1M than for Movielens Small. Surprisingly, on Yahoo! Movies, VSM performs much better, and the errors are almost negligible. The dierence between the two modelscould be due to many reasons. In the following we analyze possible relations between such behaviors and two of them: popularity bias in the dataset and characteristics of side information. On the one hand, if the dataset is aected by popularity bias, it would be a well-studied cause of confusion for LIME-RS. On the other hand, the characteristics of the side information associated with the datasets could dramatically inuence the performance of the two recommendation models. To assess these hypotheses, we have evaluated (see Table 4) the recommendation lists produced by Att-Item-kNN and VSM considering nDCG, Hit Rate (HR), Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR). Table 4 shows that the chosen datasets are strongly aected by popularity bias. Indeed, MostPop is the best performing approach, and the two Table 4 Results of the experiments on the models involved in the experiments. Models are optimized according to the value of nDCG. "personalized" models fail to produce accurate results. This triggers the second aspect that concerns the quality of the content. The results suggest that the side information is not good enough to boost the recommendation systems in producing meaningful recommendations. In fact, the three datasets seem to have an informative content that is not adequate to generate appealing recommendations. We observe that, from an informative point of view, the Yahoo! Movies dataset is slightly more complete: 22 genres against the 18 genres available on Movielens. Although the VSM model does not show excellent performance, in combination with LIME-RS, it provides explanations that are very reliable in terms of constancy (see Table 2) and adherence (see Table 3) to the actual content of the items being explained. From the designer perspective,there is also a pragmatic way to look at the experimental results. Suppose a developer needs an o-the-shelf way of generating explanations for recommendations, and chooses LIME-RS to do that. Our results suggest that if the explainer employs a Movielens dataset with Att-Item-kNN model, then it is better to run the explainer several times. Indeed, the rst feature obtained for the explanation could change around 1 time every 5 trials (rst column of Table 2), and once such a feature is obtained, it is better to check whether this feature is really among the ones describing the item, since 1 time out of 4 the feature can be wrong (rst column of Table 3). Moreover, if the explainer employs the Yahoo! Movies dataset with VSM model, then probably there is no need to run the explainer twice, since its behavior is constant 97% of the times, while the feature is wrong only 10% of the times. However, the low performance of such a model is to be taken into account. In this paper we shed a rst light on the eectiveness of LIME-RS as a black-box explanation model in a recommendation scenario. We propose two dierent measures to understand how reliable an explanation based on LIME-RS is: (i) constancy was used to assess the impact of the random sampling phase of LIME-RS on the provided explanation – ideally the explanation should remain constant in spite of the sample used to obtain it; (ii) adherence was proposed to understand the reconstructive power of LIME-RS with respect to the features that belong to the item involved in the explanation – ideally, LIME-RS should provide an explanation that always adheres to the actual features of the recommended item. To test both constancy and adherence, we trained and optimized two content-based recommendation models: Attribute Item-kNN (Att-Item-kNN), and a classical Vector Space Model. For each model, and for all datasets exploited in the study, we generated recommendation lists for all users. We exploited the rst item of these top-10 lists to produce the explanations that were then the subject of our investigation. It turned out that for models built with a large collaborative input such as Att-Item-kNN, LIME-RS produces fairly constant explanations up to a length of three features. Moreover, these explanations turn out to be adherent with respect to the item between 65% and 75% of the cases in which only the rst feature of the weighted vector of explanations is considered. VSM shows a dierent behavior where explanations are much more constant, but suer a lot in terms of adherence, except for the Yahoo! Movies dataset for which the explanation model showed outstanding performance despite the poor ability of VSM to provide sound recommendations to users. In our experiments, some evidence started to emerge highlighting that the adopted explanation model is conditioned not only by the accuracy of the black-box model it tries to explain but also by the quality of the side information used to train the model. The latter result deserves to be adequately investigated to search for a link at a higher level of detail. We plan to apply our experiments also to other recommendation models, to see whether the problems with adherence and constancy that we found for the two tested models show up also in other situations. We will also investigate what impact structured knowledge has on this performance by exploiting models capable of leveraging this type of content. In addition, it would also be the case to try dierent reference domains with richer datasets of side information to understand what impact content quality has on this type of explainer. The authors acknowledge partial support of PID2019108965GB-I00, PON ARS01_00876 BIO-D, Casa delle Tecnologie Emergenti della Città di Matera, PON ARS01_00821 FLET4.0, PIA Servizi Locali 2.0, H2020 Passapartout - Grant n. 101016956, PIA ERP4.0, and IPZS-PRJ4_IA_NORMATIVO.