CCS Concepts: • Computing methodologies → Ranking; • Information systems → Content analysis and feature selection; Similarity measures; Language models. ACM Reference Format: Dor Lavi. 2021. Learning to Match Job Candidates Using Multilingual Bi-Encoder BERT. In Fifteenth ACM Conference on Recommender Systems (RecSys ’21), September 27-October 1, 2021, Amsterdam, Netherlands. ACM, New York, NY, U SA, 2 pages. https://doi.org/10.1145/3460231.3474609 ABSTRACT Randstad is the global leader in the HR services industry. We support people and organizations in realizing their true potential by combining the power o f today’s technology with our passion for people. In 2020, we helpe d more than two million candidates ﬁnd a meaningful job with our 236,100 clients. Randstad is active in 38 markets around the world and has top-three positions in al most half of these. In 2020, Randstad had on average 34,680 corporate employees and generated revenue of € 20.7 billion. Each day, at Randstad, we employ industry-scale recommender systems to recommend thousands of candidates to our clients, and the other way around; vacancies to job seekers. Our “Talent Recommender” recommender system is based on a heterogeneous collection of input data: CVs, vacancy texts (job descriptions) and structured data (e.g., the location of a candidate or vacancy). The goal of the system is to recommend the best candidates (tal ents) to each open vacancy. CVs are user-generated PDF ﬁles. It goes without saying that parsing those ﬁles to plain text can b e a challenge in itself and therefore out of scope for this talk. On the other hand, vacancies are usually structured formatted text. We should be aware that due to the diﬀerence in structure and preprocessing steps, that the input t o the subsequent steps is inevitably noisy. Most NLP research in t ext similarity is based on the assumption that 2 pieces of information are the same but written diﬀerently [ 2 documents complement one another like pieces in a puzzle, together they create the bigger picture, rather than 2 similar paintings. Some o f our biggest challenges with the “Talent Rec ommender” stem from d ealing with the diverse nature of our textual sources of data: vacancies and CVs. While both capture similar information, they are inherently diﬀerent in many ways. First, the information in CVs and vacancies are similar, bu t there exists a vocabulary gap, where grammar and context diﬀer. For example, where “I have 10 years of experience as an instructor” in a CV shares no word overlap with “We are looking for a talented tutor” in a vacancy, both cases express similar information regarding “experience in the ﬁeld of education,” we need to overcome the synonyms gap “instru ctor” and “tutor.” In addition, the sentence structure 1]. Like two artists that paint the same landscape, but each with its own style. However, in our case the is completely diﬀerent, CVs are typically written in “storytelling mode” “I have...,” while the vacancy is in “exploration mode” “we are looking.. . ” The second challenge is multilinguality. Since we are a multinational company that operates all across the globe, developing a model per language is not scalable in our case. We ult imately would like one maintainable model that supports as many l anguages as possible. Our last challenge is cross language similarity [4]. In some of the countries we operate, there is a high p ercentage of job seekers that are not native to that country. For example, many of the job descriptions in the Netherlands are in Dutch, however around 10% of the CVs are in English. Classic text models, like TF-IDF and Word2vec, captu re information within one language, but hardly connect between languages. Simply put, even if trained on multiple languages each language will have its own cluster in space. So “logistics” in English and “logistiek” in Dutch are embedded in a completely diﬀerent point in space, even though the meaning is the same. Furthermore, we know that the language of CV correlates with nationality and therefore c an be a proxy discriminator. Due to the impact of these systems and the risks of unintended algorithmic bias and discrimination, HR is marked as a high risk domain in the recently published EC Artiﬁcial Intelligence Act [2]. To avoid discriminating against nationality we would l ike to recommend a c andidate to the vacancy no matter which language the CV is written in. That is of course only if language is not a requirement for that vacancy. In this talk, we will show how we used our internal history of candidate placements to generate labeled CV-vacancy pairs dataset. Afterwards we ﬁne-tune a multilingual BERT with bi encoder struct ure [3] over this dataset, by adding a cosine similarity log loss layer. We will explain how using the mentioned structure helps us overcome most of the challenges described above, and how it enables us to build a maintainable and scalable pipeline to match CVs and vacancies. In addition, we show how we gain a better semantic understanding, and learn to bridge the vocabulary gap. Finally, we highlight how multilingual transformers help us handle cross language barrier and might reduce discrimination. SPEAKER BIO Dor Lavi is a senior data scientist at Randstad. Dor has experience in both academia and industry environments. He worked as a researcher in Tel Aviv University. His research focu sed on the impact of Israeli regulation on t he real estate market in Israel. He worked as a data scientist in a variety of technology companies, some of them are tech giants such as Booking.com and others are smaller scale up s like Similarweb. At Randstad, Dor is responsible for the core recommendation system for that match between job seekers and vacancies.