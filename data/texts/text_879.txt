Abstract Session-based recommender systems aim to improve recommendations in short-term sessions that can be found across many platforms. A critical challenge is to accurately model user intent with only limited evidence in these short sessions. For example, is a ﬂower bouquet being viewed meant as part of a wedding purchase or for home decoration? Such diﬀerent perspectives greatly impact what should be recommended next. Hence, this paper proposes a novel sessionbased recommendation system empowered by hypergraph attention networks. Three unique properties of the proposed approach are: (i) it constructs a hypergraph for each session to model the item correlations deﬁned by various contextual windows in the session simultaneously, to uncover item meanings; (ii) it is equipped with hypergraph attention layers to generate item embeddings by ﬂexibly aggregating the contextual information from correlated items in the session; and (iii) it aggregates the dynamic item representations for each session to infer the general purpose and current need, which is decoded to infer the next interesting item in the session. Through experiments on three benchmark datasets, we ﬁnd the proposed model is eﬀective in generating informative dynamic item embeddings and providing more accurate recommendations compared to the state-of-the-art. 1 Introduction Recommendation systems are ubiquitous, acting as an essential component in online platforms to help users discover items of interest. In many practical scenarios, a recommendation system needs to infer the next interaction for a user based only on the short-term prior interactions within a particular session. Previous approaches that learn a static model of users [10, 11, 22, 31, 34] or rely on long-term user behaviors [9, 17, 28, 30] are not well-suited for such scenarios and could lead to poor predictive power. In contrast, session-based recommendation has attracted increasing attention, with recent approaches showing promising performance in inferring user interests in these (often anonymous) short-term sessions [16, 19, 21, 23, 32]. A critical issue is how items are treated in such session-based recommendation approaches. The individual items can reveal user intent, but they only provide limited evidence. To illustrate, consider the example in Figure 1. In diﬀerent sessions, the same ﬂower bouquet can be viewed diﬀerently, i.e., as part of a wedding party purchase, an option for home decoration, or Figure 1: An example with three sessions. The same ﬂower bouquet appears in each session, but with a diﬀerent purpose in each. The righthand side shows contextual windows of size 2 and 3 including this ﬂower bouquet. Items in the same contextual window are correlated; and diﬀerent contextual windows may have diﬀerent levels of importance for characterizing an item. a gift for Mother’s Day. However, if we independently consider Flower Bouquet A, it might be viewed as exactly the same item across sessions. In a sense, the meaning of an item (and what it reveals about user intent) could be inferred from contextual windows , each of which contains a set of consecutive items showing up together within a session. In Session C, bouquet A is likely for a wedding party since the user clicks it right between other wedding-related items, while the bouquet in Session A may be a Mother’s Day gift since it is clicked along with items speciﬁed for “Mom.” Hence, we propose to exploit these contextual windows to model session-wise item representations that can robustly capture user intent with only limited evidence available in short sessions. However, there are several key challenges in eliciting the user intent signal among items from the contextual windows in each session: (1) conventional graph structures and graph neural networks [2, 3, 7, 8, 15] are designed to model the pairwise connections between items, which are not always suﬃcient since we need to consider contextual windows connecting various numbers of items ranging from two to many. For example, for Session B in Figure 1, the pairwise linkage between bouquet A and B is not enough to reveal that bouquet A is meant for home decoration. But such evidence could be inferred by analyzing the triadic relations among both the bouquets and a vase as deﬁned by Contextual Window 1. To this end, we adopt the hypergraph [1, 4, 6, 29] structure to model the correlations amongst items in diﬀerent contextual windows within each session. In a session hypergraph, each node denotes an item, and a hyperedge connects the collection of items that show up together within a contextual window. This hypergraph structure supports capturing correlations among items deﬁned by contextual windows, which could be arbitrary-order depending on the usage scenario; (2) while propagating and aggregating the user intent evidence within a session hypergraph, some items are informative, but others may not be. Moreover, diﬀerent contextual windows may bring in diﬀerent levels of evidence for how items are represented. Thus, a key challenge is how to carefully highlight the informative items on each hyperedge and also emphasize the evidence from hyperedges with larger impacts. To tackle the aforementioned challenges, we propose SHARE: a Session-based Hypergraph Attention Network for REcommendation. Speciﬁcally, the proposed SHARE has three unique characteristics: • First, it captures contextual information with sliding windows, whereby items appearing in the same window are connected with a hyperedge. By applying multiple sliding windows on the session sequence, it is able to model a session considering contextual windows of varying sizes simultaneously in the hypergraph structure. • Second, it incorporates a carefully-designed hypergraph attention network to extract the user intent evidence from contextual windows, which is able to pay more attention on the informative items (nodes) and also emphasize the evidence from contextual windows (hyperedges) with larger impacts. • Third, the session-wise item embeddings resulting from a stack of hypergraph attention layers can then be fed into a self-attention layer to infer both the general intent and current interests in the session, which are decoded jointly to generate the next-item recommendation for this session. With experiments on three benchmark datasets for session-based recommendation, we show that such a hypergraph-based approach is better suited than conventional graphs for modeling item correlations within sessions. The proposed SHARE is eﬀective in predicting the next interesting items, and signiﬁcantly outperforms the state-of-the-art in session-based recommendation. 2 Related Work Session-based Recommendation. Given sequential user behaviors in a session, session-based recommendation systems aim to infer the subsequent behavior. Since these sessions usually exclude login information and take place in the short-term (e.g., less than a half hour) where user intent is local to the session, previous works that elicit user preferences from long-term historic behaviors or generate user embeddings based on their identiﬁcation [11, 28, 30] are not well-suited to providing recommendations for these anonymous short-term sessions. Though matrix factorization-based and neighborbased methods can be applied for session-based recommendation [18, 24], they ignore the transitional patterns between items. Thus, FPMC [23] has been proposed to extend matrix factorization with Markov Chains, which is able to predict the next item considering the transition from the last item. With the advancement of neural networks, diﬀerent neural mechanisms have been applied to handle sequential session data. In [12, 13], the authors adopt recurrent neural networks to learn sequential patterns from all sessions and infer the next items with the output of the last layer. NARM [16] utilizes an attention layer to aggregate items in the session and capture the main purpose of each session. Furthermore, STAMP [19] designs another attention component to emphasize the short-term interest in the session. To capture the local dependencies between items, SR-GNN [32] applies the conventional graph neural network to model the transitions between items in a session graph. In contrast, we propose to utilize hypergraphs and an attention mechanism to model item correlations and user intent with the contextual windows in sessionbased recommendation. Graph-based Recommendation. Since graphs can be a good ﬁt for modeling the interactions between itemitem, user-user, or user-item, there are works in developing recommendation systems centering around diﬀerent graph structures. Translation-based recommendation systems [9, 20] treat all the items as nodes and users as the connections between items consumed subsequently. These models embed users and items into a similar space by minimizing the translation loss, which requires clear user identiﬁcation (which is often not available in session-based scenarios). Recently, several eﬀorts apply newly introduced Graph Neural Networks (GNNs) for recommendation [2, 31, 33], in which diﬀerent GNN models have been designed for representation learning in graph structured data. GCMC [2] consists of neural graph autoencoders to reconstruct the user-item rating graph. And in NGCF [31], the authors propose to construct a user-item bipartite graph and utilize multiple graph neural layers to capture the multi-hop collaborative signals between users and items. In social recommendation, the relations between users can be exploited with the social graph [5, 32]. For example, DiﬀNet [5] adopts a Graph Convolutional Network to model the diﬀusion of user embeddings among their social connections. Unlike SHARE, these methods are built on conventional graph structures (not hypergraphs). And they are not designed for the special characteristics of session-based recommendation, like sequential user actions in short-lived sessions without user identiﬁcation. 3 The Proposed Model - SHARE In this section, we propose a novel session-based recommendation model with Hypergraph Attention Networks to exploit the contextual windows within each individual session. In the following, we begin with the problem setup and then structure the design of SHARE around three research questions: (i) Given that items falling into the same contextual window are correlated, how do we construct a hypergraph for each session to model the correlations among items from a variety of contextual windows simultaneously? (ii) Considering these contextual windows, how do we update an item embedding with the user intent evidence that propagates in and across the contextual windows? (iii) With these session-wise item embeddings, how can we infer the next interesting item by extracting both the general interest and current need in the session? 3.1 Problem Setup. In session-based recommendation [16, 21, 32], given the sequence of items which have been interacted within a session, the goal is to predict the next item. Let I = {i, i, ..., i} denote the set of N unique items in the system. These items start with a set of embeddings {i, i..., i}, each of which is a trainable embedding associated with the item ID. An anonymous session s consisting of a sequence of t actions (i.e., the items interacted within the session) can be denoted by s = [i, i,..., i], in which the items are sorted in chronological order and irepresents the pitem interacted within the session. A session-based recommendation system should predict the next possible action (i.e., i) based on the previous t actions. That is, we want to generate the preference scores for all the items in I based on the sequence of actions in session s. Then the top-K preferred items can be treated as candidates for recommendation. 3.2 Session Hypergraph Construction. With a sequence of interacted items in session s, we propose to model the session as a hypergraph before exploiting the item correlations in diﬀerent (possibly, overlapping) contextual windows. Let G= {V, E} denote the hypergraph constructed from session s, in which the node set Vconsists of all the unique items appearing in this session. Each hyperedge e ∈ Ewill connect all the items falling into the speciﬁc contextual window. As in Figure 2, we can apply a sliding window with size w on the item sequence for the session to identify all of the contextual windows of size w in this session, with which the items appearing in the same window can be treated as items falling into the same contextual window and thus will be connected with a hyperedge. Hence, to exploit contextual windows of varying sizes in the session, we can apply sliding windows of varying sizes on the item sequence. Let Erepresent the collection of all the hyperedges constructed with such a sliding window of size w on session s. Then we gather hyperedges based on diﬀerent sliding windows together to be the set of hyperedges Efor session s with E= E∪ E∪ ...E, in which W is the maximum size of contextual windows that we consider in the model. We repeat this process to construct the unique hypergraph for each of the sessions. 3.3 Hypergraph Attention Networks. With the hypergraph structure, we want to learn the representations for nodes (items) considering the correlations among nodes (items) deﬁned by diﬀerent hyperedges. In the following, we introduce a novel hypergraph attention network (HGAT) which is able to aggregate the user intent evidence that propagates in and across contextual windows while updating the node representations in a hypergraph. There are recent works extending neural networks from a conventional graph to a hypergraph by generalizing the convolution operation [1, 6, 29], with which the neighboring node features will be ﬁrstly aggregated to the common hyperedges and then propagated to the node. They usually treat nodes equally while characterizing the hyperedge or use a pre-deﬁned weight matrix to model the importance of information propagated via diﬀerent hyperedges. However, in session hypergraphs, for nodes belonging to the same contextual window, some of them may be informative in revealing user intent, but others may not be. And the evidence propagating via diﬀerent hyperedges (contextual windows) may bring in diﬀerent levels of impacts to an item. Thus, we require a new representation learning process for hypergraphs to capture these special needs. To tackle this problem, we propose to generalize the attention mechanism for hypergraphs so that we can highlight the informative items on each hyperedge and also emphasize the evidence from hyperedges with larger impacts. In the following, we will explain the node representation learning process with the proposed hypergraph attention layer (see Figure 2) in two steps: Node to Hyperedge. Since information can propagate among neighboring nodes via hyperedges, they are the key factor for node representation learning in hyper- Figure 2: The structure of SHARE: It applies multiple sliding windows to capture the contextual information to construct a hypergraph. With a well-designed HGAT network, it is able to generate the item embeddings revealing their meanings in the speciﬁc session. The sequence of session-wise item embeddings is fed into the self-attention layer to generate an embedding for the session, which is decoded for the preference scores on items. graph. With the special hypergraph structure, instead of directly updating each node with the the neighboring node information, we ﬁrstly treat each hyperedge as an interlayer between nodes and aggregate all the information propagating via the hyperedge. We take the graph Gconstructed by session s as an example. The operation can be applied on hypergraphs for other sessions. Let {n, n, ..., n} = {i, i, ..., i} denote the node input for the ﬁrst HGAT layer, which is the initial item embeddings for the set of unique items {i, i, ..., i} appearing in G. Since some nodes on a hyperedge are informative but others may not be, we should pay varying attention on the information from these nodes while aggregating them together. Let mdenote the information propagating via hyperedge from node t on the (1)HGAT layer. We can aggregate the information from each of the nodes connected by with the attention operation to generate the representation eas: in which Ndenotes all the nodes connected by hyperedge and urepresents a trainable node-level context vector for the (1)HGAT layer. WandˆW are the transform matrices and αdenotes the attention score of node t on hyperedge . We use a function S(·, ·) to calculate the similarity between the node embedding and context vector. Empirically, we use Scaled Dot-Product Attention to calculate the attention scores [14, 26], which is deﬁned as: where D is the dimension size and can be used for normalization while calculating the similarity scores. Hyperedge to Node. To update the embedding for a node, we need to aggregate the contents from all its connected hyperedges. Similarly, we utilize the attention mechanism to perform the aggregation in order to model the signiﬁcance of diﬀerent hyperedges. Let mdenote information (user intent evidence) from hyperedge to node t. Given the set of hyperedges Ywhich are connected to node t, its update embedding is calculated as: where W,ˆWand Ware the trainable matrices used to transform the vector before calculating the attention scores for the (1)HGAT layer. βindicates the impact of hyperedge on node t. As in the last step, we use the Scaled Dot-Product Attention formula deﬁned by Equation 3.2 to calculate S. The resulting ncan be treated as the updated embedding for note t by aggregating information from its neighboring region in the hypergraph. High-order Propagation. While a single HGAT layer can capture the information from direct neighbors, we construct a Hypergraph Attention Network by stacking multiple HGAT layers to model the multi-hop highorder information propagation in the hypergraph. In this Hypergraph Attention Network, the output of the (l − 1)HGAT layer is the input for the llayer. Thus the outputs (i.e., node embeddings) from the last layer (L) inherit the contextual information from all the previous layers and can be used to characterize the items in this session. For each of the session hypergraph, such an Hypergraph Attention Network is able to generate session-wise item embeddings to reﬂect user intent in the session, by highlighting the informative items on each hyperedge and also emphasizing the evidence from hyperedges with larger impacts. 3.4 Next-Item Prediction. To infer the next interesting item, our goal is to generate an embedding which can encode both the general interest and the current need of the session. We adopt the idea from selfattention [14, 26] to achieve this goal. Since the general interest can be uncovered by the aggregation of all the items and the current need is revealed by the last item, we want to aggregate the items in the session while paying more attention on those items which are highly correlated to the last one. Thus, we can treat the last item in the session as the query and the sequence of items in the session as both keys and values, leading to the design of the self-attention layer in Figure 2. For a session interacted with [i, i, ..., i] sequentially, we will lookup the corresponding node embeddings from the output of the hypergraph attention networks to get [n, n, ..., n]. We will transform the item embeddings with W, Wand Wto generate the query vectors, key vectors, and value vectors correspondingly. Then we can aggregate the sequence of embeddings with: in which the attention scores S is deﬁned in Equation 3.2. According to observations in previous research [14, 32], the order of items is less likely to be related to the general interest in short-term sequences and could introduce noise in modeling such short-term sequences. We omit the order information while handling the item sequences in session-based recommendation. With this carefully-designed self-attention layer, the resulting hencodes both the general interest and the multiplication between hand the latent factor of item v to predict the preference score of session s on v using p= hi. Let p= [p, p, ..., p] denote the predicted preference scores of session s on all of the N items in the system, which will be processed with a Softmax layer to generate the ﬁnal scores such that:ˆy= softmax(p). We use a one-hot vector y= [y, y, ..., y] to denote the ground-truth item of session s. During the training phrase, we calculate the cross-entropy loss for all training sessions SPP with L = −ylog ˆy. Then we can train the model using back-propagation. In the testing phase, given an unseen session, we can construct a new hypergraph and feed it into the hypergraph attention network. And then we can calculate its preference scores on all the items and recommend the items ranked among the top. 4 Experiments In this section, we conduct several experiments to evaluate how the proposed SHARE model performs in session-based recommendation. 4.1 Data. We adopt two public datasets that have been widely used to evaluate session-based recommendation: YooChoose and Diginetica [13, 16, 21, 32]. Yoochoose contains sessions of click events from an online retailer in Europe and was originally published as part of the 2015 RecSys Challenge. Diginetica contains sessions of product transaction data from an online retailer and was released as part of the 2016 CIKM Cup. We keep sessions with length longer than 1 and items which appear in at least 5 sessions. For Yoochoose, we test on all the sessions happening on the last day in the dataset while training all the sessions before that. Only items appearing in the training set are considered. As for Diginetica, we split the dataset and use the sessions happening in the last 7-days for testing. Furthermore, we adopt a standard sequence preprocessing method used in previous work [19, 25, 32] to generate the session sequences and labels. Then we get the ﬁnal Diginetica dataset as described in Table 1. However, it is not necessary to train on the entire set of training sequences from YooChoose since it is extremely large and training only on a fraction of recent sessions can lead to better prediction performance based on the experimental results in [25]. As in [16, 19, 32], we sort all of the training sequences generated from YooChoose, and retrieve the most recent 1/64 and 1/4 to be the training samples in Y ooChooseand Y ooChoose(listed in Table 1). Note that Y ooChooseand Y ooChoose share the same set of testing samples. In addition, for fair comparison, the training samples and testing samples in all of the three datasets are exactly the same as in [16, 19, 21, 32]. Table 2: Comparison of Diﬀerent Models. All the results are in percentage (%). The best performing method in each column is boldfaced, and the second best method is marked with †. ∗ indicates that the improvement of the best result is statistically signiﬁcant compared with the next-best result with p < 0.05. 4.2.1 Evaluation Metrics. We aim to evaluate how each model performs in predicting the next item in each session. Thus, the experiments follow the leave-oneout setting with one ground-truth item to be tested for each session in the test set. As in previous works for session-based recommendation [16, 21, 32], we adopt both Mean Reciprocal Rank (MRR@K) and Hit Rate (Hit@K) as evaluation metrics. Given the ranked list of items predicted for each session, Hit@K measures the probability that the ground-truth item is within the top-K. Let rdenote the ranking of the ground-truth item for session s. Then, Hit@K = 1 if r≤ K and Hit@K = 0 otherwise. As for MRR@K, it measures the average ranking of the ground-truth items among the lists. That is MRR@K =if r≤ K otherwise MRR@K = 0. Then we take the average values of MRR and Hit Rate over all the sessions in the test set and report the results. 4.2.2 Baselines. • S-Pop: This simple baseline recommends the most popular items based on their popularity in the current session. Ties are broken up using the popularity values based on the whole training set. • KNN [24]: This method recommends items which are most similar to items clicked in the current session. Each item is represented with a binary vector indicating all the sessions it appears in and cosine similarity is used to deﬁne their similarity. • MF [22]: This matrix factorization-based model is trained with Bayesian personalized ranking loss. Factorization and Markov Chains, it is able to infer the next item based on sequential behaviors. • GRU4Rec+ [12]: This baseline utilizes Gated Recurrent Unit (GRU) to model sequential patterns, with a new ranking loss functions tailored to RNNs in session-based recommendation. • NARM [16]: Besides learning the sequential behavior with GRU, it also includes an attention layer to extract the session’s main purpose, which are combined together to infer the preference scores. • STAMP [19]: This model combines both general interests from long-term memory and current interests from the short-term memory of the last-clicks in session-based recommendation. • RPNet [21]: Considering the repeat consumption in each session, this model can recommend unclicked items in the explore mode while recommending repeated items in the repeat mode. • SR-GNN [32]: This is the state-of-the-art for session-based recommendation with graph neural networks. It models session sequences as graphstructured data and designs a graph neural network to capture the transition between items. Recommendation is made with the composition of the whole session and the last click. 4.2.3 Parameter Tuning. All of the experiments are conducted on a server machine equipped with a 12 GB Titan Xp GPU. We use Adam as the optimizer with a learning rate set to be 0.001 for all datasets. The batch size for Y ooChooseis 100. Since there are more training sessions in both Y ooChooseand Diginetica, we set their batch size to be 300. For a fair comparison, the dimension of the item embedding is set to be 100 as in the baseline methods. We grid search for the dropout rate in {0.1, 0.2, 0.3, 0.4, 0.5, 0.6} and L2 regularization in {10, 10, 10, 10, 10}. The dropout is set to be 0.3 and the L2 regularization is set to be 10for all the datasets. We train each model for 50 epochs or until its performance does not improve for the validation set after 5 epochs. We ﬁne-tune the maximum size of the sliding window in {2, 3, 4, 5, 6} and the number of HGAT layer in {1, 2, 3, 4} for diﬀerent datasets. As in previous works, we randomly sample 10% of the training sessions as validation for parameter tuning. 4.3 Overall Evaluation. To compare SHARE with the baseline models, we summarize the overall results Table 3: Ablation Test Result. All the results are in percentage (%). ∗ indicates that the improvement of the best result is statistically signiﬁcant compared with the next-best result with p < 0.05. in Table 2. As in previous work [16, 19, 32], we report MRR and Hit Rate at K=20 for the overall comparison with the baselines. We ﬁnd that SHARE outperforms all of the baselines under each of the metrics for sessionbased recommendation over all of the datasets. Starting from the simplest method, since MF is unable to capture sequential information, it performs poorly in session-based recommendation. Integrating with Markov Chains, FPMC can improve MF, but it is still not a good ﬁt for modeling the short-term user behaviors and performs worse than KNN. With advances in neural networks, we observe that GRU4Rec+ achieves much better results than the non-neural models in sequential pattern modeling. However, in comparison to GRU4Rec+, we see that NARM and STAMP are better suited to extract the general interests from each short-term anonymous session by aggregating items with weighted attention scores. Emphasizing the item clicked most recently helps STAMP achieve better performance than NARM. Furthermore, by predicting and modeling the repeat consumption in sessions, RPNet can outperform the attention-based and GRU-based models in this recommendation scenario. As the state-of-the-art for session-based recommendation, SR-GNN outperforms RPNet, verifying the effectiveness of modeling item transitions with appropriate graph structure to obtain accurate user representations in sessions. However, the conventional graph structure is insuﬃcient in capturing the correlations amongst items across diﬀerent contextual windows. Since SHARE is speciﬁcally designed with this challenge in mind, we observe that it achieves signiﬁcant improvements compared with SR-GNN. Further, we observe that the performance of SHARE on Y ooChooseis even better than SR-GNN on Y ooChoose. Though Y ooChooseand Y ooChoosecontain diﬀerent numbers of training sessions, they are tested on exactly the same set of sessions. That is, with much less training data, SHARE can recommend more accurately than the state-of-the-art model trained with more data. 4.4 Ablation Analysis. To examine the design of the proposed model, we conduct an ablation test to com- Figure 3: Size of contextual windows vs. Hit Rate. pare SHARE with three of its variants and report the results in Table 3. In Model (1), we remove the Hypergraph attention network from SHARE, meaning that a set of static item embeddings are directly fed into the self-attention layer for diﬀerent sessions. Instead of utilizing the hypergraph structure, Model (2) constructs a conventional graph for each session based on pairwise item co-occurrence within the session and use a Graph Attention Networks (GAT) [27] to generate the dynamic item embeddings. In Model (3), we train a hypergraph convolution networks [6] instead of HGAT network to generate the dynamic item embeddings. To compare them under diﬀerent conditions, we report their performance under both K = 10 and K = 20. Overall, SHARE can outperform all of its variants for both K values, indicating the eﬀectiveness of its design. Without the hypergraph component, though it includes the self-attention layer to learn the sequential patterns, Model (1) achieves the weakest results since it does not consider the item correlations within each session and uses the same embedding for an item across diﬀerent sessions. Then in Model (2), building on top of Model (1), we adopt the conventional graph structure and GAT networks [27] to obtain dynamic item representations considering the consecutive transitions of items in each session. This model can outperform model (1), illustrating the necessity of modeling the insession item correlations with a graph-based structure. Next, to take advantage of the hypergraph structure for modeling the complex item correlations in each session, we construct a hypergraph for each session following the same procedure as in SHARE. But in Model (3), we generate the item embedding with hypergraph convolu- Figure 4: Number of HGAT Layers vs. Hit Rate. tional networks [6], which aggregates information from the neighboring hyperedges with the convolutional operation. It can achieve better performance than using conventional graphs, which shows the eﬀectiveness of the hypergraph structure and the necessity of modeling the contextual windows for item representation learning. However, it still falls behind SHARE since it does not consider the informativeness of nodes and the different impacts from contextual windows. 4.5 Study of SHARE. In this section, we conduct further studies on three core factors in SHARE: size of the contextual window, depth of information propagation and length of the sessions. In the following, due to the limit of space, as Y ooChooseand Y ooChoose shares the same set of testing sessions, we show the results trained with Y ooChooseto uncover the patterns for YooChoose. Size of contextual window. We ﬁrst examine the performance of SHARE by varying the maximum size of contextual windows that we consider while constructing the hypergraphs (in Section 3.2). We use the 1-layer HGAT network to avoid the inﬂuence of high-order information propagation. Note that maximum size = 1 is similar to the case that no hypergraph is constructed for each session, meaning that items use static embeddings across sessions. As in Figure 3, for YooChoose, at the beginning, the performance of SHARE improves as contextual windows with larger size are incorporated into the hypergraphs. In Diginetica, the best choice of window size is smaller that in YooChoose. The reason could be that the length of sessions is shorter in Diginetica, and thus we need only consider smaller contextual windows. These observations show that contextual windows can have signiﬁcant contribution on characterizing items in each session and the proposed framework is eﬀective in modeling various contextual windows simultaneously. High-order Propagation. We can stack multiple HGAT layers to model the information ﬂowing among items with high-order connections in hypergraph. To visualize the high-order information propagation, while setting the maximum window size to be constant (i.e., Figure 5: Session Length vs. Recommendation Quality. 2), we show the resulting Hit Rate in Figure 4 by varying the number of HGAT layers in SHARE. In each dataset, starting with a single HGAT layer, we ﬁnd that the performance is improved by stacking one more layer, indicating the importance of modeling the information via high-order connections. For Diginetica, stacking more than two layers will worsen the performance since the sessions in this dataset are short in general. Using too many layers will bring in noise for the representation learning process. Meanwhile, the optimized number of layers for YooChoose is larger than that for Diginetica. Thus, we conclude that SHARE can capture both the direct and high-order connections among items in the hypergraphs and lead to accurate recommendation. Short vs. Long Sessions. To fully understand how SHARE performs in modeling sessions under diﬀerent circumstances, we group the sessions based on their length and test how SHARE and SR-GNN perform for each group of sessions. As in Figure 5, SHARE provides better recommendations for all session lengths in comparison with SR-GNN. Both models can make more accurate predictions for short action sequences than the longer ones, since they omit the order information which may be important for long sessions with adequate user behaviors. However, since the percentage of long sessions is low in real-world datasets for session-based recommendation, the main purpose is to boost the recommendation for sessions with fewer items. Indeed, the proposed framework is suitable for such practical scenarios (i.e., short-term sessions) characterized by only a limited number of sequential actions. 5 Conclusion and Future Work To recommend the next interesting items in short-term sessions, we are motivated to exploit the correlations among items within various contextual windows in each session to better model their dynamic meanings across sessions. In this work, we propose a novel session-based recommendation system – SHARE – which is empowered by the hypergraph structure and hypergraph attention networks. With experiments on three real-world benchmark datasets, we ﬁnd that the proposed SHARE is able to generate eﬀective session-wise item embeddings and thus provide more accurate recommendation compared with baseline models. While in this work we mainly studying how to generate session-wise item representations, in the future we want to explore how to capture the item dynamics along time and over diﬀerent domains in session-based recommendation. Meanwhile, we are also interested in extending the proposed hypergraph attention networks for representation learning in other research areas.