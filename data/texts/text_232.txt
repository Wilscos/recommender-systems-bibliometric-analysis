This paper introduces a simple and eective form of data augmentation for recommender systems. A paraphrase similarity model is applied to widely available textual data – such as reviews and product descriptions – yielding new semantic relations that are added to the user-item graph. This increases the density of the graph without needing further labeled data. The data augmentation is evaluated on a variety of recommendation algorithms, using Euclidean, hyperbolic, and complex spaces, and over three categories of Amazon product reviews with diering characteristics. Results show that the data augmentation technique provides signicant improvements to all types of models, with the most pronounced gains for knowledge graph-based recommenders, particularly in cold-start settings, leading to state-of-the-art performance. • Information systems → Recommender systems. recommender systems, data augmentation, knowledge graphs Recommender systems (RS) estimate users’ preferences for items to provide personalized recommendations and a better user experience. The underlying assumption is that users may be interested in items selected by people who share similar interactions with them. By mining interaction records, a model implicitly learns user-user and item-item similarities, and exploits them for recommendations [65]. One of the major challenges for RS is the long tail of users with sparse interaction data. Making recommendations for new users or about new items with little data is dicult (the so called cold-start problem). To alleviate this issue, a considerable strand of research has explored incorporating side information to augment the interaction data [70]. A prominent branch of this work explores using textual descriptions and reviews, which often exist alongside rating or purchase data [52,72]. However, a recent re-evaluation of such techniques indicates that the benets are marginal, especially in cold-start scenarios. For example, modern deep learning-based models yield minimal changes in performance when reviews are masked [63]. In this paper we introduce a simple and eective form of data augmentation that leverages pre-trained textual semantic similarity models. The models are applied to widely available textual data, like product descriptions and reviews, yielding new relations between items. In this manner, we complement the implicit item similarity learnt from interactions by introducing explicit semantic relations based on textual attributes. We explore how these relations guide models to group semantically similar items and boost the recommendation system’s performance. The data augmentation technique is evaluated on a variety of models where the user-item graph can naturally be extended with new relations. Many of these models are variants of knowledge graph recommenders since they provide an expressive and unied framework for modelling side information between users, items, and related entities [90]. Analysis of local and global geometric measures of the generated graphs indicate that the augmented graphs are better represented in hyperbolic spaces (see §4). Therefore, we also explore a variety of representational alternatives for recommender systems, including Euclidean [4,84,88], complex [69] and hyperbolic [3, 11] geometries. Finally, we analyze how the proposed relations are more ecient at encoding semantic information present in textual descriptions compared to baselines that extract latent features from raw text. We nd that our technique is more eective at reducing the generalization error, and this is particularly notable in cold-start settings. Furthermore, we analyze which type of text is more helpful for this task, noting that the product description can provide very condensed and useful information to draw semantic relations. In this work we investigate two types of inductive biases: a data-dependent bias, by augmenting the graph relations via pretrained language models, and a geometric bias, through the choice of a metric space to embed the relations. By means of a thorough assessment, we show how they complement each other. As we enlarge the data, the hyperbolic properties of the graph become more evident. In summary, we make the following contributions: •We propose an unsupervised data augmentation technique by explicitly mining semantic relations derived from items’ text that boosts the performance, and is particularly eective in cold-start settings. •We provide a thorough analysis of local and global structural aspects of the user-item graph, and its augmented version, which indicates that the graph is better represented in hyperbolic space rather than in Euclidean. •We explore KG methods developed in Euclidean, hyperbolic and complex spaces, and showcase how they achieve state-ofthe-art performance for recommendations, when leveraged with the appropriate relations. A popular approach to incorporate side information in recommender systems is to exploit user reviews and item descriptions, which often exist alongside rating or purchase data [52,72]. A textual review is much more expressive than a single rating, and the underlying assumption is that reviews are eective user/item descriptors [23,51], thus they can be used to learn better latent features [8, 13]. In a conventional deep learning architecture, these features are used in matrix factorization [94]. In such a setup, the model is burdened with the task of learning an implicit similarity function that should emerge from the textual input and the user’s purchase history. Based on this anity, alike users and items are grouped and leveraged for recommendations. However, Sachdeva & McAuley [63] show that recent models yield minimal changes in performance when reviews are masked. They observe that reviews are more eective when used as a regularizer [31,52], rather than as side data to extract latent features, and this behavior is accentuated in cold-start scenarios. This paper introduces a dierent approach to benet from textual attributes: it proposes leveraging advances in textual similarity models [9] to feed a recommender explicit content-based similarities via new edges in the interaction graph. This requires no supervision and and complements the implicit similarity function that the model learns, without increasing the computational complexity. The data augmentation increases the density in the graph and serves as an ecient regularizer without the need to reduce the eective capacity of the model [30]. The goal of the proposed method is to augment the user-item interaction graph with item-item relations. These relations are based on the semantic similarity between the item descriptions and reviews. Initially we collect all the available text for each item𝑖in the set of itemsI. This includes metadata, such as item name, descriptions and reviews. We experiment with various lters, as heuristic measures of the helpfulness of dierent types of text (e.g. top-k longest reviews, or only the metadata). To compute text embeddings we employ the Universal Sentence Encoder (USE) [9], as it has shown good performance on sentence similarity benchmarks, and it can be applied without any further ne-tuning. Moreover, the average review length in purchase datasets tends to be one paragraph [1,52], and USE has also been pre-trained to encode paragraphs composed of more than one sentence. We compute one embedding𝑒∈ Rfor each review (or descriptor) 𝑗 corresponding to the item 𝑖. The nal embedding for item𝑖is the result of taking the mean of all its review embeddings. Once we have assigned an embedding to all items, we employ cosine similarity to compute the similarity between them. Finally, we extend the original user-item training set with the semantic relations between pairs of items. We lter out low-similarity pairs, and select the top-k highest similarities to be added as relations. Given that we extend the user-item graph by adding semantic relations, we need to account for the dierent types of edges present in the augmented graph. We model this multi-relational graph as a knowledge graph, since they oer a exible approach to add multiple relations between diverse entities. In this section we describe knowledge graphs, and their application in recommender systems. Knowledge graphs (KGs) are multi-relational graphs where nodes represent entities and typed-edges represent relationships among entities. They are popular data structures for representing heterogeneous knowledge in the shape of (head, relation, tail) triples, which can be queried and used in downstream applications. The usual approach to work with KGs is to learn representations of entities and relations as vectors, for some choice of spaceS(typicallyR), such that the KG structure is preserved. More formally, letG = (E, R, T ) be a knowledge graph whereEis the set of entities,Ris the set of relations andT ⊂ E × R × Eis the set of triples stored in the graph. Most of the KG embedding methods learn vectorsh, t ∈ R forℎ, 𝑡 ∈ E, andr ∈ Rfor𝑟 ∈ R. The likelihood of a triple to be correct is evaluated using a model specic score function 𝜙 : E × R × E → R. Knowledge graph embedding methodshave been widely adopted into the recommendation problem as an eective tool to model side information [27]. Multiple relations between users, items, and heterogeneous entities can be mapped into the KG and incorporated to alleviate data sparsity and enhance the recommendation performance [90]. Knowledge graph-based recommender systems can be seen as multi-task models, with well-established advantages. Learning several tasks (relations) at a time reduces the risk of over-tting by generalizing the shared entity representations [92]. The data augmentation also improves the generalization of the model and acts as an eective regularizer [30]. Furthermore, akin relations can help the model to learn dierent types of entity interactions, such as similarities, that are nally exploited for recommendations [62]. Multi-relational knowledge graphs exhibit an intricate and varying structure as a result of the logical properties of the relationships they encode [43,54,68]. An item can be connected to dierent entities by symmetric, anti-symmetric, or hierarchical relations. To capture these non-trivial patterns more expressive operators become necessary. In Table 1, we show KG embedding methods, along with their operators, and in which RS work they have been applied. It can be seen that previous work integrating KG into RS has applied a rather narrow set of methods. These are the translational approaches TransH [84], TransR [47], or TransD [33], which are extensions of TransE [4]. We notice that the state of the art in the eld of KG embedding methods has advanced in recent years, as more compound and expressive operators have been developed. However, recommender systems have continued to apply outdated models and have not proted from the current progress. Recent approaches propose to embed the graph into non-Euclidean geometries such as hyperbolic spaces [3,11,37], to model embeddings over the complex numbersC[40,69,77], or apply quaternion algebra [91]. In the next section we describe methods that combine dierent operators and achieve SotA performance on KG completion tasks. RotatE [69]: Maps entities and relations to the complex vector spaceCand denes each relation as a rotation in the complex plane from the source entity to the target entity. Given a triple (h, r, t), it is expected thatt ≈ h ◦ r, where◦denotes the Hadamard (element-wise) product. Rotations are chosen since they can simultaneously model and infer inversion, composition, symmetric or anti-symmetric patterns. MuRP [3]: By establishing a comparison with word analogies through hyperbolic distances [76], the authors propose a scoring function based on relation-specic Möbius multiplication on the head entity, and Möbius addition [22] on the tail entity: whereh, r, t ∈ H;𝑏, 𝑏∈ Rare scalar biases for the head and tail entities respectively, and 𝑑is the hyperbolic distance (Eq 2). RotRef [11]: Extends MuRP with rotations and reections in hyperbolic space by learning relationship-specic isometries through Givens transformations.The result of these operations is combined with an attention mechanism in the tangent space [12]. We augment the user-item graph by exploiting heterogeneous relations between items and other entities. Our starting point is the user-item graph, composed of solely(𝑢𝑠𝑒𝑟, 𝑏𝑢𝑦, 𝑖𝑡𝑒𝑚)triples. We augment this bipartite graph with the aforementioned semantic relations. These are(𝑖𝑡𝑒𝑚_𝐴,ℎ𝑎𝑠_𝑠𝑒𝑚𝑎𝑛𝑡𝑖𝑐_𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦, 𝑖𝑡𝑒𝑚_𝐵)triples, meaning that there is a semantic overlap between the descriptors corresponding to these items. Previous work has explored relations between items and diverse entities such as product brands and categories [1,86,93], or movie directors and actors [7,87,90], depending on the dataset. To the best of our knowledge, this paper is the rst to explore using pre-trained semantic similarity models to create new relations. The graph augmentation modies its size and structure. A change in its connectivity aects the optimal embedding space to operate [25]. Thus, we analyze this aspect in the next section. The predominant approach to deal with graphs has been to embed them in an Euclidean space. Nonetheless, graphs from many diverse domains exhibit non-Euclidean features [5]. In particular, several RS datasets exhibit power-law degree distributions [10], and properties of scale-free networks [6,36], which imply a latent hyperbolic geometry [38]. If we match the geometry of the target embedding space to the structure of the data, we can improve the representation delity [25,49]. With the goal of understanding which type of Riemannian manifold would be more suitable as embedding space, we analyze dierent structural aspects and geometric properties of the graph. Our analysis shows that when we augment the relationships in the graph, the added edges modify its connectivity and structure, making it more hyperbolic-like. To investigate the recommendation problem with regard to dierent relationships and geometries, we focus on the Amazon dataset [52,56], as it is a standard benchmark for RS, and it provides item reviews and metadata in the form of textual descriptions. Nonetheless, our analysis generalizes to RS datasets that exhibit a latent hyperbolic geometry, studied in [6, 10, 36]. Specically, we adopt the 5-core split for the branches "Musical Instruments" (MusIns), "Video Games" (VGames) and "Arts, Crafts and Sewing" (Arts&Crafts), which form a diverse dataset in size and domain. Besides the semantic relations, we also add relationships available on the dataset that have already been explored in previous work [1, 86, 93]. These are: • also_bought: users who bought item A also bought item B. •also_view: users who bought the item A also viewed item B. • category: the item belongs to one or more categories. • brand: the item belongs to one brand. The number of each type of relation added to the nal augmented graph is reported in Table 2. Figure 1: Visualization of Ollivier-Ricci curvature for dierent graphs. Left to right: a) "MusIns" user-item, b) "MusIns" augmented graph, c) "VGames" user-item, d) "VGames" augmented graph. Node size represents node degree. The vast majority of nodes and edges in red depict negative curvature, in correspondence with the negative curvature of hyperbolic space. Hyperbolic geometry is a non-Euclidean geometry with constant negative curvature. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures [57,64]. Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance from the root [15]. Thus, hyperbolic space can be seen as the continuous analogue to a discrete tree-like structure. Embedding norm represents depth in the hierarchy, and distance between embeddings the anity or similarity of the respective items [48]. In this work, we analyze models operating in the𝑛-dimensional Poincaré ball: H= {𝑥 ∈ R: ||𝑥 || < 1}.. For two points𝑥,𝑦 ∈ Hthe distance in this space is dened as: Curvature is a geometric property that describes the local shape of an object. If we draw two parallel paths on a surface with positive curvature like a sphere, these two paths move closer to each other while for a negatively curved surface like a saddle, these two paths tend to be apart. There are multiple notions of curvature in Riemannian manifolds, with varying granularity. In the interest of space, we only recall a key notion: hyperbolic spaces have constant negative curvature, Euclidean spaces have zero curvature (at) and spherical spaces are positively curved. Table 2: Number of users, items and each type of relations added on dierent branches of the Amazon dataset. Discrete data such as graphs do not have manifold structure. Thus, curvature analogs are necessary to provide a measure that satises similar properties [17]. In this work, we apply the OllivierRicci curvature to analyze the graphs [58]. Since this type of curvature characterizes the space locally, we plot the results in Figure 1. We can observe that nodes and edges in the user-item graphs exhibit a very negative curvature (in red color). Negatively curved edges are highly related to graph connectivity, and removing them would result in a disconnected graph [55]. As we add more relationships, the augmented graph becomes much more connected, therefore these edges play a less important role. Nonetheless, the overall curvature, as shown by the vast majority of nodes and edges, remains negative. The correspondence with the negative curvature of hyperbolic space suggests that both, the user-item and the augmented graphs would prot from a representation in that geometry, rather than in an Euclidean one. Also known as Gromov hyperbolicity [24],𝛿-hyperbolicity quanties with a single number the hyperbolicity of a given metric space. The smaller the𝛿is, the more hyperbolic-like or negatively-curved the space is. This measure has also been adapted to graphs [21]. We report the𝛿-mean and𝛿-max in Table 3. We can see that both measures decrease when we compare the user-item (U-I) graph with the augmented one (Augmen). The metric shows that, as we add more relationships to the initial user-item graph, it becomes more hyperbolic-like. This global metric complements the local curvature and also indicates that the graph ts into a hyperbolic space. Table 3: Statistics of graphs derived from dierent branches of the Amazon dataset, without relations (user-item, "U-I") and with all relations (augmented, "Augmen"). 𝛿-mean and 𝛿max refer to the 𝛿-hyperbolicity (lower is more hyperbolic). We experiment with dierent approaches to represent the multirelational interaction graph for the task of generating recommendations. Our aim is to compare recent KG techniques, with a particular focus on the ones operating in hyperbolic spaces, with KG methods applied in previous work and SotA recommender systems. Baselines: The recommender system baselines are: •BPR [61]: Standard collaborative ltering baseline for RS based on matrix factorization (MF) with Bayesian personalized ranking. •NeuMF [29]: Fuses MF with a multi-layer perceptron to learn latent user/item features and model interactions. We enhance this model by initializing the item features with pre-trained embeddings. •CML++ [31]: Metric learning baseline that encodes interactions in a joint space using Euclidean distance. Following [63], we learn extra user and item biases. • HyperML [79]: Hyperbolic adaptation of CML. Since reproducing all the works that adopt KGs is unfeasible (see Table 1), and in cases such as [1,90,93] the KG method is applied practically without modications, we directly employ the KG models themselves. The selected methods are: • TransE [4]: Translation-based KG method. • TransH [84]: Hyperplane translation-based KG method. •DistMul [88]: Based on multi-linear product, a generalization of the dot product. • RotatE [69]: Performs rotations in the complex plane C. • RotRef [11]: Based on rotations and reections. • MuR [3]: Based on multiplications and additions. For RotRef and MuR we compare to the Euclidean and hyperbolic versions. Data. To evaluate the models we utilize the branches of the Amazon dataset introduced in §4.1. Since it is very costly to rank all the available items, following [29,79], we randomly select100 samples which the user has not interacted with, and rank the ground truth amongst these samples. To generate evaluation splits, the penultimate and last item the user has interacted with are withheld as dev and test sets respectively. Implementation details: Given dierences in preprocessing strategies, we reproduce the baselines based on their public implementations. To ensure consistency in the results we conduct a hyperparameter search for all methods on the validation set. All models are trained with the Adam optimizer [35], and operate with64 latent dimensions. We train for1000epochs with early stopping on the dev set. To optimize parameters in hyperbolic models we apply tangent space optimization [12]. We choose the best learning rate for each method from from{0.0001, 0.0005, 0 .001, 0.005} and batch-size from{256, 512, 1024}. In each case, we report the average of three runs. Preliminary experiments with multi-task losses (splitting the loss between KG and RS components as in [7,81,87]) did not show signicant improvements therefore we disregard this approach. Models that incorporate item features (NeuMF and CML++) are fed with the text embeddings used to compute the semantic similarities. In this way, all models have access to information extracted from the same sources. Setup: We evaluate the models in two setups: User-item, where we only employ the user-item interactions, and Augmented, where we utilize the graph with all the added relationships. Moreover, for the Augmented case we follow the standard data extension protocol by adding inverse relations to the train split [11,40] . That is, for each triple (ℎ, 𝑟, 𝑡), we also add the inverse (𝑡, 𝑟, ℎ). Evaluation protocols and Metrics: To evaluate the recommendation performance of KG methods we only look at the buy relation. For each user𝑢we rank the items𝑖according to the scoring function𝜙 (𝑢, 𝑏𝑢𝑦, 𝑖). We adopt normalized discounted cumulative gain (DCG) and hit ratio (HR), both at10, as well-established ranking evaluation metrics for recommendations. Research Questions: Through our experiments we aim to answer the following questions: RQ1How do KG methods perform compared to recently published RS? RQ2 What is the impact of the data augmentation? RQ3How important are dierent relations to improving recommendations? RQ4 Which text attributes are most helpful? RQ5 How do dierent metric spaces compare? In Table 4 we report the results for all models. Regarding the useritem results, we can observe that NeuMF is a very strong baseline, surpassing the performance of all other RS, and several KG methods. This can be explained by the fact that NeuMF and CML++ have access to more information than other baselines, since they are fed with embeddings generated from the items’ text. However, RotRef and MuR outperform all models. These models are designed to deal with multi-relational graphs, but in this case they only see one type of relation (𝑏𝑢𝑦). Although they operate solely based on the useritem interactions, the compound operators that they incorporate allow them for a more expressive representation of the bipartite graph, which results in improved recommendations. KG methods applied only on the user-item graph report a very high performance due to their enhanced representation capacity, thus we consider they should be adopted as hard-to-beat baselines in further research in recommender systems. We can see that all models (except for HyperML in MusIns) have a signicant boost in performance when we train them on the augmented graph , with gains up to32.6%for RotatE over the useritem data in MusIns. The proposed densication process reduces the sparsity by adding meaningful relations between the entities. All models, including the RS that are not designed to incorporate multi-relational information, benet from the augmented data. In this setup, the RS can be thought as models that aim to predict the plausibility of an interaction between any two entities (not only user-item). Although they do not account for each particular type of Table 4: Results for "User-item" and "Augmented" graph setups, for models operating in Euclidean (R), hyperbolic (H) and complex (C) space. Δ% shows the hit rate improvement over the user-item graph when data is augmented. All HR improvements are statistically signicant (one-tailed Mann-Whitney U test, 𝑝 ≤ 0.05). relation, they prot from the extended training set and this results in an enhanced generalization, which contributes to cluster users and items in a way that improves the recommendation. In this setting, TransE and TransH show a greater relative increase in their performance (higherΔ%) compared to recommender system baselines. Since the purpose of the KG models is to cope with multi-relational data, they can exploit the augmented relations in a much better way than the RS baselines. Although the rotations in the complex plane of RotatE oer noticeable improvements of more than17%in all branches with the augmented graph, it is outperformed by translational approaches such as TransE. HyperML and CML++ do not show large gains or high performances with the added relations. The reason for this is that metric learning approaches that lack relational operators are ill-posed algebraic systems when there is a large number of interactions (see [74], Thm 2.1). MuR and RotRef are the best performing models also in this setup. These results highlight how heterogeneous sources of information, when leveraged through adequate tools and formulations, allow models to exploit the augmented data and achieve considerable performance gains. Hyperbolic and Euclidean models show very competitive results for MusIns and Arts&Crafts. It has been shown that in lowdimensional setups (𝑑 ∈ [2, 32]) hyperbolic space oers signicant improvements [11,44,57]. Since we operate with64dimensions, both models exhibit a similar representation capacity on these datasets. Nonetheless, MuRP outperforms its Euclidean counterpart in VGames for both setups. These ndings are in line with the previous analysis (§4), and they emphasize the importance of choosing a suitable metric space that ts the data distribution as a powerful and ecient inductive bias. Since the amount of space in the hyperbolic representation grows exponentially compared to the Euclidean one, this model is able to accommodate entities in a better way, unfolding latent hierarchies in the graph (see §6.5). Finally, we also demonstrate the recent developments in KGs, and their enriched representation capacities. Advanced KG methods achieve a much better performance than their predecessors and outperform RS explicitly designed for the task in both setups. In this section we investigate the contribution of each individual relation to the results of MuRP, which we consider the best performing model. Besides evaluating on the test split, we create a subset with the2%of users with fewer number of interactions. They represent users aected by the cold-start problem, since they exhibit very few interactions with items. We refer to this split as "Cold Test". The results of the ablation are presented in Table 5. When we look at individual performance on the test set, we see that each relation brings improvements over the user-item graph (buy relation), which highlights the key role of data augmentation to boost the performance of these models. We notice that the semantic relation is the best in the "Musical Instruments" branch, whereas in the other two branches, it is the second. The relation also_bought seems to be more helpful in those cases. also_bought is mined from behavioral patterns of users with respect to complementary products that are usually bought together [53], and it plays a fundamental role in predicting user purchases [85]. However, the relations also_bought and also_view are derived from the entire dataset, and not only from the train split. These relations incorporate information between users and items on the dev/test set, posing considerable advantages over the brand, category and semantic relations. Finally, all relationships combined outperform the individual setups for all branches of the dataset in the test set, in line with previous results [93]. This demonstrate how KG methods can leverage heterogeneous information in an unied manner, and shows the scalability of the approach to new relation types. Cold-start Problem. We analyze how dierent relations aect users in cold-start settings, by looking at the2%of users with the fewest number of interactions (Cold Test). In this case, the semantic relation brings a very pronounced boost in performance:15.4%, Table 5: Relation ablation for MuRP model, for dierent branches of the Amazon dataset. Results in bold show the best performing relation combination. "Cold Test" refers to the 2% of users with the smallest number of interactions. Δ% shows the improvements of each relation with respect to only using the relation buy. 12.6%and7.2%for MusIns, VGames and Arts&Crafts respectively, much more than any other relation. This shows the eectiveness of semantic relations to densify the graph, with remarkable improvements particularly over sparse users and items. These observations are in line with previous research that has shown how reviews, when used as regularizers, are particularly helpful to alleviate coldstart problems [63]. Moreover, we notice that the performance for "Cold Test" tends to be better than for "Test" in MusIns and Arts&Crafts. We hypothesize that this is caused by modelling users as a single point in the space. When users exhibit a large number of interactions with distinct items, it becomes more dicult to place the user embedding close to all their preferences. 6.4.1 Relations vs Features. We analyze the eectiveness of semantic relations to model side information extracted from textual descriptions when compared to dierent ways of incorporating latent features proposed in previous work. We consider the following models: CML++ utilizes the item features as an explicit regularizer, NeuMF initializes the item embeddings with the features, and Narre [13] uses TextCNN [34] to extract features from the text. We do not compare to KG approaches since they do not incorporate latent features. For CML++ and NeuMF we employ the text embeddings used to compute the semantic similarities as item features. Results for the ablation are reported in Table 6. Compared to baselines that do not incorporate any side information, we see that relations bring a larger improvement than features for CML++ and Narre (17.1 vs 3.7 and 10.6 vs 7.4 relative improvement respectively). Although these models are not specifically designed to incorporate multi-relational information, data augmentation via semantic relations seems to be more eective than exploiting textual features. For NeuMF, the transfer learning technique of initializing the item embeddings with textual features proves to be very eective. Nonetheless, when semantic relations are combined with features, the performance is improved. This ablation showcases the ecacy of semantic relations to model information extracted from textual data. They can be seamlessly integrated with latent features, and the improvements of the combined models demonstrate that they provide complementary information for recommendations. 6.4.2 Type of Text for Semantic Relation. We analyze which type of text is more useful for extracting features and capturing item similarities. To do so, we lter the available text for each item according to dierent criteria: (a) Only textual metadata, such as product name, description and categorical labels, (b) only reviews, (c) only reviews with high level of sentiment polarity, and (d) metadata + top-k longest reviews. We repeat the method described in §2.2 to pre-process relations, and encode all text with USE. We compare CML++ and NeuMF, which are models that incorporate item features, and MuRP. In all cases we run experiments with the buy + semantic training set, in order to analyze the addition of semantic information only. Results for "MusIns" are reported in Table 7. We can see that adding features on top of relations degrade the performance of CML++ (the same behavior can be noticed in Table 6). This model integrates item features as a regularizer to correct the projection into the target embedding space. In this setup, the semantic relations are already contributing in that regard, therefore the integration of extra features seems to mislead the model. On the other hand, NeuMF has a drastic drop in performance when features are removed. In this case, the features and relations extracted from text that combine metadata with the longest reviews helps the most in the recommendations, followed by utilizing all the available reviews as input. Finally, MuRP does not use any features and solely learns from the relations available in the graph. We again see that using metadata combined with the longest reviews is the most useful text to learn item dis/similarities. This is due to the fact that longer reviews tend to be more descriptive about the items. However, results using only metadata show competitive performance as well. In the three models, reviews with high polarity (i.e. very "positive/negative" reviews rather than "neutral" ones) do not seem to carry extra descriptive information such that it can be leveraged for similarities. ✗ feat, ✓ rel 55.57 17.1 44.13 6.3 50.98 10.6 ✓ feat, ✗ rel 49.22 3.7 49.64 19.6 49.50 7.4 ✓ feat, ✓ rel 53.95 13.7 50.80 22.4 51.22 11.1 Table 6: Ablation of adding item features and/or semantic relations for the MusIns branch. Table 7: Comparison of mo dels that exploit textual information extracted from dierent types of text. All models are trained over the user-item graph + semantic relations. This ablation suggests that to learn useful dis/similarities between items for recommendations, it is convenient to combine the longest reviews with the item metadata. Nonetheless, it is noticeable that metadata by itself can be leveraged to obtain competitive results. We consider this a relevant outcome since, in the absence of lengthy reviews, a brief and accurate item name and description might as well oer remarkable performance gains. 6.4.3 Encoder Analysis. This ablation is related to the choice of a pre-trained encoder that captures text similarities in an unsupervised manner. We compare the performance of USE to BERT [19], without applying any ne-tuning, and Sentence-BERT [60], which is an adaptation of BERT with a siamese network, ne-tuned for sentence similarity. We analyze the MuRP model with the same four criteria for ltering the available text, and we compare to the setup without adding the semantic relations as well. Results for the "MusIns" are reported in Table 8. We observe that when we use BERT as the encoder, the performance of the user-item graph extended with semantic relations is worse than using the user-item graph alone. In Figure 2 we show the cosine similarities for50random item embeddings. We see that for the BERT encoder, which has not been pre-trained on semantic similarity objectives, most items are very similar to each other, and this hinders the model from clustering them. This setup can be considered as an ablative experiment of the original model, where we add random semantic relations. This demonstrates the importance of the semantic information contained in the connections that we create, and how the model is able to leverage them. Sentence-BERT shows improvements over BERT. Nevertheless, we nd USE to be the most eective encoder that captures review and metadata dis/similarities. Sentence-BERT is more competent at distinguishing degrees of similarity than BERT, but Figure 2 shows that the patterns detected are alike. On the other hand, USE is able to identify a broader range of dis/similarities between items, that results in an improved performance for recommendations. Our experiments showed that hyperbolic methods can oer improvements over systems operating on Euclidean space or with complex numbers. Moreover, since hyperbolic space is naturally equipped for embedding hierarchical structures, its self-organizing properties make it amenable to capture dierent types of hierarchies as a by-product of the learning process. In the resulting embeddings, the norm represents depth in the hierarchy. As explained in §3.4, each item in the Amazon dataset has a set of categorical labels that Table 8: Performance of MuRP model on MusIns for dierent encoders, and dierent ltering heuristics. describes which categories the item belongs to. We analyze the hyperbolic and Euclidean versions of MuR, and report the Spearman correlation between the norms of the embeddings for each category and the number of interactions of that category: • Hyperbolic: −0.52 • Euclidean: −0.06 The correlation is moderate to high for the hyperbolic model, whereas for the Euclidean model it is non-existent. This indicates that more "general" categories (with more interactions) have a shorter norm, which is expected when embedding a hierarchy in a hyperbolic space [64], while the origin of the space has no particular meaning in the Euclidean model. To shed light on this aspect, we reconstruct the hierarchies that the hyperbolic and Euclidean models learn. To do so, we randomly select a category embedding, and iteratively look at the closest neighbor that has less or equal norm (this would be the parent category). We report the hierarchies for "Electric Guitar Bags & Cases" and "Footswitches" from the "MusIns" dataset in Table 9. We notice that the hyperbolic hierarchy is much more concise and precise, compared to the Euclidean one. The hyperbolic model builds dierent "short" hierarchies, accommodating the labels in a more spread way, whereas the Euclidean model learns a "tall" tree of categories, which results in a much more noisy arrangement. This shows that the hyperbolic model automatically infers the hierarchy arising from the label distribution [48,50], and oers a more interpretable space. Furthermore, the model achieves this as a by-product of the learning process with augmented relations, without being specically trained for this purpose. Data Augmentation: Data augmentation plays an important role in machine learning [39,67], as it reduces the generalization error without aecting the eective capacity of the model [30]. In RS, augmentation techniques have been applied by extending co-purchased Figure 2: Cosine similarity of BERT (left), S-BERT (center) and USE (right) review embe ddings for 50 items. products [85], generating adversarial pseudo user-item interactions [82], or casting dierent user actions as purchases [71,78]. Also by exploiting item side information, such as images [16], audio [46] or video [14] features. We propose a new unsupervised method to learn similarity relations between items (or users) based on semantic text models applied to textual attributes. Our work expands the strand of research that incorporates review information as regularization technique [63], and notably improves the performance for the cold-start. Recommenders using Text: Previous work has mined text to use it as regularizer [31,52], or as latent features to learn better user and item representations [13,94]. However, Sachdeva & McAuley argue that the benet of using reviews for recommendation is overstated, and the reported gains are only possible under a narrow set of conditions [63]. They specically note that reviews seem to provide little benet as features, but help more when used for regularization. The simple data augmentation method proposed in this paper oers another way to think about using textual information. The added relations improve user and item representations, like adding features can, but without needing to increase the representation size. Thus it can also be seen as providing the benets of regularization, but without directly constraining model expressivity, as for instance dropout does. Knowledge Graph Recommenders: Previous work integrating KG into RS has applied a narrow set of representational methods, favoring Euclidean translational approaches [27] (see Table 1). The experiments in this paper expand on previous analysis by including more recent KG embedding methods. Our results show that newer methods signicantly benet from the introduced data augmentation and outperform not only previous KG recommenders, but also other state-of-the-art recommendation systems. Hyperbolic Space: The advantages of hyperbolic space have been argued for in a wide variety of application domains: question answering [75], machine translation [26], language modeling [20], hierarchical classication [48,50], and taxonomy renement [2,41] among others. In RS, hyperbolic geometry naturally emerges in several datasets [6,36], and hyperbolic spaces have been applied in combination with metric learning approaches [10,79]. In this work, we expand on these studies and carry out a structural analysis of the properties of user-item graphs extended by our data augmentation, which shows that hyperbolic methods may have an important Table 9: Category hierarchies and norm of each category label for hyperbolic and Euclidean spaces. role to play for both interpretability of recommendations, and for enabling higher performance by leveraging lower representational dimensionality. In this work we propose a simple unsupervised data augmentation technique that adds semantic relations to the user-item graph based on applying pre-trained language models to widely available textual attributes. This can be regarded as a data-dependent prior that introduces an eective inductive bias, without increasing the computational cost of models at inference time. By exploring a variety of modern KG methods, we observe that recent advances, when combined with our data augmentation technique, result in state-of-the-art RS performance (RQ1, §6.1). Moreover, the proposed data augmentation improves the performance of all analyzed models, including those that are not designed to handle multi-relational information (RQ2, §6.2). Thus, the technique can be considered architecture-agnostic. Our ablation study highlights the impact of the semantic relations particularly in cold-start settings (RQ3, §6.3). Regarding the choice of textual inputs, the study reveals that using either reviews or brief product descriptions are both eective (RQ4, §6.4). An important branch of further work here is to explore if these results generalise to denser domains, as we notice anecdotal evidence that the benet of this data augmentation diminishes as the average degree of nodes increases. Finally, our analysis of structural properties of the graphs, which can be extended to more datasets that exhibit a latent hyperbolic geometry [6,10,36], reveals that recommenders can benet from operating in these metric spaces. In particular, we remark how hyperbolic space improves the interpretability for recommendations (RQ5, §6.5). This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1 and the Klaus Tschira Foundation, Heidelberg, Germany.